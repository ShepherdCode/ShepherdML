{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Analysis - Nucleus rollup\n",
    "Rollup the nucleus statistics per patch. All classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Plan\n",
    "* Run CellProfiler on 80K patches. Make CSV files.\n",
    "* Record bounding box of every nucleus of every patch.\n",
    "* Run CNN on 80K patches. \n",
    "* For each class c, label correctly classified patches c_Cor.\n",
    "* For each class c, label in correctly classified patches c_Inc.\n",
    "* Run CNN attention on 80K patches. Make heatmaps.\n",
    "* Compute average heatmap color per nucleus bounding box.\n",
    "* Set aside test set: 20% of images (and all their patch data) per class.\n",
    "* Possibly set aside patches with too little tissue, too many RBC, or too few nuclei.\n",
    "* Remove useless columns such as XY locations.\n",
    "* Add dispersion columns such as deciles.\n",
    "* Train a Cor/Inc binary classifier for each class.\n",
    "* Evaluate the model by cross-validation over training data.\n",
    "* If the model is accurate, extract important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 11:07:07.417977\n",
      "scikit-learn version 1.0.2\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "print('scikit-learn version',sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES=[0,1,2,3,4,5]\n",
    "# out of memory processing class 0 which is 12 GB\n",
    "CLASSES=[6,7,1,2,3,4,5]   # 6 and 7 are first and second half of 0 respectively\n",
    "FILEPATHS=[0,1,2,3,4,5,6,7]\n",
    "FILEPATHS[0]='/home/jrm/Adjeroh/Naved/CP_80K/Output0/'\n",
    "FILEPATHS[1]='/home/jrm/Adjeroh/Naved/CP_80K/Output1/'\n",
    "FILEPATHS[2]='/home/jrm/Adjeroh/Naved/CP_80K/Output2/'\n",
    "FILEPATHS[3]='/home/jrm/Adjeroh/Naved/CP_80K/Output3/'\n",
    "FILEPATHS[4]='/home/jrm/Adjeroh/Naved/CP_80K/Output4/' \n",
    "FILEPATHS[5]='/home/jrm/Adjeroh/Naved/CP_80K/Output5/'\n",
    "FILEPATHS[6]='/home/jrm/Adjeroh/Naved/CP_80K/Output6/' \n",
    "FILEPATHS[7]='/home/jrm/Adjeroh/Naved/CP_80K/Output7/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 11:07:07.741586\n",
      "Process /home/jrm/Adjeroh/Naved/CP_80K/Output6/ to Nucleus_Rollup_6.csv\n"
     ]
    }
   ],
   "source": [
    "from CellProfiler_Util import CP_Util\n",
    "for c in CLASSES:\n",
    "    print(datetime.datetime.now())\n",
    "    outfile = f'Nucleus_Rollup_%1d.csv'%c\n",
    "    cputil = CP_Util(FILEPATHS[c])\n",
    "    print('Process',FILEPATHS[c],'to',outfile)\n",
    "    cputil.train_test_split() \n",
    "    cputil.validate_split()\n",
    "    train_set=cputil.get_train_patches()\n",
    "    nuc = cputil.get_nuclei()\n",
    "    rollup = nuc.groupby(['PatchNumber']).describe() ## this is slow\n",
    "    rollup.columns=rollup.columns.map('_'.join)  ## helps random forest code\n",
    "    rollup.to_csv(outfile)\n",
    "print(datetime.datetime.now())\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Nucleus_Rollup_5.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory considerations\n",
    "Output0 could not be processed in one chunk. The nucleus file was 12 GB. Our Alien computer only has 8 GB RAM, and this notebook crashed trying the groupby/describe step on that file. So we broke the class 0 data into two chunks of equal size. This notebook used 85% RAM temporarily, then 50% growing to 60%. To break the file...\n",
    "\n",
    "cat Process100_Nucleus.csv | awk '{c++; if (c==1 || c<500010) print $0;}' > Nucleus_0_1.csv\n",
    "\n",
    "cat Process100_Nucleus.csv | awk '{c++; if (c==1 || c>=500010) print $0;}' > Nucleus_0_2.csv\n",
    "\n",
    "In the Output6 directory, we put Nucleus 0 1 and a link to the Image.csv file. In the Output7 directory, we put Nucleus 0 2 and a link to the Image.csv file. After processing, we concatenated classes 6 and 7 into 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
