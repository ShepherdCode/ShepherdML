{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep RNN \n",
    "In notebook 11, we managed to train an RNN with one SimpleRNN layer of 4 neurons.\n",
    "Here, we will add layers and neurons.\n",
    "\n",
    "We will hold off on the stratified split by sequence length.\n",
    "To save RAM and CPU during testing, we will only use short sequences.\n",
    "\n",
    "We will hold off on trying the word2vec embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.keras.backend.set_floatx('float32')  # save RAM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and encode the sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load ncRNA.fasta\n",
      "Load pcRNA.fasta\n",
      "Encoder categories\n",
      "Ecode the non-coding sequences\n",
      "shape: (15173,)\n",
      "element 0 is a sequence: \n",
      "[[0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n",
      "element 0,0 is one letter: [0. 0. 0. 1.]\n",
      "Encode the protein-coding sequences\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13126,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIN_SEQ_LEN=200\n",
    "MAX_SEQ_LEN=25000 ### this screens 4 outliers in the complete dataset\n",
    "MAX_SEQ_LEN=2000  ### we use this to reduce training RAM and CPU\n",
    "DEFLINE='>'\n",
    "ncfile='ncRNA.fasta' \n",
    "pcfile='pcRNA.fasta' \n",
    "#ncfile='tiny.ncRNA.fasta' # 10 seqs for faster debugging\n",
    "#pcfile='tiny.pcRNA.fasta' \n",
    "\n",
    "# Assume file was preprocessed to contain one line per seq.\n",
    "# Returned structure is ndarray of ndarray i.e no python lists inside.\n",
    "def load_fasta(filename):\n",
    "    seqs=[]\n",
    "    with open (filename,'r') as infile:\n",
    "        for line in infile:\n",
    "            if line[0]!=DEFLINE and len(line)>=MIN_SEQ_LEN and len(line)<=MAX_SEQ_LEN:\n",
    "                line=line.rstrip()\n",
    "                chars=np.array(list(line))\n",
    "                seqs.append(chars.reshape(-1, 1)) # reshaped changes (any,) to (any,1)\n",
    "    nparray=np.array(seqs)\n",
    "    return nparray\n",
    "\n",
    "print(\"Load \"+ncfile)\n",
    "nc_seqs = load_fasta(ncfile)\n",
    "print(\"Load \"+pcfile)\n",
    "pc_seqs = load_fasta(pcfile)\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
    "#seq=tf.reshape(nc_seqs[0],shape=(-1, 1)) # tensor flow version\n",
    "seq=nc_seqs[0].reshape(-1, 1)\n",
    "encoder.fit(seq)\n",
    "print(\"Encoder categories\")\n",
    "encoder.categories_\n",
    "\n",
    "print(\"Ecode the non-coding sequences\")\n",
    "nc_list=[]\n",
    "for seq in nc_seqs:\n",
    "    encoded=encoder.transform(seq)  \n",
    "    nc_list.append(encoded)\n",
    "    \n",
    "nc_all=np.array(nc_list) # .reshape(-1,1)\n",
    "print(\"shape: \"+str(nc_all.shape))\n",
    "print(\"element 0 is a sequence: \\n\"+str(nc_all[0]))\n",
    "print(\"element 0,0 is one letter: \"+str(nc_all[0][0]))\n",
    "\n",
    "print(\"Encode the protein-coding sequences\")\n",
    "pc_list=[]\n",
    "for seq in pc_seqs:\n",
    "    encoded=encoder.transform(seq)\n",
    "    pc_list.append(encoded)\n",
    "\n",
    "pc_all=np.array(pc_list) # .reshape(-1,1)\n",
    "pc_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sequences, shape of labels\n",
      "shape of train\n",
      "(22639,) (22639,)\n",
      "shape of test\n",
      "(5660,) (5660,)\n",
      "Convert numpy array to python 3D array\n",
      "Partition = 18111\n",
      "Partition train/validation\n",
      "Lengths of train,valid\n",
      "(18111, 4528)\n",
      "Convert to tensors\n",
      "<class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'>\n",
      "(18111, None, None)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(18111,)\n",
      "<class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'>\n",
      "(4528, None, None)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(4528,)\n"
     ]
    }
   ],
   "source": [
    "nc_labels=np.zeros(shape=(17711))\n",
    "pc_labels=np.ones(shape=(20152))\n",
    "#nc_labels=np.zeros(shape=(10))  # fast debugging\n",
    "#pc_labels=np.ones(shape=(10))\n",
    "\n",
    "#nc_labeled=np.concatenate((nc_all,nc_labels),axis=1)\n",
    "#pc_labeled=np.concatenate((pc_all,pc_labels),axis=1)\n",
    "all_seqs=np.concatenate((nc_all,pc_all),axis=0)\n",
    "all_labels=np.concatenate((nc_labels,pc_labels),axis=0)\n",
    "\n",
    "print(\"shape of sequences, shape of labels\")\n",
    "all_seqs.shape,all_labels.shape\n",
    "\n",
    "splitter = ShuffleSplit(n_splits=1, test_size=0.2, random_state=37863)\n",
    "for train_index,test_index in splitter.split(all_seqs):\n",
    "    train_seqs =   all_seqs[train_index]\n",
    "    train_labels = all_labels[train_index]\n",
    "    test_seqs =    all_seqs[test_index]\n",
    "    test_labels =  all_labels[test_index]\n",
    " \n",
    "print(\"shape of train\")\n",
    "print(train_seqs.shape,train_labels.shape)\n",
    "print(\"shape of test\")\n",
    "print(test_seqs.shape,test_labels.shape)\n",
    "\n",
    "print(\"Convert numpy array to python 3D array\")\n",
    "def numpy_to_python_3D(np_seqs):\n",
    "    one_set = []\n",
    "    tlen = len(np_seqs)\n",
    "    for i in range(tlen): # for every sequence in set\n",
    "        one_seq = []\n",
    "        slen = len(np_seqs[i])\n",
    "        for j in range(slen): # for ever letter in sequence\n",
    "            one_letter=np_seqs[i][j]\n",
    "            one_seq.append(one_letter)\n",
    "        one_set.append(one_seq)\n",
    "    return one_set\n",
    "train_seqs = numpy_to_python_3D(train_seqs)\n",
    "test_seqs = numpy_to_python_3D(test_seqs)\n",
    "train_labels = train_labels.tolist()\n",
    "test_labels = test_labels.tolist()\n",
    "\n",
    "# Now that values are shuffled, partition gives random sample.\n",
    "data_size=len(train_seqs)\n",
    "PARTITION=int(data_size*0.8)\n",
    "print(\"Partition = \"+str(PARTITION))\n",
    "\n",
    "print(\"Partition train/validation\")\n",
    "X_train=train_seqs[:PARTITION]\n",
    "X_valid=train_seqs[PARTITION:]\n",
    "y_train=train_labels[:PARTITION]\n",
    "y_valid=train_labels[PARTITION:]\n",
    "print(\"Lengths of train,valid\")\n",
    "print((len(X_train),len(X_valid)))\n",
    "\n",
    "# Free memory for the next step\n",
    "nc_seqs=None\n",
    "pc_seqs=None\n",
    "all_seqs=None\n",
    "nc_labels=None\n",
    "pc_labels=None\n",
    "train_seqs=None\n",
    "train_labels=None\n",
    "\n",
    "print(\"Convert to tensors\")\n",
    "X_train = tf.ragged.constant(X_train)  # This takes a long time and hogs memory\n",
    "print(type(X_train))\n",
    "print(X_train.shape)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "print(type(y_train))\n",
    "print(y_train.shape)\n",
    "X_valid = tf.ragged.constant(X_valid)\n",
    "print(type(X_valid))\n",
    "print(X_valid.shape)\n",
    "y_valid = tf.convert_to_tensor(y_valid)\n",
    "print(type(y_valid))\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build the model\n",
      "Build the training environment\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (None, None, 16)          336       \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, None, 16)          528       \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, None, 16)          528       \n",
      "_________________________________________________________________\n",
      "simple_rnn_5 (SimpleRNN)     (None, 1)                 18        \n",
      "=================================================================\n",
      "Total params: 1,410\n",
      "Trainable params: 1,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Build the model\")\n",
    "seq_len=None  # none indicates variable length\n",
    "input_features=4  # one hot encoding of DNA means 4 categories\n",
    "rnn2 = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(16, return_sequences=True, input_shape=[seq_len,input_features]),\n",
    "    keras.layers.SimpleRNN(16, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(16, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(1),\n",
    "])\n",
    "\n",
    "print(\"Build the training environment\")\n",
    "bc=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "rnn2.compile(loss=bc, optimizer=\"Adam\",metrics=[\"accuracy\"])\n",
    "rnn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the model\n",
      "708/708 [==============================] - ETA: 0s - loss: 1.6904 - accuracy: 0.4942"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'logs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2d34d64c6bbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# ValueError: Input 0 of layer sequential_1 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, None, None, None]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    860\u001b[0m           val_x, val_y, val_sample_weight = (\n\u001b[1;32m    861\u001b[0m               data_adapter.unpack_x_y_sample_weight(validation_data))\n\u001b[0;32m--> 862\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m    863\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1089\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'logs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "print(\"Train the model\")\n",
    "history = rnn2.fit(X_train,y_train,epochs=1,validation_data=(X_valid,y_valid))\n",
    "# ValueError: Input 0 of layer sequential_1 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, None, None, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Visualize training history.\")\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources.\n",
    "[Working with RNNs](https://keras.io/guides/working_with_rnns/).\n",
    "[Recurrent Neural Networks with Keras](https://www.tensorflow.org/guide/keras/rnn#rnns_with_listdict_inputs_or_nested_inputs).\n",
    "Function tf.convert_to_tensor [docs](https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor).\n",
    "Function tf.reshape [docs](https://www.tensorflow.org/api_docs/python/tf/reshape).\n",
    "Ragged Tensors [tutorial](https://www.tensorflow.org/guide/tensor#ragged_tensors)\n",
    "and [docs](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor#documenting_raggedtensor_shapes_2) and [module](https://www.tensorflow.org/api_docs/python/tf/ragged).\n",
    "Incredible speedup for convert to tensor by sirfz on [stackoverflow](https://stackoverflow.com/questions/44353509/tensorflow-tf-constant-initializer-is-very-slow).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
