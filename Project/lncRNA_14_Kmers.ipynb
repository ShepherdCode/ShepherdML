{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "lncRNA_14_Kmers.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFXUwj5sKlBa",
        "colab_type": "text"
      },
      "source": [
        "# RNN on RNA K-mers\n",
        "\n",
        "Our previous RNN used one-hot encoding per single letter.\n",
        "Here we try one-hot encoding per K-mer.\n",
        "For this experiment, K=3.\n",
        "\n",
        "On the first try, this hit 15 GB RAM just building the nc_seqs tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF0oFz5oKlBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "tf.keras.backend.set_floatx('float32')  # save RAM\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHTgt2mcKlBs",
        "colab_type": "text"
      },
      "source": [
        "Load and encode the sequence data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHgYYvUPKlBt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "a1728f02-612d-4f95-b630-02571f7f9b61"
      },
      "source": [
        "MIN_SEQ_LEN=200   ### by definition, lncRNA have min len 200\n",
        "MIN_SEQ_LEN=1000  ### we use this to reduce training RAM and CPU\n",
        "MAX_SEQ_LEN=25000 ### this screens 4 outliers in the complete dataset\n",
        "MAX_SEQ_LEN=2000  ### we use this to reduce training RAM and CPU\n",
        "DEFLINE='>'\n",
        "ncfile='ncRNA.fasta' \n",
        "pcfile='pcRNA.fasta' \n",
        "#ncfile='tiny.ncRNA.fasta' # 10 seqs for faster debugging\n",
        "#pcfile='tiny.pcRNA.fasta' # 10 seqs for faster debugging\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "# !ls drive/'My Drive'/'Colab Notebooks'/*\n",
        "ncfile='drive/My Drive/Colab Notebooks/ncRNA.fasta'\n",
        "pcfile='drive/My Drive/Colab Notebooks/pcRNA.fasta'\n",
        "\n",
        "K=3   # K-mer length = word length\n",
        "\n",
        "# Assume file was preprocessed to contain one line per seq.\n",
        "# Returned structure is ndarray of ndarray i.e no python lists inside.\n",
        "def load_fasta(filename):\n",
        "    seqs=[]\n",
        "    with open (filename,'r') as infile:\n",
        "        for line in infile:\n",
        "            if line[0]!=DEFLINE and len(line)>=MIN_SEQ_LEN and len(line)<=MAX_SEQ_LEN:\n",
        "                line=line.rstrip()\n",
        "                linelen=len(line)\n",
        "                kmers=[]\n",
        "                for i in range(linelen-K+1): # e.g. k=3,|line=4| => range=2 so i={0,1}\n",
        "                    kmer=line[i:i+K]\n",
        "                    kmers.append(kmer)\n",
        "                kmers=np.array(kmers)\n",
        "                seqs.append(kmers.reshape(-1, 1)) # reshaped changes (any,) to (any,1)\n",
        "    nparray=np.array(seqs)\n",
        "    return nparray\n",
        "\n",
        "print(\"Load \"+ncfile)\n",
        "nc_seqs = load_fasta(ncfile)\n",
        "print(\"Load \"+pcfile)\n",
        "pc_seqs = load_fasta(pcfile)\n",
        "\n",
        "encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
        "#seq=tf.reshape(nc_seqs[0],shape=(-1, 1)) # tensor flow version\n",
        "seq=nc_seqs[0].reshape(-1, 1)\n",
        "encoder.fit(seq)\n",
        "print(\"Encoder categories\")\n",
        "print(str(encoder.categories_))\n",
        "\n",
        "print(\"Ecode the non-coding sequences\")\n",
        "nc_list=[]\n",
        "for seq in nc_seqs:\n",
        "    encoded=encoder.transform(seq)  \n",
        "    nc_list.append(encoded)\n",
        "    \n",
        "nc_all=np.array(nc_list) # .reshape(-1,1)\n",
        "print(\"shape: \"+str(nc_all.shape))\n",
        "print(\"sequence 0 starts \")\n",
        "print(nc_seqs[0][:7])\n",
        "print(\"element 0 encoding: \")\n",
        "print(nc_all[0][:5])\n",
        "\n",
        "print(\"Encode the protein-coding sequences\")\n",
        "pc_list=[]\n",
        "for seq in pc_seqs:\n",
        "    encoded=encoder.transform(seq)\n",
        "    pc_list.append(encoded)\n",
        "\n",
        "pc_all=np.array(pc_list) # .reshape(-1,1)\n",
        "pc_all.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Load drive/My Drive/Colab Notebooks/ncRNA.fasta\n",
            "Load drive/My Drive/Colab Notebooks/pcRNA.fasta\n",
            "Encoder categories\n",
            "[array(['AAA', 'AAC', 'AAG', 'AAT', 'ACA', 'ACC', 'ACG', 'ACT', 'AGA',\n",
            "       'AGC', 'AGG', 'AGT', 'ATA', 'ATC', 'ATG', 'ATT', 'CAA', 'CAC',\n",
            "       'CAG', 'CAT', 'CCA', 'CCC', 'CCG', 'CCT', 'CGA', 'CGC', 'CGG',\n",
            "       'CGT', 'CTA', 'CTC', 'CTG', 'CTT', 'GAA', 'GAC', 'GAG', 'GAT',\n",
            "       'GCA', 'GCC', 'GCG', 'GCT', 'GGA', 'GGC', 'GGG', 'GGT', 'GTA',\n",
            "       'GTC', 'GTG', 'GTT', 'TAA', 'TAC', 'TAG', 'TAT', 'TCA', 'TCC',\n",
            "       'TCG', 'TCT', 'TGA', 'TGC', 'TGG', 'TGT', 'TTA', 'TTC', 'TTG',\n",
            "       'TTT'], dtype='<U3')]\n",
            "Ecode the non-coding sequences\n",
            "shape: (3761,)\n",
            "sequence 0 starts \n",
            "[['TCA']\n",
            " ['CAG']\n",
            " ['AGC']\n",
            " ['GCC']\n",
            " ['CCT']\n",
            " ['CTC']\n",
            " ['TCC']]\n",
            "element 0 encoding: \n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "Encode the protein-coding sequences\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5864,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvMpBuH0KlB8",
        "colab_type": "text"
      },
      "source": [
        "Create train and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tbhs-U-KlB8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e44dbf08-14ed-4c7b-f77d-bffe8977d9bc"
      },
      "source": [
        "nc_labels=np.zeros(shape=(17711))\n",
        "pc_labels=np.ones(shape=(20152))\n",
        "#nc_labels=np.zeros(shape=(10))  # fast debugging\n",
        "#pc_labels=np.ones(shape=(10))\n",
        "\n",
        "#nc_labeled=np.concatenate((nc_all,nc_labels),axis=1)\n",
        "#pc_labeled=np.concatenate((pc_all,pc_labels),axis=1)\n",
        "all_seqs=np.concatenate((nc_all,pc_all),axis=0)\n",
        "all_labels=np.concatenate((nc_labels,pc_labels),axis=0)\n",
        "\n",
        "print(\"shape of sequences, shape of labels\")\n",
        "all_seqs.shape,all_labels.shape\n",
        "\n",
        "splitter = ShuffleSplit(n_splits=1, test_size=0.2, random_state=37863)\n",
        "for train_index,test_index in splitter.split(all_seqs):\n",
        "    train_seqs =   all_seqs[train_index]\n",
        "    train_labels = all_labels[train_index]\n",
        "    test_seqs =    all_seqs[test_index]\n",
        "    test_labels =  all_labels[test_index]\n",
        " \n",
        "print(\"shape of train\")\n",
        "print(train_seqs.shape,train_labels.shape)\n",
        "print(\"shape of test\")\n",
        "print(test_seqs.shape,test_labels.shape)\n",
        "\n",
        "print(\"Convert numpy array to python 3D array\")\n",
        "def numpy_to_python_3D(np_seqs):\n",
        "    one_set = []\n",
        "    tlen = len(np_seqs)\n",
        "    for i in range(tlen): # for every sequence in set\n",
        "        one_seq = []\n",
        "        slen = len(np_seqs[i])\n",
        "        for j in range(slen): # for ever letter in sequence\n",
        "            one_letter=np_seqs[i][j]\n",
        "            one_seq.append(one_letter)\n",
        "        one_set.append(one_seq)\n",
        "    return one_set\n",
        "train_seqs = numpy_to_python_3D(train_seqs)\n",
        "test_seqs = numpy_to_python_3D(test_seqs)\n",
        "train_labels = train_labels.tolist()\n",
        "test_labels = test_labels.tolist()\n",
        "\n",
        "# Now that values are shuffled, partition gives random sample.\n",
        "data_size=len(train_seqs)\n",
        "PARTITION=int(data_size*0.8)\n",
        "print(\"Partition = \"+str(PARTITION))\n",
        "\n",
        "print(\"Partition train/validation\")\n",
        "X_train=train_seqs[:PARTITION]\n",
        "X_valid=train_seqs[PARTITION:]\n",
        "y_train=train_labels[:PARTITION]\n",
        "y_valid=train_labels[PARTITION:]\n",
        "print(\"Lengths of train,valid\")\n",
        "print((len(X_train),len(X_valid)))\n",
        "\n",
        "# Free memory for the next step\n",
        "nc_seqs=None\n",
        "pc_seqs=None\n",
        "all_seqs=None\n",
        "nc_labels=None\n",
        "pc_labels=None\n",
        "train_seqs=None\n",
        "train_labels=None\n",
        "\n",
        "print(\"Convert to tensors\")\n",
        "tensor = tf.ragged.constant(X_train)  # This takes a long time and hogs memory\n",
        "X_train = tensor\n",
        "tensor=None\n",
        "print(type(X_train))\n",
        "print(X_train.shape)\n",
        "tensor = tf.convert_to_tensor(y_train)\n",
        "y_train = tensor\n",
        "tensor=None\n",
        "print(type(y_train))\n",
        "print(y_train.shape)\n",
        "tensor = tf.ragged.constant(X_valid)\n",
        "X_valid=tensor\n",
        "tensor=None\n",
        "print(type(X_valid))\n",
        "print(X_valid.shape)\n",
        "tensor = tf.convert_to_tensor(y_valid)\n",
        "y_valid = tensor\n",
        "tensor=None\n",
        "print(type(y_valid))\n",
        "print(y_valid.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of sequences, shape of labels\n",
            "shape of train\n",
            "(7700,) (7700,)\n",
            "shape of test\n",
            "(1925,) (1925,)\n",
            "Convert numpy array to python 3D array\n",
            "Partition = 6160\n",
            "Partition train/validation\n",
            "Lengths of train,valid\n",
            "(6160, 1540)\n",
            "Convert to tensors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-2yAOndKlCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Build the model\")\n",
        "seq_len=None  # none indicates variable length\n",
        "input_features=4**K   # 64 DNA K-mers at K=3\n",
        "rnn2 = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(16, return_sequences=True, input_shape=[seq_len,input_features]),\n",
        "    keras.layers.SimpleRNN(16, return_sequences=True),\n",
        "    keras.layers.SimpleRNN(16, return_sequences=True),\n",
        "    keras.layers.SimpleRNN(1),\n",
        "])\n",
        "\n",
        "print(\"Build the training environment\")\n",
        "bc=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "rnn2.compile(loss=bc, optimizer=\"Adam\",metrics=[\"accuracy\"])\n",
        "rnn2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBoVjg06KlCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Train the model\")\n",
        "history = rnn2.fit(X_train,y_train,epochs=5,validation_data=(X_valid,y_valid))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGVmufeoKlCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Visualize training history.\")\n",
        "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0,1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quH4crn8KlCR",
        "colab_type": "text"
      },
      "source": [
        "Resources.\n",
        "[Working with RNNs](https://keras.io/guides/working_with_rnns/).\n",
        "[Recurrent Neural Networks with Keras](https://www.tensorflow.org/guide/keras/rnn#rnns_with_listdict_inputs_or_nested_inputs).\n",
        "Function tf.convert_to_tensor [docs](https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor).\n",
        "Function tf.reshape [docs](https://www.tensorflow.org/api_docs/python/tf/reshape).\n",
        "Ragged Tensors [tutorial](https://www.tensorflow.org/guide/tensor#ragged_tensors)\n",
        "and [docs](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor#documenting_raggedtensor_shapes_2) and [module](https://www.tensorflow.org/api_docs/python/tf/ragged).\n",
        "Incredible speedup for convert to tensor by sirfz on [stackoverflow](https://stackoverflow.com/questions/44353509/tensorflow-tf-constant-initializer-is-very-slow).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QeTNy7HKlCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}