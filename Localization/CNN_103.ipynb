{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PG-tGRnlFLA3"
   },
   "source": [
    "# CNN + LSTM + ATTN (fail)\n",
    "We want to add layers for Attention and Pooling.\n",
    "Unfortunately, it isn't so easy.\n",
    "\n",
    "We probably must abandon the keras Sequential() model \n",
    "and switch to using the Functional() api to build the model as a graph.\n",
    "\n",
    "Even though keras has an Attention layer,\n",
    "we'll need to implement a subclass.\n",
    "\n",
    "There are many types of attention, including two very popular ones.\n",
    "We'll need to study the Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RmwUsVLFLA6",
    "outputId": "d393332d-ef0a-4f97-c75f-304f964759e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-17 15:50:26.793068\n",
      "Python 3.10.0\n",
      "sklearn 1.1.2\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())\n",
    "from platform import python_version\n",
    "print('Python',python_version())\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "import sklearn   # pip install --upgrade scikit-learn\n",
    "print('sklearn',sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUtGXPrcFLA8"
   },
   "source": [
    "We prevously used sklearn.model_selection.ShuffleSplit   \n",
    "Now we avoid it due to this note in the \n",
    "[documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html):\n",
    "Note: contrary to other cross-validation strategies, random splits do not guarantee that all folds will be different, although this is still very likely for sizeable datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PRX-UEr8FLA8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-17 15:50:30.882012: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "dt='float32'\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "tf.random.set_seed(42) \n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Masking\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Attention\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "K=4\n",
    "RCI_THRESHOLD=0.0\n",
    "CFILTERS=16\n",
    "FILTERSIZE=10\n",
    "RCELLS=16\n",
    "EPOCHS=5\n",
    "FOLDS=5      \n",
    "EMBED_DIMEN = 4 # arbitrary hyperparameter\n",
    "BREAK = True   # break after first fold\n",
    "MINLEN=1000\n",
    "MAXLEN=2000   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OlzN9OdsFWEU",
    "outputId": "1583083c-f133-4840-961b-160326b70585"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jasonmiller/WVU/Localization/TrainTest/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print('Running on CoLab')\n",
    "    PATH='/content/drive/'\n",
    "    drive.mount(PATH)\n",
    "    DATA_DIR=PATH+'My Drive/data/Localization/TrainTest/'  # must end in \"/\"\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    DATA_DIR = 'D:/Adjeroh/Localization/TrainTest/'   # Windows\n",
    "    DATA_DIR = '/Users/jasonmiller/WVU/Localization/TrainTest/'    # Mac\n",
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LnkpVKdMFLA-"
   },
   "outputs": [],
   "source": [
    "GENES_FILE =    'CNRCI_coding_train_genes.csv'\n",
    "RCI_FILE =      'CNRCI_coding_train_RCI.gc42.csv'\n",
    "SEQUENCE_FILE = 'CNRCI_coding_train_transcripts.gc42.csv'\n",
    "COUNTS_FILE=    'CNRCI_coding_train_counts.K4.gc42.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3p4QzQJFLA_",
    "outputId": "c4c5769f-4e7a-4f6b-eaae-e32bbc166929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell line for today: 1 = H1.hESC\n"
     ]
    }
   ],
   "source": [
    "def get_ordered_list():\n",
    "    ordered_list = \\\n",
    "    ['A549',\\\n",
    "      'H1.hESC',\\\n",
    "      'HeLa.S3',\\\n",
    "      'HepG2',\\\n",
    "      'HT1080',\\\n",
    "      'HUVEC',\\\n",
    "      'MCF.7',\\\n",
    "      'NCI.H460',\\\n",
    "      'NHEK',\\\n",
    "      'SK.MEL.5',\\\n",
    "      'SK.N.DZ',\\\n",
    "      'SK.N.SH',\\\n",
    "      'GM12878',\\\n",
    "      'K562',\\\n",
    "      'IMR.90']\n",
    "    return ordered_list\n",
    "CELL_LINE_NUMBER=1\n",
    "all_cell_lines = get_ordered_list()\n",
    "cell_line_name = all_cell_lines[CELL_LINE_NUMBER]\n",
    "print('Cell line for today:',CELL_LINE_NUMBER,'=',cell_line_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtqdpJOxFLBA"
   },
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p35ehKV3Kq0z",
    "outputId": "6a1c58b7-007f-4e46-905d-c6f17e5e7fd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n",
      "[20, 16]\n",
      "[63, 57]\n",
      "[64, 0]\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        self.cache=dict() \n",
    "        self.vals = {'A':0, 'C':1, 'G':2, 'T':3}\n",
    "        \n",
    "    def load_gene_rci_values(self,filepath,cell_line):\n",
    "        '''\n",
    "        Load from RCI csv file.\n",
    "        Return dict with keys=gene:str and values=RCI:float.\n",
    "        '''\n",
    "        gene_to_rci = {}\n",
    "        with open (filepath,'r') as handle:\n",
    "            header = None\n",
    "            for row in handle:\n",
    "                if header is None:\n",
    "                    header = row # skip file's header line\n",
    "                else:\n",
    "                    line = row.strip()\n",
    "                    fields = line.split(',')\n",
    "                    gene_id = fields[0]\n",
    "                    rci_val = fields[cell_line+1]\n",
    "                    if rci_val != \"nan\":\n",
    "                        # Convert string nan to float(nan)\n",
    "                        gene_to_rci[gene_id] = float(rci_val)\n",
    "        return gene_to_rci\n",
    "    \n",
    "    def seq_to_kmer_values(self,rna,K):\n",
    "        # The cache may represent more than one K. Probably not a problem.\n",
    "        N_indicator = 0 # indicator value\n",
    "        vec=[] # seq converted to list of K-mers \n",
    "        length = len(rna)\n",
    "        for i in range(length-K+1):\n",
    "            kmer = rna[i:i+K]\n",
    "            if 'N' in kmer:\n",
    "                value = N_indicator\n",
    "            elif kmer in self.cache.keys():\n",
    "                value = self.cache[kmer]\n",
    "            else:\n",
    "                value = 0\n",
    "                for j in range(K):\n",
    "                    value *= 4   \n",
    "                    nextnuc = kmer[j] \n",
    "                    nucval = self.vals[nextnuc]\n",
    "                    value += nucval\n",
    "                value += 1   # NNN => 0, AAA => 1\n",
    "                self.cache[kmer] = value\n",
    "            vec.append(value)\n",
    "        return vec\n",
    "\n",
    "    def rci_to_label(self,rci):\n",
    "        CYTO_LABEL = 1\n",
    "        NUCLEAR_LABEL = 0\n",
    "        # cnrci = log (cyto-to-nuclear ratio)\n",
    "        # rci > 0 implies cytoplasmic\n",
    "        if rci > RCI_THRESHOLD:\n",
    "            return CYTO_LABEL\n",
    "        return NUCLEAR_LABEL\n",
    "\n",
    "    def load_sequence(self,filepath):\n",
    "        labels=[]\n",
    "        allids=[]\n",
    "        allseq=[]\n",
    "        NREPEAT = str('N'*MAXLEN)\n",
    "        with open (filepath,'r') as handle:\n",
    "            header = None\n",
    "            for row in handle:\n",
    "                if header is None:\n",
    "                    header = row\n",
    "                else:\n",
    "                    line    = row.strip()\n",
    "                    fields  = line.split(',')\n",
    "                    tran_id = fields[0]  # with version number\n",
    "                    gene_id = fields[1]        # without version number\n",
    "                    seq_len = int(fields[3])\n",
    "                    seq_txt = fields[4]\n",
    "                    if seq_len>=MINLEN and seq_len<=MAXLEN and gene_id in gene_to_rci.keys():\n",
    "                        allids.append( (gene_id,tran_id) )\n",
    "                        rci_val = gene_to_rci[gene_id]\n",
    "                        rci_label = self.rci_to_label(rci_val)\n",
    "                        labels.append(rci_label)\n",
    "                        if seq_len<MAXLEN:\n",
    "                            seq_txt = seq_txt + NREPEAT\n",
    "                            seq_txt = seq_txt[:MAXLEN]\n",
    "                        hot_vec = self.seq_to_kmer_values(seq_txt,K)\n",
    "                        allseq.append(hot_vec)\n",
    "        return labels,allids,allseq\n",
    "    \n",
    "loader = DataLoader()\n",
    "# test it\n",
    "print(loader.seq_to_kmer_values('AAAA',3))\n",
    "print(loader.seq_to_kmer_values('CATT',3))\n",
    "print(loader.seq_to_kmer_values('TTGA',3))\n",
    "print(loader.seq_to_kmer_values('TTTN',3))\n",
    "# test it\n",
    "print(loader.rci_to_label(-0.9))\n",
    "print(loader.rci_to_label(1.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYoYDc93FLBB",
    "outputId": "10831256-91c8-488c-86d8-8843d197632a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-17 15:50:42.108465\n",
      "Load RCI values...\n",
      "Num RCI: 13000\n",
      "Example RCI: [('ENSG00000000003', 1.85734), ('ENSG00000000005', 5.88264), ('ENSG00000000419', 2.58954)]\n",
      "Load genes...\n",
      "2022-12-17 15:50:42.151659\n",
      "Load sequence...\n",
      "2022-12-17 15:50:55.540001\n",
      "Num IDs: 13143\n",
      "Example IDs: [('ENSG00000186827', 'ENST00000379236.4'), ('ENSG00000127054', 'ENST00000620829.4'), ('ENSG00000235098', 'ENST00000520296.5'), ('ENSG00000187730', 'ENST00000638411.1')]\n",
      "Count 6959 ones out of 13143 labels.\n",
      "Example labels: [0, 0, 0, 0]\n",
      "Num counts: 13143\n",
      "Example sequence: [60, 238, 182, 213, 81, 67, 11, 43, 171, 169, 164, 142, 54, 216, 95, 122, 229, 147, 75, 44, 175, 186, 229, 148, 78, 54, 216, 94, 118, 215, 89, 100, 142, 56, 223, 122, 231, 153, 98, 136, 31, 122, 230, 150, 87, 91, 107, 170, 168, 159, 122, 230, 151, 89, 97, 132, 13, 52, 206, 54, 216, 94, 118, 214, 86, 87, 91, 108, 175, 185, 225, 130, 6, 23, 91, 107, 171, 171, 170, 167, 155, 106, 168, 159, 123, 234, 165, 147, 73, 34, 133, 17, 67, 9, 35, 139, 41, 162, 133, 19, 76, 46, 183, 218, 102, 150, 88, 95, 122, 230, 152, 95, 122, 230, 151, 90, 102, 150, 87, 91, 105, 161, 130, 6, 24, 95, 121, 225, 131, 9, 33, 131, 11, 41, 163, 138, 39, 153, 97, 130, 8, 30, 118, 214, 85, 83, 74, 40, 160, 126, 248, 222, 120, 223, 122, 230, 149, 83, 74, 39, 153, 100, 143, 59, 236, 175, 185, 226, 133, 19, 74, 39, 153, 98, 135, 27, 107, 169, 163, 140, 47, 187, 234, 166, 150, 86, 85, 82, 70, 24, 95, 124, 239, 187, 235, 170, 167, 155, 106, 167, 155, 106, 166, 149, 83, 75, 42, 168, 160, 127, 249, 225, 131, 10, 37, 147, 75, 41, 163, 139, 41, 164, 143, 59, 236, 174, 182, 215, 90, 101, 146, 69, 20, 78, 54, 215, 92, 109, 180, 206, 53, 212, 79, 57, 225, 131, 9, 35, 137, 33, 131, 9, 35, 140, 46, 182, 213, 82, 69, 18, 70, 22, 85, 82, 72, 31, 123, 235, 169, 162, 135, 28, 111, 185, 225, 130, 5, 20, 78, 56, 222, 120, 224, 128, 254, 246, 215, 89, 99, 137, 35, 139, 42, 167, 156, 110, 182, 216, 95, 122, 229, 147, 74, 38, 149, 83, 75, 41, 162, 135, 27, 106, 165, 145, 66, 6, 24, 96, 126, 246, 214, 85, 82, 70, 22, 88, 94, 117, 212, 77, 52, 206, 54, 213, 83, 74, 39, 156, 110, 182, 213, 82, 70, 23, 90, 101, 147, 74, 38, 151, 90, 102, 149, 82, 70, 24, 94, 119, 220, 112, 189, 244, 207, 58, 230, 150, 87, 89, 99, 138, 37, 148, 78, 53, 211, 73, 35, 138, 38, 151, 90, 104, 159, 124, 239, 185, 225, 132, 16, 62, 246, 213, 83, 73, 35, 137, 35, 139, 42, 165, 147, 74, 38, 152, 95, 123, 233, 163, 137, 36, 144, 63, 251, 234, 166, 152, 95, 122, 231, 153, 98, 134, 22, 87, 90, 102, 151, 91, 108, 175, 185, 226, 134, 24, 95, 124, 240, 191, 251, 235, 170, 165, 145, 67, 9, 35, 139, 42, 168, 159, 123, 235, 170, 166, 151, 90, 104, 158, 118, 214, 86, 86, 86, 87, 92, 109, 180, 206, 53, 211, 74, 37, 147, 74, 39, 153, 98, 136, 31, 122, 232, 160, 128, 256, 254, 245, 211, 73, 35, 137, 33, 131, 9, 35, 139, 43, 170, 165, 146, 71, 25, 97, 131, 10, 39, 153, 97, 132, 14, 55, 218, 102, 152, 94, 117, 209, 67, 10, 37, 147, 73, 35, 139, 42, 167, 154, 104, 159, 122, 232, 159, 122, 232, 159, 122, 230, 151, 90, 103, 155, 107, 169, 163, 138, 40, 159, 123, 235, 171, 170, 166, 150, 85, 83, 74, 37, 148, 79, 59, 234, 166, 150, 86, 87, 91, 105, 163, 139, 41, 162, 134, 21, 84, 80, 61, 242, 198, 23, 90, 102, 151, 91, 106, 168, 160, 127, 252, 239, 188, 238, 181, 211, 74, 37, 146, 72, 31, 121, 227, 138, 39, 153, 99, 139, 42, 166, 149, 83, 74, 37, 146, 70, 24, 96, 128, 255, 249, 227, 139, 41, 162, 134, 22, 88, 94, 117, 211, 74, 39, 154, 102, 152, 94, 120, 221, 114, 198, 21, 82, 70, 24, 95, 123, 235, 170, 166, 152, 94, 118, 214, 86, 85, 83, 74, 38, 149, 82, 71, 27, 108, 175, 185, 227, 139, 41, 162, 134, 22, 85, 82, 70, 22, 88, 95, 123, 234, 165, 148, 79, 57, 228, 142, 54, 214, 86, 86, 88, 94, 117, 212, 78, 53, 210, 70, 24, 94, 118, 214, 86, 85, 83, 74, 38, 149, 82, 71, 25, 100, 142, 56, 222, 118, 216, 95, 121, 227, 139, 43, 172, 174, 182, 215, 91, 106, 165, 147, 75, 41, 163, 139, 44, 175, 187, 234, 167, 155, 106, 168, 159, 122, 229, 147, 74, 40, 158, 120, 223, 121, 227, 139, 43, 171, 170, 166, 150, 86, 85, 83, 76, 47, 187, 234, 166, 152, 95, 123, 233, 161, 131, 10, 38, 150, 85, 82, 70, 24, 95, 122, 230, 150, 88, 94, 118, 216, 94, 118, 213, 82, 71, 27, 106, 165, 147, 75, 44, 174, 181, 211, 74, 39, 156, 110, 183, 219, 105, 161, 131, 10, 37, 147, 75, 43, 170, 166, 152, 95, 123, 234, 168, 158, 117, 211, 74, 37, 146, 70, 23, 91, 107, 169, 163, 139, 43, 170, 167, 154, 102, 151, 90, 102, 150, 86, 85, 83, 74, 40, 159, 122, 230, 151, 90, 102, 150, 86, 87, 92, 110, 182, 216, 96, 126, 248, 222, 119, 219, 105, 163, 137, 35, 139, 43, 169, 163, 138, 40, 159, 122, 230, 152, 94, 117, 211, 74, 38, 151, 90, 102, 150, 86, 86, 86, 88, 96, 127, 250, 232, 159, 124, 238, 183, 218, 102, 151, 90, 101, 147, 73, 33, 132, 15, 58, 230, 150, 86, 88, 94, 117, 210, 71, 28, 110, 183, 218, 102, 150, 88, 95, 123, 235, 170, 166, 150, 86, 86, 85, 84, 78, 56, 222, 117, 211, 75, 42, 166, 150, 86, 86, 86, 88, 96, 126, 246, 216, 95, 123, 235, 171, 171, 172, 175, 186, 230, 150, 88, 94, 119, 219, 106, 168, 158, 120, 223, 124, 239, 186, 230, 149, 83, 73, 34, 134, 22, 86, 85, 83, 75, 42, 168, 157, 114, 199, 27, 106, 168, 160, 126, 246, 216, 95, 122, 230, 150, 86, 86, 86, 87, 90, 103, 154, 101, 147, 75, 42, 167, 155, 105, 163, 137, 36, 143, 60, 240, 190, 247, 218, 102, 152, 95, 123, 234, 165, 147, 74, 37, 147, 75, 41, 163, 138, 40, 158, 118, 216, 95, 122, 231, 155, 105, 161, 131, 10, 37, 147, 73, 33, 130, 6, 24, 95, 123, 234, 166, 150, 87, 91, 106, 168, 159, 123, 233, 163, 138, 40, 159, 122, 230, 150, 87, 90, 102, 151, 89, 98, 134, 24, 94, 118, 216, 95, 122, 231, 155, 106, 165, 147, 73, 33, 131, 11, 41, 163, 138, 40, 159, 123, 233, 163, 137, 35, 138, 39, 154, 103, 154, 103, 154, 102, 150, 85, 82, 69, 19, 74, 40, 159, 122, 232, 159, 123, 234, 167, 154, 102, 150, 87, 89, 99, 137, 34, 134, 23, 90, 102, 150, 88, 95, 122, 231, 154, 102, 150, 86, 85, 81, 66, 7, 25, 98, 135, 27, 106, 167, 154, 102, 151, 89, 99, 139, 41, 163, 138, 40, 159, 122, 229, 147, 74, 39, 155, 106, 167, 154, 103, 155, 107, 171, 170, 166, 150, 88, 95, 122, 232, 159, 123, 236, 175, 186, 232, 159, 121, 225, 130, 6, 21, 82, 71, 27, 106, 167, 154, 103, 155, 106, 167, 154, 102, 149, 82, 72, 31, 122, 232, 159, 123, 234, 166, 150, 88, 95, 122, 230, 150, 86, 86, 86, 86, 85, 83, 75, 43, 171, 170, 166, 150, 86, 86, 87, 91, 107, 170, 168, 158, 118, 215, 91, 105, 162, 134, 22, 86, 86, 86, 85, 82, 70, 22, 86, 87, 92, 110, 182, 214, 87, 91, 107, 169, 162, 136, 30, 120, 223, 122, 230, 150, 87, 91, 106, 167, 153, 99, 138, 38, 150, 86, 86, 86, 87, 91, 105, 161, 131, 11, 43, 171, 171, 171, 172, 174, 182, 214, 87, 91, 106, 166, 150, 88, 95, 122, 230, 152, 94, 117, 211, 74, 39, 154, 103, 155, 106, 166, 150, 85, 83, 74, 39, 153, 99, 140, 46, 182, 213, 81, 67, 11, 41, 163, 137, 36, 143, 57, 226, 135, 27, 107, 171, 171, 170, 168, 157, 115, 203, 42, 168, 158, 120, 223, 123, 235, 170, 165, 146, 69, 17, 67, 9, 36, 143, 59, 234, 168, 158, 119, 219, 105, 161, 131, 9, 34, 135, 25, 99, 138, 38, 150, 86, 86, 86, 85, 81, 65, 3, 9, 34, 136, 30, 119, 219, 105, 162, 135, 27, 105, 163, 137, 35, 139, 41, 162, 134, 22, 86, 87, 89, 99, 137, 34, 135, 27, 106, 165, 147, 74, 40, 159, 124, 240, 191, 251, 235, 172, 175, 186, 229, 147, 75, 43, 171, 171, 170, 166, 150, 85, 82, 72, 30, 118, 215, 91, 107, 170, 166, 149, 81, 67, 10, 40, 158, 118, 213, 83, 74, 40, 159, 123, 233, 163, 139, 43, 171, 170, 166, 151, 91, 106, 167, 154, 102, 151, 89, 99, 139, 43, 171, 169, 161, 131, 11, 43, 171, 170, 168, 160, 128, 256, 254, 246, 214, 85, 83, 75, 43, 172, 174, 182, 213, 82, 69, 18, 72, 31, 122, 230, 150, 86, 88, 95, 123, 235, 170, 168, 160, 126, 246, 214, 88, 96, 125, 244, 207, 58, 230, 151, 92, 110, 181, 211, 74, 38, 150, 86, 88, 93, 114, 200, 32, 126, 246, 213, 82, 69, 18, 69, 19, 75, 42, 167, 154, 103, 155, 108, 173, 179, 203, 43, 171, 171, 169, 162, 136, 30, 120, 222, 118, 213, 84, 79, 59, 233, 164, 143, 59, 235, 171, 169, 163, 139, 41, 163, 139, 42, 166, 150, 86, 85, 83, 74, 38, 150, 86, 88, 95, 121, 227, 139, 41, 162, 135, 28, 110, 181, 210, 70, 21, 81, 67, 12, 47, 187, 233, 162, 134, 23, 92, 111, 187, 233, 164, 143, 57, 226, 135, 28, 110, 184, 223, 122, 229, 147, 74, 40, 160, 126, 247, 220, 111, 187, 235, 171, 171, 171, 170, 166, 152, 95, 124, 238, 184, 223, 123, 234, 168, 159, 124, 239, 187, 233, 163, 137, 35, 140, 45, 178, 197, 18, 72, 30, 119, 219, 107, 172, 174, 184, 224, 126, 245, 211, 75, 43, 169, 163, 138, 37, 147, 75, 43, 171, 169, 164, 142, 55, 217, 98, 135, 27, 107, 171, 169, 163, 137, 34, 134, 22, 88, 95, 122, 230, 149, 82, 72, 31, 122, 232, 159, 121, 226, 135, 27, 105, 163, 139, 41, 163, 138, 37, 146, 70, 24, 95, 122, 232, 159, 121, 226, 134, 21, 81, 66, 5, 20, 79, 59, 235, 171, 170, 168, 159, 121, 225, 131, 10, 40, 159, 123, 235, 171, 170, 166, 150, 87, 90, 102, 150, 88, 94, 117, 209, 67, 9, 36, 142, 54, 215, 91, 107, 170, 166, 150, 85, 83, 74, 38, 149, 82, 70, 21, 81, 66, 6, 22, 88, 95, 122, 231, 155, 107, 170, 166, 150, 86, 87, 91, 105, 163, 138, 39, 153, 99, 137, 33, 130, 8, 30, 119, 219, 106, 165, 146, 69, 19, 75, 41, 163, 137, 35, 138, 37, 147, 74, 38, 150, 88, 96, 127, 252, 238, 182, 214, 86, 86, 85, 82, 71, 25, 98, 135, 27, 106, 166, 149, 82, 71, 28, 110, 182, 214, 86, 86, 88, 93, 116, 207, 59, 233, 163, 139, 43, 171, 171, 170, 166, 149, 82, 71, 26, 102, 150, 88, 96, 127, 250, 230, 151, 91, 108, 174, 181, 209, 65, 2, 8, 32, 126, 245, 210, 70, 22, 85, 81, 67, 10, 37, 147, 75, 41, 163, 137, 33, 132, 15, 59, 235, 169, 162, 134, 24, 96, 127, 251, 234, 168, 158, 120, 221, 114, 200, 32, 126, 246, 213, 83, 75, 43, 171, 170, 166, 150, 86, 86, 87, 89, 98, 134, 22, 88, 96, 126, 246, 214, 85, 83, 74, 38, 152, 94, 120, 223, 124, 239, 188, 240, 191, 249, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "print(\"Load RCI values...\")\n",
    "loader = DataLoader()\n",
    "filepath = DATA_DIR+RCI_FILE\n",
    "gene_to_rci = loader.load_gene_rci_values(filepath,CELL_LINE_NUMBER)\n",
    "print('Num RCI:', len(gene_to_rci.keys()))\n",
    "print('Example RCI:', list(gene_to_rci.items())[:3])\n",
    "print(\"Load genes...\")\n",
    "all_genes = list(gene_to_rci.keys())\n",
    "print(datetime.now())\n",
    "print('Load sequence...')\n",
    "filepath = DATA_DIR+SEQUENCE_FILE\n",
    "labels,allids,allseq = loader.load_sequence(filepath)\n",
    "print(datetime.now())\n",
    "print('Num IDs:',len(allids))\n",
    "print('Example IDs:',[allids[x] for x in [10, 20, 30, 40]] )\n",
    "print('Count',np.count_nonzero(labels),'ones out of',len(labels),'labels.')\n",
    "print('Example labels:',[labels[x] for x in [10, 20, 30, 40]] )\n",
    "print('Num counts:',len(allseq))\n",
    "print('Example sequence:',allseq[3])\n",
    "loader = None  # drop K-mer cache to save RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDZ6siB_Kq04"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AwMbRjm0FLBF"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ALPHABET=4**K+1  # NUMBER OF DISTINCT KMERS POSSIBLE, add one if N gets mask value\n",
    "    ADJUST_LENGTH = MAXLEN-K+1  # fixed length sequences\n",
    "    rnn = Sequential()\n",
    "    embed_layer = Embedding(ALPHABET,EMBED_DIMEN,input_length=ADJUST_LENGTH,mask_zero=True)   \n",
    "    cnn1_layer = Conv1D(CFILTERS, FILTERSIZE)\n",
    "    rnn1_layer = Bidirectional( LSTM(RCELLS, return_sequences=True) )\n",
    "    att1_layer = Attention()\n",
    "    output_layer = Dense(1,activation='sigmoid',dtype=dt)\n",
    "\n",
    "    rnn.add(embed_layer)\n",
    "    rnn.add(cnn1_layer)\n",
    "    rnn.add(rnn1_layer)\n",
    "    rnn.add(att1_layer)\n",
    "    rnn.add(output_layer)\n",
    "\n",
    "    bc=BinaryCrossentropy(from_logits=False)\n",
    "    print(\"COMPILE\")\n",
    "    rnn.compile(loss=bc, optimizer=\"Adam\",metrics=[\"accuracy\"])\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "id": "clj-wufgFLBF",
    "outputId": "dd7144ae-00a4-4db6-fdba-c4827a91722a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-17 15:50:55.605065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-17 15:50:55.638081: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"attention\" (type Attention).\n\nAttention layer must be called on a list of inputs, namely [query, value] or [query, value, key]. Received: Tensor(\"Placeholder:0\", shape=(None, 1988, 32), dtype=float32).\n\nCall arguments received by layer \"attention\" (type Attention):\n  • inputs=tf.Tensor(shape=(None, 1988, 32), dtype=float32)\n  • mask=None\n  • training=None\n  • return_attention_scores=False\n  • use_causal_mask=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mnow())\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msummary())\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m rnn\u001b[38;5;241m.\u001b[39madd(cnn1_layer)\n\u001b[1;32m     13\u001b[0m rnn\u001b[38;5;241m.\u001b[39madd(rnn1_layer)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43matt1_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m rnn\u001b[38;5;241m.\u001b[39madd(output_layer)\n\u001b[1;32m     17\u001b[0m bc\u001b[38;5;241m=\u001b[39mBinaryCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/layers/attention/base_dense_attention.py:211\u001b[0m, in \u001b[0;36mBaseDenseAttention._validate_call_args\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    209\u001b[0m class_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m layer must be called on a list of inputs, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnamely [query, value] or [query, value, key]. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    215\u001b[0m     )\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m layer accepts inputs list of length 2 or 3, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnamely [query, value] or [query, value, key]. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    221\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"attention\" (type Attention).\n\nAttention layer must be called on a list of inputs, namely [query, value] or [query, value, key]. Received: Tensor(\"Placeholder:0\", shape=(None, 1988, 32), dtype=float32).\n\nCall arguments received by layer \"attention\" (type Attention):\n  • inputs=tf.Tensor(shape=(None, 1988, 32), dtype=float32)\n  • mask=None\n  • training=None\n  • return_attention_scores=False\n  • use_causal_mask=False"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "model=build_model()\n",
    "print(model.summary())  # Print this only once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgrC1alOKq07"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9xiFzNbFLBE"
   },
   "outputs": [],
   "source": [
    "class CrossValidator():\n",
    "    def __init__(self,epochs,folds,quick_test=False,pred_threshold=0.5):\n",
    "        self.epochs = epochs\n",
    "        self.folds = folds\n",
    "        self.quick_test = quick_test\n",
    "        self.threshold = pred_threshold # e.g. score>0.5 => class 1\n",
    "    \n",
    "    def get_gene_subset(self,all_genes,sub_index):\n",
    "        sub_genes = set()\n",
    "        for index in sub_index:\n",
    "            one_gene = all_genes[index]\n",
    "            sub_genes.add(one_gene)\n",
    "        return sub_genes\n",
    "    \n",
    "    def get_X_y(self,gene_set,allids,allX,allY):\n",
    "        cnt = len(allids)\n",
    "        subsetX=[]\n",
    "        subsetY=[]\n",
    "        if cnt != len(allX) or cnt!= len(allY):\n",
    "            raise Exception('Lengths differ')\n",
    "        for i in range(cnt):\n",
    "            gene_id,tran_id = allids[i]\n",
    "            if gene_id in gene_set:\n",
    "                oneX = allX[i]\n",
    "                oneY = allY[i]\n",
    "                subsetX.append(oneX)\n",
    "                subsetY.append(oneY)\n",
    "        subsetX = np.array(subsetX)\n",
    "        subsetY = np.array(subsetY).reshape((-1,1))\n",
    "        return subsetX,subsetY\n",
    "    \n",
    "    def do_cross_validation(self):\n",
    "        cv_accuracy=[]\n",
    "        cv_precision=[]\n",
    "        cv_recall=[]\n",
    "        cv_f1=[]\n",
    "        fold=0\n",
    "        print(datetime.now())\n",
    "        print('splitting')\n",
    "        # KFold shuffles once before making the partitions\n",
    "        splitter = KFold(n_splits=self.folds,shuffle=True,random_state=42)\n",
    "        for train_index,valid_index in splitter.split(all_genes):\n",
    "            fold += 1\n",
    "            print('Fold',fold)\n",
    "            train_genes = self.get_gene_subset(all_genes,train_index)\n",
    "            valid_genes = self.get_gene_subset(all_genes,valid_index)\n",
    "            X_train,y_train = self.get_X_y(train_genes,allids,allseq,labels)\n",
    "            X_valid,y_valid = self.get_X_y(valid_genes,allids,allseq,labels)\n",
    "\n",
    "            print('Training example')\n",
    "            print(X_train[0])\n",
    "\n",
    "            print('Train sizes',X_train.shape,y_train.shape)\n",
    "            print('Valid sizes',X_valid.shape,y_valid.shape)\n",
    "            print('Train set ones/size',\n",
    "                  np.count_nonzero(y_train),'/',len(y_train))\n",
    "            print('Valid set ones/size',\n",
    "                  np.count_nonzero(y_valid),'/',len(y_valid))\n",
    "\n",
    "            print(\"BUILD MODEL\")\n",
    "            model=build_model()\n",
    "\n",
    "            print(\"FIT\")\n",
    "            print(datetime.now())\n",
    "            history=model.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
    "                    epochs=self.epochs, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
    "                    validation_data=(X_valid,y_valid) )\n",
    "\n",
    "            pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "            plt.grid(True)\n",
    "            plt.gca().set_ylim(0,1)\n",
    "            plt.show()\n",
    "\n",
    "            print(\"Compute valiation accuracy\")\n",
    "            print(datetime.now())\n",
    "            yhat_pred=model.predict(X_valid, verbose=0) \n",
    "            print('Range of scores:',np.min(yhat_pred),'to',np.max(yhat_pred))\n",
    "            yhat_classes=np.where(yhat_pred > self.threshold, 1, 0)\n",
    "            print('Predicted zeros and ones',np.count_nonzero(yhat_classes==0),np.count_nonzero(yhat_classes==1))\n",
    "            # accuracy: (tp + tn) / (p + n)\n",
    "            accuracy = accuracy_score(y_valid, yhat_classes)*100.\n",
    "            # precision tp / (tp + fp)\n",
    "            precision = precision_score(y_valid, yhat_classes)*100.\n",
    "            # recall: tp / (tp + fn)\n",
    "            recall = recall_score(y_valid, yhat_classes)*100.\n",
    "            # f1: 2 tp / (2 tp + fp + fn)\n",
    "            f1 = f1_score(y_valid, yhat_classes)*100.\n",
    "            print('Accuracy: %.2f%% Precision: %.2f%% Recall: %.2f%% F1: %.2f%%' % (accuracy,precision,recall,f1)) \n",
    "            print(datetime.now())\n",
    "            cv_accuracy.append(accuracy)\n",
    "            cv_precision.append(precision)\n",
    "            cv_recall.append(recall)\n",
    "            cv_f1.append(f1)\n",
    "            if self.quick_test:   \n",
    "                print('Break -- this was for code testing only')\n",
    "                break\n",
    "        print()\n",
    "        return cv_accuracy, cv_precision, cv_recall, cv_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XC9m0W-pFLBH",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(datetime.now())\n",
    "cvdo = CrossValidator(EPOCHS,FOLDS,BREAK)\n",
    "cv_accuracy, cv_precision, cv_recall, cv_f1 = cvdo.do_cross_validation()   \n",
    "print(\"Cross validation %d folds %d epochs\" % (FOLDS,EPOCHS)) \n",
    "print(\" accuracy mean %.2f%% +/- %.2f\" % (np.mean(cv_accuracy), np.std(cv_accuracy)))\n",
    "print(\" precision mean %.2f%% +/- %.2f\" % (np.mean(cv_precision), np.std(cv_precision)))\n",
    "print(\" recall mean %.2f%% +/- %.2f\" % (np.mean(cv_recall), np.std(cv_recall)))\n",
    "print(\" F1 mean %.2f%% +/- %.2f\" % (np.mean(cv_f1), np.std(cv_f1)))\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thQspN3Nga5S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
