{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PG-tGRnlFLA3"
   },
   "source": [
    "# LSTM + CNN + GlobalAvgPool \n",
    "Model: Embed no mask, LSTM ret seq, Conv1D, GlobalAvgPool\n",
    "\n",
    "Coding len 200-4000\n",
    "\n",
    "Killed after 2 epochs. Each epoch took 1.5 hours on Colab GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RmwUsVLFLA6",
    "outputId": "fa49df17-8c0f-4f48-9134-fcc693fee9cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-19 21:38:57.479441\n",
      "Python 3.8.16\n",
      "sklearn 1.0.2\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())\n",
    "from platform import python_version\n",
    "print('Python',python_version())\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "import sklearn   # pip install --upgrade scikit-learn\n",
    "print('sklearn',sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUtGXPrcFLA8"
   },
   "source": [
    "We prevously used sklearn.model_selection.ShuffleSplit   \n",
    "Now we avoid it due to this note in the \n",
    "[documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html):\n",
    "Note: contrary to other cross-validation strategies, random splits do not guarantee that all folds will be different, although this is still very likely for sizeable datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PRX-UEr8FLA8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "dt='float32'\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "tf.random.set_seed(42) \n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Masking\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Attention\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "K=4\n",
    "RCI_THRESHOLD=0.0\n",
    "CFILTERS=64\n",
    "FILTERSIZE=8\n",
    "RCELLS=16\n",
    "DCELLS=8\n",
    "EPOCHS=5\n",
    "FOLDS=5      \n",
    "EMBED_DIMEN = 4 # arbitrary hyperparameter\n",
    "BREAK = False   # break after first fold\n",
    "MINLEN=200\n",
    "MAXLEN=4000   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OlzN9OdsFWEU",
    "outputId": "0ed1f0ff-e86f-474f-b2b9-158d7564653b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CoLab\n",
      "Mounted at /content/drive/\n",
      "/content/drive/My Drive/data/Localization/TrainTest/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print('Running on CoLab')\n",
    "    PATH='/content/drive/'\n",
    "    drive.mount(PATH)\n",
    "    DATA_DIR=PATH+'My Drive/data/Localization/TrainTest/'  # must end in \"/\"\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    DATA_DIR = 'D:/Adjeroh/Localization/TrainTest/'   # Windows\n",
    "    DATA_DIR = '/Users/jasonmiller/WVU/Localization/TrainTest/'    # Mac\n",
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LnkpVKdMFLA-"
   },
   "outputs": [],
   "source": [
    "GENES_FILE =    'CNRCI_coding_train_genes.csv'\n",
    "RCI_FILE =      'CNRCI_coding_train_RCI.gc42.csv'\n",
    "SEQUENCE_FILE = 'CNRCI_coding_train_transcripts.gc42.csv'\n",
    "COUNTS_FILE=    'CNRCI_coding_train_counts.K4.gc42.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3p4QzQJFLA_",
    "outputId": "0d44e94c-78be-4ae4-8f99-562ba66e3a87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell line for today: 1 = H1.hESC\n"
     ]
    }
   ],
   "source": [
    "def get_ordered_list():\n",
    "    ordered_list = \\\n",
    "    ['A549',\\\n",
    "      'H1.hESC',\\\n",
    "      'HeLa.S3',\\\n",
    "      'HepG2',\\\n",
    "      'HT1080',\\\n",
    "      'HUVEC',\\\n",
    "      'MCF.7',\\\n",
    "      'NCI.H460',\\\n",
    "      'NHEK',\\\n",
    "      'SK.MEL.5',\\\n",
    "      'SK.N.DZ',\\\n",
    "      'SK.N.SH',\\\n",
    "      'GM12878',\\\n",
    "      'K562',\\\n",
    "      'IMR.90']\n",
    "    return ordered_list\n",
    "CELL_LINE_NUMBER=1\n",
    "all_cell_lines = get_ordered_list()\n",
    "cell_line_name = all_cell_lines[CELL_LINE_NUMBER]\n",
    "print('Cell line for today:',CELL_LINE_NUMBER,'=',cell_line_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtqdpJOxFLBA"
   },
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p35ehKV3Kq0z",
    "outputId": "d1a67970-3ca4-4d15-f31b-f68c8d64570f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n",
      "[20, 16]\n",
      "[63, 57]\n",
      "[64, 0]\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        self.cache=dict() \n",
    "        self.vals = {'A':0, 'C':1, 'G':2, 'T':3}\n",
    "        \n",
    "    def load_gene_rci_values(self,filepath,cell_line):\n",
    "        '''\n",
    "        Load from RCI csv file.\n",
    "        Return dict with keys=gene:str and values=RCI:float.\n",
    "        '''\n",
    "        gene_to_rci = {}\n",
    "        with open (filepath,'r') as handle:\n",
    "            header = None\n",
    "            for row in handle:\n",
    "                if header is None:\n",
    "                    header = row # skip file's header line\n",
    "                else:\n",
    "                    line = row.strip()\n",
    "                    fields = line.split(',')\n",
    "                    gene_id = fields[0]\n",
    "                    rci_val = fields[cell_line+1]\n",
    "                    if rci_val != \"nan\":\n",
    "                        # Convert string nan to float(nan)\n",
    "                        gene_to_rci[gene_id] = float(rci_val)\n",
    "        return gene_to_rci\n",
    "    \n",
    "    def seq_to_kmer_values(self,rna,K):\n",
    "        # The cache may represent more than one K. Probably not a problem.\n",
    "        N_indicator = 0 # indicator value\n",
    "        vec=[] # seq converted to list of K-mers \n",
    "        length = len(rna)\n",
    "        for i in range(length-K+1):\n",
    "            kmer = rna[i:i+K]\n",
    "            if 'N' in kmer:\n",
    "                value = N_indicator\n",
    "            elif kmer in self.cache.keys():\n",
    "                value = self.cache[kmer]\n",
    "            else:\n",
    "                value = 0\n",
    "                for j in range(K):\n",
    "                    value *= 4   \n",
    "                    nextnuc = kmer[j] \n",
    "                    nucval = self.vals[nextnuc]\n",
    "                    value += nucval\n",
    "                value += 1   # NNN => 0, AAA => 1\n",
    "                self.cache[kmer] = value\n",
    "            vec.append(value)\n",
    "        return vec\n",
    "\n",
    "    def rci_to_label(self,rci):\n",
    "        CYTO_LABEL = 1\n",
    "        NUCLEAR_LABEL = 0\n",
    "        # cnrci = log (cyto-to-nuclear ratio)\n",
    "        # rci > 0 implies cytoplasmic\n",
    "        if rci > RCI_THRESHOLD:\n",
    "            return CYTO_LABEL\n",
    "        return NUCLEAR_LABEL\n",
    "\n",
    "    def load_sequence(self,filepath):\n",
    "        labels=[]\n",
    "        allids=[]\n",
    "        allseq=[]\n",
    "        NREPEAT = str('N'*MAXLEN)\n",
    "        with open (filepath,'r') as handle:\n",
    "            header = None\n",
    "            for row in handle:\n",
    "                if header is None:\n",
    "                    header = row\n",
    "                else:\n",
    "                    line    = row.strip()\n",
    "                    fields  = line.split(',')\n",
    "                    tran_id = fields[0]  # with version number\n",
    "                    gene_id = fields[1]        # without version number\n",
    "                    seq_len = int(fields[3])\n",
    "                    seq_txt = fields[4]\n",
    "                    if seq_len>=MINLEN and seq_len<=MAXLEN and gene_id in gene_to_rci.keys():\n",
    "                        allids.append( (gene_id,tran_id) )\n",
    "                        rci_val = gene_to_rci[gene_id]\n",
    "                        rci_label = self.rci_to_label(rci_val)\n",
    "                        labels.append(rci_label)\n",
    "                        if seq_len<MAXLEN:\n",
    "                            seq_txt = seq_txt + NREPEAT\n",
    "                            seq_txt = seq_txt[:MAXLEN]\n",
    "                        hot_vec = self.seq_to_kmer_values(seq_txt,K)\n",
    "                        allseq.append(hot_vec)\n",
    "        return labels,allids,allseq\n",
    "    \n",
    "loader = DataLoader()\n",
    "# test it\n",
    "print(loader.seq_to_kmer_values('AAAA',3))\n",
    "print(loader.seq_to_kmer_values('CATT',3))\n",
    "print(loader.seq_to_kmer_values('TTGA',3))\n",
    "print(loader.seq_to_kmer_values('TTTN',3))\n",
    "# test it\n",
    "print(loader.rci_to_label(-0.9))\n",
    "print(loader.rci_to_label(1.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYoYDc93FLBB",
    "outputId": "78bda346-e8fb-41c6-b7a4-a3d79d0fbfa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-19 21:39:21.966737\n",
      "Load RCI values...\n",
      "Num RCI: 13000\n",
      "Example RCI: [('ENSG00000000003', 1.85734), ('ENSG00000000005', 5.88264), ('ENSG00000000419', 2.58954)]\n",
      "Load genes...\n",
      "2022-12-19 21:39:23.072930\n",
      "Load sequence...\n",
      "2022-12-19 21:40:39.868929\n",
      "Num IDs: 52992\n",
      "Example IDs: [('ENSG00000187634', 'ENST00000455979.1'), ('ENSG00000131591', 'ENST00000421241.7'), ('ENSG00000162571', 'ENST00000379289.6'), ('ENSG00000186827', 'ENST00000699974.1')]\n",
      "Count 25873 ones out of 52992 labels.\n",
      "Example labels: [0, 0, 0, 0]\n",
      "Num counts: 52992\n",
      "Example sequence: [147, 73, 35, 138, 38, 150, 85, 83, 74, 37, 147, 73, 36, 142, 54, 214, 88, 95, 122, 231, 155, 106, 167, 156, 112, 190, 247, 218, 103, 153, 99, 139, 43, 172, 175, 187, 235, 169, 162, 135, 27, 107, 169, 161, 131, 10, 39, 155, 107, 170, 168, 159, 123, 235, 169, 161, 131, 12, 46, 183, 219, 107, 170, 166, 151, 89, 99, 139, 43, 169, 161, 129, 1, 3, 12, 46, 184, 223, 121, 225, 131, 9, 34, 135, 26, 104, 160, 125, 244, 207, 60, 238, 182, 213, 81, 67, 11, 43, 171, 169, 164, 142, 54, 216, 95, 122, 229, 147, 75, 44, 175, 186, 229, 148, 78, 54, 216, 94, 118, 215, 89, 100, 142, 56, 223, 122, 231, 153, 98, 136, 31, 122, 230, 150, 87, 91, 107, 170, 168, 159, 122, 230, 151, 89, 97, 132, 13, 52, 206, 54, 216, 94, 118, 214, 86, 87, 91, 108, 175, 185, 225, 130, 6, 23, 91, 107, 171, 171, 170, 167, 155, 106, 168, 159, 123, 234, 165, 147, 73, 34, 133, 17, 67, 9, 35, 139, 41, 162, 133, 19, 76, 46, 183, 218, 102, 150, 88, 95, 122, 230, 152, 95, 122, 230, 151, 90, 102, 150, 87, 91, 105, 161, 130, 6, 24, 95, 121, 225, 131, 9, 33, 131, 11, 41, 163, 138, 39, 153, 97, 130, 8, 30, 118, 214, 85, 83, 74, 40, 160, 126, 248, 222, 120, 223, 122, 230, 149, 83, 74, 39, 153, 100, 143, 59, 236, 175, 185, 226, 133, 19, 74, 39, 153, 98, 135, 27, 107, 169, 163, 140, 47, 187, 234, 166, 150, 86, 85, 82, 70, 24, 95, 124, 239, 187, 235, 170, 167, 155, 106, 167, 155, 106, 166, 149, 83, 75, 42, 168, 160, 127, 249, 225, 131, 10, 37, 147, 75, 41, 163, 139, 41, 164, 143, 59, 236, 174, 182, 215, 90, 101, 146, 69, 20, 78, 54, 215, 92, 109, 180, 206, 53, 212, 79, 57, 225, 131, 9, 35, 137, 33, 131, 9, 35, 140, 46, 182, 213, 82, 69, 18, 70, 22, 85, 82, 72, 31, 123, 235, 169, 162, 135, 28, 111, 185, 225, 130, 5, 20, 78, 56, 222, 120, 224, 128, 254, 246, 215, 89, 99, 137, 35, 139, 42, 167, 156, 110, 182, 216, 95, 122, 229, 147, 74, 38, 149, 83, 75, 41, 162, 135, 27, 106, 165, 145, 66, 6, 24, 96, 126, 246, 214, 85, 82, 70, 22, 88, 94, 117, 212, 77, 52, 206, 54, 213, 83, 74, 39, 156, 110, 182, 213, 82, 70, 23, 90, 101, 147, 74, 38, 151, 90, 102, 149, 82, 70, 24, 94, 119, 220, 112, 189, 244, 207, 58, 230, 150, 87, 89, 99, 138, 37, 148, 78, 53, 211, 73, 35, 138, 38, 151, 90, 104, 159, 124, 239, 185, 225, 132, 16, 62, 246, 213, 83, 73, 35, 137, 35, 139, 42, 165, 147, 74, 38, 152, 95, 123, 233, 163, 137, 36, 144, 63, 251, 234, 166, 152, 95, 122, 231, 153, 98, 134, 22, 87, 90, 102, 151, 91, 108, 175, 185, 226, 134, 24, 95, 124, 240, 191, 251, 235, 170, 165, 145, 67, 9, 35, 139, 42, 168, 159, 123, 235, 170, 166, 151, 90, 104, 158, 118, 214, 86, 86, 86, 87, 92, 109, 180, 206, 53, 211, 74, 37, 147, 74, 39, 153, 98, 136, 31, 122, 232, 160, 128, 256, 254, 245, 211, 73, 35, 137, 33, 131, 9, 35, 139, 43, 170, 165, 146, 71, 25, 97, 131, 10, 39, 153, 97, 132, 14, 55, 218, 102, 152, 94, 117, 209, 67, 9, 35, 139, 42, 167, 154, 104, 159, 122, 232, 159, 122, 232, 159, 122, 230, 151, 90, 103, 155, 107, 169, 163, 138, 40, 159, 123, 235, 171, 170, 166, 150, 85, 83, 74, 37, 148, 79, 59, 234, 166, 150, 86, 87, 91, 105, 163, 139, 41, 162, 134, 21, 84, 80, 61, 242, 198, 23, 90, 102, 151, 91, 106, 168, 160, 127, 252, 239, 188, 238, 181, 211, 74, 37, 146, 72, 31, 121, 227, 138, 39, 153, 99, 139, 42, 166, 149, 83, 74, 37, 146, 70, 24, 96, 128, 255, 249, 227, 139, 41, 162, 134, 22, 88, 94, 117, 211, 74, 39, 154, 102, 152, 94, 120, 221, 114, 198, 21, 82, 70, 24, 95, 123, 235, 170, 166, 152, 94, 118, 214, 86, 85, 83, 74, 38, 149, 82, 71, 27, 108, 175, 185, 227, 139, 41, 162, 134, 22, 85, 82, 70, 22, 88, 95, 123, 234, 165, 148, 79, 57, 228, 142, 54, 214, 86, 86, 88, 94, 117, 212, 78, 53, 210, 70, 24, 94, 118, 214, 86, 85, 83, 74, 38, 149, 82, 71, 25, 100, 142, 56, 222, 118, 216, 95, 121, 227, 139, 43, 172, 174, 182, 215, 91, 106, 165, 147, 75, 41, 163, 139, 44, 175, 187, 234, 167, 155, 106, 168, 159, 122, 229, 147, 74, 40, 158, 120, 223, 121, 227, 139, 43, 171, 170, 166, 150, 86, 85, 83, 76, 47, 187, 234, 166, 152, 95, 123, 233, 161, 131, 10, 38, 150, 85, 82, 70, 24, 95, 122, 230, 150, 88, 94, 118, 216, 94, 118, 213, 82, 71, 27, 106, 165, 147, 75, 44, 174, 181, 211, 74, 39, 156, 110, 183, 219, 105, 161, 131, 10, 37, 147, 75, 43, 170, 166, 152, 95, 123, 234, 168, 158, 117, 211, 74, 37, 146, 70, 23, 91, 107, 169, 163, 139, 43, 170, 167, 154, 102, 151, 90, 102, 150, 86, 85, 83, 74, 40, 159, 122, 230, 151, 90, 102, 150, 86, 87, 92, 110, 182, 216, 96, 126, 248, 222, 119, 219, 105, 163, 137, 35, 139, 43, 169, 163, 138, 40, 159, 122, 230, 152, 94, 117, 211, 74, 38, 151, 90, 102, 150, 86, 86, 86, 88, 96, 127, 250, 232, 159, 124, 238, 183, 218, 102, 151, 90, 101, 147, 73, 33, 132, 15, 58, 230, 150, 86, 88, 94, 117, 210, 71, 28, 110, 183, 218, 102, 150, 88, 95, 123, 235, 170, 166, 150, 86, 86, 85, 84, 78, 56, 222, 117, 211, 75, 42, 166, 150, 86, 86, 86, 88, 96, 126, 246, 216, 95, 123, 235, 171, 171, 172, 175, 186, 230, 150, 88, 94, 119, 219, 106, 168, 158, 120, 223, 124, 239, 186, 230, 149, 83, 73, 34, 134, 22, 86, 85, 83, 75, 42, 168, 157, 114, 199, 27, 106, 168, 160, 126, 246, 216, 95, 122, 230, 150, 86, 86, 86, 87, 90, 103, 154, 101, 147, 75, 42, 167, 155, 105, 163, 137, 36, 143, 60, 240, 190, 247, 218, 102, 152, 95, 123, 234, 165, 147, 74, 37, 147, 75, 41, 163, 138, 40, 158, 118, 216, 95, 122, 231, 155, 105, 161, 131, 10, 37, 147, 73, 33, 130, 6, 24, 95, 123, 234, 166, 150, 87, 91, 106, 168, 159, 123, 233, 163, 138, 40, 159, 122, 230, 150, 87, 90, 102, 151, 89, 98, 134, 24, 94, 118, 216, 95, 122, 231, 155, 106, 165, 147, 73, 33, 131, 11, 41, 163, 138, 40, 159, 123, 233, 163, 137, 35, 138, 39, 154, 103, 154, 103, 154, 102, 150, 85, 82, 69, 19, 74, 40, 159, 122, 232, 159, 123, 234, 167, 154, 102, 150, 87, 89, 99, 137, 34, 134, 23, 90, 102, 150, 88, 95, 122, 231, 154, 102, 150, 86, 85, 81, 66, 7, 25, 98, 135, 27, 106, 167, 154, 102, 151, 89, 99, 139, 41, 163, 138, 40, 159, 122, 229, 147, 74, 39, 155, 106, 167, 154, 103, 155, 107, 171, 170, 166, 150, 88, 95, 122, 232, 159, 123, 236, 175, 186, 232, 159, 121, 225, 130, 6, 21, 82, 71, 27, 106, 167, 154, 103, 155, 106, 167, 154, 102, 149, 82, 72, 31, 122, 232, 159, 123, 234, 166, 150, 88, 95, 122, 230, 150, 86, 86, 86, 86, 85, 83, 75, 43, 171, 170, 166, 150, 86, 86, 87, 91, 107, 170, 168, 158, 118, 215, 91, 105, 162, 134, 22, 86, 86, 86, 85, 82, 70, 22, 86, 87, 92, 110, 182, 214, 87, 91, 107, 169, 162, 136, 30, 120, 223, 122, 230, 150, 87, 91, 106, 167, 153, 99, 138, 38, 150, 86, 86, 86, 87, 91, 105, 161, 131, 11, 43, 171, 171, 171, 172, 174, 182, 214, 87, 91, 106, 166, 150, 88, 95, 122, 230, 152, 94, 117, 211, 74, 39, 154, 103, 155, 106, 166, 150, 85, 83, 74, 39, 153, 99, 140, 46, 182, 213, 81, 67, 11, 41, 163, 137, 36, 143, 57, 226, 135, 27, 107, 171, 171, 170, 168, 157, 115, 203, 42, 168, 158, 120, 223, 123, 235, 170, 165, 146, 69, 17, 67, 9, 36, 143, 59, 234, 168, 158, 119, 219, 105, 161, 131, 9, 34, 135, 25, 99, 138, 38, 150, 86, 86, 86, 85, 81, 65, 3, 9, 34, 136, 30, 119, 219, 105, 162, 135, 27, 105, 163, 137, 35, 139, 41, 162, 134, 22, 86, 87, 89, 99, 137, 34, 135, 27, 106, 165, 147, 74, 40, 159, 124, 240, 191, 251, 235, 172, 175, 186, 229, 147, 75, 43, 171, 171, 170, 166, 150, 85, 82, 72, 30, 118, 215, 91, 107, 170, 166, 149, 81, 67, 10, 40, 158, 118, 213, 83, 74, 40, 159, 123, 233, 163, 139, 43, 171, 170, 166, 151, 91, 106, 167, 154, 102, 151, 89, 99, 139, 43, 171, 169, 161, 131, 11, 43, 171, 170, 168, 160, 128, 256, 254, 246, 214, 85, 83, 75, 43, 172, 174, 182, 213, 82, 69, 18, 72, 31, 122, 230, 150, 86, 88, 95, 123, 235, 170, 168, 160, 126, 246, 214, 88, 96, 125, 244, 207, 58, 230, 151, 92, 110, 181, 211, 74, 38, 150, 86, 88, 93, 114, 200, 32, 126, 246, 213, 82, 69, 18, 69, 19, 75, 42, 167, 154, 103, 155, 108, 173, 179, 203, 43, 171, 171, 169, 162, 136, 30, 120, 222, 118, 213, 84, 79, 59, 233, 164, 143, 59, 235, 171, 169, 163, 139, 41, 163, 139, 42, 166, 150, 86, 85, 83, 74, 38, 150, 86, 88, 95, 121, 227, 139, 41, 162, 135, 28, 110, 181, 210, 70, 21, 81, 67, 12, 47, 187, 233, 162, 134, 23, 92, 111, 187, 233, 164, 143, 57, 226, 135, 28, 110, 184, 223, 122, 229, 147, 74, 40, 160, 126, 247, 220, 111, 187, 235, 171, 171, 171, 170, 166, 152, 95, 124, 238, 184, 223, 123, 234, 168, 159, 124, 239, 187, 233, 163, 137, 35, 140, 45, 178, 197, 18, 72, 30, 119, 219, 107, 172, 174, 184, 224, 126, 245, 211, 75, 43, 169, 163, 138, 37, 147, 75, 43, 171, 169, 164, 142, 55, 217, 98, 135, 27, 107, 171, 169, 163, 137, 34, 134, 22, 88, 95, 122, 230, 149, 82, 72, 31, 122, 232, 159, 121, 226, 135, 27, 105, 163, 139, 41, 163, 138, 37, 146, 70, 24, 95, 122, 232, 159, 121, 226, 134, 21, 81, 66, 5, 20, 79, 59, 235, 171, 170, 168, 159, 121, 225, 131, 10, 40, 159, 123, 235, 171, 170, 166, 150, 87, 90, 102, 150, 88, 94, 117, 209, 67, 9, 36, 142, 54, 215, 91, 107, 170, 166, 150, 85, 83, 75, 44, 175, 187, 234, 166, 149, 83, 75, 42, 167, 154, 102, 152, 95, 123, 235, 170, 166, 151, 89, 99, 140, 48, 192, 256, 254, 248, 221, 114, 199, 28, 111, 187, 234, 166, 149, 83, 74, 40, 160, 126, 246, 214, 86, 87, 92, 111, 187, 234, 168, 158, 120, 223, 122, 230, 149, 82, 72, 31, 122, 229, 147, 74, 38, 149, 82, 70, 21, 81, 66, 6, 22, 88, 95, 122, 231, 155, 107, 170, 166, 150, 86, 87, 91, 105, 163, 138, 39, 153, 99, 137, 33, 130, 8, 30, 119, 219, 106, 165, 146, 69, 19, 75, 41, 163, 137, 35, 138, 37, 147, 74, 38, 150, 88, 96, 127, 252, 238, 182, 214, 86, 86, 85, 82, 71, 25, 98, 135, 27, 106, 166, 149, 82, 71, 28, 110, 182, 214, 86, 86, 88, 93, 116, 207, 59, 233, 163, 139, 43, 171, 171, 170, 166, 149, 82, 71, 26, 102, 150, 88, 96, 127, 250, 230, 151, 91, 108, 174, 181, 209, 65, 2, 8, 32, 126, 245, 210, 70, 22, 85, 81, 67, 10, 37, 147, 75, 41, 163, 137, 33, 132, 15, 59, 235, 169, 162, 134, 24, 96, 127, 251, 234, 168, 158, 120, 221, 114, 200, 32, 126, 246, 213, 83, 75, 43, 171, 170, 166, 150, 86, 86, 87, 89, 98, 134, 22, 88, 96, 126, 246, 214, 85, 83, 74, 38, 152, 94, 120, 223, 124, 239, 188, 240, 191, 249, 227, 139, 44, 176, 191, 250, 230, 151, 91, 107, 171, 171, 172, 173, 179, 203, 43, 171, 172, 175, 187, 235, 171, 170, 166, 149, 82, 69, 18, 69, 17, 65, 4, 14, 56, 222, 118, 213, 83, 75, 41, 163, 138, 38, 149, 82, 70, 21, 82, 72, 30, 117, 209, 66, 5, 18, 69, 17, 68, 15, 59, 234, 166, 150, 88, 95, 122, 230, 152, 94, 118, 214, 85, 82, 70, 23, 90, 104, 160, 128, 253, 244, 208, 64, 254, 248, 224, 128, 254, 247, 219, 108, 176, 192, 254, 247, 219, 105, 164, 143, 58, 229, 145, 65, 1, 2, 5, 17, 65, 1, 1, 1, 4, 16, 64, 256, 253, 241, 193, 1, 3, 9, 33, 129, 1, 4, 15, 60, 239, 185, 226, 136, 32, 126, 245, 209, 65, 3, 11, 41, 161, 129, 3, 11, 41, 161, 130, 5, 17, 65, 4, 16, 64, 256, 254, 245, 209, 65, 3, 9, 34, 136, 32, 127, 251, 235, 171, 171, 169, 163, 140, 47, 185, 225, 131, 11, 42, 165, 147, 73, 35, 138, 38, 152, 95, 123, 236, 175, 186, 229, 147, 73, 36, 143, 59, 233, 162, 135, 25, 99, 139, 44, 174, 184, 223, 122, 229, 147, 73, 34, 135, 27, 105, 163, 139, 43, 170, 165, 147, 73, 35, 139, 44, 175, 187, 236, 175, 187, 233, 161, 131, 11, 43, 171, 170, 166, 149, 83, 75, 43, 171, 170, 166, 152, 95, 122, 229, 147, 75, 42, 166, 152, 94, 118, 214, 86, 86, 88, 95, 123, 233, 161, 130, 8, 31, 123, 235, 169, 162, 136, 31, 123, 236, 174, 184, 222, 119, 219, 108, 174, 184, 223, 122, 232, 159, 121, 226, 135, 28, 110, 181, 211, 75, 43, 172, 174, 181, 211, 74, 40, 158, 118, 214, 86, 86, 87, 90, 103, 155, 105, 163, 138, 40, 159, 121, 226, 136, 32, 126, 245, 211, 74, 37, 147, 74, 38, 150, 85, 82, 69, 19, 74, 40, 159, 124, 239, 187, 235, 171, 170, 168, 160, 126, 245, 211, 74, 37, 147, 74, 38, 149, 82, 69, 18, 70, 21, 83, 74, 38, 150, 85, 83, 74, 38, 150, 85, 83, 74, 38, 150, 85, 83, 74, 40, 158, 120, 222, 119, 217, 100, 141, 50, 199, 28, 112, 192, 255, 251, 236, 174, 184, 224, 128, 254, 245, 212, 79, 58, 232, 159, 121, 225, 129, 1, 1, 4, 13, 49, 193, 4, 13, 49, 196, 13, 49, 193, 3, 10, 38, 152, 95, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "print(\"Load RCI values...\")\n",
    "loader = DataLoader()\n",
    "filepath = DATA_DIR+RCI_FILE\n",
    "gene_to_rci = loader.load_gene_rci_values(filepath,CELL_LINE_NUMBER)\n",
    "print('Num RCI:', len(gene_to_rci.keys()))\n",
    "print('Example RCI:', list(gene_to_rci.items())[:3])\n",
    "print(\"Load genes...\")\n",
    "all_genes = list(gene_to_rci.keys())\n",
    "print(datetime.now())\n",
    "print('Load sequence...')\n",
    "filepath = DATA_DIR+SEQUENCE_FILE\n",
    "labels,allids,allseq = loader.load_sequence(filepath)\n",
    "print(datetime.now())\n",
    "print('Num IDs:',len(allids))\n",
    "print('Example IDs:',[allids[x] for x in [10, 20, 30, 40]] )\n",
    "print('Count',np.count_nonzero(labels),'ones out of',len(labels),'labels.')\n",
    "print('Example labels:',[labels[x] for x in [10, 20, 30, 40]] )\n",
    "print('Num counts:',len(allseq))\n",
    "print('Example sequence:',allseq[3])\n",
    "loader = None  # drop K-mer cache to save RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDZ6siB_Kq04"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AwMbRjm0FLBF"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ALPHABET=4**K+1  # NUMBER OF DISTINCT KMERS POSSIBLE, add one if N gets mask value\n",
    "    ADJUST_LENGTH = MAXLEN-K+1  # fixed length sequences\n",
    "    rnn = Sequential()\n",
    "    embed_layer = Embedding(ALPHABET,EMBED_DIMEN,input_length=ADJUST_LENGTH,mask_zero=False)   \n",
    "    rnn1_layer = Bidirectional( LSTM(RCELLS, return_sequences=True) )\n",
    "    cnn1_layer = Conv1D(CFILTERS, FILTERSIZE)\n",
    "    #pool_layer = MaxPooling1D(pool_size=3)\n",
    "    pool_layer = GlobalAveragePooling1D()\n",
    "    flat_layer = Flatten()\n",
    "    den1_layer = Dense(DCELLS)\n",
    "    drop_layer = Dropout(0.5)\n",
    "    output_layer = Dense(1,activation='sigmoid',dtype=dt)\n",
    "\n",
    "    rnn.add(embed_layer)\n",
    "    rnn.add(rnn1_layer)\n",
    "    rnn.add(cnn1_layer)\n",
    "    rnn.add(pool_layer)\n",
    "    #rnn.add(flat_layer)\n",
    "    #rnn.add(den1_layer)\n",
    "    #rnn.add(drop_layer)\n",
    "    rnn.add(output_layer)\n",
    "\n",
    "    bc=BinaryCrossentropy(from_logits=False)\n",
    "    print(\"COMPILE\")\n",
    "    rnn.compile(loss=bc, optimizer=\"Adam\",metrics=[\"accuracy\"])\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "clj-wufgFLBF",
    "outputId": "aa3d928d-e00a-4582-8345-d6a5e1b89617"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-19 21:40:39.920246\n",
      "COMPILE\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 3997, 4)           1028      \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 3997, 32)         2688      \n",
      " l)                                                              \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 3990, 64)          16448     \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 64)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,229\n",
      "Trainable params: 20,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "model=build_model()\n",
    "print(model.summary())  # Print this only once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgrC1alOKq07"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "W9xiFzNbFLBE"
   },
   "outputs": [],
   "source": [
    "class CrossValidator():\n",
    "    def __init__(self,epochs,folds,quick_test=False,pred_threshold=0.5):\n",
    "        self.epochs = epochs\n",
    "        self.folds = folds\n",
    "        self.quick_test = quick_test\n",
    "        self.threshold = pred_threshold # e.g. score>0.5 => class 1\n",
    "    \n",
    "    def get_gene_subset(self,all_genes,sub_index):\n",
    "        sub_genes = set()\n",
    "        for index in sub_index:\n",
    "            one_gene = all_genes[index]\n",
    "            sub_genes.add(one_gene)\n",
    "        return sub_genes\n",
    "    \n",
    "    def get_X_y(self,gene_set,allids,allX,allY):\n",
    "        cnt = len(allids)\n",
    "        subsetX=[]\n",
    "        subsetY=[]\n",
    "        if cnt != len(allX) or cnt!= len(allY):\n",
    "            raise Exception('Lengths differ')\n",
    "        for i in range(cnt):\n",
    "            gene_id,tran_id = allids[i]\n",
    "            if gene_id in gene_set:\n",
    "                oneX = allX[i]\n",
    "                oneY = allY[i]\n",
    "                subsetX.append(oneX)\n",
    "                subsetY.append(oneY)\n",
    "        subsetX = np.array(subsetX)\n",
    "        subsetY = np.array(subsetY).reshape((-1,1))\n",
    "        return subsetX,subsetY\n",
    "    \n",
    "    def do_cross_validation(self):\n",
    "        cv_accuracy=[]\n",
    "        cv_precision=[]\n",
    "        cv_recall=[]\n",
    "        cv_f1=[]\n",
    "        fold=0\n",
    "        print(datetime.now())\n",
    "        print('splitting')\n",
    "        # KFold shuffles once before making the partitions\n",
    "        splitter = KFold(n_splits=self.folds,shuffle=True,random_state=42)\n",
    "        for train_index,valid_index in splitter.split(all_genes):\n",
    "            fold += 1\n",
    "            print('Fold',fold)\n",
    "            train_genes = self.get_gene_subset(all_genes,train_index)\n",
    "            valid_genes = self.get_gene_subset(all_genes,valid_index)\n",
    "            X_train,y_train = self.get_X_y(train_genes,allids,allseq,labels)\n",
    "            X_valid,y_valid = self.get_X_y(valid_genes,allids,allseq,labels)\n",
    "\n",
    "            print('Training example')\n",
    "            print(X_train[0])\n",
    "\n",
    "            print('Train sizes',X_train.shape,y_train.shape)\n",
    "            print('Valid sizes',X_valid.shape,y_valid.shape)\n",
    "            print('Train set ones/size',\n",
    "                  np.count_nonzero(y_train),'/',len(y_train))\n",
    "            print('Valid set ones/size',\n",
    "                  np.count_nonzero(y_valid),'/',len(y_valid))\n",
    "\n",
    "            print(\"BUILD MODEL\")\n",
    "            model=build_model()\n",
    "\n",
    "            print(\"FIT\")\n",
    "            print(datetime.now())\n",
    "            history=model.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
    "                    epochs=self.epochs, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
    "                    validation_data=(X_valid,y_valid) )\n",
    "\n",
    "            pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "            plt.grid(True)\n",
    "            plt.gca().set_ylim(0,1)\n",
    "            plt.show()\n",
    "\n",
    "            print(\"Compute valiation accuracy\")\n",
    "            print(datetime.now())\n",
    "            yhat_pred=model.predict(X_valid, verbose=0) \n",
    "            print('Range of scores:',np.min(yhat_pred),'to',np.max(yhat_pred))\n",
    "            yhat_classes=np.where(yhat_pred > self.threshold, 1, 0)\n",
    "            print('Predicted zeros and ones',np.count_nonzero(yhat_classes==0),np.count_nonzero(yhat_classes==1))\n",
    "            # accuracy: (tp + tn) / (p + n)\n",
    "            accuracy = accuracy_score(y_valid, yhat_classes)*100.\n",
    "            # precision tp / (tp + fp)\n",
    "            precision = precision_score(y_valid, yhat_classes)*100.\n",
    "            # recall: tp / (tp + fn)\n",
    "            recall = recall_score(y_valid, yhat_classes)*100.\n",
    "            # f1: 2 tp / (2 tp + fp + fn)\n",
    "            f1 = f1_score(y_valid, yhat_classes)*100.\n",
    "            print('Accuracy: %.2f%% Precision: %.2f%% Recall: %.2f%% F1: %.2f%%' % (accuracy,precision,recall,f1)) \n",
    "            print(datetime.now())\n",
    "            cv_accuracy.append(accuracy)\n",
    "            cv_precision.append(precision)\n",
    "            cv_recall.append(recall)\n",
    "            cv_f1.append(f1)\n",
    "            if self.quick_test:   \n",
    "                print('Break -- this was for code testing only')\n",
    "                break\n",
    "        print()\n",
    "        return cv_accuracy, cv_precision, cv_recall, cv_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XC9m0W-pFLBH",
    "outputId": "8812f7e8-4992-4301-c910-1f780e85cff3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-19 21:40:41.029762\n",
      "2022-12-19 21:40:41.029971\n",
      "splitting\n",
      "Fold 1\n",
      "Training example\n",
      "[167 155 106 ...   0   0   0]\n",
      "Train sizes (42238, 3997) (42238, 1)\n",
      "Valid sizes (10754, 3997) (10754, 1)\n",
      "Train set ones/size 20659 / 42238\n",
      "Valid set ones/size 5214 / 10754\n",
      "BUILD MODEL\n",
      "COMPILE\n",
      "FIT\n",
      "2022-12-19 21:40:57.686594\n",
      "Epoch 1/5\n",
      "1320/1320 [==============================] - 5751s 4s/step - loss: 0.6121 - accuracy: 0.6626 - val_loss: 0.6081 - val_accuracy: 0.6605\n",
      "Epoch 2/5\n",
      "1320/1320 [==============================] - 6053s 5s/step - loss: 0.5987 - accuracy: 0.6801 - val_loss: 0.5934 - val_accuracy: 0.6837\n",
      "Epoch 3/5\n",
      " 731/1320 [===============>..............] - ETA: 44:07 - loss: 0.5942 - accuracy: 0.6851"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "cvdo = CrossValidator(EPOCHS,FOLDS,BREAK)\n",
    "cv_accuracy, cv_precision, cv_recall, cv_f1 = cvdo.do_cross_validation()   \n",
    "print(\"Cross validation %d folds %d epochs\" % (FOLDS,EPOCHS)) \n",
    "print(\" accuracy mean %.2f%% +/- %.2f\" % (np.mean(cv_accuracy), np.std(cv_accuracy)))\n",
    "print(\" precision mean %.2f%% +/- %.2f\" % (np.mean(cv_precision), np.std(cv_precision)))\n",
    "print(\" recall mean %.2f%% +/- %.2f\" % (np.mean(cv_recall), np.std(cv_recall)))\n",
    "print(\" F1 mean %.2f%% +/- %.2f\" % (np.mean(cv_f1), np.std(cv_f1)))\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thQspN3Nga5S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
