{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PG-tGRnlFLA3"
   },
   "source": [
    "# LSTM + CNN + Dense \n",
    "Compare to CNN 105, but change to LSTM before CNN, and LSTM return sequences.\n",
    "\n",
    "Ran on CoLab for 1 fold of 20 epochs, then died computing stats (???).\n",
    "\n",
    "Coding 2000-4000.\n",
    "\n",
    "Train acc = Valid acc = 63%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RmwUsVLFLA6",
    "outputId": "5e6e5899-97b6-4d92-9423-ffc7d39aef52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 15:00:51.762308\n",
      "Python 3.8.16\n",
      "sklearn 1.0.2\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())\n",
    "from platform import python_version\n",
    "print('Python',python_version())\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "import sklearn   # pip install --upgrade scikit-learn\n",
    "print('sklearn',sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUtGXPrcFLA8"
   },
   "source": [
    "We prevously used sklearn.model_selection.ShuffleSplit   \n",
    "Now we avoid it due to this note in the \n",
    "[documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html):\n",
    "Note: contrary to other cross-validation strategies, random splits do not guarantee that all folds will be different, although this is still very likely for sizeable datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PRX-UEr8FLA8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "dt='float32'\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "tf.random.set_seed(42) \n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Masking\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Attention\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "K=4\n",
    "RCI_THRESHOLD=0.0\n",
    "CFILTERS=62\n",
    "FILTERSIZE=8\n",
    "RCELLS=16\n",
    "EPOCHS=20\n",
    "FOLDS=5      \n",
    "EMBED_DIMEN = 4 # arbitrary hyperparameter\n",
    "BREAK = False   # break after first fold\n",
    "MINLEN=2000\n",
    "MAXLEN=4000   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OlzN9OdsFWEU",
    "outputId": "679f3e68-f8c3-44d4-d800-466098a42518"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CoLab\n",
      "Mounted at /content/drive/\n",
      "/content/drive/My Drive/data/Localization/TrainTest/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print('Running on CoLab')\n",
    "    PATH='/content/drive/'\n",
    "    drive.mount(PATH)\n",
    "    DATA_DIR=PATH+'My Drive/data/Localization/TrainTest/'  # must end in \"/\"\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    DATA_DIR = 'D:/Adjeroh/Localization/TrainTest/'   # Windows\n",
    "    DATA_DIR = '/Users/jasonmiller/WVU/Localization/TrainTest/'    # Mac\n",
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LnkpVKdMFLA-"
   },
   "outputs": [],
   "source": [
    "GENES_FILE =    'CNRCI_coding_train_genes.csv'\n",
    "RCI_FILE =      'CNRCI_coding_train_RCI.gc42.csv'\n",
    "SEQUENCE_FILE = 'CNRCI_coding_train_transcripts.gc42.csv'\n",
    "COUNTS_FILE=    'CNRCI_coding_train_counts.K4.gc42.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3p4QzQJFLA_",
    "outputId": "511f5361-3c6e-46eb-d8dc-c49ba7184966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell line for today: 1 = H1.hESC\n"
     ]
    }
   ],
   "source": [
    "def get_ordered_list():\n",
    "    ordered_list = \\\n",
    "    ['A549',\\\n",
    "      'H1.hESC',\\\n",
    "      'HeLa.S3',\\\n",
    "      'HepG2',\\\n",
    "      'HT1080',\\\n",
    "      'HUVEC',\\\n",
    "      'MCF.7',\\\n",
    "      'NCI.H460',\\\n",
    "      'NHEK',\\\n",
    "      'SK.MEL.5',\\\n",
    "      'SK.N.DZ',\\\n",
    "      'SK.N.SH',\\\n",
    "      'GM12878',\\\n",
    "      'K562',\\\n",
    "      'IMR.90']\n",
    "    return ordered_list\n",
    "CELL_LINE_NUMBER=1\n",
    "all_cell_lines = get_ordered_list()\n",
    "cell_line_name = all_cell_lines[CELL_LINE_NUMBER]\n",
    "print('Cell line for today:',CELL_LINE_NUMBER,'=',cell_line_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtqdpJOxFLBA"
   },
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p35ehKV3Kq0z",
    "outputId": "dbda1036-e0d6-40f3-dc6f-1742d85a6b22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n",
      "[20, 16]\n",
      "[63, 57]\n",
      "[64, 0]\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        self.cache=dict() \n",
    "        self.vals = {'A':0, 'C':1, 'G':2, 'T':3}\n",
    "        \n",
    "    def load_gene_rci_values(self,filepath,cell_line):\n",
    "        '''\n",
    "        Load from RCI csv file.\n",
    "        Return dict with keys=gene:str and values=RCI:float.\n",
    "        '''\n",
    "        gene_to_rci = {}\n",
    "        with open (filepath,'r') as handle:\n",
    "            header = None\n",
    "            for row in handle:\n",
    "                if header is None:\n",
    "                    header = row # skip file's header line\n",
    "                else:\n",
    "                    line = row.strip()\n",
    "                    fields = line.split(',')\n",
    "                    gene_id = fields[0]\n",
    "                    rci_val = fields[cell_line+1]\n",
    "                    if rci_val != \"nan\":\n",
    "                        # Convert string nan to float(nan)\n",
    "                        gene_to_rci[gene_id] = float(rci_val)\n",
    "        return gene_to_rci\n",
    "    \n",
    "    def seq_to_kmer_values(self,rna,K):\n",
    "        # The cache may represent more than one K. Probably not a problem.\n",
    "        N_indicator = 0 # indicator value\n",
    "        vec=[] # seq converted to list of K-mers \n",
    "        length = len(rna)\n",
    "        for i in range(length-K+1):\n",
    "            kmer = rna[i:i+K]\n",
    "            if 'N' in kmer:\n",
    "                value = N_indicator\n",
    "            elif kmer in self.cache.keys():\n",
    "                value = self.cache[kmer]\n",
    "            else:\n",
    "                value = 0\n",
    "                for j in range(K):\n",
    "                    value *= 4   \n",
    "                    nextnuc = kmer[j] \n",
    "                    nucval = self.vals[nextnuc]\n",
    "                    value += nucval\n",
    "                value += 1   # NNN => 0, AAA => 1\n",
    "                self.cache[kmer] = value\n",
    "            vec.append(value)\n",
    "        return vec\n",
    "\n",
    "    def rci_to_label(self,rci):\n",
    "        CYTO_LABEL = 1\n",
    "        NUCLEAR_LABEL = 0\n",
    "        # cnrci = log (cyto-to-nuclear ratio)\n",
    "        # rci > 0 implies cytoplasmic\n",
    "        if rci > RCI_THRESHOLD:\n",
    "            return CYTO_LABEL\n",
    "        return NUCLEAR_LABEL\n",
    "\n",
    "    def load_sequence(self,filepath):\n",
    "        labels=[]\n",
    "        allids=[]\n",
    "        allseq=[]\n",
    "        NREPEAT = str('N'*MAXLEN)\n",
    "        with open (filepath,'r') as handle:\n",
    "            header = None\n",
    "            for row in handle:\n",
    "                if header is None:\n",
    "                    header = row\n",
    "                else:\n",
    "                    line    = row.strip()\n",
    "                    fields  = line.split(',')\n",
    "                    tran_id = fields[0]  # with version number\n",
    "                    gene_id = fields[1]        # without version number\n",
    "                    seq_len = int(fields[3])\n",
    "                    seq_txt = fields[4]\n",
    "                    if seq_len>=MINLEN and seq_len<=MAXLEN and gene_id in gene_to_rci.keys():\n",
    "                        allids.append( (gene_id,tran_id) )\n",
    "                        rci_val = gene_to_rci[gene_id]\n",
    "                        rci_label = self.rci_to_label(rci_val)\n",
    "                        labels.append(rci_label)\n",
    "                        if seq_len<MAXLEN:\n",
    "                            seq_txt = seq_txt + NREPEAT\n",
    "                            seq_txt = seq_txt[:MAXLEN]\n",
    "                        hot_vec = self.seq_to_kmer_values(seq_txt,K)\n",
    "                        allseq.append(hot_vec)\n",
    "        return labels,allids,allseq\n",
    "    \n",
    "loader = DataLoader()\n",
    "# test it\n",
    "print(loader.seq_to_kmer_values('AAAA',3))\n",
    "print(loader.seq_to_kmer_values('CATT',3))\n",
    "print(loader.seq_to_kmer_values('TTGA',3))\n",
    "print(loader.seq_to_kmer_values('TTTN',3))\n",
    "# test it\n",
    "print(loader.rci_to_label(-0.9))\n",
    "print(loader.rci_to_label(1.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYoYDc93FLBB",
    "outputId": "877bcbd4-d91c-40a5-da12-3ca419fcba35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 15:01:25.487619\n",
      "Load RCI values...\n",
      "Num RCI: 13000\n",
      "Example RCI: [('ENSG00000000003', 1.85734), ('ENSG00000000005', 5.88264), ('ENSG00000000419', 2.58954)]\n",
      "Load genes...\n",
      "2022-12-21 15:01:28.100591\n",
      "Load sequence...\n",
      "2022-12-21 15:01:55.192749\n",
      "Num IDs: 18084\n",
      "Example IDs: [('ENSG00000162571', 'ENST00000379289.6'), ('ENSG00000131584', 'ENST00000354700.10'), ('ENSG00000162576', 'ENST00000477278.3'), ('ENSG00000197530', 'ENST00000505820.7')]\n",
      "Count 8030 ones out of 18084 labels.\n",
      "Example labels: [0, 0, 0, 0]\n",
      "Num counts: 18084\n",
      "Example sequence: [60, 238, 182, 213, 81, 67, 11, 43, 171, 169, 164, 142, 54, 216, 95, 122, 229, 147, 75, 44, 175, 186, 229, 148, 78, 54, 216, 94, 118, 215, 89, 100, 142, 56, 223, 122, 231, 153, 98, 136, 31, 122, 230, 150, 87, 91, 107, 170, 168, 159, 122, 230, 151, 89, 97, 132, 13, 52, 206, 54, 216, 94, 118, 214, 86, 87, 91, 108, 175, 185, 225, 130, 6, 23, 91, 107, 171, 171, 170, 167, 155, 106, 168, 159, 123, 234, 165, 147, 73, 34, 133, 17, 67, 9, 35, 139, 41, 162, 133, 19, 76, 46, 183, 218, 102, 150, 88, 95, 122, 230, 152, 95, 122, 230, 151, 90, 102, 150, 87, 91, 105, 161, 130, 6, 24, 95, 121, 225, 131, 9, 33, 131, 11, 41, 163, 138, 39, 153, 97, 130, 8, 30, 118, 214, 85, 83, 74, 40, 160, 126, 248, 222, 120, 223, 122, 230, 149, 83, 74, 39, 153, 100, 143, 59, 236, 175, 185, 226, 133, 19, 74, 39, 153, 98, 135, 27, 107, 169, 163, 140, 47, 187, 234, 166, 150, 86, 85, 82, 70, 24, 95, 124, 239, 187, 235, 170, 167, 155, 106, 167, 155, 106, 166, 149, 83, 75, 42, 168, 160, 127, 249, 225, 131, 10, 37, 147, 75, 41, 163, 139, 41, 164, 143, 59, 236, 174, 182, 215, 90, 101, 146, 69, 20, 78, 54, 215, 92, 109, 180, 206, 53, 212, 79, 57, 225, 131, 9, 35, 137, 33, 131, 9, 35, 140, 46, 182, 213, 82, 69, 18, 70, 22, 85, 82, 72, 31, 123, 235, 169, 162, 135, 28, 111, 185, 225, 130, 5, 20, 78, 56, 222, 120, 224, 128, 254, 246, 215, 89, 99, 137, 35, 139, 42, 167, 156, 110, 182, 216, 95, 122, 229, 147, 74, 38, 149, 83, 75, 41, 162, 135, 27, 106, 165, 145, 66, 6, 24, 96, 126, 246, 214, 85, 82, 70, 22, 88, 94, 117, 212, 77, 52, 206, 54, 213, 83, 74, 39, 156, 110, 182, 213, 82, 70, 23, 90, 101, 147, 74, 38, 151, 90, 102, 149, 82, 70, 24, 94, 119, 220, 112, 189, 244, 207, 58, 230, 150, 87, 89, 99, 138, 37, 148, 78, 53, 211, 73, 35, 138, 38, 151, 90, 104, 159, 124, 239, 185, 225, 132, 16, 62, 246, 213, 83, 73, 35, 137, 35, 139, 42, 165, 147, 74, 38, 152, 95, 123, 233, 163, 137, 36, 144, 63, 251, 234, 166, 152, 95, 122, 231, 153, 98, 134, 22, 87, 90, 102, 151, 91, 108, 175, 185, 226, 134, 24, 95, 124, 240, 191, 251, 235, 170, 165, 145, 67, 9, 35, 139, 42, 168, 159, 123, 235, 170, 166, 151, 90, 104, 158, 118, 214, 86, 86, 86, 87, 92, 109, 180, 206, 53, 211, 74, 37, 147, 74, 39, 153, 98, 136, 31, 122, 232, 160, 128, 256, 254, 245, 211, 73, 35, 137, 33, 131, 9, 35, 139, 43, 170, 165, 146, 71, 25, 97, 131, 10, 39, 153, 97, 132, 14, 55, 218, 102, 152, 94, 117, 209, 67, 10, 37, 147, 73, 35, 139, 42, 167, 154, 104, 159, 122, 232, 159, 122, 232, 159, 122, 230, 151, 90, 103, 155, 107, 169, 163, 138, 40, 159, 123, 235, 171, 170, 166, 150, 85, 83, 74, 37, 148, 79, 59, 234, 166, 150, 86, 87, 91, 105, 163, 139, 41, 162, 134, 21, 84, 80, 61, 242, 198, 23, 90, 102, 151, 91, 106, 168, 160, 127, 252, 239, 188, 238, 181, 211, 74, 37, 146, 72, 31, 121, 227, 138, 39, 153, 99, 139, 42, 166, 149, 83, 74, 37, 146, 70, 24, 96, 128, 255, 249, 227, 139, 41, 162, 134, 22, 88, 94, 117, 211, 74, 39, 154, 102, 152, 94, 120, 221, 114, 198, 21, 82, 70, 24, 95, 123, 235, 170, 166, 152, 94, 118, 214, 86, 85, 83, 74, 38, 149, 82, 71, 27, 108, 175, 185, 227, 139, 41, 162, 134, 22, 85, 82, 70, 22, 88, 95, 123, 234, 165, 148, 79, 57, 228, 142, 54, 214, 86, 86, 88, 94, 117, 212, 78, 53, 210, 70, 24, 94, 118, 214, 86, 85, 83, 74, 38, 149, 82, 71, 25, 100, 142, 56, 222, 118, 216, 95, 121, 227, 139, 43, 172, 174, 182, 215, 91, 106, 165, 147, 75, 41, 163, 139, 44, 175, 187, 234, 167, 155, 106, 168, 159, 122, 229, 147, 74, 40, 158, 120, 223, 121, 227, 139, 43, 171, 170, 166, 150, 86, 85, 83, 76, 47, 187, 234, 166, 152, 95, 123, 233, 161, 131, 10, 38, 150, 85, 82, 70, 24, 95, 122, 230, 150, 88, 94, 118, 216, 94, 118, 213, 82, 71, 27, 106, 165, 147, 75, 44, 174, 181, 211, 74, 39, 156, 110, 183, 219, 105, 161, 131, 10, 37, 147, 75, 43, 170, 166, 152, 95, 123, 234, 168, 158, 117, 211, 74, 37, 146, 70, 23, 91, 107, 169, 163, 139, 43, 170, 167, 154, 102, 151, 90, 102, 150, 86, 85, 83, 74, 40, 159, 122, 230, 151, 90, 102, 150, 86, 87, 92, 110, 182, 216, 96, 126, 248, 222, 119, 219, 105, 163, 137, 35, 139, 43, 169, 163, 138, 40, 159, 122, 230, 152, 94, 117, 211, 74, 38, 151, 90, 102, 150, 86, 86, 86, 88, 96, 127, 250, 232, 159, 124, 238, 183, 218, 102, 151, 90, 101, 147, 73, 33, 132, 15, 58, 230, 150, 86, 88, 94, 117, 210, 71, 28, 110, 183, 218, 102, 150, 88, 95, 123, 235, 170, 166, 150, 86, 86, 85, 84, 78, 56, 222, 117, 211, 75, 42, 166, 150, 86, 86, 86, 88, 96, 126, 246, 216, 95, 123, 235, 171, 171, 172, 175, 186, 230, 150, 88, 94, 119, 219, 106, 168, 158, 120, 223, 124, 239, 186, 230, 149, 83, 73, 34, 134, 22, 86, 85, 83, 75, 42, 168, 157, 114, 199, 27, 106, 168, 160, 126, 246, 216, 95, 122, 230, 150, 86, 86, 86, 87, 90, 103, 154, 101, 147, 75, 42, 167, 155, 105, 163, 137, 36, 143, 60, 240, 190, 247, 218, 102, 152, 95, 123, 234, 165, 147, 74, 37, 147, 75, 41, 163, 138, 40, 158, 118, 216, 95, 122, 231, 155, 105, 161, 131, 10, 37, 147, 73, 33, 130, 6, 24, 95, 123, 234, 166, 150, 87, 91, 106, 168, 159, 123, 233, 163, 138, 40, 159, 122, 230, 150, 87, 90, 102, 151, 89, 98, 134, 24, 94, 118, 216, 95, 122, 231, 155, 106, 165, 147, 73, 33, 131, 11, 41, 163, 138, 40, 159, 123, 233, 163, 137, 35, 138, 39, 154, 103, 154, 103, 154, 102, 150, 85, 82, 69, 19, 74, 40, 159, 122, 232, 159, 123, 234, 167, 154, 102, 150, 87, 89, 99, 137, 34, 134, 23, 90, 102, 150, 88, 95, 122, 231, 154, 102, 150, 86, 85, 81, 66, 7, 25, 98, 135, 27, 106, 167, 154, 102, 151, 89, 99, 139, 41, 163, 138, 40, 159, 122, 229, 147, 74, 39, 155, 106, 167, 154, 103, 155, 107, 171, 170, 166, 150, 88, 95, 122, 232, 159, 123, 236, 175, 186, 232, 159, 121, 225, 130, 6, 21, 82, 71, 27, 106, 167, 154, 103, 155, 106, 167, 154, 102, 149, 82, 72, 31, 122, 232, 159, 123, 234, 166, 150, 88, 95, 122, 230, 150, 86, 86, 86, 86, 85, 83, 75, 43, 171, 170, 166, 150, 86, 86, 87, 91, 107, 170, 168, 158, 118, 215, 91, 105, 162, 134, 22, 86, 86, 86, 85, 82, 70, 22, 86, 87, 92, 110, 182, 214, 87, 91, 107, 169, 162, 136, 30, 120, 223, 122, 230, 150, 87, 91, 106, 167, 153, 99, 138, 38, 150, 86, 86, 86, 87, 91, 105, 161, 131, 11, 43, 171, 171, 171, 172, 174, 182, 214, 87, 91, 106, 166, 150, 88, 95, 122, 230, 152, 94, 117, 211, 74, 39, 154, 103, 155, 106, 166, 150, 85, 83, 74, 39, 153, 99, 140, 46, 182, 213, 81, 67, 11, 41, 163, 137, 36, 143, 57, 226, 135, 27, 107, 171, 171, 170, 168, 157, 115, 203, 42, 168, 158, 120, 223, 123, 235, 170, 165, 146, 69, 17, 67, 9, 36, 143, 59, 234, 168, 158, 119, 219, 105, 161, 131, 9, 34, 135, 25, 99, 138, 38, 150, 86, 86, 86, 85, 81, 65, 3, 9, 34, 136, 30, 119, 219, 105, 162, 135, 27, 105, 163, 137, 35, 139, 41, 162, 134, 22, 86, 87, 89, 99, 137, 34, 135, 27, 106, 165, 147, 74, 40, 159, 124, 240, 191, 251, 235, 172, 175, 186, 229, 147, 75, 43, 171, 171, 170, 166, 150, 85, 82, 72, 30, 118, 215, 91, 107, 170, 166, 149, 81, 67, 10, 40, 158, 118, 213, 83, 74, 40, 159, 123, 233, 163, 139, 43, 171, 170, 166, 151, 91, 106, 167, 154, 102, 151, 89, 99, 139, 43, 171, 169, 161, 131, 11, 43, 171, 170, 168, 160, 128, 256, 254, 246, 214, 85, 83, 75, 43, 172, 174, 182, 213, 82, 69, 18, 72, 31, 122, 230, 150, 86, 88, 95, 123, 235, 170, 168, 160, 126, 246, 214, 88, 96, 125, 244, 207, 58, 230, 151, 92, 110, 181, 211, 74, 38, 150, 86, 88, 93, 114, 200, 32, 126, 246, 213, 82, 69, 18, 69, 19, 75, 42, 167, 154, 103, 155, 108, 173, 179, 203, 43, 171, 171, 169, 162, 136, 30, 120, 222, 118, 213, 84, 79, 59, 233, 164, 143, 59, 235, 171, 169, 163, 139, 41, 163, 139, 42, 166, 150, 86, 85, 83, 74, 38, 150, 86, 88, 95, 121, 227, 139, 41, 162, 135, 28, 110, 181, 210, 70, 21, 81, 67, 12, 47, 187, 233, 162, 134, 23, 92, 111, 187, 233, 164, 143, 57, 226, 135, 28, 110, 184, 223, 122, 229, 147, 74, 40, 160, 126, 247, 220, 111, 187, 235, 171, 171, 171, 170, 166, 152, 95, 124, 238, 184, 223, 123, 234, 168, 159, 124, 239, 187, 233, 163, 137, 35, 140, 45, 178, 197, 18, 72, 30, 119, 219, 107, 172, 174, 184, 224, 126, 245, 211, 75, 43, 169, 163, 138, 37, 147, 75, 43, 171, 169, 164, 142, 55, 217, 98, 135, 27, 107, 171, 169, 163, 137, 34, 134, 22, 88, 95, 122, 230, 149, 82, 72, 31, 122, 232, 159, 121, 226, 135, 27, 105, 163, 139, 41, 163, 138, 37, 146, 70, 24, 95, 122, 232, 159, 121, 226, 134, 21, 81, 66, 5, 20, 79, 59, 235, 171, 170, 168, 159, 121, 225, 131, 10, 40, 159, 123, 235, 171, 170, 166, 150, 87, 90, 102, 150, 88, 94, 117, 209, 67, 9, 36, 142, 54, 215, 91, 107, 170, 166, 150, 85, 83, 75, 44, 175, 187, 234, 166, 149, 83, 75, 42, 167, 154, 102, 152, 95, 123, 235, 170, 166, 151, 89, 99, 140, 48, 192, 256, 254, 248, 221, 114, 199, 28, 111, 187, 234, 166, 149, 83, 74, 40, 160, 126, 246, 214, 86, 87, 92, 111, 187, 234, 168, 158, 120, 223, 122, 230, 149, 82, 72, 31, 122, 229, 147, 74, 38, 149, 82, 70, 21, 81, 66, 6, 22, 88, 95, 122, 231, 155, 107, 170, 166, 150, 86, 87, 91, 105, 163, 138, 39, 153, 99, 137, 33, 130, 8, 30, 119, 219, 106, 165, 146, 69, 19, 75, 41, 163, 137, 35, 138, 37, 147, 74, 38, 150, 88, 96, 127, 252, 238, 182, 214, 86, 86, 85, 82, 71, 25, 98, 135, 27, 106, 166, 149, 82, 71, 28, 110, 182, 214, 86, 86, 88, 93, 116, 207, 59, 233, 163, 139, 43, 171, 171, 170, 166, 149, 82, 71, 26, 102, 150, 88, 96, 127, 250, 230, 151, 91, 108, 174, 181, 209, 65, 2, 8, 32, 126, 245, 210, 70, 22, 85, 81, 67, 10, 37, 147, 75, 41, 163, 137, 33, 132, 15, 59, 235, 169, 162, 134, 24, 96, 127, 251, 234, 168, 158, 120, 221, 114, 200, 32, 126, 246, 213, 83, 75, 43, 171, 170, 166, 150, 86, 86, 87, 89, 98, 134, 22, 88, 96, 126, 246, 214, 85, 83, 74, 38, 152, 94, 120, 223, 124, 239, 188, 240, 191, 249, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "print(\"Load RCI values...\")\n",
    "loader = DataLoader()\n",
    "filepath = DATA_DIR+RCI_FILE\n",
    "gene_to_rci = loader.load_gene_rci_values(filepath,CELL_LINE_NUMBER)\n",
    "print('Num RCI:', len(gene_to_rci.keys()))\n",
    "print('Example RCI:', list(gene_to_rci.items())[:3])\n",
    "print(\"Load genes...\")\n",
    "all_genes = list(gene_to_rci.keys())\n",
    "print(datetime.now())\n",
    "print('Load sequence...')\n",
    "filepath = DATA_DIR+SEQUENCE_FILE\n",
    "labels,allids,allseq = loader.load_sequence(filepath)\n",
    "print(datetime.now())\n",
    "print('Num IDs:',len(allids))\n",
    "print('Example IDs:',[allids[x] for x in [10, 20, 30, 40]] )\n",
    "print('Count',np.count_nonzero(labels),'ones out of',len(labels),'labels.')\n",
    "print('Example labels:',[labels[x] for x in [10, 20, 30, 40]] )\n",
    "print('Num counts:',len(allseq))\n",
    "print('Example sequence:',allseq[3])\n",
    "loader = None  # drop K-mer cache to save RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDZ6siB_Kq04"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AwMbRjm0FLBF"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ALPHABET=4**K+1  # NUMBER OF DISTINCT KMERS POSSIBLE, add one if N gets mask value\n",
    "    ADJUST_LENGTH = MAXLEN-K+1  # fixed length sequences\n",
    "    rnn = Sequential()\n",
    "    embed_layer = Embedding(ALPHABET,EMBED_DIMEN,input_length=ADJUST_LENGTH,mask_zero=True)   \n",
    "    rnn1_layer = Bidirectional( LSTM(RCELLS, return_sequences=True) )\n",
    "    cnn1_layer = Conv1D(CFILTERS, FILTERSIZE)\n",
    "    den1_layer = Dense(RCELLS)\n",
    "    drop_layer = Dropout(0.5)\n",
    "    output_layer = Dense(1,activation='sigmoid',dtype=dt)\n",
    "\n",
    "    rnn.add(embed_layer)\n",
    "    rnn.add(rnn1_layer)\n",
    "    rnn.add(cnn1_layer)\n",
    "    rnn.add(den1_layer)\n",
    "    rnn.add(drop_layer)\n",
    "    rnn.add(output_layer)\n",
    "\n",
    "    bc=BinaryCrossentropy(from_logits=False)\n",
    "    print(\"COMPILE\")\n",
    "    rnn.compile(loss=bc, optimizer=\"Adam\",metrics=[\"accuracy\"])\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "clj-wufgFLBF",
    "outputId": "f07b5e1a-2651-46c2-8ec5-743c5027c84a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 15:01:55.239163\n",
      "COMPILE\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 3997, 4)           1028      \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 3997, 32)         2688      \n",
      " l)                                                              \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 3990, 62)          15934     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3990, 16)          1008      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 3990, 16)          0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3990, 1)           17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,675\n",
      "Trainable params: 20,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "model=build_model()\n",
    "print(model.summary())  # Print this only once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgrC1alOKq07"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "W9xiFzNbFLBE"
   },
   "outputs": [],
   "source": [
    "class CrossValidator():\n",
    "    def __init__(self,epochs,folds,quick_test=False,pred_threshold=0.5):\n",
    "        self.epochs = epochs\n",
    "        self.folds = folds\n",
    "        self.quick_test = quick_test\n",
    "        self.threshold = pred_threshold # e.g. score>0.5 => class 1\n",
    "    \n",
    "    def get_gene_subset(self,all_genes,sub_index):\n",
    "        sub_genes = set()\n",
    "        for index in sub_index:\n",
    "            one_gene = all_genes[index]\n",
    "            sub_genes.add(one_gene)\n",
    "        return sub_genes\n",
    "    \n",
    "    def get_X_y(self,gene_set,allids,allX,allY):\n",
    "        cnt = len(allids)\n",
    "        subsetX=[]\n",
    "        subsetY=[]\n",
    "        if cnt != len(allX) or cnt!= len(allY):\n",
    "            raise Exception('Lengths differ')\n",
    "        for i in range(cnt):\n",
    "            gene_id,tran_id = allids[i]\n",
    "            if gene_id in gene_set:\n",
    "                oneX = allX[i]\n",
    "                oneY = allY[i]\n",
    "                subsetX.append(oneX)\n",
    "                subsetY.append(oneY)\n",
    "        subsetX = np.array(subsetX)\n",
    "        subsetY = np.array(subsetY).reshape((-1,1))\n",
    "        return subsetX,subsetY\n",
    "    \n",
    "    def do_cross_validation(self):\n",
    "        cv_accuracy=[]\n",
    "        cv_precision=[]\n",
    "        cv_recall=[]\n",
    "        cv_f1=[]\n",
    "        fold=0\n",
    "        print(datetime.now())\n",
    "        print('splitting')\n",
    "        # KFold shuffles once before making the partitions\n",
    "        splitter = KFold(n_splits=self.folds,shuffle=True,random_state=42)\n",
    "        for train_index,valid_index in splitter.split(all_genes):\n",
    "            fold += 1\n",
    "            print('Fold',fold)\n",
    "            train_genes = self.get_gene_subset(all_genes,train_index)\n",
    "            valid_genes = self.get_gene_subset(all_genes,valid_index)\n",
    "            X_train,y_train = self.get_X_y(train_genes,allids,allseq,labels)\n",
    "            X_valid,y_valid = self.get_X_y(valid_genes,allids,allseq,labels)\n",
    "\n",
    "            print('Training example')\n",
    "            print(X_train[0])\n",
    "\n",
    "            print('Train sizes',X_train.shape,y_train.shape)\n",
    "            print('Valid sizes',X_valid.shape,y_valid.shape)\n",
    "            print('Train set ones/size',\n",
    "                  np.count_nonzero(y_train),'/',len(y_train))\n",
    "            print('Valid set ones/size',\n",
    "                  np.count_nonzero(y_valid),'/',len(y_valid))\n",
    "\n",
    "            print(\"BUILD MODEL\")\n",
    "            model=build_model()\n",
    "\n",
    "            print(\"FIT\")\n",
    "            print(datetime.now())\n",
    "            history=model.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
    "                    epochs=self.epochs, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
    "                    validation_data=(X_valid,y_valid) )\n",
    "\n",
    "            pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "            plt.grid(True)\n",
    "            plt.gca().set_ylim(0,1)\n",
    "            plt.show()\n",
    "\n",
    "            print(\"Compute valiation accuracy\")\n",
    "            print(datetime.now())\n",
    "            yhat_pred=model.predict(X_valid, verbose=0) \n",
    "            print('Range of scores:',np.min(yhat_pred),'to',np.max(yhat_pred))\n",
    "            yhat_classes=np.where(yhat_pred > self.threshold, 1, 0)\n",
    "            print('Predicted zeros and ones',np.count_nonzero(yhat_classes==0),np.count_nonzero(yhat_classes==1))\n",
    "            # accuracy: (tp + tn) / (p + n)\n",
    "            accuracy = accuracy_score(y_valid, yhat_classes)*100.\n",
    "            # precision tp / (tp + fp)\n",
    "            precision = precision_score(y_valid, yhat_classes)*100.\n",
    "            # recall: tp / (tp + fn)\n",
    "            recall = recall_score(y_valid, yhat_classes)*100.\n",
    "            # f1: 2 tp / (2 tp + fp + fn)\n",
    "            f1 = f1_score(y_valid, yhat_classes)*100.\n",
    "            print('Accuracy: %.2f%% Precision: %.2f%% Recall: %.2f%% F1: %.2f%%' % (accuracy,precision,recall,f1)) \n",
    "            print(datetime.now())\n",
    "            cv_accuracy.append(accuracy)\n",
    "            cv_precision.append(precision)\n",
    "            cv_recall.append(recall)\n",
    "            cv_f1.append(f1)\n",
    "            if self.quick_test:   \n",
    "                print('Break -- this was for code testing only')\n",
    "                break\n",
    "        print()\n",
    "        return cv_accuracy, cv_precision, cv_recall, cv_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XC9m0W-pFLBH",
    "outputId": "f9b3825e-59e7-40a3-d1f2-4b3fe76734ba",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-21 15:02:00.193602\n",
      "2022-12-21 15:02:00.193918\n",
      "splitting\n",
      "Fold 1\n",
      "Training example\n",
      "[167 155 106 ...   0   0   0]\n",
      "Train sizes (14388, 3997) (14388, 1)\n",
      "Valid sizes (3696, 3997) (3696, 1)\n",
      "Train set ones/size 6390 / 14388\n",
      "Valid set ones/size 1640 / 3696\n",
      "BUILD MODEL\n",
      "COMPILE\n",
      "FIT\n",
      "2022-12-21 15:02:06.306532\n",
      "Epoch 1/20\n",
      "450/450 [==============================] - 115s 217ms/step - loss: 0.6395 - accuracy: 0.6358 - val_loss: 0.6369 - val_accuracy: 0.6197\n",
      "Epoch 2/20\n",
      "450/450 [==============================] - 95s 211ms/step - loss: 0.6489 - accuracy: 0.6225 - val_loss: 0.6469 - val_accuracy: 0.6336\n",
      "Epoch 3/20\n",
      "450/450 [==============================] - 94s 209ms/step - loss: 0.6365 - accuracy: 0.6387 - val_loss: 0.6321 - val_accuracy: 0.6362\n",
      "Epoch 4/20\n",
      "450/450 [==============================] - 96s 213ms/step - loss: 0.6477 - accuracy: 0.6195 - val_loss: 0.6575 - val_accuracy: 0.6028\n",
      "Epoch 5/20\n",
      "450/450 [==============================] - 95s 212ms/step - loss: 0.6415 - accuracy: 0.6301 - val_loss: 0.6292 - val_accuracy: 0.6404\n",
      "Epoch 6/20\n",
      "450/450 [==============================] - 96s 213ms/step - loss: 0.6306 - accuracy: 0.6444 - val_loss: 0.6345 - val_accuracy: 0.6393\n",
      "Epoch 7/20\n",
      "450/450 [==============================] - 96s 214ms/step - loss: 0.6489 - accuracy: 0.6103 - val_loss: 0.6570 - val_accuracy: 0.6165\n",
      "Epoch 8/20\n",
      "450/450 [==============================] - 97s 217ms/step - loss: 0.6367 - accuracy: 0.6398 - val_loss: 0.6488 - val_accuracy: 0.6136\n",
      "Epoch 9/20\n",
      "450/450 [==============================] - 95s 212ms/step - loss: 0.6537 - accuracy: 0.6109 - val_loss: 0.6395 - val_accuracy: 0.6260\n",
      "Epoch 10/20\n",
      "450/450 [==============================] - 96s 214ms/step - loss: 0.6402 - accuracy: 0.6334 - val_loss: 0.6653 - val_accuracy: 0.5564\n",
      "Epoch 11/20\n",
      "450/450 [==============================] - 97s 216ms/step - loss: 0.6496 - accuracy: 0.6223 - val_loss: 0.6370 - val_accuracy: 0.6305\n",
      "Epoch 12/20\n",
      "450/450 [==============================] - 96s 214ms/step - loss: 0.6489 - accuracy: 0.6213 - val_loss: 0.6454 - val_accuracy: 0.6187\n",
      "Epoch 13/20\n",
      "450/450 [==============================] - 99s 220ms/step - loss: 0.6593 - accuracy: 0.6035 - val_loss: 0.6562 - val_accuracy: 0.6042\n",
      "Epoch 14/20\n",
      "450/450 [==============================] - 103s 228ms/step - loss: 0.6473 - accuracy: 0.6228 - val_loss: 0.6426 - val_accuracy: 0.6235\n",
      "Epoch 15/20\n",
      "450/450 [==============================] - 105s 234ms/step - loss: 0.6380 - accuracy: 0.6378 - val_loss: 0.6357 - val_accuracy: 0.6336\n",
      "Epoch 16/20\n",
      "450/450 [==============================] - 100s 222ms/step - loss: 0.6344 - accuracy: 0.6397 - val_loss: 0.6443 - val_accuracy: 0.6380\n",
      "Epoch 17/20\n",
      "450/450 [==============================] - 100s 222ms/step - loss: 0.6402 - accuracy: 0.6311 - val_loss: 0.6402 - val_accuracy: 0.6306\n",
      "Epoch 18/20\n",
      "450/450 [==============================] - 100s 223ms/step - loss: 0.6341 - accuracy: 0.6420 - val_loss: 0.6334 - val_accuracy: 0.6309\n",
      "Epoch 19/20\n",
      "450/450 [==============================] - 99s 219ms/step - loss: 0.6303 - accuracy: 0.6432 - val_loss: 0.6319 - val_accuracy: 0.6328\n",
      "Epoch 20/20\n",
      "450/450 [==============================] - 99s 219ms/step - loss: 0.6366 - accuracy: 0.6402 - val_loss: 0.6412 - val_accuracy: 0.6343\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute valiation accuracy\n",
      "2022-12-21 15:35:00.627359\n",
      "Range of scores: 0.12841798 to 0.8139247\n",
      "Predicted zeros and ones 10484616 4262424\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8f9f88d0500f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcvdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossValidator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFOLDS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBREAK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcv_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvdo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cross validation %d folds %d epochs\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mFOLDS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" accuracy mean %.2f%% +/- %.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-8ab1504c62e0>\u001b[0m in \u001b[0;36mdo_cross_validation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted zeros and ones'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat_classes\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat_classes\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m# accuracy: (tp + tn) / (p + n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;31m# precision tp / (tp + fp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     94\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[1;32m     95\u001b[0m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and unknown targets"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "cvdo = CrossValidator(EPOCHS,FOLDS,BREAK)\n",
    "cv_accuracy, cv_precision, cv_recall, cv_f1 = cvdo.do_cross_validation()   \n",
    "print(\"Cross validation %d folds %d epochs\" % (FOLDS,EPOCHS)) \n",
    "print(\" accuracy mean %.2f%% +/- %.2f\" % (np.mean(cv_accuracy), np.std(cv_accuracy)))\n",
    "print(\" precision mean %.2f%% +/- %.2f\" % (np.mean(cv_precision), np.std(cv_precision)))\n",
    "print(\" recall mean %.2f%% +/- %.2f\" % (np.mean(cv_recall), np.std(cv_recall)))\n",
    "print(\" F1 mean %.2f%% +/- %.2f\" % (np.mean(cv_f1), np.std(cv_f1)))\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thQspN3Nga5S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
