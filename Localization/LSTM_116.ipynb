{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PG-tGRnlFLA3"
   },
   "source": [
    "# LSTM\n",
    "Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RmwUsVLFLA6",
    "outputId": "6b2cbe80-916b-4d5d-c1c8-498343c1c6d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-26 16:49:03.295794\n",
      "Python 3.10.0\n",
      "sklearn 1.1.2\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())\n",
    "from platform import python_version\n",
    "print('Python',python_version())\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "import sklearn   # pip install --upgrade scikit-learn\n",
    "print('sklearn',sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUtGXPrcFLA8"
   },
   "source": [
    "We prevously used sklearn.model_selection.ShuffleSplit   \n",
    "Now we avoid it due to this note in the \n",
    "[documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html):\n",
    "Note: contrary to other cross-validation strategies, random splits do not guarantee that all folds will be different, although this is still very likely for sizeable datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PRX-UEr8FLA8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "dt='float32'\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "tf.random.set_seed(42) \n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import KFold\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Masking\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "K=3\n",
    "EPOCHS=10\n",
    "FOLDS=5  \n",
    "BREAK = False   # break after first fold\n",
    "MAXLEN=4000   # this is problematic as some genes will be excluded entirely or partially\n",
    "MINLEN=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OlzN9OdsFWEU",
    "outputId": "daac96a8-9914-4038-f5c8-4dd9eed3004b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jasonmiller/WVU/Localization/TrainTest/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print('Running on CoLab')\n",
    "    PATH='/content/drive/'\n",
    "    drive.mount(PATH)\n",
    "    DATA_DIR=PATH+'My Drive/data/Localization/TrainTest/'  # must end in \"/\"\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    DATA_DIR = 'D:/Adjeroh/Localization/TrainTest/'   # Windows\n",
    "    DATA_DIR = '/Users/jasonmiller/WVU/Localization/TrainTest/'    # Mac\n",
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LnkpVKdMFLA-"
   },
   "outputs": [],
   "source": [
    "GENES_FILE =    'CNRCI_noncoding_train_genes.csv'\n",
    "RCI_FILE =      'CNRCI_noncoding_train_RCI.gc42.csv'\n",
    "SEQUENCE_FILE = 'CNRCI_noncoding_train_transcripts.gc42.csv'\n",
    "COUNTS_FILE=    'CNRCI_noncoding_train_counts.K4.gc42.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3p4QzQJFLA_",
    "outputId": "b69a75e4-cc45-4e9d-c1f9-6ce3b19b8741"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell line for today: 1 = H1.hESC\n"
     ]
    }
   ],
   "source": [
    "def get_ordered_list():\n",
    "    ordered_list = \\\n",
    "    ['A549',\\\n",
    "      'H1.hESC',\\\n",
    "      'HeLa.S3',\\\n",
    "      'HepG2',\\\n",
    "      'HT1080',\\\n",
    "      'HUVEC',\\\n",
    "      'MCF.7',\\\n",
    "      'NCI.H460',\\\n",
    "      'NHEK',\\\n",
    "      'SK.MEL.5',\\\n",
    "      'SK.N.DZ',\\\n",
    "      'SK.N.SH',\\\n",
    "      'GM12878',\\\n",
    "      'K562',\\\n",
    "      'IMR.90']\n",
    "    return ordered_list\n",
    "CELL_LINE_NUMBER=1\n",
    "all_cell_lines = get_ordered_list()\n",
    "cell_line_name = all_cell_lines[CELL_LINE_NUMBER]\n",
    "print('Cell line for today:',CELL_LINE_NUMBER,'=',cell_line_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtqdpJOxFLBA"
   },
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYoYDc93FLBB",
    "outputId": "e50612e6-3a7a-47ad-9555-9d03ac5433a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-26 16:49:16.599508\n",
      "Num RCI: 3327\n",
      "[('ENSG00000099869', 1.0), ('ENSG00000116652', 1.65208), ('ENSG00000117242', -0.793877)]\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "# labels\n",
    "filepath = DATA_DIR+RCI_FILE\n",
    "gene_to_rci = {}\n",
    "with open (filepath,'r') as handle:\n",
    "    header = None\n",
    "    for row in handle:\n",
    "        if header is None:\n",
    "            header = row\n",
    "        else:\n",
    "            line = row.strip()\n",
    "            fields = line.split(',')\n",
    "            gene_id = fields[0]\n",
    "            rci_val = fields[CELL_LINE_NUMBER+1]\n",
    "            # Could implement Gudenas style threshold here\n",
    "            if rci_val != \"nan\":\n",
    "                gene_to_rci[gene_id] = float(rci_val)\n",
    "print('Num RCI:', len(gene_to_rci.keys()))\n",
    "print(list(gene_to_rci.items())[:3])\n",
    "all_genes = list(gene_to_rci.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bNrOKHddFycs",
    "outputId": "9c4cb2cd-9cc7-4aa6-eff8-b233e338ef6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1], [1, 1, 1]]\n",
      "[[2, 1, 4], [1, 4, 4]]\n",
      "[[4, 4, 3], [4, 3, 1]]\n",
      "[[4, 4, 4], [0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "def seq_to_kmers(rna,K):\n",
    "    VALS = {'A':1, 'C':2, 'G':3, 'T':4, 'N':0}\n",
    "    length = len(rna)\n",
    "    vec = []\n",
    "    nmer = [0] * K\n",
    "    for i in range(length-K+1):\n",
    "        kmer=[]\n",
    "        for j in range(K):\n",
    "            nextnuc = rna[i+j]\n",
    "            if nextnuc=='N':\n",
    "                kmer = nmer   # indicate an N\n",
    "                break\n",
    "            nucval = VALS[nextnuc]\n",
    "            kmer.append(nucval)\n",
    "        vec.append(kmer)\n",
    "    return vec\n",
    "print(seq_to_kmers('AAAA',3))\n",
    "print(seq_to_kmers('CATT',3))\n",
    "print(seq_to_kmers('TTGA',3))\n",
    "print(seq_to_kmers('TTTN',3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ooyTeQr-FLBB",
    "outputId": "645cac53-12e2-443b-e49f-3d16c822f2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-26 16:49:16.643755\n",
      "Load sequence\n",
      "2022-11-26 16:49:50.807673\n",
      "Num IDs: 8564\n",
      "Num labels: 8564\n",
      "Num counts: 8564\n"
     ]
    }
   ],
   "source": [
    "# one hot\n",
    "print(datetime.now())\n",
    "print('Load sequence')\n",
    "filepath = DATA_DIR+SEQUENCE_FILE\n",
    "labels=[]\n",
    "allids=[]\n",
    "allseq=[]\n",
    "NREPEAT = str('N'*MAXLEN)\n",
    "with open (filepath,'r') as handle:\n",
    "    header = None\n",
    "    for row in handle:\n",
    "        if header is None:\n",
    "            header = row\n",
    "        else:\n",
    "            line    = row.strip()\n",
    "            fields  = line.split(',')\n",
    "            tran_id = fields[0]  # with version number\n",
    "            gene_id = fields[1]        # without version number\n",
    "            seq_len = int(fields[3])\n",
    "            seq_txt = fields[4]\n",
    "            if seq_len>=MINLEN and seq_len<=MAXLEN and gene_id in gene_to_rci.keys():\n",
    "                rci_val = gene_to_rci[gene_id]\n",
    "                allids.append( (gene_id,tran_id) )\n",
    "                labels.append(rci_val)\n",
    "                if seq_len<MAXLEN:\n",
    "                    seq_txt = seq_txt + NREPEAT\n",
    "                    seq_txt = seq_txt[:MAXLEN]\n",
    "                hot_vec = seq_to_kmers(seq_txt,K)\n",
    "                allseq.append(hot_vec)\n",
    "print(datetime.now())\n",
    "print('Num IDs:',len(allids))\n",
    "#print('Examples:',[allids[x] for x in [10, 20, 30, 40]] )\n",
    "print('Num labels:',len(labels))\n",
    "#print('Examples:',[labels[x] for x in [10, 20, 30, 40]] )\n",
    "print('Num counts:',len(allseq))\n",
    "#print('Example:',allseq[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "W9xiFzNbFLBE"
   },
   "outputs": [],
   "source": [
    "def get_gene_subset(all_genes,sub_index):\n",
    "    sub_genes = set()\n",
    "    for index in sub_index:\n",
    "        one_gene = all_genes[index]\n",
    "        sub_genes.add(one_gene)\n",
    "    return sub_genes\n",
    "def get_X_y(gene_set,allids,allX,allY,threshold):\n",
    "    cnt = len(allids)\n",
    "    subsetX=[]\n",
    "    subsetY=[]\n",
    "    if cnt != len(allX) or cnt!= len(allY):\n",
    "        raise Exception('Lengths differ')\n",
    "    for i in range(cnt):\n",
    "        gene_id,tran_id = allids[i]\n",
    "        if gene_id in gene_set:\n",
    "            oneX = allX[i]\n",
    "            oneY = allY[i]\n",
    "            if oneY < threshold:\n",
    "                Yvalue = int(0)\n",
    "            else:\n",
    "                Yvalue = int(1)\n",
    "            subsetX.append(oneX)\n",
    "            subsetY.append(Yvalue)\n",
    "    subsetX = np.array(subsetX)\n",
    "    subsetY = np.array(subsetY).reshape((-1,1))\n",
    "    return subsetX,subsetY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AwMbRjm0FLBF"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ALPHABET=4**K+1  # NUMBER OF DISTINCT KMERS POSSIBLE, add one if N gets mask value\n",
    "    EMBED_DIMEN = 16 # arbitrary hyperparameter\n",
    "    ADJUST_LENGTH = MAXLEN-K+1  # fixed length sequences\n",
    "    rnn = Sequential()\n",
    "    mask_layer = Masking(mask_value=0, input_shape=(ADJUST_LENGTH, K))\n",
    "    # This layer automatically converts inputs to one-hot using alphabet size.\n",
    "    # embed_layer = Embedding(ALPHABET,EMBED_DIMEN,mask_zero=True);   # crashes with mask_value=True\n",
    "    # rnn1_layer = LSTM(16, return_sequences=True, input_shape=[MAXLEN,DIMEN]) \n",
    "    rnn1_layer = LSTM(32, return_sequences=True)\n",
    "    # rnn1_layer = Bidirectional( LSTM(32, return_sequences=True) )\n",
    "    # rnn1_layer = LSTM(16, return_sequences=True) \n",
    "    # rnn2_layer = LSTM(16, return_sequences=True)\n",
    "    # Dense can handle sequence input. Is it the best thing to do?\n",
    "    # dense1_layer = Dense(16,activation='sigmoid',dtype=dt)\n",
    "    # dropout1_layer = Dropout(0.50)\n",
    "    output_layer = Dense(1,activation='sigmoid',dtype=dt)\n",
    "\n",
    "    rnn.add(mask_layer)\n",
    "    #rnn.add(embed_layer)\n",
    "    rnn.add(rnn1_layer)\n",
    "    #rnn.add(rnn2_layer)\n",
    "    #rnn.add(dense1_layer)\n",
    "    #rnn.add(dropout1_layer)\n",
    "    #rnn.add(dense2_layer)\n",
    "    rnn.add(output_layer)\n",
    "\n",
    "    bc=BinaryCrossentropy(from_logits=False)\n",
    "    print(\"COMPILE\")\n",
    "    rnn.compile(loss=bc, optimizer=\"Adam\",metrics=[\"accuracy\"])\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "clj-wufgFLBF",
    "outputId": "3113304d-657d-42a3-9bdc-966599b59c60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-26 16:53:13.742988\n",
      "COMPILE\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking (Masking)           (None, 3998, 3)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 3998, 32)          4608      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3998, 1)           33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,641\n",
      "Trainable params: 4,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "model=build_model()\n",
    "print(model.summary())  # Print this only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CqO0a9evFLBG",
    "outputId": "b021d5af-9467-4ff7-d156-973b045d2feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-26 16:53:23.604473\n"
     ]
    }
   ],
   "source": [
    "SCALING = False\n",
    "def do_cross_validation(eps):\n",
    "    cv_scores = []\n",
    "    fold=0\n",
    "    print(datetime.now())\n",
    "    print('splitting')\n",
    "    # KFold shuffles once before making the partitions\n",
    "    splitter = KFold(n_splits=FOLDS,shuffle=True,random_state=42)\n",
    "    for train_index,valid_index in splitter.split(all_genes):\n",
    "        fold += 1\n",
    "        print('Fold',fold)\n",
    "        train_genes = get_gene_subset(all_genes,train_index)\n",
    "        valid_genes = get_gene_subset(all_genes,valid_index)\n",
    "        X_train,y_train = get_X_y(train_genes,allids,allseq,labels,0)\n",
    "        X_valid,y_valid = get_X_y(valid_genes,allids,allseq,labels,0)\n",
    "\n",
    "        if SCALING:\n",
    "            print('scaling')\n",
    "            print(datetime.now())\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(X_train)\n",
    "            X_train = scaler.transform(X_train)\n",
    "            X_valid = scaler.transform(X_valid)\n",
    "\n",
    "        print('Train sizes',X_train.shape,y_train.shape)\n",
    "        print('Valid sizes',X_valid.shape,y_valid.shape)\n",
    "        print('Train set ones/size',\n",
    "              np.count_nonzero(y_train),'/',len(y_train))\n",
    "        print('Valid set ones/size',\n",
    "              np.count_nonzero(y_valid),'/',len(y_valid))\n",
    "\n",
    "        print(\"BUILD MODEL\")\n",
    "        model=build_model()\n",
    "\n",
    "        print(\"FIT\")\n",
    "        print(datetime.now())\n",
    "        history=model.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
    "                epochs=EPOCHS, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
    "                validation_data=(X_valid,y_valid) )\n",
    "\n",
    "        pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "        plt.grid(True)\n",
    "        plt.gca().set_ylim(0,1)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Compute valiation accuracy\")\n",
    "        print(datetime.now())\n",
    "        scores = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "        print(datetime.now())\n",
    "        cv_scores.append(scores[1] * 100)\n",
    "        \n",
    "        if BREAK:\n",
    "            break\n",
    "        \n",
    "    print()\n",
    "    return cv_scores\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XC9m0W-pFLBH",
    "outputId": "2f8654f0-2bf6-4672-d2f8-1f3f34669ca8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-26 16:53:24.327170\n",
      "2022-11-26 16:53:24.327689\n",
      "splitting\n",
      "Fold 1\n",
      "Train sizes (6777, 3998, 3) (6777, 1)\n",
      "Valid sizes (1787, 3998, 3) (1787, 1)\n",
      "Train set ones/size 2754 / 6777\n",
      "Valid set ones/size 765 / 1787\n",
      "BUILD MODEL\n",
      "COMPILE\n",
      "FIT\n",
      "2022-11-26 16:53:43.288487\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/losses.py\", line 140, in __call__\n        return losses_utils.compute_weighted_loss(\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/losses_utils.py\", line 326, in compute_weighted_loss\n        losses, _, sample_weight = squeeze_or_expand_dimensions(  # pylint: disable=unbalanced-tuple-unpacking\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/losses_utils.py\", line 212, in squeeze_or_expand_dimensions\n        sample_weight = tf.squeeze(sample_weight, [-1])\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 3998 for '{{node binary_crossentropy/weighted_loss/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](Cast)' with input shapes: [?,3998].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mnow())\n\u001b[0;32m----> 2\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m \u001b[43mdo_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCross validation acc mean \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m (+/- \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (np\u001b[38;5;241m.\u001b[39mmean(cv_scores), np\u001b[38;5;241m.\u001b[39mstd(cv_scores)))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mnow())\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mdo_cross_validation\u001b[0;34m(eps)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFIT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mnow())\n\u001b[0;32m---> 37\u001b[0m history\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# batch_size=10, default=32 works nicely\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# verbose=1 for ascii art, verbose=0 for none\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(history\u001b[38;5;241m.\u001b[39mhistory)\u001b[38;5;241m.\u001b[39mplot(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     42\u001b[0m plt\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/86/qhfj0q755sg2hlnb93j1gngm0000gn/T/__autograph_generated_file448n2tb9.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/losses.py\", line 140, in __call__\n        return losses_utils.compute_weighted_loss(\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/losses_utils.py\", line 326, in compute_weighted_loss\n        losses, _, sample_weight = squeeze_or_expand_dimensions(  # pylint: disable=unbalanced-tuple-unpacking\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/losses_utils.py\", line 212, in squeeze_or_expand_dimensions\n        sample_weight = tf.squeeze(sample_weight, [-1])\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 3998 for '{{node binary_crossentropy/weighted_loss/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](Cast)' with input shapes: [?,3998].\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "cv_scores = do_cross_validation(EPOCHS)    \n",
    "print(\"Cross validation acc mean %.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riMTsEsCFLBJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
