{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-supervised clustering\n",
    "Suppose We only have labels for a few instances. Use those as the initial centroids.Apply labels to other instances transitively.\n",
    "\n",
    "## Active learning:\n",
    "Get centroids, ask experts to label those, and repeat. (The book mentions this but does not have an exercise for it.)\n",
    "\n",
    "## MNIST\n",
    "[Geron](https://github.com/ageron/handson-ml2/blob/master/09_unsupervised_learning.ipynb) \n",
    "applies this to a random \"blob\" dataset. We will try it on digits. Our results are never as good as his."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8537"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Logistic regression requires 2 dimensions: X and y.\n",
    "num_pixels = 784\n",
    "X_train1D = X_train.reshape(X_train.shape[0],num_pixels)\n",
    "X_test1D = X_test.reshape(X_test.shape[0],num_pixels)\n",
    "# Logistic regression fails to converge in reasonable num iterations.\n",
    "# Scaling seems to help though pixel colors shouldn't need it.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(X_train1D)\n",
    "X_train1D=scaler.transform(X_train1D)\n",
    "X_test1D=scaler.transform(X_test1D)\n",
    "# n_jobs doesn't use threads on my Mac\n",
    "# To avoid errors about non-convergence, we boost max_iter to 1000.\n",
    "log = LogisticRegression(max_iter=1000)\n",
    "log.fit(X_train1D[:1000],y_train[:1000])\n",
    "# This takes 15 min on the 60,000 samples. Score = 0.9211\n",
    "# It takes 5 min on 1000 samples. Score = 0.8537\n",
    "score1=log.score(X_test1D,y_test)\n",
    "score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6223"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish a baseline: logistic regression trained on first 50\n",
    "# with no boost from k-means.\n",
    "n_labels = 50\n",
    "X_exemplars = X_train1D[:n_labels]\n",
    "y_exemplars = y_train[:n_labels]\n",
    "# n_jobs doesn't use threads on my Mac\n",
    "log = LogisticRegression(max_iter=1000)\n",
    "log.fit(X_exemplars,y_exemplars)\n",
    "score2 = log.score(X_test1D,y_test)\n",
    "score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-cc535be0e76b>:7: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  km.fit(X_train1D)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.701"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This was my idea.\n",
    "# Use first 50 instances as initial centroids for K-means.\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "K = n_labels\n",
    "km = KMeans(n_clusters=K,init=X_exemplars)\n",
    "km.fit(X_train1D)\n",
    "X_distances = km.transform(X_train1D)\n",
    "# Now we have 60K instances whose features are 50 distances to centroids.\n",
    "# The minimum distance is 0 and we should have at least 50 of those.\n",
    "centroid_index = np.argmin(X_distances, axis=0)\n",
    "X_centroids = X_train1D[centroid_index]\n",
    "y_centroids = y_train[centroid_index]\n",
    "log = LogisticRegression(max_iter=1000)\n",
    "log.fit(X_centroids,y_centroids)\n",
    "score3 = log.score(X_test1D,y_test)\n",
    "score3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try again without setting the initial centroids.\n",
    "# This runs for 5 min, a little less if we limit the initializations to 10.\n",
    "km = KMeans(n_clusters=K,n_init=10)\n",
    "km.fit(X_train1D)\n",
    "X_distances = km.transform(X_train1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using argmin, we train on 50 instances\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6843"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we have 60K instances whose features are 50 distances to centroids.\n",
    "# The minimum distance is 0 and we should have at least 50 of those.\n",
    "centroid_index = np.argmin(X_distances, axis=0)\n",
    "X_centroids = X_train1D[centroid_index]\n",
    "y_centroids = y_train[centroid_index]\n",
    "print(\"Using argmin, we train on %d instances\"%(len(X_centroids)))\n",
    "log = LogisticRegression(max_iter=1000)\n",
    "log.fit(X_centroids,y_centroids)\n",
    "score3 = log.score(X_test1D,y_test)\n",
    "score3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using min distance per dimension, we train on 50 instances\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6843"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should do the same as the above using loop instead of argmin.\n",
    "BIG=10000\n",
    "num_centroids=len(X_distances[0])\n",
    "centroid_indices=[-1]*num_centroids\n",
    "mins=[BIG]*num_centroids\n",
    "i= -1\n",
    "for distances in X_distances:\n",
    "    i += 1\n",
    "    for j in range(num_centroids):\n",
    "        if distances[j]<mins[j]:\n",
    "            mins[j]=distances[j]\n",
    "            centroid_indices[j]=i\n",
    "X_centroids = X_train1D[centroid_indices]\n",
    "y_centroids = y_train[centroid_indices]\n",
    "print(\"Using min distance per dimension, we train on %d instances\"%(len(X_centroids)))\n",
    "log = LogisticRegression(max_iter=1000)\n",
    "log.fit(X_centroids,y_centroids)\n",
    "score4 = log.score(X_test1D,y_test)\n",
    "score4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At max distance 7, we train on 440 instances\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6508"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build on the above.\n",
    "# Add more instances in neighborhood of each centroid.\n",
    "# As above, use each instance's true label regardless of cluster.\n",
    "# To avoid re-inserting a centroid, reuse centroid_indices from above.\n",
    "MAX_DISTANCE = 7 \n",
    "i=0\n",
    "neighbor_indices=[]\n",
    "neighbor_centroid=[]\n",
    "for distances in X_distances:\n",
    "    for d in distances:\n",
    "        if d<MAX_DISTANCE and i not in centroid_indices:\n",
    "            neighbor_indices.append(i)\n",
    "            neighbor_centroid.append(j)  # will need this later\n",
    "            break\n",
    "    i += 1\n",
    "combined_indices=centroid_indices+neighbor_indices\n",
    "X_neighbors = X_train1D[combined_indices]\n",
    "y_neighbors = y_train[combined_indices]\n",
    "print(\"At max distance %d, we train on %d instances\"%(MAX_DISTANCE,len(X_neighbors)))\n",
    "log = LogisticRegression(max_iter=1000)\n",
    "log.fit(X_neighbors,y_neighbors)\n",
    "neighbors5=len(X_neighbors) # save for later\n",
    "score5 = log.score(X_test1D,y_test)\n",
    "score5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 440 instances and 440 labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5018"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Improve on above using label propagation.\n",
    "# Reuse variables from above.\n",
    "new_labels=[]\n",
    "for i in centroid_indices:\n",
    "    new_labels.append(y_train[i])\n",
    "for j in neighbor_centroid:\n",
    "    new_labels.append(y_train[j])\n",
    "print(\"Train on %d instances and %d labels\"%(len(X_neighbors),len(new_labels)))\n",
    "log = LogisticRegression(max_iter=1000)\n",
    "log.fit(X_neighbors,new_labels)\n",
    "neighbors6=len(X_neighbors) # save for later\n",
    "score6 = log.score(X_test1D,y_test)\n",
    "score6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.853700 LogReg trained on 1000.\n",
      "0.622300 LogReg trained on first 50.\n",
      "0.684300 LogReg trained on 50 centroids (guided).\n",
      "0.684300 LogReg trained on 50 centroids (unguided).\n",
      "0.650800 440 neighbors LogReg, K-means, (original labels).\n",
      "0.501800 440 neighbors LogReg, K-means, (labels propagation).\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"%f %s\"%(score1,\"LogReg trained on 1000.\"))\n",
    "print(\"%f %s\"%(score2,\"LogReg trained on first 50.\"))\n",
    "print(\"%f %s\"%(score3,\"LogReg trained on 50 centroids (guided).\"))\n",
    "print(\"%f %s\"%(score4,\"LogReg trained on 50 centroids (unguided).\"))\n",
    "print(\"%f %d neighbors %s\"%(score5,neighbors5,\"LogReg, K-means, (original labels).\"))\n",
    "print(\"%f %d neighbors %s\"%(score6,neighbors6,\"LogReg, K-means, (labels propagation).\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
