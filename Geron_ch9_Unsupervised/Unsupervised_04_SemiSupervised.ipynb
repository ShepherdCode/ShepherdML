{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Semi-supervised clustering.\n",
    "# We only have labels for a few instances.\n",
    "# Use those as the initial centroids.\n",
    "# Apply labels to other instances transitively.\n",
    "\n",
    "# Active learning:\n",
    "# Get centroids, ask experts to label those, and repeat.\n",
    "\n",
    "import ssl\n",
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Logistic regression requires 2 dimensions: X and y.\n",
    "num_pixels = 784\n",
    "X_train1D = X_train.reshape(X_train.shape[0],num_pixels)\n",
    "X_test1D = X_test.reshape(X_test.shape[0],num_pixels)\n",
    "# Logistic regression fails to converge in reasonable num iterations.\n",
    "# Scaling seems to help though pixel colors shouldn't need it.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(X_train1D)\n",
    "X_train1D=scaler.transform(X_train1D)\n",
    "X_test1D=scaler.transform(X_test1D)\n",
    "# n_jobs doesn't use threads on my Mac\n",
    "log = LogisticRegression(max_iter=1000)\n",
    "log.fit(X_train1D[:1000],y_train[:1000])\n",
    "# This takes 15 min on the 60,000 samples so we ran it on 1,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1=log.score(X_test1D,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the data weren't labeled, we could manually label few.\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "n_labels = 3\n",
    "X_exemplars = X_train[:n_labels]\n",
    "i=0\n",
    "y_exemplars=np.array([5,0,4])  # manual\n",
    "#for sample in X_exemplars:\n",
    "#    plt.imshow(sample,cmap=\"binary\")\n",
    "#    print(i)\n",
    "#    print(y_exemplars[i])\n",
    "#    plt.show()\n",
    "#    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish a baseline: logistic regression trained on first 50\n",
    "# with no boost from k-means.\n",
    "n_labels = 50\n",
    "X_exemplars = X_train1D[:n_labels]\n",
    "y_exemplars = y_train[:n_labels]\n",
    "# n_jobs doesn't use threads on my Mac\n",
    "log = LogisticRegression(max_iter=1000)\n",
    "log.fit(X_exemplars,y_exemplars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "score2 = log.score(X_test1D,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1133: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return self.fit(X, sample_weight=sample_weight)._transform(X)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next use 50 labeled instances as initial centroids for K-means.\n",
    "from sklearn.cluster import KMeans\n",
    "K = 50\n",
    "km = KMeans(n_clusters=K,init=X_exemplars)\n",
    "X_train_distances = km.fit_transform(X_train1D)\n",
    "X_train_distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57689, 38110, 36238,  4585, 50557, 23171,  6690, 39871,  7252,\n",
       "        5417, 31258, 42267, 54288, 46323,   638,  5569, 52986, 59803,\n",
       "       18376, 29282,  2648, 11846, 35840, 19400,  9092, 50474, 22948,\n",
       "       13239, 18448, 25392, 28298, 59352, 41442, 49565,  8368, 41321,\n",
       "       12191, 35502,  6156, 40979, 19552, 48106, 51068,   423, 33444,\n",
       "       24088, 51519, 25440, 28596,  4808])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we have 60K instances whose features are 50 distances to centroids.\n",
    "# The minimum distance is 0 and we should have at least 50 of those.\n",
    "centroid_index = np.argmin(X_train_distances, axis=0)\n",
    "centroid_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_centroids = X_train1D[centroid_index]\n",
    "y_centroids = y_train[centroid_index]\n",
    "log = LogisticRegression(max_iter=1000)\n",
    "log.fit(X_centroids,y_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "score3 = log.score(X_test1D,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try again without setting the initial centroids.\n",
    "km = KMeans(n_clusters=K)\n",
    "X_distances = km.fit_transform(X_train1D)\n",
    "centroid_index = np.argmin(X_distances, axis=0)\n",
    "X_centroids = X_train1D[centroid_index]\n",
    "y_centroids = y_train[centroid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:    2.3s finished\n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression(max_iter=1000)\n",
    "log.fit(X_centroids,y_centroids)\n",
    "score4 = log.score(X_test1D,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.853700 LogReg trained on 1000.\n",
      "0.622300 LogReg trained on first 50.\n",
      "0.701000 LogReg trained on 50 centroids (guided).\n",
      "0.734300 LogReg trained on 50 centroids (unguided).\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"%f %s\"%(score1,\"LogReg trained on 1000.\"))\n",
    "print(\"%f %s\"%(score2,\"LogReg trained on first 50.\"))\n",
    "print(\"%f %s\"%(score3,\"LogReg trained on 50 centroids (guided).\"))\n",
    "print(\"%f %s\"%(score4,\"LogReg trained on 50 centroids (unguided).\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion. \n",
    "# If we only had 50 labels, the situation is worse than 1000 labels.\n",
    "# But, we can improve accuracy by preprocessing with K-means,\n",
    "# replacing the 50 labeled instances with 50 centroids.\n",
    "# Initializing K-means with the 50 labeled instances did not help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 50)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next try label propagation.\n",
    "# After clustering, label all the instances close to their centroids.\n",
    "X_distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4562, 784)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try several cutoffs. Choose one that generates a decent sized train set.\n",
    "zero=0.000000001\n",
    "threshold=10\n",
    "distance=X_distances[0]\n",
    "n_centroids=len(distance)\n",
    "centroid_labels=[0]*n_centroids\n",
    "for i in range(len(X_distances)):\n",
    "    distance=X_distances[i]\n",
    "    for j in range(len(distance)):\n",
    "        if distance[j]<zero:\n",
    "            centroid_labels[j]=y_train[i]\n",
    "            break\n",
    "central_indices=[]\n",
    "central_values=[]\n",
    "for i in range(len(X_distances)):\n",
    "    distance=X_distances[i]\n",
    "    for j in range(len(distance)):\n",
    "        if distance[j]<threshold:     \n",
    "            central_indices.append(i)\n",
    "            central_values.append(centroid_labels[j])\n",
    "            break\n",
    "X_central = X_train1D[central_indices]\n",
    "X_central.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4562,)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_central = np.array(central_values)\n",
    "y_central.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.098"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = LogisticRegression(max_iter=1000)\n",
    "log.fit(X_central,y_central)\n",
    "score5 = log.score(X_test1D,y_test)\n",
    "score5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
