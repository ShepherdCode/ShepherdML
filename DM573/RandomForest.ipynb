{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "See our notebook on Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest algorithm can be used for classification or regression.\n",
    "RF is robust to missing values, numeric & categorical features, \n",
    "and numeric features at different scales. \n",
    "A trained RF is explainable; \n",
    "it can rank the features by their importance to the decision making process.\n",
    "\n",
    "A decision tree, DT, is an ok classifier. \n",
    "Here is how to build a DT. \n",
    "Always operate on the most impure node i.e. \n",
    "one that still receives 50% cancer & 50% normal instances.\n",
    "For that node, select a feature & threshold that is optimal for splitting. \n",
    "For example, age & 50 would be a good choice \n",
    "if age>50 would put most of the cancer cases on the right, \n",
    "and age<=50 would put most of the normal cases on the left. \n",
    "\n",
    "But a single DT tends to overfit.\n",
    "That means high variance and poor generalization.\n",
    "For a single decision tree method like CART, it is critical to prune the tree.\n",
    "Also, a single DT can fail to classify data with missing values.   \n",
    "\n",
    "Another solution is random forest,\n",
    "which is an ensemble of many trees.\n",
    "RF is a strong classifier built from weak trees.\n",
    "Each tree incorporates randomness so each is slightly different (and wrong).\n",
    "The ensemble layer aggregates the tree decisions.\n",
    "The ensemble layer can ask each tree for its best guess (called winner-take-all), \n",
    "then apply majority rule; this is what the orginal RF paper suggested.\n",
    "Or it can ask each tree to assign a probability to each class, and sum those up;\n",
    "this is what the sklearn RandomForestClassifier does.\n",
    "\n",
    "There are several ways to make random trees.\n",
    "You can build each tree from a random subset of the training data.\n",
    "Or you can build each tree from a random subset of the features.\n",
    "Or you can do both. This is what the sklearn RandomForestClassifier does.\n",
    "\n",
    "When using a random subset of instances (called the bag), \n",
    "you can estimate the tree's accuracy on the remaining instances \n",
    "(called out-of-bag, or OOB)\n",
    "to arrive at an OOB score per tree.\n",
    "Then, the ensemble layer can weight each tree's vote based on its OOB.\n",
    "The sklearn RandomForestClassifier has options to do this.\n",
    "\n",
    "See sklearn [Ensemble Methods](https://scikit-learn.org/stable/modules/ensemble.html)\n",
    "section 1.11.2.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble methods\n",
    "(This is from older notes, mistakenly posted in the Timeseries Forecasting notebook.)\n",
    "\n",
    "Decision trees (DT) are simple and random forest (RF) is popular.   \n",
    "RF is a bag of trees.  \n",
    "See [MachineLearningMastery](https://machinelearningmastery.com/random-forest-for-time-series-forecasting/)\n",
    "\n",
    "Bagging = bootstrap + aggregation\n",
    "* Bootstrap i.e. form data subsets by random sampling with replacement.\n",
    "* Train a model, such as DT, for each subset.\n",
    "* Aggregate i.e. by average of all predictions.\n",
    "\n",
    "Training\n",
    "* Transform the time series data into a (features,label) training set.\n",
    "* Conduct supervised learning.\n",
    "* Validate on unseen data. \n",
    "* Out-of-bag (OOB) error: measure each model on the data it didn't see.\n",
    "\n",
    "Cross-validation\n",
    "* Use walk-forward validation.\n",
    "* Choose a cut point. Train on data before the cut. Test on data after the cut.\n",
    "* For k-fold, walk the cut point forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
