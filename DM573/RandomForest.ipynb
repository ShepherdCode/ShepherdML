{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "See Decision Tree and Forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest algorithm can be used for classification or regression.\n",
    "RF is robust to missing values, numeric & categorical features, \n",
    "and numeric features at different scales. \n",
    "A trained RF is explainable; \n",
    "it can rank the features by their importance to the decision making process.\n",
    "\n",
    "A decision tree, DT, is an ok classifier. \n",
    "Here is how to build a DT. \n",
    "Always operate on the most impure node i.e. \n",
    "one that still receives 50% cancer & 50% normal instances.\n",
    "For that node, select a feature & threshold that is optimal for splitting. \n",
    "For example, age & 50 would be a good choice \n",
    "if age>50 would put most of the cancer cases on the right, \n",
    "and age<=50 would put most of the normal cases on the left. \n",
    "\n",
    "But a single DT tends to overfit.\n",
    "That means high variance and poor generalization.  \n",
    "For a single decision tree method like CART, it is critical to prune the tree.  \n",
    "In RF, it is ok if each tree overfits.\n",
    "\n",
    "One solution is random forest,\n",
    "which is an ensemble of many trees.\n",
    "RF is a strong classifier built from weak trees.\n",
    "Each tree incorporates randomness so each is slightly different (and wrong).\n",
    "The ensemble layer aggregates the tree decisions.\n",
    "The ensemble layer can ask each tree for its best guess (called winner-take-all), \n",
    "then apply majority rule; this is what the orginal RF paper suggested.\n",
    "Or it can ask each tree to assign a probability to each class, and sum those up;\n",
    "this is what the sklearn RandomForestClassifier does.\n",
    "\n",
    "There are several ways to make random trees.\n",
    "You can build each tree from a random subset of the training data.\n",
    "Or you can build each tree from a random subset of the features.\n",
    "Or you can do both. This is what the sklearn RandomForestClassifier does.\n",
    "\n",
    "When using a random subset of instances (called the bag), \n",
    "you can estimate the tree's accuracy on the remaining instances \n",
    "(called out-of-bag, or OOB)\n",
    "to arrive at an OOB score per tree.\n",
    "Then, the ensemble layer can weight each tree's vote based on its OOB.\n",
    "The sklearn RandomForestClassifier has options to do this.\n",
    "\n",
    "See sklearn [Ensemble Methods](https://scikit-learn.org/stable/modules/ensemble.html)\n",
    "section 1.11.2.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
