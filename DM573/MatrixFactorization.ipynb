{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9005f4",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e3434",
   "metadata": {},
   "source": [
    "# LU\n",
    "Any square matrix can be factored into an upper-diagonal and lower-diagonal matrix.  \n",
    "This corresponds to Gaussian elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c282d70",
   "metadata": {},
   "source": [
    "## Eigen\n",
    "Some matrices can be factored into values and eigenvectors.   \n",
    "The matrix must be square and diagonalizable.   \n",
    "Some factorizations yield matrices of complex numbers.   \n",
    "The eigenvalues can be ordered by importance for dimensionality reduction.   \n",
    "The eigenvectors can be scaled or normalized to unit length.   \n",
    "The eigenvalues can also be scaled, so they are non-unique.   \n",
    "Use lambda $\\lambda$ for one eigenvalue, Lambda $\\Lambda$ for diagonal matrix of all.   \n",
    "Use A for a matrix e.g. rows of data, columns of features.    \n",
    "\n",
    "An eigenvector of A stretches or shrinks but does not rotate when multiplied by A.    \n",
    "$A v = \\lambda v$   \n",
    "Matrix A = (column) eigenvectors times Lambda times (row) eigenvectors.   \n",
    "$A = Q \\Lambda Q^{-1}$   \n",
    "The eigenvalues are found by solving the system of linear equations    \n",
    "$det(A - \\lambda I) = 0$   \n",
    "The eigenvectors are found by solving another system of linear equations    \n",
    "$(A - \\lambda_i I)v_i = 0$   \n",
    "\n",
    "As a special case, when A is real and symmetric, \n",
    "there exist orthonormal eigenvectors    \n",
    "$A = Q \\Lambda Q^{T}$   \n",
    "This is the case when A is a covariance matrix, as with PCA.   \n",
    "(When a matrix $M$ is orthogonal, $M^{-1} = M^T$.)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e6098",
   "metadata": {},
   "source": [
    "## PCA\n",
    "Eigen decomposition of the covariance matrix.   \n",
    "The matrix satisfies the special case of being square, real, symmetric, positive semi-definite.   \n",
    "PCA chooses an orthonormal basis of eigenvectors,\n",
    "with eigenvalues ordered by the portion of variance explained.   \n",
    "Covariance = $P \\Lambda P^{-1}$   \n",
    "This can be used for dimensionality reduction.  \n",
    "No use of labels, so this is unsupervised.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e647a",
   "metadata": {},
   "source": [
    "## ICA\n",
    "ICA vectors are not orthonormal and are not ranked.   \n",
    "ICA can discover higher-order interactions.   \n",
    "(PCA only discovers linear combinations but its vecors are ranked.)\n",
    "No use of labels, so this is unsupervised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c846bc",
   "metadata": {},
   "source": [
    "## SVD\n",
    "Data D = { colums of features, rows of instances }.   \n",
    "SVD finds    \n",
    "$D = U \\Sigma V^{-1}$ where   \n",
    "U = matrix of instances (rows) transformed to latent features   \n",
    "$\\Sigma$ = ranking of latent features   \n",
    "V = matrix of vectors (rows) that compose latent features by combinations of given features.   \n",
    "Objective function: use of top k latent features minimizes reconstruction loss, for any k.\n",
    "\n",
    "The SVD factorization is unique and always exists.   \n",
    "SVD does not use labels, so it is unsupervised.\n",
    "$\\Sigma$ = is diagonal but not square (last rows could be zero)      \n",
    "\n",
    "### SVD vs PCA\n",
    "SVD devises a different coordinate system for the data (like PCA).    \n",
    "SVD lossless, but subset of latent features gives dimensionality reduction (like PCA).   \n",
    "SVD does not use the eigen decomposition (unlike PCA).   \n",
    "SVD can operate on rectangular matrices (unlike eigen decomposition).        \n",
    "SVD decomposes the data matrix (not the covariance, as in PCA).   \n",
    "SVD generates three matrices (unlike PCA which generates two).       \n",
    "The middle matrix is diagonal (in SVD and PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb6acea",
   "metadata": {},
   "source": [
    "## LDA\n",
    "See [Wikipedia](https://en.wikipedia.org/wiki/Linear_discriminant_analysis).\n",
    "See also our Linear Discriminant Analysis notebook.\n",
    "\n",
    "LDA finds the optimal linear boundary between the labeled instances.     \n",
    "LDA casts the data into a lower dimension.    \n",
    "LDA can be used for classification or dimensionality reduction.   \n",
    "LDA assumes Gaussian normal distributions.   \n",
    "LDA assumes homoscedacity: equal variance and covariance.  \n",
    "(When these conditions aren't met, use QDA instead.)   \n",
    "\n",
    "LDA only explores linear combinations of features.   \n",
    "LDA only finds linear decision boundaries (unlike QDA).    \n",
    "The data determine the orientation of the decision boundary hyperplane,\n",
    "but a given parameter (confidence to predict class 1) determines its position.    \n",
    "LDA is less effective if features are correlated.  \n",
    "\n",
    "Objective functions: \n",
    "* maximize SS_between/SS_within ratio, and\n",
    "* maximize classification accuracy.   \n",
    "\n",
    "LDA uses the eigen decomposition (like PCA),\n",
    "but LDA applies it to label scatter (not just feature covariance).      \n",
    "LDA is specific to continuous numerical features.   \n",
    "LDA uses labels, so LDA is supervised classification.   \n",
    "\n",
    "Otsu's method is related to LDA.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b50919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
