{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Rule\n",
    "$P(y=y_i|X) = P(X|y_i) P(y_i) / P(X)$  \n",
    "\n",
    "One data point X is represented by a vector of attributes.  \n",
    "The set of ground truth labels y contains i classes, each denoted yi.  \n",
    "In words, the formula says:   \n",
    "Prob of yi given X = Prob of X given yi * Prob of yi / Prob of X over all y   \n",
    "In other words:  \n",
    "Prob that X is in class yi = Prob of X when yi is the class * Prob of class yi / Prob of X independent of y   \n",
    "In other words:  \n",
    "Posterior probability = Class conditional probability * Prior probability / Marginal probability  \n",
    "\n",
    "#### Normalizer\n",
    "Denominator = $P(X) = \\sum_i [ P(X|y_i) P(y_i)]$  \n",
    "\n",
    "The denominator normalizes the result to a probability in range 0 to 1.  \n",
    "The marginal probability of X means the probability of X regardless of y.    \n",
    "The numerator has the likelihood of X under one yi, but\n",
    "the denominator has the likelihood of X under all y.   \n",
    "Rembember to weight each P(X|y) by the corresponding P(y).  \n",
    "\n",
    "#### Derivation\n",
    "The joint probability of X and y equals the conditional probability\n",
    "of X given y, times the marginal probability of y.  \n",
    "$P(X,y) = P(X|y)P(y)$   \n",
    "\n",
    "By symmetry, you can interchange X and y, so  \n",
    "$P(X,y) = P(X|y)P(y) = P(y|X)P(X)$   \n",
    "or  \n",
    "$P(X|y)P(y) = P(y|X)P(X)$   \n",
    "\n",
    "Now just divide both sides by P(X)...   \n",
    "$P(y|X) = P(X|y)P(y)/P(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian models\n",
    "\n",
    "### Bayesian models are generative\n",
    "To train a model, ignore the denominator.  \n",
    "Estimate P(y=yi) based on the label frequencies in the training data.  \n",
    "Learn P(X|y) from the training data, given a yi for each xi.   \n",
    "By learning a probability distribution, we learn a generating function.  \n",
    "This makes the model generative.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian models can work on continuous features\n",
    "\n",
    "Choice 1: Discretize features by binning them, and apply Bayes formula.  \n",
    "Must be careful, as binning strategy matters.\n",
    "\n",
    "Choice 2: Learn parameters for a distribution.  \n",
    "Assume each feature has a normal distribution of values, per class.  \n",
    "For feature j and class i:  \n",
    "$P(X_j=x_j|Y=y_i) $  \n",
    "$= \\mathcal{N}(\\mu_{ij},\\sigma_{ij})$  \n",
    "$= \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{-1}{2}[\\frac{x-\\mu}{\\sigma}]^2}$\n",
    "\n",
    "Estimate the mean and standard deviation, per class, from the training data.  \n",
    "$\\mu_{ij} = \\sum(x_{ij})/n$  \n",
    "$\\sigma_{ij} = \\sqrt{ \\sum(x_{ij}-\\mu_{ij})/(n-1) }$\n",
    "\n",
    "During Bayesian inference, \n",
    "given an X,\n",
    "use the above to compute likelihood P(X|yi).  \n",
    "Then multiply by the prior P(yi).  \n",
    "Repeat for each class yi.  \n",
    "Predict the class with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "Probabilistic.  \n",
    "Generative.  \n",
    "Predictive.  \n",
    "Robust to irrelevant features -- they affect every class equally.  \n",
    "Robust to missing feature values -- just skip them while computing products.  \n",
    "(Assume only a few features are missing per test instance\n",
    "and missingness of a feature is not correlated to the class.)\n",
    "\n",
    "Assumes independence of features.  \n",
    "Fairly robust to violations of the assumption.  \n",
    "\n",
    "Addresses problem of exponential complexity supported by insufficient data:   \n",
    "$P(X|y_i)$ involves a sum over all possible joint probabilities of feature combinations.     \n",
    "\n",
    "Solution: naively assume the features of X are independent.  \n",
    "With this assumption,    \n",
    "$P(X|y_i) = \\prod_j P(X_j|y_i)$\n",
    "\n",
    "Feature correlation can undermine the assumption.  \n",
    "But to negatively affect predictions, \n",
    "features need to be correlated differently in different classes.   \n",
    "Thus, Naive Bayes is somewhat robust to the assumption. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Psuedocounts for NB with categorical features\n",
    "The training data may have zero instances of category c in some feature j in class i.  \n",
    "If we use zero probability,\n",
    "this entire product becomes zero, as does all our posteriors:  \n",
    "$P(X|y_i) = \\prod_j P(X_j|y_i)$\n",
    "\n",
    "Solution 0: Frequentist estimate without adjustment (can be zero)  \n",
    "$P(X_i = c | y) = (n_{cy}) / (n_{y})$ \n",
    "\n",
    "Solution 1: Laplace estimate increases the numerator and denominator  \n",
    "$P(X_i = c | y) = (n_{cy} + p) / (n_{y} + v_i)$   \n",
    "where  \n",
    "$n_{cy}$ = number of class=y instances where feature i has value c  \n",
    "$n_{y}$ = number of instances in class y  \n",
    "$p$ = a psuedocount, usually 1   \n",
    "$v_i$ = number of distinct values of feature i  \n",
    "\n",
    "Solution 2: m-estimate (one of many maximum-likelihood estimators)  \n",
    "$P(X_i = c | y) = (n_{cy} + mp) / (n_{y} + m)$   \n",
    "where  \n",
    "$n_{cy}$ = number of class=y instances where feature i has value c  \n",
    "$n_{y}$ = number of instances in class y  \n",
    "$p$ = a prior guess of the non-zero probability of value c   \n",
    "$m$ = hyper-parameter weight for p representing confidence in p  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Decision Theory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Bayes rule: $P(y=y_i|X) = P(X|y_i) P(y_i) / P(X)$    \n",
    "Ignore the denominator (monotonic anyway).  \n",
    "Use the log (monotonic anyway).   \n",
    "Use natural log to balance e in the Gauss pdf.  \n",
    "To: $g_i(x) = ln(p(x|y_i)) + ln(p(y_i))$    \n",
    "\n",
    "Decision rule:   \n",
    "In the two-class case, our model is a dichotomizer.  \n",
    "Classify x as class i if $g_i(x) > g_j(x)$  \n",
    "Equivalently, classify x as class i if $g(x) > 0$ where $g(x) = g_i(x) - g_j(x)$  \n",
    "\n",
    "Decision boundary:   \n",
    "Hyperplane where $g_i(x) = g_j(x)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate Gaussian:   \n",
    "$\\bar X \\sim \\mathcal{N} (\\bar \\mu,\\Sigma)=(\\frac{1}{\\sqrt{|\\Sigma|} * (2\\pi)^{D/2}})e^{[\\frac{-1}{2} * (\\bar X - \\bar \\mu)^{T}*\\Sigma^{-1}*(\\bar X - \\bar \\mu)]}$  \n",
    "   \n",
    "Dichotomizer:   \n",
    "$g_i(x) = ln(p(x|y_i)) + ln(p(y_i))$    \n",
    "\n",
    "Substitute Gauss (where M = Mahalanobis distance):   \n",
    "$g_i(x)=ln(\\mathcal{N}) + ln(p(y_i))$  \n",
    "$= ln(1)-ln(\\Sigma^{1/2} (2\\pi)^{D/2}) + \\frac{-1}{2}M  + ln(p(y_i))$   \n",
    "$= 0 -\\frac{1}{2}ln(\\Sigma) -\\frac{D}{2}ln(2\\pi) -M/2 + ln(p(y_i))$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
