{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes \n",
    "See lecture slides 03.  \n",
    "See Duda & Hart section 2.6.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Rule\n",
    "$P(y=y_i|X) = P(X|y_i) P(y_i) / P(X)$  \n",
    "\n",
    "Prob of y=yi given X = Prob of X given yi * Prob of yi / Prob of X over all y   \n",
    "Posterior prob = Class conditional prob * Prior prob / Marginal prob    \n",
    "Posterior prob = Likelihood of model * Prior prob / Prob of X indepdent of y    \n",
    "\n",
    "P(covid|cough) = P(cough|covid) * P(covid) / P(cough)   \n",
    "P(rain|clouds) = P(clouds|rain) * P(rain) / P(clouds)   \n",
    "\n",
    "Each X is a vector of attributes.  \n",
    "Each yi is potentially the label for X. \n",
    "\n",
    "#### Denominator is a normalizer\n",
    "Denominator = $P(X) = \\sum_i [ P(X|y_i) P(y_i)]$  \n",
    "\n",
    "The denominator normalizes the result to a probability in range 0 to 1.  \n",
    "The marginal probability of X means the probability of X regardless of y.    \n",
    "The numerator has the likelihood of X under one yi, but\n",
    "the denominator has the likelihood of X under all y.   \n",
    "Being Bayesian, we weight each P(X|y) by the corresponding P(y).  \n",
    "\n",
    "The denominator is the same for every class.  \n",
    "The denominator can be ignored by a classifier.  \n",
    "\n",
    "#### Derivation\n",
    "The joint probability of X and y equals the conditional probability\n",
    "of X given y, times the marginal probability of y.  \n",
    "$P(X,y) = P(X|y)P(y)$   \n",
    "\n",
    "By symmetry, you can interchange X and y, so  \n",
    "$P(X,y) = P(X|y)P(y) = P(y|X)P(X)$   \n",
    "\n",
    "Thus,    \n",
    "$P(y|X)P(X)= P(X|y)P(y)$   \n",
    "\n",
    "Now just divide both sides by P(X)   \n",
    "$P(y|X) = P(X|y)P(y)/P(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian models\n",
    "\n",
    "### Bayesian models are generative\n",
    "To train a model, ignore the denominator.  \n",
    "Estimate P(y=yi) based on the label frequencies in the training data.  \n",
    "Learn P(X|y) from the training data, given a yi for each xi.   \n",
    "By learning a probability distribution, we learn a generating function.  \n",
    "This makes the model generative.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian models can work on continuous features\n",
    "\n",
    "Choice 1: Discretize features by binning them, and apply Bayes formula.  \n",
    "Must be careful, as binning strategy matters.\n",
    "\n",
    "Choice 2: Learn parameters for a distribution.  \n",
    "Assume each feature has a normal distribution of values, per class.  \n",
    "For feature j and class i:  \n",
    "$P(X_j=x_j|Y=y_i) $  \n",
    "$= \\mathcal{N}(\\mu_{ij},\\sigma_{ij})$  \n",
    "$= \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{-1}{2}[\\frac{x-\\mu}{\\sigma}]^2}$\n",
    "\n",
    "Estimate the mean and variance, per feature per class, from the training data.  \n",
    "$\\mu_{ij} = \\sum(x_{ij})/n$  \n",
    "$\\sigma_{ij}^2 = \\frac{\\sum(x_{ij}-\\mu_{ij})^2}{(n-1)}$    \n",
    "\n",
    "During Bayesian inference, \n",
    "given an X,\n",
    "use the above to compute likelihood P(X|yi).  \n",
    "Then multiply by the prior P(yi).  \n",
    "Repeat for each class yi.  \n",
    "Predict the class with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "NBC is: Probabilistic. Generative. Predictive.  \n",
    "\n",
    "See Duda & Hart 2.6\n",
    "\n",
    "Not enough data to represent every feature combination? Assume independent features.    \n",
    "Addresses problem of exponential complexity but insufficient data.   \n",
    "\n",
    "Without the assumption,    \n",
    "$P(X|y_i) = $ sum over all possible joint probabilities \n",
    "i.e. all feature combinations.   \n",
    "With the assumption,    \n",
    "$P(X|y_i) = \\prod_j P(X_j|y_i)$    \n",
    "\n",
    "Assumes independence of features, which is unlikely.    \n",
    "But NBC is Fairly robust to violations of the assumption.  \n",
    "Robust to irrelevant features -- they affect every class equally.  \n",
    "\n",
    "Robust to missing feature values -- just skip them while computing products.  \n",
    "Assume only a few features are missing per test instance.   \n",
    "Assume \"missingness\" is random i.e. not correlated to the class.\n",
    "\n",
    "Feature correlation undermines the assumption.  \n",
    "But to negatively affect predictions, \n",
    "features need to be correlated differently in different classes.   \n",
    "For example, price correlates to volume for milk but not caviar.   \n",
    "This is unlikely to escape your notice.   \n",
    "Thus, Naive Bayes is somewhat robust to the assumption. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Psuedocounts for NB with categorical features\n",
    "The training data may have zero instances of category c in some feature j in class i.  \n",
    "If we use zero probability,\n",
    "this entire product becomes zero, as does all our posteriors:  \n",
    "$P(X|y_i) = \\prod_j P(X_j|y_i)$\n",
    "\n",
    "Solution 0: Frequentist estimate without adjustment (can be zero)  \n",
    "$P(X_i = c | y) = (n_{cy}) / (n_{y})$ \n",
    "\n",
    "Solution 1: Laplace estimate increases the numerator and denominator  \n",
    "$P(X_i = c | y) = (n_{cy} + p) / (n_{y} + v_i)$   \n",
    "where  \n",
    "$n_{cy}$ = number of class=y instances where feature i has value c  \n",
    "$n_{y}$ = number of instances in class y  \n",
    "$p$ = a psuedocount, usually 1   \n",
    "$v_i$ = number of distinct values of feature i, i.e. total pseudocounts  \n",
    "\n",
    "Solution 2: m-estimate (one of many maximum-likelihood estimators)  \n",
    "$P(X_i = c | y) = (n_{cy} + mp) / (n_{y} + m)$   \n",
    "where  \n",
    "$n_{cy}$ = number of class=y instances where feature i has value c  \n",
    "$n_{y}$ = number of instances in class y  \n",
    "$p$ = a prior guess of the non-zero probability of value c   \n",
    "$m$ = hyper-parameter weight for p representing confidence in p  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Decision Theory\n",
    "From Bayes rule: $P(y=y_i|X) = P(X|y_i) P(y_i) / P(X)$    \n",
    "Ignore the denominator (monotonic anyway).  \n",
    "Use the log (monotonic anyway).   \n",
    "Use natural log to balance e in the Gauss pdf.  \n",
    "Result:\n",
    "\n",
    "$g_i(x) = ln(p(x|y_i)) + ln(p(y_i))$    \n",
    "\n",
    "In the two-class case, our model is a dichotomizer.  \n",
    "\n",
    "Decision rule:   \n",
    "Classify x as class i if $g_i(x) > g_j(x)$  \n",
    "\n",
    "Equivalently, classify x as class i if $g(x) > 0$ where $g(x) = g_i(x) - g_j(x)$  \n",
    "\n",
    "Decision boundary:   \n",
    "Hyperplane where $g_i(x) = g_j(x)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Gaussian   \n",
    "$\\bar X \\sim \\mathcal{N} (\\bar \\mu,\\Sigma)=(\\frac{1}{\\sqrt{|\\Sigma|} * (2\\pi)^{D/2}})e^{[\\frac{-1}{2} * (\\bar X - \\bar \\mu)^{T}*\\Sigma^{-1}*(\\bar X - \\bar \\mu)]}$  \n",
    "   \n",
    "Dichotomizer:   \n",
    "$g_i(x) = ln(p(x|y_i)) + ln(p(y_i))$    \n",
    "\n",
    "Use M for Mahalanobis distance:   \n",
    "$M = [(x - \\mu)^{T} \\Sigma^{-1} (x - \\mu)]$\n",
    "\n",
    "Substitute a Gaussian for the PDF p(x|y):   \n",
    "$g_i(x) = [ln(\\mathcal{N})] + ln(p(y_i))$  \n",
    "$g_i(x) = [ln(\\operatorname{numerator}) - ln(\\operatorname{denominator}) + \\operatorname{exponent}] + ln(p(y_i))$  \n",
    "$g_i(x) = ln(1)-ln(\\Sigma^{1/2} (2\\pi)^{D/2}) + \\frac{-1}{2}M  + ln(p(y_i))$   \n",
    "$g_i(x) = 0 -\\frac{1}{2}ln(\\Sigma) -\\frac{D}{2}ln(2\\pi) -M/2 + ln(p(y_i))$   \n",
    "The middle term is constant w.r.t. class i, so ignore it.   \n",
    "$g_i(x) = -\\frac{1}{2}ln(\\Sigma) -M/2 + ln(p(y_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: Features are independent (no covariance), features have same variance \n",
    "Graphical interpretation in 2D:   \n",
    "every PDF (circle) has different mean but the same radius.\n",
    "Boundary is perpendicular line between circles.\n",
    "\n",
    "The decision boundary is a hyperplance orthogonal to the line between the means.   \n",
    "The placement of the hyperplane depends on the priors.  \n",
    "\n",
    "From above, we have:  \n",
    "$g_i(x) = -\\frac{1}{2}ln(\\Sigma) -M/2 + ln(p(y_i))$\n",
    "\n",
    "$\\Sigma$ is constant for every class i so ignore the first term of gi(x).  \n",
    "\n",
    "Every class i has the same variance. \n",
    "So the covariance matrix is diagonal of variances:   \n",
    "$\\Sigma = \\sigma^2 I$  \n",
    "\n",
    "Also $\\Sigma^{-1} = \\frac{1}{\\sigma^2}$   \n",
    "\n",
    "So, the Mahalnobis distance M,   \n",
    "$M = [(x - \\mu)^{T} \\Sigma^{-1} (x - \\mu)]$\n",
    "\n",
    "reduces to Euclidian distance M:   \n",
    "$M = \\frac{(x-\\mu_i)^2}{\\sigma^2}$\n",
    "\n",
    "and the second term is   \n",
    "$-\\frac{(x-\\mu_i)^2}{2\\sigma^2}$   \n",
    "\n",
    "So \n",
    "$g_i(x) = -\\frac{(x-\\mu_i)^2}{2\\sigma^2} + ln(p(y_i))$   \n",
    "$g_i(x) = -\\frac{x^2-2x\\mu_i+\\mu_i^2}{2\\sigma^2} + ln(p(y_i))$   \n",
    "\n",
    "But $x^2$ is independent of class i so it can be ignored.  \n",
    "$g_i(x) = -\\frac{-2x\\mu_i+\\mu_i^2}{2\\sigma^2} + ln(p(y_i))$   \n",
    "$g_i(x) = x\\frac{\\mu_i}{\\sigma^2} - \\frac{\\mu_i^2}{2\\sigma^2} + ln(p(y_i))$   \n",
    "$g_i(x) = w_i x + b_i + c_i$   \n",
    "Thus, $g_i(x)$ is a linear discriminant\n",
    "where w is the weight vector \n",
    "and vector b is the bias or threshold.   \n",
    "The prior (last term: ln(p(yi))) merely shifts the decision plane left or right.  \n",
    "\n",
    "If x is midway between the means of class i and class j,\n",
    "and both classes have equal variance,\n",
    "then it is equally likely to have come from either class,\n",
    "and the decision depends entirely on the priors (the last term).\n",
    "\n",
    "The lecture slides arrive at the same by different math.  \n",
    "Place x at the center of the two means\n",
    "because the probabilities of each class are the same\n",
    "and ln(P1/P2) = ln(1) = 0 erases the complex term\n",
    "leaving just half the sum of the means.\n",
    "\n",
    "Note.   \n",
    "Exteme prior can pull the boundary to one side of that mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: Features have same covariance per class \n",
    "Graphical interpretation in 2D:   \n",
    "every PDF (elipse) has different mean but same size and orientation.\n",
    "If the elipses are \"tilted\" right, so is the decision boundary.\n",
    "\n",
    "Same as case 1:   \n",
    "The decision boundary is still linear: a line, plane, or hyperplane.   \n",
    "Boundary placement depends on the priors.    \n",
    "Exteme prior can pull the boundary to one side of that mean.\n",
    "\n",
    "Different from case 1:   \n",
    "The plane may be \"tilted\" i.e. not orthogonal to the line between the means.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 3: Features have arbitrary variance and covariance \n",
    "Graphical interpretation in 2D:   \n",
    "every PDF (elipse) has different mean, different size, different orientation.\n",
    "\n",
    "The decision boundary could be any shape,\n",
    "including hypersphere or hyperparabola.\n",
    "\n",
    "For two PDFs with same mean but different variance,\n",
    "the contor lines are concentric.\n",
    "Points close in are likely from the PDF with low variance,\n",
    "but points far out are likely from the PDF with high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Belief Network\n",
    "Also called Bayesian Network.   \n",
    "From Duda & Hart.   \n",
    "Also [Jason Brownlee](https://machinelearningmastery.com/introduction-to-bayesian-belief-networks/)\n",
    "\n",
    "This network model tries to get around the Naive assumption.   \n",
    "What if features are not independent?   \n",
    "Take a middle road and let certain chosen features depend on others.\n",
    "\n",
    "P(a) is independent. P(b) is independent.    \n",
    "P(d) depends on b: P(d|b).     \n",
    "P(c) depends on a, d: P(c|a,d).   \n",
    "\n",
    "The BN must be designed with domain expertise.   \n",
    "Use a probabilistic graphical network such as a HMM, but use a DAG.   \n",
    "The graph models conditional independence (no edges) and dependence (edges).   \n",
    "\n",
    "Python implementation: PyMC3 on Theano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
