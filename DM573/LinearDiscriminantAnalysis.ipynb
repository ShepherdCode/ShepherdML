{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283f5100",
   "metadata": {},
   "source": [
    "# LDA = Linear Discriminant Analysis\n",
    "Using labeled data, maximize the (between-group)/(within-group) scatter.\n",
    "\n",
    "Supervised.   \n",
    "Separates the labels by linear combination of continuous features, assumed independent.   \n",
    "Lower-dimensional space maximizes between-vs-within class scatter.    \n",
    "The discriminant may not exist.    \n",
    "\n",
    "LDA was derived from Fisher's Discriminant Analysis. \n",
    "The statistics was extended for dimensionality reduction. \n",
    "\n",
    "Good tutorial [here](https://www.knowledgehut.com/blog/data-science/linear-discriminant-analysis-for-machine-learning). Good video from UVa [here](https://youtu.be/IMfLXEOksGc). Code sample [blog](https://www.python-engineer.com/courses/mlfromscratch/14-lda/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51adeefe",
   "metadata": {},
   "source": [
    "### Summary steps\n",
    "The labeled data points are in a d-dimensional space for d features.   \n",
    "Fisher's Score uses between-scatter in numerator and within-scatter in denominator.    \n",
    "There is a d-dimensional vector w that maximizes Fisher's Score.   \n",
    "Of all hyperplanes perpendicular to w, one maximally separates the classes.   \n",
    "Use least squares regression to find its intercept: wx+b=0.    \n",
    "Now w is linear combination of features.    \n",
    "But w is also a latent feature that maximally separates classes.    \n",
    "We can recursively add more latent features = dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe2581e",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "LDA is sensitive to its assumpitons.\n",
    "If these assumptions are violated, poor discriminant functions result.\n",
    "\n",
    "Assume independent features (for learning a linear combination).\n",
    "Assume independent data instances (for learning the placement or intercept of each discriminant.\n",
    "Assume the data are generated by one Gaussian distribution per class. \n",
    "Assume homoscedacity i.e. same variance everywhere in in each class.\n",
    "Thus, points from any one class form a circular sombrero,\n",
    "and all the sombreros are the same size. \n",
    "Look for a linear combination of features that explains the class labels.\n",
    "Finally, place lines or hyperplanes between sombreros.\n",
    "(Yes, LDA is rather simplistic. See QDA.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a600d57",
   "metadata": {},
   "source": [
    "Each discriminant function creates latent features (like an eigenvector).\n",
    "Each discriminant gets a discriminant score (like an eigenvalue).\n",
    "much like how each eigenvector has an eigenvalue. \n",
    "\n",
    "Say you use the top 3 linear discriminants.\n",
    "These would draw 3 hyperplanes,\n",
    "assigning points and populations to classes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c15b96",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "### Likelihood\n",
    "Consider each class using its assumed mean & stdev.\n",
    "In LDA, \n",
    "each point has a probability of coming from this Gaussian (computed with the PDF).\n",
    "The class as a whole has a likelihood based on these data.\n",
    "\n",
    "### Maximum likelihood\n",
    "In LDA, each point is assigned to the class model that explains it best.\n",
    "(Like K-means Clustering?)\n",
    "\n",
    "### Bayes decision rule.\n",
    "In LDA, draw a hyperplane between the Gaussians.\n",
    "Assign data points to classes based on which side of the line.\n",
    "The line placement can incorporate priors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7220ea",
   "metadata": {},
   "source": [
    "## LDA as classifier\n",
    "LDA is a classifier.\n",
    "But it is typically used for dimensionality reduction prior to classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102fa1ef",
   "metadata": {},
   "source": [
    "## Related methods\n",
    "### LDA and PCA\n",
    "It helps to run PCA before LDA. Why?\n",
    "### LDA vs QDA\n",
    "QDA = quadratic discriminant analysis.   \n",
    "QDA allows different variances and yields parabolic discriminants.  \n",
    "LDA assumes same variances and yields hyperplanes as discriminants.  \n",
    "### LDA and ANOVA\n",
    "ANOVA requires categorical independent variables, continuous dependent variables.   \n",
    "LDA requires continuous independent variables, categorical dependent variables.   \n",
    "### LDA and Logistic Regression\n",
    "LDA assumes normally distributed data.  \n",
    "Otherwise, use logistic regression.  \n",
    "### LDA and PCA\n",
    "LDA is related to eigenvalues of the covariance matrix and thus to PCA.  \n",
    "Part of computing the LDA involves computing the eigen decomposition of the (Scatter_Between)/(Scatter_Within) matrix.\n",
    "### Otsu's Method\n",
    "Otsu's method is related to LDA.   \n",
    "Otsu is for greyscale image analysis.   \n",
    "Otsu chooses the black/which threshold that maximizes pixel-to-class assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bda2f7b",
   "metadata": {},
   "source": [
    "## Scikit-Learn\n",
    "The scikit-learn [LDA](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) \n",
    "has these features.\n",
    "\n",
    "Inputs:\n",
    "* Solver algorithm. Choices: SVD avoids the covariance matrix; Eigen and Least Squares must compute covariance and can use shrinkage. \n",
    "* Shrinkage algorithm: for large covariance matrices, it is better to apply a shrinkage algorithm that preserves variance but removes outliers.\n",
    "* Priors: if not given, inferred from frequencies in the data.\n",
    "* Target dimensions: if not given, uses min(#classes-1,#features).\n",
    "\n",
    "Outputs:\n",
    "* The vectors w and the intercepts b.\n",
    "* Means (centroids) per class.\n",
    "* Explained variance ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db18573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
