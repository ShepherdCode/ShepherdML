{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283f5100",
   "metadata": {},
   "source": [
    "# LDA = Linear Discriminant Analysis\n",
    "Supervised learning.   \n",
    "Using labeled data, \n",
    "LDA projects to a lower-dimensional space \n",
    "to maximize the (between-group)/(within-group) scatter.    \n",
    "LDA separates the labels by a linear combination of features.\n",
    "\n",
    "For example, project 2D points onto a 1D line,\n",
    "but chose the line that maximizes between/within scatter.\n",
    "Or, project 3D points onto a 2D plane,\n",
    "but chose the plane that maximizes between/within scatter.\n",
    "\n",
    "For the two-class problem, the solution is computed directly using covariances.    \n",
    "For the multi-class problem, the solution requires an optimization step.   \n",
    "The compute relies on eigen decomposition of covB/covW = covW$^{-1}$covB.\n",
    "\n",
    "Assume the features are continuous and independent.   \n",
    "Assume there is one covariance matrix i.e. covaraince does not vary by class.    \n",
    "Geometrically, every class has different mean but the same ellipsoid distrution.  \n",
    "All the decision boundaries are linear (line, plane, hyperplane).   \n",
    "\n",
    "For classes with different ellipsoid distributions, \n",
    "step up to QDA = quadratic discriminant analysis, \n",
    "and estimate a covariance matrix for each class.\n",
    "QDA gives parabolic decision boundaries.\n",
    "\n",
    "LDA is infeasible at very high dimesions.   \n",
    "The discriminant may not exist.    \n",
    "\n",
    "LDA was derived from Fisher's Discriminant Analysis,\n",
    "extending the statistics to also do dimensionality reduction.    \n",
    "LDA was conceived for two-class problems,\n",
    "but was extended for multi-class problems.\n",
    "\n",
    "Good tutorial at [knowledgehut](https://www.knowledgehut.com/blog/data-science/linear-discriminant-analysis-for-machine-learning). \n",
    "Good video from [UVa](https://youtu.be/IMfLXEOksGc). \n",
    "Code sample at [python-engineer](https://www.python-engineer.com/courses/mlfromscratch/14-lda/).\n",
    "\n",
    "Formulas given on MultivariateStats\n",
    "for [two-class](https://multivariatestatsjl.readthedocs.io/en/latest/lda.html)\n",
    "and [multi-class](https://multivariatestatsjl.readthedocs.io/en/latest/mclda.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e26ccd5",
   "metadata": {},
   "source": [
    "## Define scatter\n",
    "Sum of the pair-wise distances.  \n",
    "Within-group scatter for every instance i in class c:    \n",
    "$\\Sigma_i(\\bar{x}_i-\\bar{\\mu_c})(\\bar{x}_i-\\bar{\\mu_c})^T$   \n",
    "Between-group scatter for all classes c vs overall mean $\\bar{\\mu}$:   \n",
    "$\\Sigma_c(\\bar{\\mu}_c-\\bar{\\mu})(\\bar{\\mu}_c-\\bar{\\mu})^T$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51adeefe",
   "metadata": {},
   "source": [
    "## LDA steps\n",
    "The labeled data points are in a d-dimensional space for d features.   \n",
    "Measure scatter within the same label, and between groups of labels.  \n",
    "Fisher's Score uses between-scatter in numerator and within-scatter in denominator.    \n",
    "There is a d-dimensional vector w that maximizes Fisher's Score.   \n",
    "Of all hyperplanes perpendicular to w, one maximally separates the classes.   \n",
    "Use least squares regression to find its intercept: wx+b=0.    \n",
    "Now w is linear combination of features.    \n",
    "But w is also a latent feature that maximally separates classes.    \n",
    "We can recursively add more latent features = dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe2581e",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "LDA is sensitive to its assumpitons.\n",
    "If these assumptions are violated, poor discriminant functions result.\n",
    "\n",
    "Assume independent features (for learning a linear combination).\n",
    "Assume independent data instances (for learning the placement or intercept of each discriminant.\n",
    "Assume the data are generated by one Gaussian distribution per class. \n",
    "Assume homoscedacity i.e. same variance everywhere in in each class.\n",
    "Thus, points from any one class form a circular sombrero,\n",
    "and all the sombreros are the same size. \n",
    "Look for a linear combination of features that explains the class labels.\n",
    "Finally, place lines or hyperplanes between sombreros.\n",
    "(Yes, LDA is rather simplistic. See QDA.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a600d57",
   "metadata": {},
   "source": [
    "Each discriminant function creates latent features (like an eigenvector).\n",
    "Each discriminant gets a discriminant score (like an eigenvalue).\n",
    "\n",
    "Say you use the top 3 linear discriminants.\n",
    "These would draw 3 hyperplanes,\n",
    "assigning points and populations to classes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c15b96",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "### Likelihood\n",
    "Consider each class using its assumed mean & stdev.\n",
    "In LDA, \n",
    "each point has a probability of coming from this Gaussian (computed with the PDF).\n",
    "The class as a whole has a likelihood based on these data.\n",
    "\n",
    "### Maximum likelihood\n",
    "In LDA, each point is assigned to the class model that explains it best.\n",
    "(Like K-means Clustering?)\n",
    "\n",
    "### Bayes decision rule\n",
    "In LDA, draw a hyperplane between the Gaussians.\n",
    "Assign data points to classes based on which side of the line.\n",
    "The line placement can incorporate priors.\n",
    "\n",
    "### Distance\n",
    "Use Mahalanobis distance which incorporates the covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7220ea",
   "metadata": {},
   "source": [
    "## LDA as classifier\n",
    "LDA is a classifier.\n",
    "But it is typically used for dimensionality reduction prior to classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102fa1ef",
   "metadata": {},
   "source": [
    "## Related methods\n",
    "### LDA and PCA\n",
    "LDA and PCA use eigenvalues of the covariance matrix.  \n",
    "LDA is supervised but PCA is unsupervised.\n",
    "\n",
    "At high dimensions, LDA overfits the scatter of the data seen.  \n",
    "This is because scatter just happens, even in random data, \n",
    "when the number of dimensions approaches the number of data points.\n",
    "A shrinkage method, such as PCA, can help.\n",
    "Doing PCA before LDA will reduce the dimensions for LDA.\n",
    "### LDA vs QDA\n",
    "LDA assumes same variances and covariances.   \n",
    "LDA yields hyperplanes as discriminants.   \n",
    "QDA = quadratic discriminant analysis.    \n",
    "QDA allows different variances and covariances.   \n",
    "QDA yields parabolic discriminants.  \n",
    "### LDA and ANOVA are opposites\n",
    "ANOVA requires categorical independent variables, continuous dependent variables.   \n",
    "LDA requires continuous independent variables, categorical dependent variables.   \n",
    "### LDA and Logistic Regression\n",
    "LDA assumes normally distributed data.  \n",
    "Otherwise, use logistic regression.  \n",
    "### Otsu's Method\n",
    "Otsu's method is related to LDA.   \n",
    "Otsu is for greyscale image analysis.   \n",
    "Otsu chooses the black/which threshold that maximizes pixel-to-class assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bda2f7b",
   "metadata": {},
   "source": [
    "## Scikit-Learn\n",
    "The scikit-learn [LDA](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) \n",
    "has these features.\n",
    "\n",
    "Inputs:\n",
    "* Solver algorithm. Choices: Eigen and Least_Squares both compute covariance, and both can use shrinkage methods; SVD avoids the covariance matrix. \n",
    "* Shrinkage algorithm: for large covariance matrices i.e. high dimensions, it is recommended  to apply a shrinkage algorithm that preserves variance but removes outliers.\n",
    "* Priors: if not given, inferred from frequencies in the data.\n",
    "* Target dimensions: if not given, uses min(#classes-1,#features).\n",
    "\n",
    "Outputs:\n",
    "* The vectors w and the intercepts b.\n",
    "* Means (centroids) per class.\n",
    "* Explained variance ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db18573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
