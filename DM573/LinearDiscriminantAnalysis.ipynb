{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f6c6e5",
   "metadata": {},
   "source": [
    "# LDA = Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292e5400",
   "metadata": {},
   "source": [
    "## LDA\n",
    "Original form invented by Fisher. Since extended to LDA and dimensionality reduction. Good tutorial [here](https://www.knowledgehut.com/blog/data-science/linear-discriminant-analysis-for-machine-learning). Good video from UVa [here](https://youtu.be/IMfLXEOksGc). Code sample [blog](https://www.python-engineer.com/courses/mlfromscratch/14-lda/).\n",
    "\n",
    "Supervised learning, maximizes between/within scatter.\n",
    "\n",
    "This is rather simplistic. \n",
    "It assumes each class is generated by a Guassian with the same variance.\n",
    "That is, each class forms a circular sombrero of same size. \n",
    "If these assumptions are violated, poor discriminant functions result.\n",
    "The LDA discriminators between classes are lines or hyperplanes.\n",
    "\n",
    "One step up is QDA, quadratic discriminant analysis, \n",
    "which allows different variances, and yields parabolic discriminants.\n",
    "\n",
    "Assume independent features (for learning a linear combination).\n",
    "Assume independent data instances (for learning the placement or intercept of each discriminant.\n",
    "Assume the data are generated by one Gaussian distribution per class. \n",
    "Assume homoscedacity i.e. same variance everywhere. \n",
    "Look for a linear combination of features that explains the class labels.\n",
    "\n",
    "Uses likelihood.\n",
    "Consider each class using its assumed mean & stdev.\n",
    "Each point has a probability of coming from this mean, & stdev (use the Gaussian PDF).\n",
    "The class has a likelihood based on these data.\n",
    "\n",
    "Uses maximum likelihood. \n",
    "Each point is assigned to the class model that explains it best.\n",
    "\n",
    "Invokes the Bayes decision rule.\n",
    "Draw a line between Gaussians and assign points to classes\n",
    "based on whether they are left or right of the line.\n",
    "The line placement can incorporate priors.\n",
    "\n",
    "Each discriminant function creates latent features and gets a discriminant score, much like how each eigenvector has an eigenvalue. The top 3 linear discriminants would draw 3 lines or hyperplanes between populations.\n",
    "\n",
    "Otsu's method is related. In greyscale image analysis, it chooses the black/which threshold that maximizes pixel-to-class assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ade23",
   "metadata": {},
   "source": [
    "LDA is a classifier but it is typically used for dimensionality reduction prior to classification. LDA is related to eigenvalues of the covariance matrix and thus to PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f5419c",
   "metadata": {},
   "source": [
    "Here is my take. The labeled data points are in a d-dimensional space for d features. Fisher's Score uses between-scatter in numerator and within-scatter in denominator. There is a d-dimensional vector w that maximizes Fisher's Score. Of all hyperplanes perpendicular to w, one maximally separates the classes. Use least squares regression to find its intercept: wx+b=0. Now w is linear combination of features but also a latent feature that maximally separates classes. We can recursively add more latent features = dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5df124a",
   "metadata": {},
   "source": [
    "LDA is associated with eigenvalues. Part of computing the LDA involves computing the eigen decomposition of the Scatter_Between/Scatter_Within matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fa3e33",
   "metadata": {},
   "source": [
    "The scikit-learn [LDA](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) \n",
    "has these features.\n",
    "\n",
    "Inputs:\n",
    "Solver algorithm: SVD avoids the covariance matrix; Eigen and Least Squares must compute covariance and can use shrinkage. \n",
    "Shrinkage algorithm: for large covariance matrices, it is better to apply a shrinkage algorithm that preserves variance but removes outliers.\n",
    "Priors: if not given, inferred from frequencies in the data.\n",
    "Target dimensions: if not given, uses min(#classes-1,#features).\n",
    "\n",
    "Outputs:\n",
    "The vectors w and the intercepts b.\n",
    "Means (centroids) per class.\n",
    "Explained variance ratio. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
