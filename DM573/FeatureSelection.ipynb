{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "Too many features is a problem due to\n",
    "\n",
    "* NumFeatures > NumInstances\n",
    "* Curse of dimensionality\n",
    "* Worse-than-linear algorithm complexity\n",
    "* Model confusion from irrelevent features\n",
    "* Overfitting to variance in irrelevant features\n",
    "\n",
    "After FEATURE EXTRACTION,   \n",
    "we need to reduce complexity by  \n",
    "FEATURE SELECTION or DIMENSIONALITY REDUCTION.\n",
    "\n",
    "But finding the optimal subset is NP-hard.\n",
    "Also, cannot just use features with high correlation (predictive value)\n",
    "due to important feature-interaction effects. \n",
    "So there are heuristics and domain-specific rules.\n",
    "* Which features can we do without? \n",
    "* Which features can we ignore? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separability\n",
    "See our notebook on separability.  \n",
    "Separability over all features sets the upper limit on the learning accuracy.  \n",
    "Max-Min problem: maximally reduce features while minimally reducing separability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "Find features that are highly correlated to the lables.\n",
    "\n",
    "Pearson's correlation between feature f and class y   \n",
    "$R = \\frac {cov(f,y)} {\\sqrt{var(f)var(y)}}$   \n",
    "$R = \\frac {\\sum[(f-\\bar{f})(y-\\bar{y})]} {\\sqrt{\\sum[(f-\\bar{f})^2\\sum(y-\\bar{y})^2]}}$ \n",
    "Usually use $|R|$ or $R^2$.  \n",
    "\n",
    "Problem: Correlation only detects linear relationships.  \n",
    "Important features may be predictive of class collectively \n",
    "but uncorrelated to class individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information\n",
    "Find a feature subset that conveys the same information \n",
    "as the labels. \n",
    "\n",
    "Compared to correlation, \n",
    "mutual information detects non-linear relationships,\n",
    "but is harder to compute.\n",
    "\n",
    "Information:  \n",
    "$I(x) = p(x) ln(p(x))$  \n",
    "\n",
    "Mutual information between feature xi and class y,\n",
    "uses the joint probability divided by the marginal probabilities:  \n",
    "$I(x_i,y) = p(x_i,y) ln[p(x_i,y)/p(x_i)p(y)]$\n",
    "\n",
    "With discrete variables, \n",
    "and using frequencies as empirical estimates of probabilities:  \n",
    "$I(X,Y) = \\sum_x \\sum_y P(X=x,Y=y) ln[P(X=x,Y=y)/P(X=x)P(Y=y)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Random forest ranks features by importance,\n",
    "and you can try using the top-ranked features.\n",
    "\n",
    "Random Forest computes \n",
    "Gini Impurity or Information Gain at each node of each decision tree.\n",
    "The overall importance is a combination of these.\n",
    "\n",
    "This is a form of random guess plus hill climbing. \n",
    "This is supervised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter vs Wrapper methods\n",
    "Empirically, try classification while leaving out some features.\n",
    "\n",
    "### Filter methods\n",
    "These are independent of any model.   \n",
    "Also called unsupervised.   \n",
    "May not be optimal for a chosen model.    \n",
    "\n",
    "Examples: \n",
    "* Choose features that improve clustering and reduce outliers.\n",
    "* Ignore some features that correlate with other features.\n",
    "* Choose features that correlate with the label (supervised).\n",
    "\n",
    "### Wrapper methods\n",
    "These are specific to a model.   \n",
    "Also called supervised.   \n",
    "Test goodness by cross-validation using the model of interest.  \n",
    "\n",
    "Methods for feature selection with a wrapper \n",
    "* Forward selection ... start with one feature and iteratively add more.   \n",
    "* Reverse selection ... start with full feature set and iteratively remove a feature. \n",
    "* Bidirectional selection ... after each feature addition, check if any previous could now be removed.\n",
    "\n",
    "Note these are greedy and compute-intense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
