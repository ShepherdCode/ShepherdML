{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b430b8cb",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "This is a linear classifier. \n",
    "It finds a linear decision boundary.\n",
    "It is optimal in terms of maximum likelihood.\n",
    "Use a regularization term to avoid overfitting.\n",
    "\n",
    "Subtle difference with Bayesian classifier:\n",
    "* Bayes: Assume each class has a characteristic PDF of features. Choose a boundary.\n",
    "* Logistic: Assume class is predictable from features. Find the boundary.\n",
    "\n",
    "This is not regression.\n",
    "It is so named because it is similar to linear regression. \n",
    "Logistic regression uses a logistic function where linear regression uses squared error.\n",
    "\n",
    "Slides and videos from Herman Kamper [link](https://www.kamperh.com/data414/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa9c114",
   "metadata": {},
   "source": [
    "## Logistic\n",
    "Parameter k controls steepness, usually 1.   \n",
    "Parameter m is midpoint, usually 0.   \n",
    "Parameter L is maximum, usually 1.   \n",
    "Logistic = $f(x) = \\frac{L}{1+e^{-k(x-m)}}$    \n",
    "\n",
    "The standard logistic function is sigmoid:   \n",
    "Sigmoid = $f(x) = \\frac{1}{1+e^{-x}}$    \n",
    "\n",
    "Sigmoid is the inverse of the logit function.   \n",
    "The logit is the log of the odds of a probability i.e. ln(P(true)/P(false))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3869606b",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "The boundary is a hyperplane.  \n",
    "$sum=(\\Theta_0 + \\Sigma_{i=1}^d \\Theta_i x_i )$  \n",
    "Constant $\\Theta_0$ is the offset.  \n",
    "$P(class=+1|\\bar{X}) = \\frac{1}{1+e^{-sum}}$ = logistic(sum)  \n",
    "$P(class=-1|\\bar{X}) = \\frac{1}{1+e^{sum}}$  \n",
    "The two probabilities always sum to 1.  \n",
    "Points in class +1 have positive sum, tiny denominator, prob close to 1.   \n",
    "Points in class -1 have negative sum, huge denominator, prob close to 0.   \n",
    "Points on the boundary have sum=0, denominator=2, prob=1/2.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb016f",
   "metadata": {},
   "source": [
    "## Training\n",
    "The classifier is trained by maximum likelihood estimation.  \n",
    "Let PPP = Product of P(class) for all points.   \n",
    "Likelihood = (PPP(class=+1)) * (PPP(class=-1))  \n",
    "Log likelihood = (sum(log(P(class=+1))) + sum(log(P(class=-1))))   \n",
    "How to maximize the log likelihood?   \n",
    "Lacking a closed form solution, we use gradient descent.   \n",
    "The gradient is computed by the partial derivatives of the log likelihood.   \n",
    "Likelihood is concave and gradient descent finds the global optimum.\n",
    "\n",
    "$L(\\Theta) = \\Pi \\frac{1}{1+e^{-sum}} \\Pi \\frac{1}{1+e^{sum}}$   \n",
    "$LL(\\Theta) = (-\\Sigma [log(1+e^{-sum})]) + (-\\Sigma [log(1+e^{sum})])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed9472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
