{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c782200",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "This is a linear classifier.\n",
    "\n",
    "Compare to Bayesian classifier:\n",
    "* Bayes: Assume each class has a characteristic PDF of features. Choose a boundary.\n",
    "* Logistic: Assume class membership prob is a function of the features. Find the boundary.\n",
    "\n",
    "This is not regression.\n",
    "It is so named because it is similar to linear regression \n",
    "(but uses a logistic function instead of the squared error)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4404201",
   "metadata": {},
   "source": [
    "## Logistic\n",
    "Parameter k controls steepness, usually 1.   \n",
    "Parameter m is midpoint, usually 0.   \n",
    "Parameter L is maximum, usually 1.   \n",
    "Logistic = $f(x) = \\frac{L}{1+e^{-k(x-m)}}$    \n",
    "\n",
    "The standard logistic function is sigmoid:   \n",
    "Sigmoid = $f(x) = \\frac{1}{1+e^{-x}}$    \n",
    "\n",
    "Sigmoid is the inverse of the logit function.   \n",
    "The logit is the log of the odds of a probability i.e. ln(P(true)/P(false))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8572b",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "The boundary is a hyperplane.  \n",
    "$sum=(\\Theta_0 + \\Sigma_{i=1}^d \\Theta_i x_i )$  \n",
    "Constant $\\Theta_0$ is the offset.  \n",
    "$P(class=+1|\\bar{X}) = \\frac{1}{1+e^{-sum}}$ = logistic(sum)  \n",
    "$P(class=-1|\\bar{X}) = \\frac{1}{1+e^{sum}}$  \n",
    "The two probabilities always sum to 1.  \n",
    "Points in class +1 have positive sum, tiny denominator, prob close to 1.   \n",
    "Points in class -1 have negative sum, huge denominator, prob close to 0.   \n",
    "Points on the boundary have sum=0, denominator=2, prob=1/2.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8708b5e5",
   "metadata": {},
   "source": [
    "## Training\n",
    "The classifier is trained by maximum likelihood estimation.  \n",
    "Let PPP = Product of P(class) for all points\n",
    "Likelihood = (PPP(class=+1)) * (PPP(class=-1))  \n",
    "Log likelihood = (sum(log(P(class=+1))) + sum(log(P(class=-1))))   \n",
    "How to maximize the log likelihood?   \n",
    "Lacking a closed form solution, we use gradient descent.   \n",
    "The gradient is computed by the partial derivatives of the log likelihood.\n",
    "\n",
    "$L(\\Theta) = \\Pi \\frac{1}{1+e^{-sum}} \\Pi \\frac{1}{1+e^{sum}}$   \n",
    "$LL(\\Theta) = (-\\Sigma [log(1+e^{-sum})]) + (-\\Sigma [log(1+e^{sum})])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a25817f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
