{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors\n",
    "Classification.   \n",
    "Lazy: no training required.   \n",
    "Local, not global.  \n",
    "Predictive, not generative.  \n",
    "Supervised, requires labeled data.   \n",
    "Instance-based learning: model-free.  \n",
    "Non-parametric: no normality assumption (but K is a critical parameter).   \n",
    "Discrete/classification or continuous/regression implementations.  \n",
    "Deductive: deduces a specific label from the general labeled data.\n",
    "Not inductive (infer a rule or pattern), unless you use it to induce decision boundaries.    \n",
    "Depends on a proxmity measure.  \n",
    "Makes non-linear, completely arbitrary, decision boundaries.  \n",
    "Sensitive to scale. Big features dominate the proximity. Normalize each feature first.  \n",
    "Susceptible to noise, especially when K is small.  \n",
    "Difficult to make predictions on data points missing some feature values.  \n",
    "Robust to feature interaction effects; but proximity function could weight these features.     \n",
    "Large numbers of irrelevant or redundant features can swamp the proximity measure.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "Project points into a (possibly lower) dimensional space.\n",
    "Given an unlabeled point P, find its K nearest neighbors.\n",
    "Label P according to average or mode of these neighbors.\n",
    "\n",
    "### Decision rules\n",
    "Choose one\n",
    "1. Winner-take-all.\n",
    "Predict the class label of the majority of neighbors. \n",
    "This is used for classification of discrete data.\n",
    "1. Predict the mediod of the neighbors. \n",
    "This is representative instance that actually exists. \n",
    "This is analagous to median and is less susceptible to outliers.\n",
    "This is used for classifcation of discrete data.\n",
    "1. Predict the centroid of the neigbhors. \n",
    "This is a point in space analogous to the mean. \n",
    "This is used for regression on continuous data.\n",
    "1. Apply weights to the neighbors.\n",
    "Weight each neighbor by 1/distance from the given point.\n",
    "Then, use one of the predictors above.\n",
    "This is less susceptible to outliers.\n",
    "\n",
    "### Discussion\n",
    "\n",
    "At K=1, kNN reduces noise: converting the input point to a representative one.\n",
    "At K=1, kNN converts an unlabled given point to a labeled one from the training set.\n",
    "\n",
    "At K>=1, kNN applies the majority class label among the K nearest neighbors.  \n",
    "\n",
    "The distance metric is usually Euclidean but could be Manhattan, etc.   \n",
    "\n",
    "The inference could be uniform/unweighted, so the K nearest neighbors contribute equally.\n",
    "Or it could be Gaussian/weighted so closest neighbors contribute the most.  \n",
    "\n",
    "The decision boundaries are extremely non-linear, regardless of K.\n",
    "Thus the results are sensitive to distributions, sampling, noise, unequal scaling.  \n",
    "\n",
    "kNN performs poorly when #dimensions > #data.\n",
    "\n",
    "Accuracy guarrantees are possible for various special cases, such as:\n",
    "K=1; binary classification on infinite data; etc.\n",
    "\n",
    "### Optimiztions\n",
    "\n",
    "For speed,\n",
    "use a reduced set of points called prototypes.\n",
    "This optimization is called condensed nearest neighbors.\n",
    "\n",
    "For speed, \n",
    "do some pre-processing to reduce avoid computing point-to-all distances.\n",
    "Precompute the pair-wise distances, sort the data, organize it as a graph or tree.\n",
    "This can reduce prediction from O(n) to O(log(n))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of K\n",
    "The best value for critical parameter K is anybody's guess.  \n",
    "It is done by heuristics. \n",
    "Or by bootstrap (testing various K on random subsets).\n",
    "Choose odd K for binary classification (just to avoid tie votes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of distance metric\n",
    "Conceive of points or vectors in N-dimentional space.  \n",
    "Usually use Euclidean distances on real data,\n",
    "Hamming distance on discrete data e.g. word frequencies of documents. \n",
    "The best distance metric can be learned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN and Other Algorithms\n",
    "Self organizing maps (SOM)\n",
    "uses the same induction rule as kNN, but uses a modified search space. \n",
    "\n",
    "KNN by itself performs poorly on high-dimensional data due to \n",
    "curse of dimensionality.\n",
    "For real-time forecasting on high-dimensional big data, KNN is applied to:\n",
    "* sketeches\n",
    "* locality sensitive hashing codes\n",
    "* the data after feature extraction\n",
    "* the data after Haar face detection\n",
    "* clusters labeled by mean-shift segmentation\n",
    "* data transformed by PCA  \n",
    "\n",
    "KNN can be used for outlier detection.\n",
    "Choose a K and R. Visit every point. \n",
    "Define outlier as any point with more than R/K neighbors labeled differently. \n",
    "\n",
    "The K Nearest Neighbors classifier is not related to K Means Clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Four numerical features\n",
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categorical labels\n",
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo run\n",
    "See model parameters at \n",
    "[sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random partition into train/test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 neighbors, 1.00 % accuracy\n",
      "2 neighbors, 0.97 % accuracy\n",
      "3 neighbors, 1.00 % accuracy\n",
      "4 neighbors, 0.97 % accuracy\n",
      "5 neighbors, 0.97 % accuracy\n",
      "6 neighbors, 0.97 % accuracy\n",
      "7 neighbors, 0.97 % accuracy\n",
      "8 neighbors, 0.95 % accuracy\n",
      "9 neighbors, 0.95 % accuracy\n"
     ]
    }
   ],
   "source": [
    "# Try various values of K\n",
    "for neighbors in range(1,10):\n",
    "    knn = KNN(neighbors)\n",
    "    knn.fit(X_train,y_train)   # does nothing but store the points and labels!\n",
    "    y_pred = knn.predict(X_test)    # aligns test points to training points!\n",
    "    scores = metrics.accuracy_score(y_test,y_pred)\n",
    "    print('%d neighbors, %4.2f %% accuracy' % (neighbors,scores) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance\n",
    "We see accuracy is highly variable. \n",
    "Run this notebook again, you get different results. \n",
    "But this is attributable to the random train/test split.\n",
    "If you use one train/test split, and just rerun the KNN cell, the results are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
