{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors\n",
    "Classification. Lazy (no training). Supervised. Non-parametric. Requires scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic idea: incorporate neighbor data to be robust to noise. \n",
    "At prediction time, given an instance, find K nearest neighbors, \n",
    "predict major class among those.\n",
    "\n",
    "Naive O(N): compute instance-vs-all.    \n",
    "Pre-processing O(lg(N)): sort or compute pair-wise distances, organize as a tree.    \n",
    "\n",
    "Problems:  \n",
    "The decision boundaries are extremely non-linear at any K.\n",
    "Thus the results are sensitive to noise and unequal scaling.\n",
    "\n",
    "There are accuracy guarrantees for various special cases\n",
    "(K=1; binary classification on infinite data; etc.).\n",
    "\n",
    "In condensed nearest neighbors, \n",
    "use a reduced set of prototypes instead of all the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision rule\n",
    "Possible return values:  \n",
    "The winner-take-all class of the majority of neighbors.  \n",
    "The centroid: a representative value, point in space, mean. \n",
    "For continuous data and regression.   \n",
    "Mediod: a representative instance, actually exists, median. \n",
    "For discrete data and classifcation.  \n",
    "A weighted mean: weight each neighbor by 1/distance.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of K\n",
    "Done by heuristics. \n",
    "Or by bootstrap (testing various K on random subsets).\n",
    "Choose odd K for binary classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of distance metric\n",
    "Conceive of points or vectors in N-dimentional space.  \n",
    "Usually use Euclidean distances on real data,\n",
    "Hamming distance on discrete data e.g. word frequencies of documents. \n",
    "The best distance metric can be learned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relations to other algorithms\n",
    "Self organizing maps (SOM): same idea on a modified search space. \n",
    "\n",
    "KNN is often the classifier used after feature extraction\n",
    "by Haar face detection, or mean-shift, or PCA, etc.  \n",
    "KNN by itself performs poorly on high-dimensional data due to \n",
    "curse of dimensionality.\n",
    "For real-time forecasting on high-dimensional big data,\n",
    "KNN is applied to sketeches or locality sensitive hashing.\n",
    "\n",
    "KNN can be used for outlier detection.\n",
    "Choose a K, find instances that are misclassified.\n",
    "Remove outliers with more than R neighbors of another class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
