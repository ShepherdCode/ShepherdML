{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors\n",
    "Classification.   \n",
    "Lazy (no training).   \n",
    "Local.  \n",
    "Predictive.  \n",
    "Supervised.   \n",
    "Instance-based learning (model-free).  \n",
    "Non-parametric (no normality assumption, but K is a critical parameter).   \n",
    "Discrete/classification or continuous/regression implementations.  \n",
    "Deductive (deduce a specific from the general) predicting one label from several).    \n",
    "Not inductive (infer a rule or pattern), unless you use it to draw decision boundaries.    \n",
    "Depends on a proxmity measure.  \n",
    "Makes non-linear (completely arbitrary) decision boundaries.  \n",
    "Susceptible to scaling (big features dominate the distance).  \n",
    "Susceptible to noise (esp with small k).  \n",
    "Difficult to use with missing values.  \n",
    "Robust to feature interaction effects.  \n",
    "Susceptible to large numbers of irrelevant or redundant features (swamps distance measure).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic ideas:    \n",
    "\n",
    "Project points into a (possibly lower) dimensional space.\n",
    "Reduce noise by converting the input point to a more representative one.\n",
    "Or, convert an unlabled given point to a labeled one from the training set.\n",
    "Or, infer a label using the major class of the K nearest neighbors.  \n",
    "\n",
    "Variations:  \n",
    "\n",
    "The choice of K is important yet entirely guess work.  \n",
    "\n",
    "The distance metric is usually Euclidean but could be Manhattan, etc.   \n",
    "\n",
    "Inference could be uniform (all K nearest neighbors contribute equally)\n",
    "or otherwise (apply Gaussian weight so closest neighbors contribute more).  \n",
    "\n",
    "The decision boundaries are extremely non-linear at any K.\n",
    "Thus the results are sensitive to distributions, sampling, noise, unequal scaling.  \n",
    "\n",
    "Speed. Instead of using all the points, \n",
    "use a reduced set of prototypes (called condensed nearest neighbors).\n",
    "\n",
    "Speed. Instead of naive instance-vs-all, which is linear,\n",
    "do pre-processing to make it logarithmic.\n",
    "Precompute pair-wise distances, sort the data, organize as a tree.    \n",
    "\n",
    "Accuracy guarrantees.\n",
    "These have been worked out for various special cases:\n",
    "K=1; binary classification on infinite data; etc.\n",
    "\n",
    "Decision rule.\n",
    "(1) Winner-take-all i.e. use the class of the majority of neighbors.\n",
    "(2) The centroid i.e. a representative value, a point in space.\n",
    "This is analogous to the mean. \n",
    "This is used for continuous data and regression.\n",
    "(3) Mediod i.e. a representative instance, one that actually exists. \n",
    "This is analagous to median. \n",
    "This is used for discrete data and classifcation.\n",
    "(4) Weighted mean i.e. weight each neighbor by 1/distance and compute the mean.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of K\n",
    "Done by heuristics. \n",
    "Or by bootstrap (testing various K on random subsets).\n",
    "Choose odd K for binary classification (just to avoid tie votes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of distance metric\n",
    "Conceive of points or vectors in N-dimentional space.  \n",
    "Usually use Euclidean distances on real data,\n",
    "Hamming distance on discrete data e.g. word frequencies of documents. \n",
    "The best distance metric can be learned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relations to other algorithms\n",
    "Self organizing maps (SOM): same idea on a modified search space. \n",
    "\n",
    "KNN is often the classifier used after feature extraction\n",
    "by Haar face detection, or mean-shift, or PCA, etc.  \n",
    "KNN by itself performs poorly on high-dimensional data due to \n",
    "curse of dimensionality.\n",
    "For real-time forecasting on high-dimensional big data,\n",
    "KNN is applied to sketeches or locality sensitive hashing.\n",
    "\n",
    "KNN can be used for outlier detection.\n",
    "Choose a K, find instances that are misclassified.\n",
    "Remove outliers with more than R neighbors of another class.\n",
    "\n",
    "K Nearest Neighbors is not related to K Means Clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data: Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo run\n",
    "See model parameters at \n",
    "[sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 neighbors, 0.95 % accuracy\n",
      "2 neighbors, 0.89 % accuracy\n",
      "3 neighbors, 0.95 % accuracy\n",
      "4 neighbors, 0.92 % accuracy\n",
      "5 neighbors, 0.89 % accuracy\n",
      "6 neighbors, 0.92 % accuracy\n",
      "7 neighbors, 0.92 % accuracy\n",
      "8 neighbors, 0.95 % accuracy\n",
      "9 neighbors, 0.92 % accuracy\n"
     ]
    }
   ],
   "source": [
    "for neighbors in range(1,10):\n",
    "    knn = KNN(neighbors)\n",
    "    knn.fit(X_train,y_train)   # does nothing but store the points and labels!\n",
    "    y_pred = knn.predict(X_test)    # aligns test points to training points!\n",
    "    scores = metrics.accuracy_score(y_test,y_pred)\n",
    "    print('%d neighbors, %4.2f %% accuracy' % (neighbors,scores) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance\n",
    "We see accuracy is highly variable. \n",
    "Run this notebook again, you get different results. \n",
    "But this is attributable to the random train/test split.\n",
    "If you use one train/test split, and just rerun the KNN cell, the results are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
