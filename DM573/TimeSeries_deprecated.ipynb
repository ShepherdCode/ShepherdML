{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series\n",
    "Formerly TimeSeries_01   \n",
    "\n",
    "This was just a first take. See version 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series models:\n",
    "* trend? \n",
    "** use linear, quadratic, or exponential least squares\n",
    "** or use autoregressive model\n",
    "* no trend? \n",
    "** use moving average \n",
    "** or use exponential smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal models:\n",
    "* use linear regression to compute coefficients of a model\n",
    "* model can be linear, quadratic, or exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplicative time series models:  \n",
    "* Y = T * C * S * I = trend * cycles * seasonal effects * irregular effects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving average models\n",
    "This is an attempt to deal with noise.  \n",
    "Think of each new data point as the middle of a window.   \n",
    "Window size should be odd for plots using window centers.   \n",
    "Gives equal weight to every part of the window.   \n",
    "This stabilizes the trend with respect to a recent outlier.  \n",
    "Describes the past; does not (directly) predict the future.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:  \n",
    "* Given data D1, D2, D3.\n",
    "* Compute mean M2 = mean(D1,D2,D3).\n",
    "* Plot M2 at time=2. \n",
    "* Given more data, we could fit a line to the plot, and predict the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential smoothing\n",
    "Whereas moving average uses a fixed size window of arbitrary size...   \n",
    "Exponential smoothing incorporates all prior data.   \n",
    "A parameter determines the relative weight of recent vs distant past.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w$: the mixing weight parameter, between zero and one.   \n",
    "$D_i$: incomimg data at time i.   \n",
    "$E_i$: exponential smoothing factor, computed with Di and Di-1, for use forecasting i+1.  \n",
    "$Y_i$: forecast for Di is Ei-1, which was computed before seeing the actual Di.   \n",
    "\n",
    "Example:\n",
    "* Given D1.  E1= D1,  Y1=n/a.    \n",
    "* Given D2.  E2= wD2 + (1-w)E1,  Y2=E1.   \n",
    "* Given D3.  E3= wD3 + (1-w)E2,  Y3=E2.      \n",
    "* Given D4.  E4= wD4 + (1-w)E3,  Y4=E3.  \n",
    "* Note exponential decay of past values: E3 = wD3 + w(1-w)D2 + (1-w)^2 D1.\n",
    "* Note exponential decay of past values: E4 = wD4 + w(1-w)D3 + w(1-w)^2 D2 + (1-w)^3 D1.\n",
    "\n",
    "Example: w=0.8 upweights the recent past\n",
    "* Given D1.  E1= D1,  Y1=n/a.    \n",
    "* Given D2.  E2= (.8)D2 + (.2)D1,  Y2=E1.   \n",
    "* Given D3.  E3= (.8)D3 + (.2)E2,  Y3=E2.     \n",
    "* Given D4.  E4= (.8)D4 + (.2)E3,  Y4=E3. \n",
    "* Note eponential decay: E3 = (.8)D3 + (.16)D2 + (.04)D1. \n",
    "* Note eponential decay: E4 = (.8)D4 + (.16)D3 + (.032)D2 + (.008)D1. \n",
    "* D1 accounts for less than 1% of the forecast at time 4.\n",
    "* Note sum of coefficients is 1.    \n",
    "\n",
    "Example: w=0.2 upweights the longer trend\n",
    "* Given D1.  E1= D1,  Y1=n/a.    \n",
    "* Given D2.  E2= (.2)D2 + (.8)D1,  Y2=E1.   \n",
    "* Given D3.  E3= (.2)D3 + (.8)E2,  Y3=E2.   \n",
    "* Given D4.  E4= (.2)D4 + (.8)E3,  Y4=E3.     \n",
    "* Note exponential decay: E3 = (.2)D3 + (.16)D2 + (.64)D1. \n",
    "* Note exponential decay: E4 = (.2)D4 + (.16)D3 + (.128)D2 + (.512)D1.\n",
    "* D1 still accounts for 50% of the forecast at time 4.     \n",
    "* Note sum of coefficients is 1.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual = error in prediction, portion of data left unexplained.  \n",
    "Ordinary Least Squares = minimize the sum of squared residuals (SSE).   \n",
    "\"Ordinary\" means as opposed to weighted or other scheme.   \n",
    "Perform linear regression to optimize the coefficients of a model.  \n",
    "Optimal means minimal SSE,\n",
    "so solve for where the partial derivatives are zero.\n",
    "\n",
    "Univariate = one regressor.   \n",
    "Multivariate = multiple regressors.  \n",
    "\n",
    "Can regress to any kind of model:\n",
    "* Linear: y = a + bx\n",
    "* Quadratic: y = a +bx + cx^2\n",
    "* Exponential: y = a + b * c^x\n",
    "* Log equivalent: log(y) = log(b) + x * log(c)\n",
    "\n",
    "Toy example with only 2 data points (solution is line between them):   \n",
    "Given data points $(x_1,y_1), (x_2,y_2)$, find a and b   \n",
    "$y_i = a + bx_i + r_i$  \n",
    "$y_i - (a + bx_i) = r_i$  \n",
    "$y_i^2 - 2y_i ( a + bx_i ) + [ a^2 + 2abx_i + (bx_i)^2 ] = r_i^2$   \n",
    "SSE = $\\Sigma r_i^2 $  \n",
    "SSE = $y_1^2+y_2^2 -2y_1(a+bx_1)-2y_2(a+bx_2) +2a^2 +2abx_1 +2abx_2 +b^2x_1^2 +b^2x_2^2$   \n",
    "$\\frac{\\delta SSE}{\\delta a} = -2y_1 -2y_2 +  4a + 2bx_1 + 2bx_2 $   \n",
    "$\\frac{\\delta SSE}{\\delta b} = -2y_1x_1 -2y_2x_2 + 2ax_1 + 2ax_2 + 2bx_1^2 + 2bx_2^2 $   \n",
    "Set both derivatives to zero to minimize SSE.   \n",
    "Substitute each data point for x and y.   \n",
    "Sum over all the data points.   \n",
    "Solve for a or b, substitute, and solve for the other.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from Wikipedia, worked out on paper:  \n",
    "Data = (1,6), (2,5), (3,7), (4,10)  \n",
    "\n",
    "$r_1^2 = [6 - (a+b)]^2$  \n",
    "$r_2^2 = [5 - (a+2b)]^2$  \n",
    "$r_3^2 = [7 - (a+3b)]^2$  \n",
    "$r_4^2 = [10 - (a+4b)]^2$  \n",
    "\n",
    "SSE = $36 - 12(a+b) + a^2 + 2ab + b^2$  \n",
    "+ $25 - 10(a+2b) + a^2 + 4ab + 4b^2$  \n",
    "+ $49 - 14(a+3b) + a^2 + 6ab + 9b^2$  \n",
    "+ $100 - 20(a+3b) + a^2 + 8ab + 16b^2$  \n",
    "\n",
    "SSE = $210 - 56a - 154b + 4a^2 + 20ab + 30b^2$  \n",
    "dSSE/da = -56 +8a + 20b = 0   \n",
    "dSSE/db = -154 + 20a + 60b = 0   \n",
    "Linear model: $y = a + bx + r = 3.5 + 1.4x + r$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA and ACF\n",
    "See [Baeldung](https://www.baeldung.com/cs/acf-pacf-plots-arma-modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity: I(d)\n",
    "The AR and MA algorithms assume stationary data,\n",
    "and produce falsely high-confidence correlations otherwise.  \n",
    "\n",
    "Most random walks (of +/- 1) are not stationary but they are correlated;\n",
    "rather than learning the correlation R value, \n",
    "we would rather learn that each delta is random.\n",
    "\n",
    "Stationary:\n",
    "* Mean is time-invariant -- can be accomplished with differencing \n",
    "* Variance is time-invariant -- homoscedacity\n",
    "* Seasonal fluctuations were removed -- can be accomplished with auto regression\n",
    "\n",
    "Statistical tests for stationarity: ADF or KPSS\n",
    "\n",
    "Differencing:\n",
    "* Convert the raw values to growth values: use t2-t1 at time t2.\n",
    "* Or conver to percents: use (t2-t1)/t1 at time t2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARMA and ARIMA\n",
    "Use a Combination of 3 models to predict value at time i.   \n",
    "The combination is controlled by 3 hyperparameters: ARIMA(p,d,q).   \n",
    "Set d=0 if the data are stationary.  \n",
    "Multivariate extensions have names like SARIMA and FARIMA.\n",
    "\n",
    "* AR(p) = p-order Auto Regression: a linear combination of times i-1 to i-p\n",
    "* I(d) = d-order Differencing: some constant amount of change, a steady increase by d\n",
    "* MA(q) = q-order Moving Average: the trend indicated by averages of windows of width q\n",
    "\n",
    "The AR(p) model predicts a linear function of the current + p preceeding values.  \n",
    "Choose p where ACF correlation tails off, or\n",
    "choose p where PACF correlation drops suddenly.\n",
    "\n",
    "The MA(q) model predicts a linear function of the current average over window q, plus q error terms, each a coefficient times a normally distributed error.  \n",
    "Choose q where ACF correlation drops suddenly, or\n",
    "choose q where PACF correlation tails off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACF and PACF \n",
    "Estimate the parameters of ARIMA.\n",
    "\n",
    "* ACF = complete auto-correlation function, assumes MA cut-off after lag q.\n",
    "* PACF = partial auto-correlation function, assumes AR cut-off after lag p.\n",
    "\n",
    "Both are plotted as vertical bars of height +1 to -1 over a background cigar.   \n",
    "Background = 95% confidence interval: outside the cigar is significant.  \n",
    "(Why does cigar get fatter going back in time? To account for multiple tests?)  \n",
    "The first vertical bar (lag=0) is always +1: time i is 100% correlated with itself.\n",
    "The second bar represents correlation of time i with time i-1 (lag=1).\n",
    "\n",
    "ACF measures correlation at exactly lag g.\n",
    "ACF tends to drop off slowly.   \n",
    "\n",
    "PACF measures correlation at lag g not explained by lag 0 to g-1.\n",
    "PACF tends to drop off fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIC and BIC\n",
    "To help choose the paramters, \n",
    "Information Criterion measures the ARIMA(p,d,q) performance.   \n",
    "\n",
    "Both reward log-likelihood.  \n",
    "Both punish the number of model parameters.  \n",
    "BIC (Bayesian) also punishes number of samples used for training.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble methods\n",
    "Decision trees (DT) are simple and random forest (RF) is popular.   \n",
    "RF is a bag of trees.  \n",
    "See [MachineLearningMastery](https://machinelearningmastery.com/random-forest-for-time-series-forecasting/)\n",
    "\n",
    "Bagging = bootstrap + aggregation\n",
    "* Bootstrap i.e. form data subsets by random sampling with replacement.\n",
    "* Train a model, such as DT, for each subset.\n",
    "* Aggregate i.e. by average of all predictions.\n",
    "\n",
    "Training\n",
    "* Transform the time series data into a (features,label) training set.\n",
    "* Conduct supervised learning.\n",
    "* Validate on unseen data. \n",
    "* Out-of-bag (OOB) error: measure each model on the data it didn't see.\n",
    "\n",
    "Cross-validation\n",
    "* Use walk-forward validation.\n",
    "* Choose a cut point. Train on data before the cut. Test on data after the cut.\n",
    "* For k-fold, walk the cut point forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residuals analysis\n",
    "Evaluate the quality of a forecasting model.   \n",
    "Measure total residual: mean absolute deviation (MAD) or sum of squares (SSE).  \n",
    "Look for bias: under at start, over at end, averages to zero.   \n",
    "Look for cycles: goes under and over every season, averages to zero.   \n",
    "Apply parsimony: prefer the simpler model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
