{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7cfa16",
   "metadata": {},
   "source": [
    "# Time Series\n",
    "This notebook was formerly named TimeSeries_02.\n",
    "\n",
    "See Charu Aggarwal, Data Mining, chapter 14, Time Series.\n",
    "\n",
    "Time series data have two components:\n",
    "1. behavioral e.g. the temperature measurement values\n",
    "1. contextual e.g. the temperature measurement times\n",
    "\n",
    "SIMILARITY:   \n",
    "To compare two time series, use:\n",
    "1. Euclidean distance (requires same number of time points)\n",
    "1. Edit distance (assume possible to mutate one into the other)\n",
    "1. Longest Common Substring (found by dynamic programming) (possibly recursive)\n",
    "1. Difference between their coefficients in wave transform (wavelet, Haar, Fourier)\n",
    "1. Align and compute distance after Dynamic Time Warp (DTW): measure differences after aligning periods with similar meaning (such as heart-valve-open and heart-valve-closed).\n",
    "1. Align and compute distance after Piecewise Aggregate Appoximiation (PAA). This uses mean or median of each bin. \n",
    "1. Align and compute distance after Symbolic Aggregate Approximation (SAX). This reduces continuous range to a few values of equal frequency. For example, turns a sine wave into a step function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4a7c8",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "Ideally, work with consecutive time points and no missing values.\n",
    "\n",
    "### Missing values\n",
    "INTERPOLATION:   \n",
    "Interpolate missing values if required. \n",
    "Linear interpolation is usually fine, but polynomial and spline can be used.\n",
    "\n",
    "### Smoothing and Noise Reduction\n",
    "PAA = piecewise aggregate approximation i.e. BINNING.\n",
    "Binning does smoothing and data reduction.\n",
    "Larger bin sizes provide more smoothing.\n",
    "Results can be sensitive to bin size.\n",
    "\n",
    "Apply non-overlapping windows. \n",
    "Replace each window with one statistic.\n",
    "Mean is more inclusive, but median is less sensitive to outliers.\n",
    "\n",
    "MOVING AVERAGE:   \n",
    "Moving average smoothing does data smoothing but not data reduction.\n",
    "\n",
    "Apply overlapping windows e.g. stride=1.\n",
    "Replace each window with one statistic.\n",
    "Mean is more inclusive, but median is less sensitive to outliers.\n",
    "\n",
    "Downsides: \n",
    "1. Window effect: You lose the first window of data\n",
    "1. Lag: Sudden big changes are hidden for a while\n",
    "1. Inversion: If wavelength is about half the window size, the waves can flip up/down.\n",
    "\n",
    "EXPONENTIAL SMOOTHING:   \n",
    "Exponential smoothing uses weighted average,\n",
    "so the most recent value counts more or less than the trend.\n",
    "Requires a smoothing parameter $\\alpha$. \n",
    "Larger values of $\\alpha$ emphasize the most recent value more.\n",
    "\n",
    "$\\hat{y}_{i} = (\\alpha)(y_{i})+(1-\\alpha)(\\hat{y}_{i-1})$   \n",
    "\n",
    "If $\\alpha=\\frac{1}{4}$ then   \n",
    "$\\hat{y}_{i} = (\\frac{1}{4})(y_{i})+(\\frac{3}{4})(\\hat{y}_{i-1})$   \n",
    "\n",
    "The recursion leads to exponential decay of older values.\n",
    "\n",
    "Notes from [Wikipedia](https://en.wikipedia.org/wiki/Exponential_smoothing).\n",
    "Exponential smoothing is just a rule of thumb.\n",
    "It is popular because it is easy to use.\n",
    "It is a low-pass filter (allow low values, but filter or attenuate high values).\n",
    "For looking ahead one timepoint, it is more reliable than moving average.\n",
    "It fails to detect trends; for a steadily increasing price, the prediction always lags.\n",
    "It incorporates infinitely many previous timepoints, \n",
    "with the coefficient $(1-\\alpha)^n$ for the value n time units ago.\n",
    "\n",
    "I have only covered simple exponential smoothing, with one parameter.\n",
    "It can be extended to have multiple parameters e.g. damping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015f18a5",
   "metadata": {},
   "source": [
    "## Transforms\n",
    "\n",
    "### Normalization\n",
    "Two ways to normalize.\n",
    "1. Range-based: (yi-min)/(max-min)\n",
    "1. Z-score: (yi-mean)/(stdev)\n",
    "\n",
    "Z-score standardization is preferable mathematically but \n",
    "range-based is computationally convenient since \n",
    "no value ever exceeds the minimum or maximum.\n",
    "\n",
    "For multivariate behavioral data on different scales,\n",
    "normalize each feature (variable) separately.\n",
    "\n",
    "### Differencing\n",
    "This captures and erases a trend, leaving a stationary timeseries.\n",
    "\n",
    "First order differencing removes a linear trend.\n",
    "Use the difference between consecutive time points. \n",
    "Replace each time point value with its delta since the previous time point. \n",
    "For example: my age keeps going up, but the difference is 1 every year. \n",
    "\n",
    "Second order differencing removes a non-linear trend.\n",
    "Use the difference of consecutive differences.\n",
    "\n",
    "### Log transform\n",
    "This may erase an an exponential trend, leaving a stationary one.\n",
    "For example: prices incorporate the compounding effects of inflation.\n",
    "Differencing doesn't help because the differences keep increasing.\n",
    "After the log transform, the differencing series is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b950b0f0",
   "metadata": {},
   "source": [
    "## Data Reduction\n",
    "### DWT = Discrete Wavelet Transform\n",
    "This transform is used for data reduction, noise reduction, data compression, and lossy image compression.\n",
    "\n",
    "DWT decomposes the time series into combinations of (coefficient * wave).\n",
    "Each wavelet captures the difference between consecutive periods.\n",
    "One wavelet captures first half vs second half, and so on, recursively.\n",
    "The coefficients are ranked; \n",
    "discard the low-order coefficients for lossy compression.\n",
    "\n",
    "Wavelets are better than Fourier for capturing one-time events such as bursts.\n",
    "\n",
    "The simplest DWT is the Haar Transform, which uses a square wave.\n",
    "For each wave, the 3 coefficients represent overall avg, left avg, right avg.\n",
    "\n",
    "Computation time is linear.\n",
    "\n",
    "### Fourier Transform\n",
    "This transform is used to describe an oscillation.\n",
    "DFT = Discrete Fourier Transform.\n",
    "If the signal were a perfect sine wave, the DFT would describe the wavelength and amplitude.\n",
    "\n",
    "Like wavelet, DFT decomposes the time series into combinations of (coefficient * wave).\n",
    "Fourier describes the global data by combinations of sinusoidal waves.\n",
    "DFT is best for describing periodic time series composed of sine waves on sine waves.\n",
    "\n",
    "The coefficients are complex numbers but \n",
    "the complex terms cancel out to give real-valued predictions.\n",
    "\n",
    "This transform allows quick time series comparison.\n",
    "Define distance between FFTs = difference in coefficients = distance between time series.\n",
    "\n",
    "DFT computation time is quadratic, \n",
    "but FFT computation time is log-linear by taking advantage of sparse matrices.\n",
    "DFT is usually calculated by FFT = Fast Fourier Transform,\n",
    "or replaced by DCT = Discrete Cosine Transform.\n",
    "\n",
    "### SAX = Symbolic Aggregate Approximation \n",
    "Choose certain values that are sybolic or representative.  \n",
    "Ideally, those values should be equally represented and equally likely.  \n",
    "Example: replace a sine wave with a square wave with 3 values: +1, -1, 0.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c196feeb",
   "metadata": {},
   "source": [
    "## ARIMA(p,d,q)\n",
    "ARIMA is for forecasting the next value of a single-valued time series.  \n",
    "For mulivariate predictions, predict each feature separately.\n",
    "\n",
    "Stationary time series have time-independent mean and variance.   \n",
    "Most time series are non-stationary but can be made stationary for ARIMA.   \n",
    "For example, prices might be steady after adjusting for inflation, or after applying a log transform.   \n",
    "Within ARIMA, second-order differencing with I(d=2) might make it stationary.\n",
    "\n",
    "Jason Brownlee shows a worked toy example in Python \n",
    "[link](https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/).\n",
    "This stats class lecture gives a much more detailed example in R\n",
    "[link](https://ademos.people.uic.edu/Chapter23.html).\n",
    "\n",
    "### I(d): differencing   \n",
    "With I(d), we predict not the actual values but the d-order differences. \n",
    "First, transform the series of measurements into a series of consecutive differences.\n",
    "Second, predict the next difference.\n",
    "Third, transform the predicted difference to a predicted measurement.\n",
    "\n",
    "At d=1, we predict first-order differences, i.e. differences of consecutive measurements.   \n",
    "$\\hat{y}_{i} = y_{i}-y_{i-1}$   \n",
    "First-order differencing models a mean that increases linearly with time.\n",
    "\n",
    "At d=2, we predict second-order differences, i.e. differences of differences.\n",
    "$\\hat{y}_{i} = \\hat{y}_{i}-\\hat{y}_{i-1}$   \n",
    "Second-order differencing models a mean that increases non-linearly with time.   \n",
    "\n",
    "Since AR(p) and MA(q) assume stationary data, apply differencing first.\n",
    "If the data are stationary, use I(d=0) or use ARMA, which is ARIMA without the I.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea58ebf",
   "metadata": {},
   "source": [
    "### AR(p): autoregression\n",
    "With AR(p), we predict the current time value by a linear combination of p previous \"lagged\" values. \n",
    "AR assumes stationarity.\n",
    "\n",
    "For p=1, the model relies on the previous value plus a term for white noise. \n",
    "Here, $\\epsilon_i$ is an unknown value from a random variable.   \n",
    "$\\hat{y}_{i} = (a_{1})(y_{i-1}) + \\epsilon_i$   \n",
    "\n",
    "For p=2, the model uses a combination of 2 previous values plus a term for white noise:    \n",
    "$\\hat{y}_{i} = (a_{1})(y_{i-1}) + (a_{2})(y_{i-2}) + \\epsilon_i$   \n",
    "\n",
    "If $a_1 = a_2 = \\frac{1}{2}$, then AR(2) is just a moving average of window size 2.  \n",
    "Other values of $a_1, a_2$ take a weighted average of the previous times.  \n",
    "For example, AR(6) could learn coefficients 0,0,.5,0,0,.5 and thus rely equally on 3 and 6 times ago.\n",
    "\n",
    "To fit this model to the data and learn the $a_i$ coefficients, \n",
    "use linear regression and least squares.  \n",
    "Each previous time window provides one linear equation.  \n",
    "Since there are more equations than unknowns,\n",
    "the system is overspecified (with contradiction).   \n",
    "So there are no solutions, just compromises and estimates.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e7b248",
   "metadata": {},
   "source": [
    "### MA(q): moving average\n",
    "With MA(q), we predict the shocks (aka innovations, deviations, residuals) from the mean.  \n",
    "MA assumes stationarity, so the mean is a given.  \n",
    "MA uses the past q deviations from the mean to predict the next deviation from the mean.   \n",
    "\n",
    "I guess if you mean-center your data, you must re-insert the mean for valid predictions.\n",
    "\n",
    "MA assumes previous shocks are predictive of future shocks.\n",
    "I think it also assumes that shocks come in regular periods.\n",
    "\n",
    "For q=1, the model predicts the next deviation based on the previous one.\n",
    "Each prediction does not depend on the previous value, \n",
    "but rather its deviation from the mean.\n",
    "Here, $\\epsilon_{i-1}$ is a known quantity, but $\\epsilon_i$ is an unknown from a random variable.    \n",
    "$\\hat{y}_{i} = (b_{1})(\\epsilon_{i-1}) + \\epsilon_i$   \n",
    "\n",
    "For q=2, the model uses the previous two deviations.\n",
    "It uses two coefficients to determine their relative importance.\n",
    "Aggarwal gives this for centered data (epsilon=deviation):   \n",
    "$\\hat{y}_{i} = (b_{1})(\\epsilon_{i-1}) + (b_{2})(\\epsilon_{i-2}) + \\epsilon_i$   \n",
    "\n",
    "But [Penn State](https://online.stat.psu.edu/stat510/lesson/2/2.1) \n",
    "gives this, which incorpotes the mean but is otherwise the same (w=deviation):   \n",
    "$x_t = \\mu + w_t + \\Theta_1 w_{t-1} + \\Theta_2 w_{t-2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c675f0",
   "metadata": {},
   "source": [
    "## AR vs MA\n",
    "AR(p) predicts the next value based on p previous values.   \n",
    "If the values were 6 and 7 previously, AR might learn to predict 8.\n",
    "\n",
    "MA(q) predicts the next deviation based on q previous deviations.   \n",
    "If the values were $\\mu+1$ and $\\mu+2$ previously, MA might learn to predict $\\mu+3$.\n",
    "\n",
    "In theory, either one would suffice on its own, but good predictions might require larger p or q.   \n",
    "By modeling the value and the deviation separately, and combining predictions, ARIMA is more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4286b223",
   "metadata": {},
   "source": [
    "## ARIMA parameter turning\n",
    "The approach described by Box & Jenkins involves trial & error, iterations, and grid search.\n",
    "\n",
    "### Auto correlation plot, ACF\n",
    "STAT 510 at [Penn State](https://online.stat.psu.edu/stat510/lesson/1/1.2) has a good explanation. \n",
    "[Duke 411](https://people.duke.edu/~rnau/411arim3.htm) shows good plots.\n",
    "\n",
    "Auto Correlation Function ACF = covariance / variance (which is similar to Pearson correlation).    \n",
    "ACF measures correlation of now to previous, aka now to lag 1, lag 2, lag 3.    \n",
    "If ACF(1) is a large fraction, then lag 1 had a big influence on now.   \n",
    "\n",
    "For lag 1, ACF is $\\rho = \\frac{\\Theta_1}{1 + \\Theta_1^2}$\n",
    "\n",
    "If ACF(1) is large, then each value is influenced by the previous.   \n",
    "Thus, it means also that lag 2 had a big influence on the lag 1 value.   \n",
    "This is where Partial autocorrelation (PCF) comes in.    \n",
    "PCF(2) is the effect of lag 2 on now, minus the effect of lag 1.   \n",
    "\n",
    "The X-axis is the \"lag\"; values 1,2,3 represent the previous 1,2,3 time points, with zero on the left.  \n",
    "The Y-axis is the (range -1 to 1) auto correlation function (ACF).  \n",
    "The autoregression plots help choose p and q for ARIMA.  \n",
    "They say to choose p from the PCF and choose q from the ACF.   \n",
    "Choose the largest X-axis value where Y is still statistically significant.   \n",
    "But the larger the value, the higher the chance of overfitting.  \n",
    "On the plots, statistical significance thresholds are indicated by horizontal thresholds above & below axis.\n",
    "The region of no significance may have a cigar shape (narrow on the left).\n",
    "\n",
    "Usually the correlations are high at X=1 \n",
    "because the current time value depends heavily on the previous.\n",
    "Usually the correlations tapers to noise after some time.\n",
    "For negative coeficients, the plot can alternate: +1, -1, +1, -1.\n",
    "For periodic time series, the plot can look sinusoidal.\n",
    "\n",
    "In periodic data, the autoregression plot looks sinusoidal.   \n",
    "An algorithm like LOESS can discern the trend and seasonal effect.     \n",
    "(LOESS = locally weighted scatterplot smoothing is a moving average technique.)   \n",
    "Example: in monthly temperature data, correlation is near +1 at 12-month lag.\n",
    "We could:\n",
    "* Preprocess the timeseries data to subtract the seasonal effect.\n",
    "* Use AR(12) so last year can predict this year, but it must learn to downweight the months inbetween.\n",
    "* Use MA(12) to model the seasonal ups and downs of every month.\n",
    "\n",
    "If AR(1) is sufficient, the ACF at lag2 is the square (smaller fraction) of the lag1 value. \n",
    "A plot of Partial Auto Correlation (PACF) subtracts this out,\n",
    "leaving the only significant bar at lag 1.\n",
    "When PACF leaves more than one bar, it is recommended to try AR(2) next.\n",
    "\n",
    "Penn State shows modeling seasonal data (beer sales).\n",
    "Their model has terms for time (due to trend) \n",
    "and time squared (due to upward curve in the trend) \n",
    "plus four terms representing seasons.\n",
    "Each seasonal term has an indicator function (1 or 0) and a learned coefficient.\n",
    "Another example uses 12-month differencing, I(12), to model seasonality.\n",
    "\n",
    "After choosing q, fit the MA(q) model to the data and learn the parameters ($b_i$ or $\\Theta_i$) \n",
    "by a hill climbing method like gradient descent.\n",
    "The system is recursive and non-linear so regression is inappropriate.\n",
    "\n",
    "### Goodness of fit\n",
    "\n",
    "To test goodness of fit, plot the residuals vs time.\n",
    "If this looks like zero-centered random noise, the model works, and the residuals are unpredictable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db1291e",
   "metadata": {},
   "source": [
    "# AIC and BIC\n",
    "These are statistical alternatives to empirical meaurement by cross-validation.  \n",
    "These statistics measure goodness of fit of model to data.   \n",
    "Both reward log likelihood of the model and both penalize model complexity.   \n",
    "They can be used to evaluate time series models.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9964051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
