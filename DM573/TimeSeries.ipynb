{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7cfa16",
   "metadata": {},
   "source": [
    "# Time Series\n",
    "This notebook was formerly named TimeSeries_02.\n",
    "\n",
    "See Charu Aggarwal, Data Mining, chapter 14, Time Series.\n",
    "\n",
    "Time series data have two components:\n",
    "1. behavioral e.g. the temperature measurement values\n",
    "1. contextual e.g. the temperature measurement times\n",
    "\n",
    "SIMILARITY:   \n",
    "To compare two time series, use:\n",
    "1. Euclidean distance (requires same number of time points)\n",
    "1. Edit distance (assume possible to mutate one into the other)\n",
    "1. Longest Common Substring (found by dynamic programming) (possibly recursive)\n",
    "1. Difference between their coefficients in wave transform (wavelet, Haar, Fourier)\n",
    "1. Align and compute distance after Dynamic Time Warp (DTW): measure differences after aligning periods with similar meaning (such as heart-valve-open and heart-valve-closed).\n",
    "1. Align and compute distance after Piecewise Aggregate Appoximiation (PAA). This uses mean or median of each bin. \n",
    "1. Align and compute distance after Symbolic Aggregate Approximation (SAX). This reduces continuous range to a few values of equal frequency. For example, turns a sine wave into a step function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4a7c8",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "Ideally, work with consecutive time points and no missing values.\n",
    "\n",
    "### Missing values\n",
    "INTERPOLATION:   \n",
    "Interpolate missing values if required. \n",
    "Linear interpolation is usually fine, but polynomial and spline can be used.\n",
    "\n",
    "### Smoothing and Noise Reduction\n",
    "PAA = piecewise aggregate approximation i.e. BINNING.\n",
    "Binning does smoothing and data reduction.\n",
    "Larger bin sizes provide more smoothing.\n",
    "Results can be sensitive to bin size.\n",
    "\n",
    "Apply non-overlapping windows. \n",
    "Replace each window with one statistic.\n",
    "Mean is more inclusive, but median is less sensitive to outliers.\n",
    "\n",
    "MOVING AVERAGE:   \n",
    "Moving average smoothing does data smoothing but not data reduction.\n",
    "\n",
    "Apply overlapping windows e.g. stride=1.\n",
    "Replace each window with one statistic.\n",
    "Mean is more inclusive, but median is less sensitive to outliers.\n",
    "\n",
    "Downsides: \n",
    "1. Window effect: You lose the first window of data\n",
    "1. Lag: Sudden big changes are hidden for a while\n",
    "1. Inversion: If wavelength is about half the window size, the waves can flip up/down.\n",
    "\n",
    "EXPONENTIAL SMOOTHING:   \n",
    "Exponential smoothing uses weighted average,\n",
    "so the most recent value counts more or less than the trend.\n",
    "Requires a smoothing parameter $\\alpha$. \n",
    "Larger values of $\\alpha$ emphasize the most recent value more.\n",
    "\n",
    "$\\hat{y}_{i} = (\\alpha)(y_{i})+(1-\\alpha)(\\hat{y}_{i-1})$   \n",
    "\n",
    "If $\\alpha=\\frac{1}{4}$ then   \n",
    "$\\hat{y}_{i} = (\\frac{1}{4})(y_{i})+(\\frac{3}{4})(\\hat{y}_{i-1})$   \n",
    "\n",
    "The recursion leads to exponential decay of older values.\n",
    "\n",
    "Notes from [Wikipedia](https://en.wikipedia.org/wiki/Exponential_smoothing).\n",
    "Exponential smoothing is just a rule of thumb.\n",
    "It is popular because it is easy to use.\n",
    "It is a low-pass filter (allow low values, but filter or attenuate high values).\n",
    "For looking ahead one timepoint, it is more reliable than moving average.\n",
    "It fails to detect trends; for a steadily increasing price, the prediction always lags.\n",
    "It incorporates infinitely many previous timepoints, \n",
    "with the coefficient $(1-\\alpha)^n$ for the value n time units ago.\n",
    "\n",
    "I have only covered simple exponential smoothing, with one parameter.\n",
    "It can be extended to have multiple parameters e.g. damping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015f18a5",
   "metadata": {},
   "source": [
    "## Transforms\n",
    "\n",
    "### Normalization\n",
    "Two ways to normalize.\n",
    "1. Range-based: (yi-min)/(max-min)\n",
    "1. Z-score: (yi-mean)/(stdev)\n",
    "\n",
    "Z-score standardization is preferable mathematically but \n",
    "range-based is computationally convenient since \n",
    "no value ever exceeds the minimum or maximum.\n",
    "\n",
    "For multivariate behavioral data on different scales,\n",
    "normalize each feature (variable) separately.\n",
    "\n",
    "### Differencing\n",
    "This captures and erases a trend, leaving a stationary timeseries.\n",
    "\n",
    "First order differencing removes a linear trend.\n",
    "Use the difference between consecutive time points. \n",
    "Replace each time point value with its delta since the previous time point. \n",
    "For example: my age keeps going up, but the difference is 1 every year. \n",
    "\n",
    "Second order differencing removes a non-linear trend.\n",
    "Use the difference of consecutive differences.\n",
    "\n",
    "### Log transform\n",
    "This may erase an an exponential trend, leaving a stationary one.\n",
    "For example: prices incorporate the compounding effects of inflation.\n",
    "Differencing doesn't help because the differences keep increasing.\n",
    "After the log transform, the differencing series is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b950b0f0",
   "metadata": {},
   "source": [
    "## Data Reduction\n",
    "### DTW = Discrete Wavelet Transform\n",
    "This transform is used for data reduction, noise reduction, data compression, and lossy image compression.\n",
    "\n",
    "DTW decomposes the time series into combinations of (coefficient * wave).\n",
    "Each wavelet captures the difference between consecutive periods.\n",
    "One wavelet captures first half vs second half, and so on, recursively.\n",
    "The coefficients are ranked; \n",
    "discard the low-order coefficients for lossy compression.\n",
    "\n",
    "Wavelets are better than Fourier for capturing one-time events such as bursts.\n",
    "\n",
    "The simplest DTW is the Haar Transform, which uses a square wave.\n",
    "For each wave, the 3 coefficients represent overall avg, left avg, right avg.\n",
    "\n",
    "Computation time is linear.\n",
    "\n",
    "### Fourier Transform\n",
    "This transform is used to describe an oscillation.\n",
    "DFT = Discrete Fourier Transform.\n",
    "\n",
    "Like DTW, DFT decomposes the time series into combinations of (coefficient * wave).\n",
    "DFT describes the global data by combinations of sinusoidal waves.\n",
    "DFT is best for describing periodic time series similar to sine waves.\n",
    "\n",
    "The coefficients are complex numbers but \n",
    "the complex terms cancel out to give real-valued predictions.\n",
    "\n",
    "This transform allows quick time series comparison.\n",
    "Define distance between FFTs = difference in coefficients = distance between time series.\n",
    "\n",
    "DFT computation time is quadratic, \n",
    "but FFT computation time is log-linear by taking advantage of sparse matrices.\n",
    "DFT is usually calculated by FFT = Fast Fourier Transform,\n",
    "or replaced by DCT = Discrete Cosine Transform.\n",
    "\n",
    "### SAX = Symbolic Aggregate Approximation \n",
    "Choose certain values that are sybolic or representative.\n",
    "Ideally, those values should be equally represented and equally likely.\n",
    "Example: replace a sine wave with a square wave with 3 values: +1, -1, 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c196feeb",
   "metadata": {},
   "source": [
    "## ARIMA(p,d,q)\n",
    "ARIMA is for forecasting the next value of a single-valued time series.\n",
    "For mulivariate predictions, predict each feature separately.\n",
    "\n",
    "Stationary time series have time-independent mean and variance. Most time series are non-stationary but can be made stationary for ARIMA. For example, prices might be steady after adjusting for inflation. The I(d) portion of ARIMA can help with this.\n",
    "\n",
    "### I(d): differencing   \n",
    "With I(d), we predict not the actual values but the d-order differences. \n",
    "First, transform the series of measurements into a series of consecutive differences.\n",
    "Second, predict the next difference.\n",
    "Third, transform the predicted difference to a predicted measurement.\n",
    "\n",
    "At d=1, we predict first-order differences, i.e. differences of consecutive measurements.   \n",
    "$\\hat{y}_{i} = y_{i}-y_{i-1}$   \n",
    "First-order differencing models a mean that increases linearly with time.\n",
    "\n",
    "At d=2, we predict second-order differences, i.e. differences of differences.\n",
    "$\\hat{y}_{i} = \\hat{y}_{i}-\\hat{y}_{i-1}$   \n",
    "Second-order differencing models a mean that increases non-linearly with time.   \n",
    "\n",
    "Since AR(p) and MA(q) assume stationary data, apply differencing first.\n",
    "If the data are stationary, use I(d=0) or use ARMA, which is ARIMA without the I.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea58ebf",
   "metadata": {},
   "source": [
    "### AR(p): autoregression\n",
    "With AR(p), we predict the current time value by a linear combination of p previous values. \n",
    "AR assumes stationarity.\n",
    "\n",
    "For p=1, the model relies on the previous value plus a term for white noise:    \n",
    "$\\hat{y}_{i} = (a_{1})(y_{i-1}) + \\epsilon_i$   \n",
    "\n",
    "For p=2, the model uses a combination of 2 previous values plus a term for white noise:    \n",
    "$\\hat{y}_{i} = (a_{1})(y_{i-1}) + (a_{2})(y_{i-2}) + \\epsilon_i$   \n",
    "\n",
    "If $a_1 = a_2 = \\frac{1}{2}$, then AR(2) is just a moving average of window size 2.  \n",
    "Other values can take a weighted average of the previous times.  \n",
    "For example, AR(9) could rely on 3, 6, 9 times ago, and ignore the intervening values.\n",
    "\n",
    "To fit this model to the data and learn the $a_i$ parameters, \n",
    "use linear regression and least squares.  \n",
    "Each previous time window provides one linear equation.  \n",
    "Since there are more equations than unknowns,\n",
    "the system is overspecified (with contradiction).   \n",
    "So there are no solutions, just compromises and estimates.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e7b248",
   "metadata": {},
   "source": [
    "### MA(q): moving average\n",
    "With MA(q), we predict the shocks i.e. deviations from the mean.\n",
    "MA assumes stationarity, predicts the mean, and counts every other value as a deviation.\n",
    "\n",
    "I guess if you mean-center your data, you must re-insert the mean for valid predictions.\n",
    "\n",
    "MA assumes previous shocks are predictive of future shocks.\n",
    "I think it also assumes that shocks come in regular periods.\n",
    "\n",
    "For q=1, the model predicts the next deviation based on the previous one.\n",
    "Each prediction does not depend on the previous value, \n",
    "but rather its deviation from the mean.    \n",
    "$\\hat{y}_{i} = (b_{1})(\\epsilon_{i-1}) + \\epsilon_i$   \n",
    "\n",
    "For q=2, the model uses the previous two deviations.\n",
    "It uses two coefficients to determine their relative importance.\n",
    "Aggarwal gives this for centered data (epsilon=deviation):   \n",
    "$\\hat{y}_{i} = (b_{1})(\\epsilon_{i-1}) + (b_{2})(\\epsilon_{i-2}) + \\epsilon_i$   \n",
    "\n",
    "But [Penn State](https://online.stat.psu.edu/stat510/lesson/2/2.1) \n",
    "gives this, which incorpotes the mean but is otherwise the same (w=deviation):   \n",
    "$x_t = \\mu + w_t + \\Theta_1 w_{t-1} + \\Theta_2 w_{t-2}$\n",
    "\n",
    "### Auto correlation plot\n",
    "STAT 510 at [Penn State](https://online.stat.psu.edu/stat510/lesson/1/1.2) has a good explanation. \n",
    "[Duke 411](https://people.duke.edu/~rnau/411arim3.htm) shows good plots.\n",
    "\n",
    "Auto Correlation Function ACF = covariance / variance (which is similar to Pearson correlation).    \n",
    "For MA(1), it is $\\rho = \\frac{\\Theta_1}{1 + \\Theta_1^2}$\n",
    "\n",
    "The autoregression plot helps choose p and q for ARIMA.  \n",
    "The X-axis is the \"lag\"; values 1,2,3 represent the previous 1,2,3 time points, with zero on the left.  \n",
    "The Y-axis is the (range -1 to 1) auto correlation function (ACF).  \n",
    "\n",
    "Usually the plot starts at almost 1 because the current time depends heavily on the previous.\n",
    "Usually it tapers to noise for no more correlation after some time.\n",
    "For negative coeficients, the plot can alternate: +1, -1, +1, -1.\n",
    "For periodic time series, the plot can look sinusoidal.\n",
    "\n",
    "Statistical significance is indicated by horizontal thresholds above & below axis.\n",
    "The region of no significance may have a cigar shape (narrow on the left).\n",
    "\n",
    "Choose p or q to incorporate as much as as there is significant signal.\n",
    "But reduce p and q to avoid overfitting.\n",
    "\n",
    "In periodic data, the autoregression plot looks sinusoidal.\n",
    "Example: in monthly temperature data, correlation is near +1 at 12-month lag,\n",
    "and AR(12) could use last year to predict this year.\n",
    "Also, correlation is near -1 at 6-month lag,\n",
    "and AR(6) could use last summer to predict this winter.\n",
    "\n",
    "If AR(1) is sufficient, the ACF at lag2 is the square (smaller fraction) of the lag1 value. \n",
    "A plot of Partial Auto Correlation (PACF) subtracts this out,\n",
    "leaving the only significant bar at lag 1.\n",
    "When PACF leaves more than one bar, it is recommended to try AR(2) next.\n",
    "\n",
    "Penn State shows modeling seasonal data (beer sales).\n",
    "Their model has terms for time (due to trend) \n",
    "and time squared (due to upward curve in the trend) \n",
    "plus four terms representing seasons.\n",
    "Each seasonal term has an indicator function (1 or 0) and a learned coefficient.\n",
    "Another example uses 12-month differencing, I(12), to model seasonality.\n",
    "\n",
    "After choosing q, fit the MA(q) model to the data and learn the parameters ($b_i$ or $\\Theta_i$) \n",
    "by a hill climbing method like gradient descent.\n",
    "The system is recursive and non-linear so regression is inappropriate.\n",
    "\n",
    "To test goodness of fit, plot the residuals vs time.\n",
    "If this looks like random noise, the model works, and the residuals are unpredictable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db1291e",
   "metadata": {},
   "source": [
    "# AIC and BIC\n",
    "These are statistical alternatives to empirical meaurement by cross-validation.  \n",
    "These statistics measure goodness of fit of model to data.   \n",
    "Both reward log likelihood of the model and both penalize model complexity.   \n",
    "They can be used to evaluate time series models.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9964051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
