{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "* PCA Discovers an axis rotation i.e. linear transformation.\n",
    "* Uses the eigen decomposition of the (symmetric positive-semidefinite) covariance matrix.\n",
    "* Prerequisite is mean centering.\n",
    "* Finds an orthonormal basis. \n",
    "* In effect, recursively performs axis rotation to where remaining variance is maximized along next axis.\n",
    "* Discovers linear transformations only. (But kernel trick can uncover non-linear ones.)\n",
    "* Under the new coordinates, there is no covariance, and the covariance matrix == the diagonalized eigenvalue matrix.\n",
    "* For dimensionality reduction (with lossy reconstruction), discard the axiis with the smallest eigenvalues.\n",
    "\n",
    "The eigen decomposition of data matrix D:   \n",
    "Scatter matrix = $D D^T$  \n",
    "Covariance matrix = $\\frac{D D^T}{n} - \\mu^T \\mu$ (mean centered)    \n",
    "Covariance = $P \\Lambda P^{-1}$   \n",
    "This factorization is possible because cov is symmetric and positive semi-definite.\n",
    "\n",
    "PCA vs SVD\n",
    "* PCA is a special case of SVD. \n",
    "* Whenever all feature means are 0, SVD==PCA.\n",
    "* PCA generates one basis for the matrix rows. SVD generates one for the rows and one for the columns. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "* Does not require mean centering.\n",
    "* Ideal for sparse non-negative matrices e.g. word vectors of documents.\n",
    "* SVD can be transformed in spectral decomposition.\n",
    "\n",
    "SVD discovers the latent factors and ranks them.\n",
    "For example, if D = movie-patrons x movie-titles,\n",
    "SVD discovers a smaller matrix of patron-preferences x movie-categories.\n",
    "\n",
    "SVD provides a lossy reconstruction of the data.\n",
    "Even better, the lost bits tend to be noise or outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$D = Q \\Sigma P^{-1}$  \n",
    "SVD uses this factorization of the data matrix.   \n",
    "This facorization always exists. \n",
    "\n",
    "D = data matrix on original axiis.   \n",
    "Q = left singular matrix of orthonormal column vectors of $D^T$.  \n",
    "$\\Sigma$ = non-negative singular values along the diagonal (in decreasing order)  \n",
    "P = right singular matrix of orthonormal column vectors of $D$.  \n",
    "\n",
    "D = (n x d) = n instances x d dimensions.  \n",
    "Q = (n x n)  \n",
    "$\\Sigma$ = (n x d) \n",
    "P = (d x d)  \n",
    "\n",
    "$P^T P = I$  \n",
    "$Q^T Q = I$  \n",
    "\n",
    "Q = eigenvectors of $D D^T$  \n",
    "P = eigenvectors of $D^T D$  \n",
    "\n",
    "Number of non-zero entries in $\\Sigma$\n",
    "equals Rank(D) and is <= min(n,d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
