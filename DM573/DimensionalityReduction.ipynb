{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "* PCA Discovers an axis rotation i.e. linear transformation.\n",
    "* Uses the eigen decomposition of the (symmetric positive-semidefinite) covariance matrix.\n",
    "* Prerequisite is mean centering.\n",
    "* Finds an orthonormal basis. \n",
    "* In effect, recursively performs axis rotation to where remaining variance is maximized along next axis.\n",
    "* Discovers linear transformations only. (But kernel trick can uncover non-linear ones.)\n",
    "* Under the new coordinates, there is no covariance, and the covariance matrix == the diagonalized eigenvalue matrix.\n",
    "* For dimensionality reduction (with lossy reconstruction), discard the axiis with the smallest eigenvalues.\n",
    "\n",
    "The eigen decomposition of data matrix D:   \n",
    "Scatter matrix = $D D^T$  \n",
    "Covariance matrix = $\\frac{D D^T}{n} - \\mu^T \\mu$ (mean centered)    \n",
    "Covariance = $P \\Lambda P^{-1}$   \n",
    "This factorization is possible because cov is symmetric and positive semi-definite.\n",
    "\n",
    "PCA vs SVD\n",
    "* PCA is a special case of SVD. \n",
    "* Whenever all feature means are 0, SVD==PCA.\n",
    "* PCA generates one basis for the matrix rows. SVD generates one for the rows and one for the columns. \n",
    "* PCA is restricted to diagonalizable (square) matrices. SVD is not.\n",
    "* PCA is applied to the covariance matrix. SVD is applied to the data matrix.\n",
    "\n",
    "PCA vs ICA\n",
    "* PCA components are orthogonal, ICA are not.\n",
    "* PCA is focused on components that are uncorrelated, but not ICA.\n",
    "* PCA uses second-order stats (variance), ICA uses higher-order.\n",
    "* PCA assumes underlying Gaussians (via variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "* Does not require mean centering.\n",
    "* Ideal for sparse non-negative matrices e.g. word vectors of documents.\n",
    "* SVD can be transformed in spectral decomposition.\n",
    "* SVD discovers the latent factors and ranks them.\n",
    "* SVD can provide a lossy reconstruction of the data.\n",
    "* SVD maximizes the energy on the reduced dimensions.\n",
    "\n",
    "Energy:\n",
    "Energy of original = sum of squared distances to origin.  \n",
    "Energy of reconstructed is unchanged by axis rotation, \n",
    "and slightly reduced by dimension reduction.\n",
    "The SVD minimizes the reconstruction error i.e.\n",
    "sum of squared distances between pairs of (original,reconstructed) points.  \n",
    "\n",
    "Latent factors:  \n",
    "Say D = (n x d) = movie-patrons x movie-titles.\n",
    "SVD discovers a latent concepts that explain both\n",
    "e.g. sci-fi might explain why some of the movie patrons like some of the movies.\n",
    "\n",
    "Noise reduction:\n",
    "The lossy part of the lossy reconstruction \n",
    "is focused on noise and outliers.\n",
    "So the lossy reconstruction can provide better training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### math\n",
    "$D = Q \\Sigma P^{-1}$  \n",
    "SVD uses this factorization of the data matrix.   \n",
    "This facorization is unique and always exists. \n",
    "\n",
    "D = data matrix on original axiis.   \n",
    "Q = left singular matrix of orthonormal column vectors of $D^T$.  \n",
    "$\\Sigma$ = non-negative singular values along the diagonal (in decreasing order)  \n",
    "P = right singular matrix of orthonormal column vectors of $D$.  \n",
    "\n",
    "D = (n x d) = n instances x d dimensions.  \n",
    "Q = (n x n)  \n",
    "$\\Sigma$ = (n x d) \n",
    "P = (d x d)  \n",
    "\n",
    "$P^T P = I$  \n",
    "$Q^T Q = I$  \n",
    "\n",
    "Q = eigenvectors of $D D^T$  \n",
    "P = eigenvectors of $D^T D$  \n",
    "\n",
    "Number of non-zero entries in $\\Sigma$\n",
    "equals Rank(D) and is <= min(n,d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of reducing dimensions to k:   \n",
    "Choose k < min(n instances, d dimensions).  \n",
    "The full dataset is $D = Q \\Sigma P^{-1}$  \n",
    "The rank=k approxization of data D(n viewers x d movies) =   \n",
    "Q(n viewers x k concepts) * (k x k, diagonal, ordered) * Pt(k concepts x d movies)  \n",
    "Note SVD provided k latent concepts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
