{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "* PCA: Unsupervised. C=ùëÉŒõùëÉ‚àí1 for covariance matrix C. This is the eigen decomposition of the covariance matrix. PCA separates the data by variance. PCA gives orthonormal basis ranked by variance explained.\n",
    "* ICA: is a higher-order PCA, but the basis is not orthonormal. \n",
    "* SVD: Unsupervised. ùê∑=ùëÑŒ£ùëÉ‚àí1 for data D. This is the lineary algebra decomposition of the data matrix, which always exists. SVD is not restricted to special cases, as PCA is. SVD discovers the minimum required latent factors (rank of D). It discovers the singular values that rank the factors.\n",
    "* LDA: Supervised. Separates the labels by linear combination of continuous features, assumed independent. Lower-dimensional space maximizes between-vs-within class scatter. May not exist. Helps to run PCA first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also: Eigen Decomposition\n",
    "* ùê¥=ùëÑŒõùëÑinv: Unsupervised. Data matrix A must be square, invertible. Q is a basis matrix. Œõ is a diagonal matrix or vector of eigenvalues in order of their utility for data reconstruction. Large eigenvalues capture trends, small eigenvalues fit the noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA = Principal Component Analysis\n",
    "PCA uses the eigen decomposition of the covariance matrix.\n",
    "\n",
    "* PCA Discovers an axis rotation i.e. linear transformation.\n",
    "* Uses the eigen decomposition of the covariance matrix.\n",
    "* The covariance matrix must be square, symmetric, positive-semidefinite, and mean centered. \n",
    "* Prerequisite is mean centering.\n",
    "* Finds an orthonormal basis. \n",
    "* In effect, recursively performs axis rotation to where remaining variance is maximized along next axis.\n",
    "* Discovers linear transformations only. (But kernel trick can uncover non-linear ones.)\n",
    "* Under the new coordinates, there is no covariance, and the covariance matrix == the diagonalized eigenvalue matrix.\n",
    "* For dimensionality reduction (with lossy reconstruction), discard the axiis with the smallest eigenvalues.\n",
    "\n",
    "The eigen decomposition of data matrix D:   \n",
    "Scatter matrix = $D D^T$  \n",
    "Covariance matrix = $\\frac{D D^T}{n} - \\mu^T \\mu$ (mean centered)    \n",
    "Covariance = $P \\Lambda P^{-1}$   \n",
    "This factorization is possible because cov is symmetric and positive semi-definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "SVD uses the factorization of one matrix into three.\n",
    "\n",
    "* Does not require mean centering.\n",
    "* Ideal for sparse non-negative matrices e.g. word vectors of documents.\n",
    "* SVD can be transformed in spectral decomposition.\n",
    "* SVD discovers the latent factors and ranks them.\n",
    "* SVD can provide a lossy reconstruction of the data.\n",
    "* SVD maximizes the energy on the reduced dimensions.\n",
    "\n",
    "Energy:\n",
    "Energy of original = sum of squared distances to origin.  \n",
    "Energy of reconstructed is unchanged by axis rotation, \n",
    "and slightly reduced by dimension reduction.\n",
    "The SVD minimizes the reconstruction error i.e.\n",
    "sum of squared distances between pairs of (original,reconstructed) points.  \n",
    "\n",
    "Latent factors:  \n",
    "Say D = (n x d) = movie-patrons x movie-titles.\n",
    "SVD discovers a latent concepts that explain both\n",
    "e.g. sci-fi might explain why some of the movie patrons like some of the movies.\n",
    "\n",
    "Noise reduction:\n",
    "The lossy part of the lossy reconstruction \n",
    "is focused on noise and outliers.\n",
    "So the lossy reconstruction can provide better training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The math behind svd\n",
    "$D = Q \\Sigma P^{-1}$  \n",
    "SVD uses this factorization of the data matrix,\n",
    "which is unique and always exists.  \n",
    "\n",
    "Intuitively, SVD breaks D into rotation, scaling, inverse rotation.  \n",
    "\n",
    "Matrix $\\Sigma$ is rectangular but diagonal.   \n",
    "Its entries are non-negative.  \n",
    "Its number of non-zero entries is the rank of D.  \n",
    "\n",
    "D = data matrix on original axiis.   \n",
    "Q = left singular matrix of orthonormal column vectors of $D^T$.  \n",
    "$\\Sigma$ = non-negative singular values along the diagonal (in decreasing order)  \n",
    "P = right singular matrix of orthonormal column vectors of $D$.  \n",
    "\n",
    "D = (n x d) = n instances x d dimensions.  \n",
    "Q = (n x n)  \n",
    "$\\Sigma$ = (n x d)   \n",
    "P = (d x d)  \n",
    "\n",
    "$P^T P = I$  \n",
    "$Q^T Q = I$  \n",
    "\n",
    "Q = eigenvectors of $D D^T$  \n",
    "P = eigenvectors of $D^T D$  \n",
    "\n",
    "Number of non-zero entries in $\\Sigma$\n",
    "equals Rank(D) and is <= min(n,d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD example\n",
    "The data is D(n viewers x d movies).   \n",
    "The n viewers are each represented by like/dislike vector of d movies.  \n",
    "Seek to explain the viewers using just k movie concepts.  \n",
    "Reduce d dimensions to k concepts:   \n",
    "Choose k < min(n instances, d dimensions).  \n",
    "The full dataset is:    \n",
    "$D = Q \\Sigma P^{-1}$  \n",
    "Now get rhe rank k approxization of the data:       \n",
    "R(k)= Q(n viewers x k concepts) * (k x k, diagonal, ordered) * Pt(k concepts x d movies)  \n",
    "Rely on SVD to provide the k latent concepts.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "Original form invented by Fisher. Since extended to LDA and dimensionality reduction. Good tutorial [here](https://www.knowledgehut.com/blog/data-science/linear-discriminant-analysis-for-machine-learning). Good video from UVa [here](https://youtu.be/IMfLXEOksGc). Code sample [blog](https://www.python-engineer.com/courses/mlfromscratch/14-lda/).\n",
    "\n",
    "Supervised learning, maximizes between/within scatter.\n",
    "\n",
    "This is rather simplistic. \n",
    "It assumes each class is generated by a Guassian with the same variance.\n",
    "That is, each class forms a circular sombrero of same size. \n",
    "If these assumptions are violated, poor discriminant functions result.\n",
    "The LDA discriminators between classes are lines or hyperplanes.\n",
    "\n",
    "One step up is QDA, quadratic discriminant analysis, \n",
    "which allows different variances, and yields parabolic discriminants.\n",
    "\n",
    "Assume independent features (for learning a linear combination).\n",
    "Assume independent data instances (for learning the placement or intercept of each discriminant.\n",
    "Assume the data are generated by one Gaussian distribution per class. \n",
    "Assume homoscedacity i.e. same variance everywhere. \n",
    "Look for a linear combination of features that explains the class labels.\n",
    "\n",
    "Uses likelihood.\n",
    "Consider each class using its assumed mean & stdev.\n",
    "Each point has a probability of coming from this mean, & stdev (use the Gaussian PDF).\n",
    "The class has a likelihood based on these data.\n",
    "\n",
    "Uses maximum likelihood. \n",
    "Each point is assigned to the class model that explains it best.\n",
    "\n",
    "Invokes the Bayes decision rule.\n",
    "Draw a line between Gaussians and assign points to classes\n",
    "based on whether they are left or right of the line.\n",
    "The line placement can incorporate priors.\n",
    "\n",
    "Each discriminant function creates latent features and gets a discriminant score, much like how each eigenvector has an eigenvalue. The top 3 linear discriminants would draw 3 lines or hyperplanes between populations.\n",
    "\n",
    "Otsu's method is related. In greyscale image analysis, it chooses the black/which threshold that maximizes pixel-to-class assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is a classifier but it is typically used for dimensionality reduction prior to classification. LDA is related to eigenvalues of the covariance matrix and thus to PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is my take. The labeled data points are in a d-dimensional space for d features. Fisher's Score uses between-scatter in numerator and within-scatter in denominator. There is a d-dimensional vector w that maximizes Fisher's Score. Of all hyperplanes perpendicular to w, one maximally separates the classes. Use least squares regression to find its intercept: wx+b=0. Now w is linear combination of features but also a latent feature that maximally separates classes. We can recursively add more latent features = dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is associated with eigenvalues.\n",
    "Part of computing the LDA involves computing\n",
    "the eigen decomposition of the Scatter_Between/Scatter_Within matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn [LDA](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) \n",
    "has these features.\n",
    "\n",
    "Inputs:\n",
    "Solver algorithm: SVD avoids the covariance matrix; Eigen and Least Squares must compute covariance and can use shrinkage. \n",
    "Shrinkage algorithm: for large covariance matrices, it is better to apply a shrinkage algorithm that preserves variance but removes outliers.\n",
    "Priors: if not given, inferred from frequencies in the data.\n",
    "Target dimensions: if not given, uses min(#classes-1,#features).\n",
    "\n",
    "Outputs:\n",
    "The vectors w and the intercepts b.\n",
    "Means (centroids) per class.\n",
    "Explained variance ratio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA vs SVD = Singular Value Decomposition\n",
    "* PCA is a special case of SVD. The give the same basis in mean-centered data.\n",
    "* Whenever all feature means are 0, SVD==PCA.\n",
    "* PCA generates one basis for the matrix rows. SVD generates one for the rows and one for the columns. \n",
    "* PCA is restricted to diagonalizable (square) matrices. SVD is not.\n",
    "* PCA is applied to the covariance matrix. SVD is applied to the data matrix.\n",
    "\n",
    "## PCA vs ICA = Independent Component Analysis\n",
    "* PCA components are orthogonal, but ICA components are not.\n",
    "* PCA removes correlation to focus on uncorrelated components. \n",
    "* ICA finds higher-order dependencies. Usually, PCA is applied first for \"whitening\" to remove correlations. \n",
    "* ICA aims to reconstruct the data from linear combination of independent signals.\n",
    "* ICA features are not ranked because they are all equally important.\n",
    "* PCA uses second-order stats (variance), ICA uses higher-order.\n",
    "* PCA assumes underlying Gaussians (via variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
