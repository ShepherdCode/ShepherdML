{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "Unsupervised analysis of features.\n",
    "* PCA finds a basis that maximizes separation by variance. \n",
    "* ICA is a non-linear, higher-order PCA, and the resulting dimensions are not orthogonal. \n",
    "* SVD always exists. It finds ranked latent factors,\n",
    "* See also our notebook on Eigen Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data matrix D:   \n",
    "Start with the data. Columns are features/dimensions. Rows are instances.\n",
    "* Scatter matrix = $D D^T$  \n",
    "* Covariance matrix = $\\frac{D D^T}{n} - \\mu^T \\mu$ (scatter normalized and mean centered)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA = Principal Component Analysis\n",
    "PCA is a linear transformation.   \n",
    "PCA finds a rigid rotation into a higher dimensional space\n",
    "such that maximum variance is explained by the first k dimensions.\n",
    "\n",
    "For data matrix A, PCA uses the eigen decomposition of the covariance matrix:  \n",
    "$cov = P \\Lambda P^{-1}$   \n",
    "\n",
    "Indirectly, PCA finds the eigenvectors of $A A^T$ or $A^T A$ (either ... it comes out equivalent).   \n",
    "PCA exploits the fact that cov is square and symmetric.    \n",
    "The eigen decomposition of cov has the property that $P P^T = P^T P = I$.\n",
    "\n",
    "* PCA requires mean-centered data.\n",
    "* PCA finds an orthonormal basis, an axis rotation, a linear transformation. In effect, PCA recursively performs axis rotation to where remaining variance is maximized along the next axis.\n",
    "* PCA can be used for dimensionality reduction.  \n",
    "* PCA uses the eigen decomposition of the covariance matrix. The decomposition is possible because the covariance matrix is square, symmetric, positive-semidefinite, and mean centered. \n",
    "* After transformation, there is no covariance. The new covariance matrix is $\\Lambda$ which is a diagonal matrix so every off-diagonal entry is zero.\n",
    "* For dimensionality reduction (with lossy reconstruction), discard the axiis with the smallest eigenvalues.\n",
    "* Each PCA axis is a linear combination of the original axiis. \n",
    "* PCA only discovers linear transformations. But the kernel trick can uncover transformations that are linear in the kernel space but non-linear in the original space. Kernel PCA computes the kernel of pairwise distance in the original space as a proxy for computing the pairwise distances in the kernel space. One kind of kernel is Gaussian.\n",
    "* PCA removes correlations. Features of the transformed data have zero covariance. \n",
    "\n",
    "### Using PCA for data transformation\n",
    "* $cov = P \\Lambda P^{-1}$   \n",
    "* Transformed data $D' = DP$   \n",
    "* Use scree plot to select minimum number of dimensions required to preserve variance.\n",
    "* Display D' to spot clusters.\n",
    "* Display D' with color=label to guage separability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "SVD uses the eigenvector decomposition of the scatter matrix.   \n",
    "\n",
    "SVD factors data matrix D matrix into 3 matrices:   \n",
    "$D = Q \\Sigma P^{-1}$  \n",
    "It is a fundamental fact of linear algebra that this is always possible.  \n",
    "\n",
    "The $\\Sigma$ matrix of factors is ranked by importance.   \n",
    "You get a lossy reconstruction if you reconstruct the data with a subset of factors. \n",
    "\n",
    "* SVD does not require mean centering.\n",
    "* SVD is ideal for sparse non-negative matrices e.g. word vectors of documents.\n",
    "* SVD can be transformed in spectral decomposition.\n",
    "* SVD discovers the latent factors and ranks them.\n",
    "* SVD can provide a lossy reconstruction of the data.\n",
    "\n",
    "SVD output size is rank of the data,\n",
    "i.e. maximum linearly independent rows (or cols).  \n",
    "SVD output matrices are orthonomal,\n",
    "i.e. product of any two rows (or cols) is zero.\n",
    "\n",
    "### SVD explained with energy\n",
    "SVD maximizes the energy on the reduced dimensions.\n",
    "Energy of original = sum of squared distances to origin.\n",
    "Energy of reconstructed is unchanged by axis rotation, \n",
    "and slightly reduced by dimension reduction.\n",
    "The SVD minimizes the reconstruction error i.e.\n",
    "sum of squared distances between pairs of (original,reconstructed) points.   \n",
    "\n",
    "Rule of thumb: retain # singular values that retains 90% of energy.  \n",
    "\n",
    "### SVD explained with latent factors  \n",
    "Say D = (n x d) = movie-patrons x movie-titles.\n",
    "SVD discovers latent concepts that explain both.\n",
    "Example: SVD might discover the latent factor \"sci-fi\" \n",
    "where some movies have \"sci-fi\" and some patrons like \"sci-fi\".\n",
    "\n",
    "Rule of thumb: retain # singular values that are explainable.\n",
    "\n",
    "### Using SVD for noise reduction\n",
    "Using SVD for lossy reconstruction, the lost part tends to be noise and outliers.\n",
    "So a lossy reconstruction can provide better training data.\n",
    "\n",
    "### Using SVD for prediction\n",
    "Suppose we have the SVD decomposition of movie preferences into viewers * concepts:   \n",
    "$D = Q \\Sigma P^{-1}$ for viewers Q and concepts P.  \n",
    "For unseen data (new customer), make a D-like vector d representing their known preferences.\n",
    "\n",
    "Either\n",
    "* Match d to most similar viewer in D. This is collaborative filtering without SVD.\n",
    "* Match d to row/viewer in Q with minimum cosine distance. This match relies on concept space rather than d's actual movie preferences.\n",
    "* Compute $d P$ to get a column vector predicts a d's concept ranks.\n",
    "* Compute $d P P^T$, projecting d back to movie space, to predict a row vector of d's preference for every movie. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The math behind svd\n",
    "$D = Q \\Sigma P^{-1}$  \n",
    "SVD uses this factorization of the data matrix,\n",
    "which is unique and always exists.  \n",
    "\n",
    "Intuitively, SVD breaks D into a rotation, a scaling, and an inverse rotation.  \n",
    "\n",
    "Matrix $\\Sigma$ is rectangular but diagonal.   \n",
    "Its entries are non-negative.  \n",
    "Its number of non-zero entries equals Rank(D) and is <= min(n,d).\n",
    "\n",
    "D = data matrix in its original axis.   \n",
    "Q = left singular matrix of orthonormal column vectors of $D^T$.  \n",
    "$\\Sigma$ = non-negative singular values along the diagonal (in decreasing order)  \n",
    "P = right singular matrix of orthonormal column vectors of $D$.  \n",
    "\n",
    "D = (n x d) = n instances x d dimensions.  \n",
    "Q = (n x n)  \n",
    "$\\Sigma$ = (n x d)   \n",
    "P = (d x d)  \n",
    "\n",
    "$P^T P = I$ because the vectors are orthonormal.   \n",
    "$Q^T Q = I$ because the vectors are orthonormal.   \n",
    "\n",
    "Q = eigenvectors of $D D^T$  \n",
    "P = eigenvectors of $D^T D$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD example\n",
    "The data is D(n viewers x d movies).   \n",
    "The n viewers are each represented by like/dislike vector of d movies.  \n",
    "Seek to explain the viewers using just k movie concepts.  \n",
    "Reduce d dimensions to k concepts:   \n",
    "Choose k < min(n instances, d dimensions).  \n",
    "By SVD, the full dataset is:    \n",
    "$D = Q \\Sigma P^{-1}$  \n",
    "Now get a rank k approxization of the data:       \n",
    "R(k)= Q(n viewers x k concepts) * (k x k, diagonal, ordered) * Pt(k concepts x d movies)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA vs SVD = Singular Value Decomposition\n",
    "* PCA is a special case of SVD. \n",
    "* SVD==PCA when all feature means are 0 i.e. when data is mean centered. \n",
    "* PCA generates one basis for the matrix rows. SVD generates one basis for the rows and one for the columns. \n",
    "* PCA is restricted to diagonalizable (square) matrices. SVD is not.\n",
    "* PCA is applied to the covariance matrix. SVD is applied to the data matrix.\n",
    "\n",
    "## PCA vs ICA = Independent Component Analysis\n",
    "* PCA components are orthogonal, but ICA components are not.\n",
    "* PCA removes correlation to focus on uncorrelated components. \n",
    "* ICA finds higher-order dependencies. Usually, PCA is applied first for \"whitening\" to remove correlations. \n",
    "* ICA aims to reconstruct the data from linear combination of independent signals.\n",
    "* ICA features are not ranked because they are all equally important.\n",
    "* PCA uses second-order stats (variance), ICA uses higher-order.\n",
    "* PCA assumes underlying Gaussians (via variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA vs MDS\n",
    "MDS = multidimensional scaling.\n",
    "\n",
    "MDS is a generalization and superset of PCA. \n",
    "MDS is usually used for visualization e.g. to show separability of two classes.   \n",
    "\n",
    "MDS will reduce the data to lower dimensions while minimizing distortion.\n",
    "MDS will maximize the agreement of pairwise distances before & after the transformation.\n",
    "MDS uses a stress() formula that penalizes distortion induced by the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
