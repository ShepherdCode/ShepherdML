{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7cfa16",
   "metadata": {},
   "source": [
    "# Time Series\n",
    "Based on Charu Aggarwal, Data Mining, chapter 14, Time Series.\n",
    "\n",
    "Time series data have two components:\n",
    "1. behavioral e.g. the temperature measurement values\n",
    "1. contextual e.g. the temperature measurement times\n",
    "\n",
    "SIMILARITY:   \n",
    "To compare two time series, use:\n",
    "1. Euclidean distance (requires same number of time points)\n",
    "1. Edit distance (assume possible to mutate one into the other)\n",
    "1. Longest Common Substring (found by dynamic programming) (possibly recursive)\n",
    "1. Difference between their coefficients in wave transform (wavelet, Haar, Fourier)\n",
    "1. Align and compute distance after Dynamic Time Warp (DTW): measure differences after aligning periods with similar meaning (such as heart-valve-open and heart-valve-closed).\n",
    "1. Align and compute distance after Piecewise Aggregate Appoximiation (PAA). This uses mean or median of each bin. \n",
    "1. Align and compute distance after Symbolic Aggregate Approximation (SAX). This reduces continuous range to a few values of equal frequency. For example, turns a sine wave into a step function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4a7c8",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "Ideally, work with consecutive time points and no missing values.\n",
    "\n",
    "### Missing values\n",
    "INTERPOLATION:   \n",
    "Interpolate missing values if required. \n",
    "Linear interpolation is usually fine, but polynomial and spline can be used.\n",
    "\n",
    "### Smoothing and Noise Reduction\n",
    "PAA = piecewise aggregate approximation i.e. BINNING.\n",
    "Binning does smoothing and data reduction.\n",
    "Larger bin sizes provide more smoothing.\n",
    "\n",
    "Apply non-overlapping windows. \n",
    "Replace each window with one statistic.\n",
    "Mean is more inclusive, but median is less sensitive to outliers.\n",
    "\n",
    "MOVING AVERAGE:   \n",
    "Moving average smoothing does data smoothing but not data reduction.\n",
    "\n",
    "Apply overlapping windows e.g. stride=1.\n",
    "Replace each window with one statistic.\n",
    "Mean is more inclusive, but median is less sensitive to outliers.\n",
    "\n",
    "Downsides: \n",
    "1. Window effect: You lose the first window of data\n",
    "1. Lag: Sudden big changes are hidden for a while\n",
    "1. Inversion: If wavelength is about half the window size, the waves can flip up/down.\n",
    "\n",
    "EXPONENTIAL SMOOTHING:   \n",
    "Exponential smoothing uses weighted average,\n",
    "so the most recent value counts more or less than the trend.\n",
    "Requires a smoothing parameter $\\alpha$. \n",
    "Larger values of $\\alpha$ emphasize the most recent value more.\n",
    "\n",
    "If $\\alpha=\\frac{1}{4}$ then   \n",
    "$\\hat{y}_{i} = (\\frac{1}{4})(y_{i})-(\\frac{3}{4})(\\hat{y}_{i-1})$   \n",
    "The recursion leads to exponential decay of older values.\n",
    "\n",
    "Notes from [Wikipedia](https://en.wikipedia.org/wiki/Exponential_smoothing).\n",
    "Exponential smoothing is just a rule of thumb.\n",
    "It is popular because it is easy to use.\n",
    "It is a low-pass filter (allow low values, but filter or attenuate high values).\n",
    "For looking ahead one timepoint, it is more reliable than moving average.\n",
    "It fails to detect trends; for a steadily increasing price, the prediction always lags.\n",
    "It incorporates infinitely many previous timepoints, \n",
    "with the coefficient $(1-\\alpha)^n$ for the value n time units ago."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015f18a5",
   "metadata": {},
   "source": [
    "## Transforms\n",
    "\n",
    "### Normalization\n",
    "Two ways to normalize.\n",
    "1. Range-based: (yi-min)/(max-min)\n",
    "1. Z-score: (yi-mean)/(stdev)\n",
    "\n",
    "Z-score standardization is preferable mathematically but \n",
    "range-based is computationally convenient since \n",
    "no value ever exceeds the minimum or maximum.\n",
    "\n",
    "For multivariate behavioral data on different scales,\n",
    "normalize each feature (variable) separately.\n",
    "\n",
    "### Differencing\n",
    "First order differencing: use the difference at consecutive time points. \n",
    "Replace each time point value with its delta since the previous time point. \n",
    "This converts a steady trend to stationary. \n",
    "For example: I am one year older each year.\n",
    "\n",
    "Second order differencing: use the difference of consecutive differences.\n",
    "\n",
    "### Log transform\n",
    "It is possible to transform an exponential trend into a stationary one.\n",
    "For example: prices incorporate the compounding effects of inflation.\n",
    "Differencing doesn't help because the differences keep increasing.\n",
    "After the log transform, the differencing series is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b950b0f0",
   "metadata": {},
   "source": [
    "## Data Reduction\n",
    "### DTW = Discrete Wavelet Transform\n",
    "This transform is also used for noise reduction, data compression, and lossy image compression.\n",
    "\n",
    "DTW decomposes the time series into waves.\n",
    "DTW generates a ranked series of coefficients; \n",
    "using just the first few provides lossy compression.\n",
    "Each wavelet captures difference between consecutive periods.\n",
    "One wavelet captures first half vs second half; and so on recursively.\n",
    "\n",
    "Wavelets capture local changes.\n",
    "Wavelets are better than Fourier for capturing one-time events such as bursts.\n",
    "\n",
    "The simplest DTW is the Haar Transform, which uses a square wave.\n",
    "For each wave, the 3 coefficients represent overall avg, left avg, right avg.\n",
    "\n",
    "Computation time is linear.\n",
    "\n",
    "### Fourier Transform\n",
    "DFT = Discrete Fourier Transform,\n",
    "usually calculated by FFT = Fast Fourier Transform\n",
    "or replaced with DCT = Discrete Cosine Transform.\n",
    "DFT describes the global data by combinations of sinusoidal waves.\n",
    "DFT is best for periodic time series similar to sine waves.\n",
    "\n",
    "The coefficients are complex numbers but \n",
    "the complex terms cancel out to give real-valued predictions.\n",
    "\n",
    "DFT makes for quick time series comparison.\n",
    "Define distance between FFTs = difference in coefficients = distance between time series.\n",
    "\n",
    "DFT computation time is quadratic, \n",
    "but FFT computation time is log-linear by taking advantage of sparse matrices.\n",
    "\n",
    "### SAX = Symbolic Aggregate Approximation \n",
    "Choose certain values that are sybolic or representative.\n",
    "Ideally, those values should be equally represented and equally likely.\n",
    "Example: replace a sine wave with a square wave with 3 values: +1, -1, 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c196feeb",
   "metadata": {},
   "source": [
    "## ARIMA(p,d,q)\n",
    "For a single-valued time series.\n",
    "\n",
    "Stationary time series have time-independent mean and variance. Most time series are non-stationary but can be made so. For example, prices might be steady after adjusting for inflation.\n",
    "\n",
    "### I(d): differencing   \n",
    "With I(d), we predict not the actual values but the d-order differences. At d=1, we predict first-order differences, which should account for a mean that increases with time:   \n",
    "$\\hat{y}_{i} = y_{i}-y_{i-1}$   \n",
    "\n",
    "At d=2, we predict second-order differences, i.e. differences of differences, which I suppose captures a bit of non-linearity:   \n",
    "$\\hat{y}_{i} = \\hat{y}_{i}-\\hat{y}_{i-1}$   \n",
    "\n",
    "Since AR(p) and MA(q) assume stationary data, apply I(d) first.\n",
    "The For stationary data, use I(d=0) or use ARMA without the I.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea58ebf",
   "metadata": {},
   "source": [
    "### AR(p): autoregression\n",
    "With AR(p), we predict the current time value by a linear combination of p previous values. \n",
    "AR assumes stationarity.\n",
    "\n",
    "For p=2, the model uses a combination of 2 previous values plus a term for white noise:    \n",
    "$\\hat{y}_{i} = (a_{1})(y_{i-1}) + (a_{2})(y_{i-2}) + \\epsilon_i$   \n",
    "\n",
    "Note that $a_1 = a_2 = \\frac{1}{2}$ means use the average of the previous two points.\n",
    "Other values mean take a weighted average of the previous times.\n",
    "\n",
    "To fit this model to the data and learn the $a_i$ parameters, \n",
    "use linear regression and least squares.\n",
    "Each previous time window provides one linear equation.\n",
    "Since there are more equations than unknowns,\n",
    "the system is overspecified (with contradiction), \n",
    "and there are no solutions, only estimates.\n",
    "\n",
    "The autoregression plot helps choose p.\n",
    "The X-axis is the \"lag\"; values 1,2,3 represent the previous 1,2,3 time points. \n",
    "The Y-axis is the correlation (range is -1 to 1).\n",
    "\n",
    "Usually the plot starts at almost 1 because the current time depends heavily on the previous.\n",
    "Usually it drops close to zero for no more correlation after some time.\n",
    "If so, choose p where the correlation is still high.\n",
    "Keep p small to avoid overfitting.\n",
    "\n",
    "In seasonal data, the autoregression plot looks like a sine wave.\n",
    "Example: in monthly temperature data, correlation is near +1 at 12-month lag,\n",
    "and AR(12) could use last year to predict this year.\n",
    "Also, correlation is near -1 at 6-month lag,\n",
    "and AR(6) could use last summer to predict this winter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e7b248",
   "metadata": {},
   "source": [
    "### MA(q): moving average\n",
    "With MA(q), we predict the shocks i.e. deviations from the mean.\n",
    "MA assumes stationarity, predicts the mean, and counts every other value as a deviation.\n",
    "\n",
    "I guess if you mean-center your data, you must re-insert the mean for valid predictions.\n",
    "\n",
    "MA assumes previous shocks are predictive of future shocks.\n",
    "I think it also assumes that shocks come in regular periods.\n",
    "\n",
    "For q=1, the model predicts the next deviation based on the previous one.\n",
    "Each prediction does not depend on the previous value, but rather its deviation.\n",
    "\n",
    "For q=2, the model uses the previous two deviations.\n",
    "It uses two coefficients to determine their relative importance.\n",
    "Aggarwal gives this for centered data (epsilon=deviation):   \n",
    "$\\hat{y}_{i} = (b_{1})(\\epsilon_{i-1}) + (b_{2})(\\epsilon_{i-2}) + \\epsilon_i$   \n",
    "\n",
    "But [Penn State](https://online.stat.psu.edu/stat510/lesson/2/2.1) \n",
    "gives this, which incorpotes the mean but is otherwise the same (w=deviation):   \n",
    "$x_t = \\mu + w_t + \\Theta_1 w_{t-1} + \\Theta_2 w_{t-2}$\n",
    "\n",
    "To fit this model to the data and learn the parameters ($b_i$ or $\\Theta_i$), \n",
    "use a hill climbing method like gradient descent.\n",
    "The system is recursive and non-linear so regression is inappropriate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5045c8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
