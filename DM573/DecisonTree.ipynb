{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "Classification or regession.  \n",
    "DT is a predictive model.  \n",
    "Tree building is inductive (infer general rules from specific data).  \n",
    "Prediction with a tree is deductive.  \n",
    "Tree building is greedy.  \n",
    "Trees don't have to be binary.  \n",
    "TDIDT: top-down induction of decision tree (general name for all algorithms).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hunt's algorithm\n",
    "Recursively split the remaining data based on the most discriminative feature.\n",
    "This is the splitting criteria used by all the other algorithms listed here.\n",
    "\n",
    "Quinlan used Hunt's algorithm.\n",
    "\n",
    "Hunt's algorithm is optimal \n",
    "if all feature combinations are present in the training data experience table.\n",
    "But this is unlikely, even if N>K! i.e. #samples > all possible feature combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quinlan's algorithms: \n",
    "Quinlan invented a serices of algorithms, each an improvement on the previous.\n",
    "\n",
    "#### ID3\n",
    "Iterative Dichotomizer. \n",
    "At each node, review the so-far unused features. \n",
    "Choose feature with max information gain or min entropy.\n",
    "Choose one or more binary rules for two or more child nodes.\n",
    "Stop when out of examples, out of features, or node is pure (leaf). \n",
    "\n",
    "Greedy, non-optimal, recursive.\n",
    "Tends to overfit. \n",
    "Can be improved (with longer run) by backtracking.\n",
    "Better suited for categorical features & classification than continuous regression.\n",
    "\n",
    "#### C4.5\n",
    "Became world's most popular machine learning tool in 1980s.\n",
    "Implemented as Weka J48.\n",
    "\n",
    "Improvments relative to ID3:\n",
    "Better at selecting thresholds on continuous features.\n",
    "Ignores missing data during entropy calculation.\n",
    "Features can be weighted.\n",
    "Prune the final tree to remove unhelpful branches.\n",
    "\n",
    "#### See5\n",
    "Improved again in See5 (commercial):\n",
    "Requires less CPU and less RAM. \n",
    "Makes smaller trees.\n",
    "Uses boosting.\n",
    "Uses feature selection (winnowing).\n",
    "Weights the different classification-error classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brieman's algorithms\n",
    "#### CART\n",
    "CART is a generic term for Classification And Regression Rree.\n",
    "\n",
    "#### Boosted trees\n",
    "Multiple trees in series.  \n",
    "Incremental refinement.  \n",
    "Similar to AdaBoot.  \n",
    "Each tree is trained to classifiy what the previous tree misclassified.  \n",
    "We say tree_2 classifies the residuals of tree_1.  \n",
    "Unclear to me: how does this work during prediction?\n",
    "\n",
    "#### Random Forests\n",
    "Multiple trees in parallel.   \n",
    "Similar to boosting, but this is called bagging (boosted aggregation).   \n",
    "Each tree built by sampling WITH replacement.   \n",
    "Each tree could be built on a random sample of the data,\n",
    "on a random sample of the features, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impurity metrics\n",
    "Each algorithm decides whether to split a node based on its impurity.  \n",
    "There are several impurity metrics.  \n",
    "Let prob(x) = proportion of examples in this node belonging to class x.  \n",
    "Sum over all classes.  \n",
    "For splits into more than two child nodes, impurity of parent is sum of impurity of children, each weighted by their portion of the examples.\n",
    "\n",
    "#### Entropy\n",
    "Entropy = sum [ -1 * prob(x) * lg (prob(x) ) ] , max=1.0\n",
    "#### Gini\n",
    "Gini = 1 - sum [ square (prob(x)) ] , max = 0.5 for binary\n",
    "#### Classification error\n",
    "Classification error = 1 - max [ scores for wrong classes ], max = 0.5 for binary\n",
    "#### Information gain\n",
    "Information gain = reduction of uncertainty = Entropy before split - Entropy after.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping\n",
    "Decision tree tends to overfit.  \n",
    "Use a stopping criteria to stop the splitting.  \n",
    "Or use tree pruning after the fact.  \n",
    "\n",
    "Stopping options.  \n",
    "Instead of splitting till nodes are pure, split till impurity is below threshold.  \n",
    "Or, stop when splitting doesn't reduce impurity by a threshold amount.  \n",
    "Or, stop when no remaining feature introduces a statistical (t-test or chi-square) change in the class distribution.    \n",
    "\n",
    "Pruning example.   \n",
    "A node has a 20:10 mix of 2 classes, error = 10/30.  \n",
    "The node has 4 leaf nodes with total error = 9/30.  \n",
    "The node was split because the split redued error.  \n",
    "But add a fudge factor of 0.5 misclassified per node.   \n",
    "Parent error = 10.5/30 but leaf error = 11/30.  \n",
    "This split should be pruned.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
