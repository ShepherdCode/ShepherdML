{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "See also Random Forest.\n",
    "\n",
    "Used for classification or regession.  \n",
    "Used on categorical or numerical data.  \n",
    "Predictive model.  \n",
    "Tree building is inductive (infer general rules from specific data).  \n",
    "Prediction with a tree is deductive.  \n",
    "Trees don't have to be binary.  \n",
    "TDIDT: top-down induction of decision tree (general name for all algorithms).  \n",
    "\n",
    "Greedy.  \n",
    "Rectilinear decision boundary.  \n",
    "Susceptible to feature interaction effect, esp with many irrelevant features.  \n",
    "Breaks if unseen data presents missing values.  \n",
    "\n",
    "Related to rules-based learners, which don't necessarily apply rules in order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hunt's algorithm\n",
    "Recursively split the remaining data based on the most discriminative feature.\n",
    "This is the splitting criteria used by all the other algorithms listed here.\n",
    "\n",
    "Quinlan used Hunt's algorithm.\n",
    "\n",
    "Hunt's algorithm is optimal \n",
    "if all feature combinations are present in the training data experience table.\n",
    "But this is unlikely, even if N>K! i.e. #samples > all possible feature combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quinlan's algorithms: \n",
    "Quinlan invented a serices of algorithms, each an improvement on the previous.\n",
    "\n",
    "#### ID3\n",
    "Iterative Dichotomizer. \n",
    "At each node, review the so-far unused features. \n",
    "Choose feature with max information gain or min entropy.\n",
    "Choose one or more binary rules for two or more child nodes.\n",
    "Stop when out of examples, out of features, or node is pure (leaf). \n",
    "\n",
    "Greedy, non-optimal, recursive.\n",
    "Tends to overfit. \n",
    "Can be improved (with longer run) by backtracking.\n",
    "Better suited for categorical features & classification than continuous regression.\n",
    "\n",
    "#### FOIL\n",
    "First Order Inductive Learner algorithm. \n",
    "Hill climbing.\n",
    "Adds one rule at a time.\n",
    "Uses separate-and-conquer (not divide-and-conquer).\n",
    "\n",
    "#### C4.5\n",
    "Became world's most popular machine learning tool in 1980s.\n",
    "Implemented as Weka J48.\n",
    "\n",
    "Improvments relative to ID3:\n",
    "Better at selecting thresholds on continuous features.\n",
    "Ignores missing data during entropy calculation.\n",
    "Features can be weighted.\n",
    "Prune the final tree to remove unhelpful branches.\n",
    "\n",
    "#### See5\n",
    "Improved again in See5 (commercial):\n",
    "Requires less CPU and less RAM. \n",
    "Makes smaller trees.\n",
    "Uses boosting.\n",
    "Uses feature selection (winnowing).\n",
    "Weights the different classification-error classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brieman's algorithms\n",
    "#### CART\n",
    "CART is a generic term for Classification And Regression Tree.\n",
    "\n",
    "#### Boosted trees\n",
    "Multiple trees in series.  \n",
    "Incremental refinement.  \n",
    "Similar to AdaBoot.  \n",
    "Each tree is trained to classifiy what the previous tree misclassified.  \n",
    "We say tree_2 classifies the residuals of tree_1.  \n",
    "Unclear to me: how does this work during prediction?\n",
    "\n",
    "#### Random Forests\n",
    "Multiple trees in parallel.   \n",
    "Similar to boosting, but this is called bagging (boosted aggregation).   \n",
    "Each tree built by sampling WITH replacement.   \n",
    "Each tree could be built on a random sample of the data,\n",
    "on a random sample of the features, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impurity metrics\n",
    "Each algorithm decides whether to split a node based on its impurity.  \n",
    "There are several impurity metrics.  \n",
    "Let prob(x) = proportion of examples in this node belonging to class x.  \n",
    "Sum over all classes.  \n",
    "For splits into more than two child nodes, impurity of parent is sum of impurity of children, each weighted by their portion of the examples.\n",
    "\n",
    "#### Entropy\n",
    "Entropy = sum [ -1 * prob(x) * lg (prob(x) ) ] , max=1.0\n",
    "#### Gini\n",
    "Gini = 1 - sum [ square (prob(x)) ] , max = 0.5 for binary\n",
    "#### Classification error\n",
    "Simple concept: a leaf \"predicts\" the majority class, so the rest is error.  \n",
    "Classification error of node = 1 - (portion in largest class)  \n",
    "Classification error of node = (portion outside largest class), max = 0.5 for binary  \n",
    "Classification error of node = (total predicted wrong) / (training instances)\n",
    "#### Information gain\n",
    "Information gain = reduction of uncertainty = Entropy before split - Entropy after.\n",
    "#### FOIL information gain\n",
    "Rule 0 finds p0 positive and n0 negative samples and t0=p0+n0.  \n",
    "Rule 1 finds p1 positive and n1 negative samples and t1=p1+n1.  \n",
    "Is the extra rule 1 worth it?  \n",
    "Gain = p1 * ( lg(p1/t1) - lg(p0/t0) )\n",
    "\n",
    "#### Collective impurity\n",
    "The collective impurity of the child nodes is the weighted sum.  \n",
    "Each child's impurity is weighted by the portion of train instances it covers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy: 0.6500224216483541 gini: 0.2777777777777777 error: 0.16666666666666663\n"
     ]
    }
   ],
   "source": [
    "# This node contains 5 of class1, 1 of class0.\n",
    "# This node will predict the majority class, class1.\n",
    "from math import log2\n",
    "class0=1\n",
    "class1=5\n",
    "total=class0+class1\n",
    "prob0=class0/total\n",
    "prob1=class1/total\n",
    "entropy = - ( prob0*log2(prob0) + prob1*log2(prob1) )\n",
    "gini = 1 - (prob0**2 + prob1**2)\n",
    "error = 1 - max(prob0,prob1)\n",
    "print ('entropy:',entropy,'gini:',gini,'error:',error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping\n",
    "Decision tree tends to overfit.  \n",
    "Use a stopping criteria to stop the splitting.  \n",
    "Or use tree pruning after the fact.  \n",
    "\n",
    "Stopping options.  \n",
    "Instead of splitting till nodes are pure, split till impurity is below threshold.  \n",
    "Or, stop when splitting doesn't reduce impurity by a threshold amount.  \n",
    "Or, stop when no remaining feature introduces a statistical (t-test or chi-square) change in the class distribution.    \n",
    "\n",
    "Pruning example.   \n",
    "A node has a 20:10 mix of 2 classes, error = 10/30.  \n",
    "The node has 4 leaf nodes with total error = 9/30.  \n",
    "The node was split because the split redued error.  \n",
    "Pessimistic error = add a penalty of e.g. 0.5 per node.   \n",
    "Then parent error = 10.5/30 but leaf error = 11/30, and this split should be pruned.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
