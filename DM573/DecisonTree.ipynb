{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "See also Random Forest.\n",
    "\n",
    "DT training is a form of supervised learning.\n",
    "\n",
    "A DT can be used for classification (categorical labels) or regession (numeric labels).\n",
    "The DT can be trained on categorical or numerical features.\n",
    "A DT can be binary, ternary, etc.  \n",
    "\n",
    "Tree building is a form of inductive reasoning: \n",
    "it infers general rules from specific data.  \n",
    "In fact, the general name for all DT algorithms is  \n",
    "Top-Down Induction of Decision Trees (TDIDT).\n",
    "\n",
    "A DT is a predictive model.\n",
    "Prediction with a tree is a form of deductive reasoning:\n",
    "it predicts a specific fact based on its general rules.\n",
    "\n",
    "Each tree node could use any feature,\n",
    "and for that feature, any threshold.\n",
    "Thus, there are too many possible trees to examine them all.\n",
    "Practical algorithms are greedy: \n",
    "they iteratively add the next best splitting rule.\n",
    "\n",
    "DT decision boundaries are rectilinear, not curved as with most non-linear classifiers.\n",
    "\n",
    "DT is less effective if there are feature interaction effects.\n",
    "(Could we use tree nodes that test for combinations of features?\n",
    "Probably, but the combinatorial explostion makes it impractical.)\n",
    "\n",
    "High dimensionality slows training, especially if many features are irrelevant.  \n",
    "\n",
    "A DT may be unusable if unseen data presents missing values\n",
    "for features that were selected during training. A RF can overcome this.\n",
    "\n",
    "Rules-based learning is a related concept. \n",
    "Rules-based learners are less strict about the order each rule is applied.\n",
    "In contrast, a DT learns a rigid set of rules and an order for applying them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hunt's algorithm\n",
    "This was the first. This is the splitting criteria used by all the other algorithms listed here, including Quinlan.\n",
    "\n",
    "Recursively split the remaining data based on the most discriminative feature.\n",
    "\n",
    "Hunt's algorithm is optimal \n",
    "if all feature combinations are present in the training data.\n",
    "Of course, this is unlikely in high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quinlan's algorithms: \n",
    "Quinlan invented a series of algorithms. Each is an improvement on the previous.\n",
    "\n",
    "#### ID3\n",
    "ID stands for Iterative Dichotomizer. \n",
    "At each node, review the so-far unused features. \n",
    "Choose feature with max information gain or min entropy.\n",
    "Choose one or more binary rules per node.\n",
    "Each node directs its input to one of two or more child nodes.\n",
    "Stop making rules when a node is out of examples, out of features, or the node is pure.\n",
    "In a pure node, all the training samples that reach it are from the same class.\n",
    "\n",
    "This is a greedy, non-optimal, recursive algorithm.\n",
    "It tends to overfit. \n",
    "It can be improved, with longer run time, by backtracking (how?).\n",
    "ID3 is better suited for classification with categorical features,  \n",
    "than for regression with continuous features (why?).\n",
    "\n",
    "#### FOIL\n",
    "FOIL stands for First Order Inductive Learner algorithm. \n",
    "This is a hill climbing algorithm.\n",
    "It adds one rule at a time.\n",
    "The strategy is separate-and-conquer (not divide-and-conquer).\n",
    "\n",
    "#### C4.5\n",
    "This became world's most popular machine learning tool in the 1980s.\n",
    "It was implemented as Weka J48.\n",
    "\n",
    "Improvments relative to ID3:\n",
    "* C4 is better at selecting thresholds on continuous features.\n",
    "* C4 gnores missing data during its entropy calculation.\n",
    "* C4, accepts weighted features.\n",
    "* C4 prunes the final tree to remove unhelpful branches.\n",
    "\n",
    "#### See5\n",
    "This was commercial software.\n",
    "* C5 requires less CPU and less RAM. \n",
    "* C5 makes smaller trees.\n",
    "* C5 uses boosting (training sequentially on the residual cases).\n",
    "* C5 uses feature selection (winnowing).\n",
    "* C5 weights the different classification-error classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brieman's algorithms\n",
    "#### CART\n",
    "CART = Classification And Regression Tree.\n",
    "CART was the acronym used by Brieman, but now it is a generic term.\n",
    "\n",
    "#### Boosted trees\n",
    "CART uses multiple trees in series (boosting) for incremental refinement.\n",
    "This was used by AdaBoost and \n",
    "implemented by [XGBoost](https://xgboost.readthedocs.io/en/stable/tutorials/model.html).\n",
    "It is implemented by sklearn as GradientBoostingClassifier.\n",
    "\n",
    "It is computationally preferable to train many small trees in series rather than one big tree. \n",
    "Each tree is trained to classifiy residuals that the previous tree misclassified.\n",
    "We say tree_2 classifies the residuals of tree_1.\n",
    "\n",
    "Unclear to me: how boosted trees are used for prediction,\n",
    "when you don't know whether the first tree's prediction was wrong.\n",
    "Documentation says we combine the scores generated by the various models,\n",
    "possibly using a weighted average, and possibly using trained weights.\n",
    "\n",
    "#### Random Forests\n",
    "RF reduces variance and avoids overitting.\n",
    "RF trains and predicts with multiple trees in parallel.\n",
    "Each tree is built on a random sample of the instances,\n",
    "on a random sample of the features, or both.\n",
    "Thus, each tree is imperfect, and the collection avoids overfitting.\n",
    "\n",
    "RF uses bagging = boosted aggregation.\n",
    "Each tree is built by sampling WITH replacement, called bootstrap.\n",
    "This ensures that each tree is independent of the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impurity metrics\n",
    "There are several impurity metrics\n",
    "used by algorithms above to decide whether to split a node.  \n",
    "\n",
    "In any DT, each node divides its training samples among its child nodes.\n",
    "If the parent assigns one child instances of one class only, that child is pure.\n",
    "\n",
    "Here, let $P(x_i)$ = proportion of examples in this node belonging to one class,\n",
    "but we intend to sum over all classes.  \n",
    "\n",
    "#### Entropy\n",
    "Entropy = $\\sum_i [ - P(x_i) * lg ( P(x_i) ) ] $   \n",
    "Using base 2 logs, lg(1/2) = -1.   \n",
    "Entropy is 0+0=0.0 at best, and $-[\\frac{1}{2}(-1)+\\frac{1}{2}(-1)]=1.0$ at worst.\n",
    "\n",
    "#### Gini\n",
    "Gini = $1 - \\sum_i [ P(x_i)^2 ] $   \n",
    "Gini is 1-(1+0)=0.0 at best, and $(0.5)^2+(0.5)^2=0.5$ at worst.\n",
    "\n",
    "#### Classification error\n",
    "Simple concept: a leaf \"predicts\" the majority class, so the rest is error.\n",
    "There are many ways to say this:\n",
    "* Classification error of node = 1 - (portion in largest class)  \n",
    "* Classification error of node = (portion outside largest class), max = 0.5 for binary  \n",
    "* Classification error of node = (total predicted wrong) / (training instances)\n",
    "\n",
    "#### Information gain\n",
    "You can measure the information gain of each node in a tree.  \n",
    "Information gain = reduction of uncertainty = Entropy before split - Entropy after split.\n",
    "\n",
    "#### FOIL information gain\n",
    "FOIL adds one rule at a time until the information gain is too small.  \n",
    "Say rule 0 finds $p_0$ positive and $n_0$ negative samples and $t_0=p_0+n_0$.  \n",
    "Say rule 1 finds $p_1$ positive and $n_1$ negative samples and $t_1=p_1+n_1$.  \n",
    "Determine whether the extra rule is worth it by:     \n",
    "Gain = $p_1 [ lg(\\frac{p_1}{t_1}) - lg(\\frac{p_0}{t_0}) ]$\n",
    "\n",
    "#### Collective impurity\n",
    "The collective impurity of a node is the weighted sum of its children,\n",
    "where each child's impurity is weighted by the portion of training instances it takes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy: 0.6500224216483541 \n",
      "gini: 0.2777777777777777 \n",
      "error: 0.16666666666666663\n"
     ]
    }
   ],
   "source": [
    "# This node contains 5 of class1, 1 of class0.\n",
    "# This node will predict the majority class, class1.\n",
    "from math import log2\n",
    "class0=1\n",
    "class1=5\n",
    "total=class0+class1\n",
    "prob0=class0/total\n",
    "prob1=class1/total\n",
    "entropy = - ( prob0*log2(prob0) + prob1*log2(prob1) )\n",
    "gini = 1 - (prob0**2 + prob1**2)\n",
    "error = 1 - max(prob0,prob1)\n",
    "print ('entropy:',entropy,'\\ngini:',gini,'\\nerror:',error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping\n",
    "Decision trees tend to overfit. Possible solutions include:\n",
    "\n",
    "### Stopping  \n",
    "During training, stop splitting nodes when they are pure enough.\n",
    "* Split till impurity is below threshold.  \n",
    "* Equivalently, stop when splitting doesn't reduce impurity by a threshold amount.  \n",
    "* Equivalently, stop when no remaining feature introduces a statistical change in the class distribution, based on t-test or chi-square test.     \n",
    "\n",
    "### Pruning    \n",
    "After training, go back and remove some leaf nodes.\n",
    "\n",
    "Say the node has a 20:10 mix of 2 classes, so classification error = 10/30.  \n",
    "The node has 4 leaf nodes with combined error = 9/30.  \n",
    "The node was split because the split reduced total error.\n",
    "But was it worth it?\n",
    "\n",
    "Compute pessimistic error by adding a penalty of e.g. 0.5 per node.   \n",
    "Then parent error = 10.5/30 but leaf error = (9+2)/30 = 11/30.\n",
    "Seen this way, the split added error. \n",
    "So, prune this split.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
