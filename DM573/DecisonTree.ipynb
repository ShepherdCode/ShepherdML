{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "See also Random Forest.\n",
    "\n",
    "Used for classification or regession.  \n",
    "Used on categorical or numerical data.  \n",
    "Trees can be binary or not.  \n",
    "\n",
    "Tree building is inductive reasoning. We infer general rules from specific data.  \n",
    "TDIDT stands for \n",
    "Top-Down Induction of Decision Trees.\n",
    "This is a general name for all DT algorithms.  \n",
    "\n",
    "A DT is a predictive model.  \n",
    "Prediction with a tree is deductive reasoning.  \n",
    "\n",
    "Practical DT algorithms are greedy.\n",
    "It is computational intractable to examine all possible trees.\n",
    "Instead, we iteratively add the next best rule.\n",
    "\n",
    "A DT chooses a rectilinear decision boundary.  \n",
    "DT is susceptible to feature interaction effects, esp with many irrelevant features.  \n",
    "A DT may be unusable if unseen data presents missing values.  \n",
    "\n",
    "DT is related to rules-based learners, \n",
    "but those don't necessarily apply rules in any order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hunt's algorithm\n",
    "Recursively split the remaining data based on the most discriminative feature.\n",
    "This is the splitting criteria used by all the other algorithms listed here,\n",
    "including Quinlan.\n",
    "\n",
    "Hunt's algorithm is optimal \n",
    "if all feature combinations are present in the training data.\n",
    "But this is unlikely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quinlan's algorithms: \n",
    "Quinlan invented a serices of algorithms, each an improvement on the previous.\n",
    "\n",
    "#### ID3\n",
    "Iterative Dichotomizer. \n",
    "At each node, review the so-far unused features. \n",
    "Choose feature with max information gain or min entropy.\n",
    "Choose one or more binary rules for two or more child nodes.\n",
    "Stop when out of examples, out of features, or node is pure.\n",
    "All the children of a pure node have the same class.\n",
    "\n",
    "This is a greedy, non-optimal, recursive algorithm.\n",
    "It tends to overfit. \n",
    "Can be improved (with longer run) by backtracking.\n",
    "Better suited for categorical features & classification than continuous regression.\n",
    "\n",
    "#### FOIL\n",
    "First Order Inductive Learner algorithm. \n",
    "This is a hill climbing algorithm.\n",
    "It adds one rule at a time.\n",
    "Uses separate-and-conquer, as opposed to divide-and-conquer.\n",
    "\n",
    "#### C4.5\n",
    "This became world's most popular machine learning tool in 1980s.\n",
    "It was implemented as Weka J48.\n",
    "\n",
    "Improvments relative to ID3:\n",
    "C4 is better at selecting thresholds on continuous features.\n",
    "C4 gnores missing data during entropy calculation.\n",
    "In C4, the features can be weighted.\n",
    "C4 prunes the final tree to remove unhelpful branches.\n",
    "\n",
    "#### See5\n",
    "This was commercial software.\n",
    "C5 requires less CPU and less RAM. \n",
    "C5 makes smaller trees.\n",
    "C5 uses boosting (training sequentially on the residual cases).\n",
    "C5 uses feature selection (winnowing).\n",
    "C5 weights the different classification-error classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brieman's algorithms\n",
    "#### CART\n",
    "CART = Classification And Regression Tree.\n",
    "CART was the acronym used by Brieman, but now it is a generic term.\n",
    "\n",
    "#### Boosted trees\n",
    "CART uses multiple trees in series (boosting) for incremental refinement.\n",
    "This was used by AdaBoot.\n",
    "Implemented by [XGBoost](https://xgboost.readthedocs.io/en/stable/tutorials/model.html).\n",
    "Implemented by sklearn as GradientBoostingClassifier.\n",
    "\n",
    "It is computationally preferable to train many small trees in series rather than one big tree. \n",
    "Each tree is trained to classifiy what the previous tree misclassified.\n",
    "We say tree_2 classifies the residuals of tree_1.\n",
    "\n",
    "Unclear to me: how boosted trees are used for prediction,\n",
    "when you don't know whether the first tree's prediction was wrong.\n",
    "Documentation says we combine the scores generated by the various models,\n",
    "possibly using a weighted average, and possibly using trained weights.\n",
    "\n",
    "#### Random Forests\n",
    "RF reduces variance and avoids overitting.\n",
    "RF trains and predicts with multiple trees in parallel.\n",
    "Each tree is built on a random sample of the instances,\n",
    "on a random sample of the features, or both.\n",
    "Thus, each tree is imperfect, and the collection avoids overfitting.\n",
    "\n",
    "RF uses bagging = boosted aggregation.\n",
    "Each tree is built by sampling WITH replacement, called bootstrap.\n",
    "This ensures that each tree is independent of the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impurity metrics\n",
    "Each algorithm decides whether to split a node based on its impurity.  \n",
    "There are several impurity metrics.  \n",
    "Let prob(x) = proportion of examples in this node belonging to class x.  \n",
    "Sum over all classes.  \n",
    "For splits into more than two child nodes, impurity of parent is sum of impurity of children, each weighted by their portion of the examples.\n",
    "\n",
    "#### Entropy\n",
    "Entropy = sum [ -1 * prob(x) * lg (prob(x) ) ] , max=1.0, min=0.0\n",
    "#### Gini\n",
    "Gini = 1 - sum [ square (prob(x)) ] , max = 0.5 for binary, min=0.0\n",
    "#### Classification error\n",
    "Simple concept: a leaf \"predicts\" the majority class, so the rest is error.\n",
    "There are many ways to say this:\n",
    "* Classification error of node = 1 - (portion in largest class)  \n",
    "* Classification error of node = (portion outside largest class), max = 0.5 for binary  \n",
    "* Classification error of node = (total predicted wrong) / (training instances)\n",
    "\n",
    "#### Information gain\n",
    "You can measure the information gain of each node in a tree.  \n",
    "Information gain = reduction of uncertainty = Entropy before split - Entropy after split.\n",
    "\n",
    "#### FOIL information gain\n",
    "FOIL adds one rule at a time until the information gain is too small.  \n",
    "Say rule 0 finds $p_0$ positive and $n_0$ negative samples and $t_0=p_0+n_0$.  \n",
    "Say rule 1 finds $p_1$ positive and $n_1$ negative samples and $t_1=p_1+n_1$.  \n",
    "Determine whether the extra rule is worth it by:     \n",
    "Gain = $p_1 [ lg(\\frac{p_1}{t_1}) - lg(\\frac{p_0}{t_0}) ]$\n",
    "\n",
    "#### Collective impurity\n",
    "The collective impurity of a node is the weighted sum of its children,\n",
    "where fractional impurity is is weighted by the portion of training instances covered.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy: 0.6500224216483541 \n",
      "gini: 0.2777777777777777 \n",
      "error: 0.16666666666666663\n"
     ]
    }
   ],
   "source": [
    "# This node contains 5 of class1, 1 of class0.\n",
    "# This node will predict the majority class, class1.\n",
    "from math import log2\n",
    "class0=1\n",
    "class1=5\n",
    "total=class0+class1\n",
    "prob0=class0/total\n",
    "prob1=class1/total\n",
    "entropy = - ( prob0*log2(prob0) + prob1*log2(prob1) )\n",
    "gini = 1 - (prob0**2 + prob1**2)\n",
    "error = 1 - max(prob0,prob1)\n",
    "print ('entropy:',entropy,'\\ngini:',gini,'\\nerror:',error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping\n",
    "Decision tree tends to overfit.  \n",
    "Use a stopping criteria to stop the splitting.  \n",
    "Or use tree pruning after the fact.  \n",
    "\n",
    "Stopping options.  \n",
    "Instead of splitting till nodes are pure, split till impurity is below threshold.  \n",
    "Or, stop when splitting doesn't reduce impurity by a threshold amount.  \n",
    "Or, stop when no remaining feature introduces a statistical (t-test or chi-square) change in the class distribution.    \n",
    "\n",
    "Pruning example.   \n",
    "A node has a 20:10 mix of 2 classes, error = 10/30.  \n",
    "The node has 4 leaf nodes with total error = 9/30.  \n",
    "The node was split because the split redued error.  \n",
    "Pessimistic error = add a penalty of e.g. 0.5 per node.   \n",
    "Then parent error = 10.5/30 but leaf error = 11/30, and this split should be pruned.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
