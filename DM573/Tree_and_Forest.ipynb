{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impurity metrics\n",
    "Let prob(x) = proportion of examples in this node belonging to class x.\n",
    "Sum over all classes.\n",
    "\n",
    "Entropy = sum [ -1 * prob(x) * lg (prob(x) ) ] , max=1.0\n",
    "\n",
    "Gini = 1 - sum [ square (prob(x)) ] , max = 0.5 for binary\n",
    "\n",
    "Classification error = 1 - max [ scores for wrong classes ], max = 0.5 for binary\n",
    "\n",
    "Information gain = reduction of uncertainty = Entropy before split - Entropy after.\n",
    "\n",
    "For splits into more than two child nodes, impurity of parent is sum of impurity of children, each weighted by their portion of the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hunt's algorithm\n",
    "This is the splitting criteria used by all the other algorithms listed here.\n",
    "It is optimal if all feature combinations are present in the training data (unlikely)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quinlan: ID3, C4.5, See5\n",
    "### ID3\n",
    "Iterative Dichotomizer. \n",
    "At each node, review the so-far unused features. \n",
    "Choose feature with max information gain or min entropy.\n",
    "Choose one or more binary rules for two or more child nodes.\n",
    "Stop when out of examples, out of features, or node is pure (leaf). \n",
    "\n",
    "Greedy, non-optimal, recursive.\n",
    "Tends to overfit. \n",
    "Can be improved (with longer run) by backtracking.\n",
    "Better suited for categorical features & classification than continuous regression.\n",
    "\n",
    "### C4.5\n",
    "Became world's most popular machine learning tool in 1980s.\n",
    "Implemented as Weka J48.\n",
    "\n",
    "Improvments relative to ID3:\n",
    "Better at selecting thresholds on continuous features.\n",
    "Ignores missing data during entropy calculation.\n",
    "Features can be weighted.\n",
    "Prune the final tree to remove unhelpful branches.\n",
    "\n",
    "Improved again in See5 (commercial):\n",
    "Requires less CPU and less RAM. \n",
    "Makes smaller trees.\n",
    "Uses boosting.\n",
    "Uses feature selection (winnowing).\n",
    "Weights the different classification-error classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brieman: CART, RF\n",
    "CART is a generic term for classification and regression tree.\n",
    "\n",
    "Boosted Trees: incremental boost as in AdaBoot. \n",
    "Build tree_2 to classify the cases misclassified by tree_1 (residuals).\n",
    "[Is tree_2 deployed before or after tree_1???]\n",
    "\n",
    "RF is one type of bagging (boosted aggregation) \n",
    "i.e. each tree built by sampling with replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest algorithm can be used for classification or regression.\n",
    "RF is robust to missing values, numeric & categorical features, \n",
    "and numeric features at different scales. \n",
    "A trained RF is explainable; \n",
    "it can rank the features by their importance to the decision making process.\n",
    "\n",
    "A decision tree, DT, is an ok classifier. \n",
    "Here is how to build a DT. \n",
    "Always operate on the most impure node i.e. \n",
    "one that still receives 50% cancer & 50% normal instances.\n",
    "For that node, select a feature & threshold that is optimal for splitting. \n",
    "For example, age & 50 would be a good choice \n",
    "if age>50 would put most of the cancer cases on the right, \n",
    "and age<=50 would put most of the normal cases on the left. \n",
    "\n",
    "But a single DT tends to overfit.\n",
    "That means high variance and poor generalization.\n",
    "\n",
    "One solution is random forest,\n",
    "which is an ensemble of many trees.\n",
    "RF is a strong classifier built from weak trees.\n",
    "Each tree incorporates randomness so each is slightly different (and wrong).\n",
    "The ensemble layer aggregates the tree decisions.\n",
    "The ensemble layer can ask each tree for its best guess (called winner-take-all), \n",
    "then apply majority rule; this is what the orginal RF paper suggested.\n",
    "Or it can ask each tree to assign a probability to each class, and sum those up;\n",
    "this is what the sklearn RandomForestClassifier does.\n",
    "\n",
    "There are several ways to make random trees.\n",
    "You can build each tree from a random subset of the training data.\n",
    "Or you can build each tree from a random subset of the features.\n",
    "Or you can do both. This is what the sklearn RandomForestClassifier does.\n",
    "\n",
    "When using a random subset of instances (called the bag), \n",
    "you can estimate the tree's accuracy on the remaining instances \n",
    "(called out-of-bag, or OOB)\n",
    "to arrive at an OOB score per tree.\n",
    "Then, the ensemble layer can weight each tree's vote based on its OOB.\n",
    "The sklearn RandomForestClassifier has options to do this.\n",
    "\n",
    "See sklearn [Ensemble Methods](https://scikit-learn.org/stable/modules/ensemble.html)\n",
    "section 1.11.2.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
