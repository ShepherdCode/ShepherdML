{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN to predict more than one value\n",
    "Previous exercise learned a series and predicted the next one.\n",
    "Now, predict a series.\n",
    "\n",
    "The book points out three ways to look ahead 10 steps.\n",
    "* Offset the training pairs i.e. on X(1), predict y(11). \n",
    "* In a loop, predict y, append the value to X, predict again.\n",
    "* Train the RNN to predict 10 values at a time. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from pathlib import Path\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Data generator and baseline for comparison.\n",
    "# Combination of 2 sine waves plus noise\n",
    "def generate_time_series (batch_size, n_steps):\n",
    "    freq1, freq2, offset1, offset2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offset1) * (freq1 * 10 + 10))\n",
    "    series += 0.2 * np.sin((time - offset2) * (freq2 * 20 + 20))\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict future 10 \n",
    "First attempt quite literally predicts the 10 future values.\n",
    "The entire training is based on predicting the 10 future values.\n",
    "(As opposed to predicting the NEXT 10 values at every step.)\n",
    "\n",
    "Alter the model to have 10 output nodes.\n",
    "Alter the training data to contain 10 future values.\n",
    "Alter the y_train to contain only the 10 future values of each instance.\n",
    "Thus, every backprop uses the error of predicting the 10 future values.\n",
    "This seems pretty dumb but it works pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = 50\n",
    "series = generate_time_series(10000,n_steps+10)\n",
    "# X = 7000 different series each with random variation\n",
    "#     * time steps 0..50\n",
    "# y = 7000 predictions\n",
    "#     * time steps 50..60\n",
    "X_train,y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
    "X_valid,y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
    "X_test,y_test = series[9000:, :n_steps], series[9000:, -10:, 0]\n",
    "y_train.shape\n",
    "# Every y is a vector of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 7s 32ms/step - loss: 0.0768 - val_loss: 0.0346\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 5s 25ms/step - loss: 0.0253 - val_loss: 0.0197\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 5s 23ms/step - loss: 0.0179 - val_loss: 0.0148\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 5s 24ms/step - loss: 0.0141 - val_loss: 0.0126\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 5s 24ms/step - loss: 0.0124 - val_loss: 0.0117\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0116 - val_loss: 0.0100\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 5s 23ms/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 5s 23ms/step - loss: 0.0106 - val_loss: 0.0094\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 5s 23ms/step - loss: 0.0101 - val_loss: 0.0089\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 5s 24ms/step - loss: 0.0095 - val_loss: 0.0102\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 5s 23ms/step - loss: 0.0097 - val_loss: 0.0084\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 5s 23ms/step - loss: 0.0095 - val_loss: 0.0099\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 5s 24ms/step - loss: 0.0094 - val_loss: 0.0092\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 5s 23ms/step - loss: 0.0092 - val_loss: 0.0087\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0093 - val_loss: 0.0092\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0089 - val_loss: 0.0078\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0092 - val_loss: 0.0113\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0095 - val_loss: 0.0095\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0087 - val_loss: 0.0083\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 5s 23ms/step - loss: 0.0090 - val_loss: 0.0082\n"
     ]
    }
   ],
   "source": [
    "rnn1 = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20,return_sequences=True,input_shape=[None,1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "rnn1.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = rnn1.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))  \n",
    "\n",
    "# loss: Value of cost function on training data.\n",
    "# val_loss: Value of cost function on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My validation loss is much worse than the book's\n",
    "# but my loss jumps up and down.\n",
    "# My poor values could be due to my small memory;\n",
    "# book does 7000 instances per batch compared to my 219.\n",
    "# The instability could be due to overfitting the small batches.\n",
    "y_pred = rnn1.predict(X_train)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict rolling 10\n",
    "Now do it the right way with a rolling definition.\n",
    "At every time step, predict the next 10\n",
    "and measure the loss on the next 10\n",
    "(as opposed to always using the same future 10).\n",
    "\n",
    "Modify the data so every y contains 10 future X values.\n",
    "Avoid look-ahead so the model remains causal\n",
    "i.e. only uses the past to predict the future.\n",
    "\n",
    "Use return_sequences=True in every layer. \n",
    "Thus, the last layer must operate on 10 time steps.\n",
    "This involves many matrix reshapes.\n",
    "Use the TimeDistributed class to handle this\n",
    "(although the Dense class is actually smart enough to do it). \n",
    "TimeDistributed is used when the output is a sequence not a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10K instances, 50 time steps each, with 10 predictions per step.\n",
    "Y = np.empty((10000,n_steps,10))\n",
    "for step_ahead in range(1,10+1):\n",
    "    Y[:,:,step_ahead-1] = series[:,step_ahead:step_ahead+n_steps,0]\n",
    "y_train = Y[:7000]\n",
    "y_valid = Y[7000:9000]\n",
    "y_test  = Y[9000:]\n",
    "\n",
    "rnn2 = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20,return_sequences=True,input_shape=[None,1]),\n",
    "    keras.layers.SimpleRNN(20,return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 6s 27ms/step - loss: 0.0191 - last_time_step_mse: 0.0070 - val_loss: 0.0172 - val_last_time_step_mse: 0.0051\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 5s 25ms/step - loss: 0.0180 - last_time_step_mse: 0.0060 - val_loss: 0.0173 - val_last_time_step_mse: 0.0057\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0180 - last_time_step_mse: 0.0060 - val_loss: 0.0173 - val_last_time_step_mse: 0.0055\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0177 - last_time_step_mse: 0.0057 - val_loss: 0.0173 - val_last_time_step_mse: 0.0054\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0177 - last_time_step_mse: 0.0058 - val_loss: 0.0186 - val_last_time_step_mse: 0.0074\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0176 - last_time_step_mse: 0.0058 - val_loss: 0.0168 - val_last_time_step_mse: 0.0048\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0173 - last_time_step_mse: 0.0055 - val_loss: 0.0172 - val_last_time_step_mse: 0.0047\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0175 - last_time_step_mse: 0.0058 - val_loss: 0.0171 - val_last_time_step_mse: 0.0054\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0174 - last_time_step_mse: 0.0055 - val_loss: 0.0163 - val_last_time_step_mse: 0.0048\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0172 - last_time_step_mse: 0.0055 - val_loss: 0.0173 - val_last_time_step_mse: 0.0062\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0171 - last_time_step_mse: 0.0054 - val_loss: 0.0164 - val_last_time_step_mse: 0.0052\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0169 - last_time_step_mse: 0.0053 - val_loss: 0.0165 - val_last_time_step_mse: 0.0048\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0169 - last_time_step_mse: 0.0053 - val_loss: 0.0168 - val_last_time_step_mse: 0.0053\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0168 - last_time_step_mse: 0.0052 - val_loss: 0.0179 - val_last_time_step_mse: 0.0065\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0168 - last_time_step_mse: 0.0053 - val_loss: 0.0177 - val_last_time_step_mse: 0.0072\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0166 - last_time_step_mse: 0.0050 - val_loss: 0.0166 - val_last_time_step_mse: 0.0051\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0166 - last_time_step_mse: 0.0051 - val_loss: 0.0166 - val_last_time_step_mse: 0.0054\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0166 - last_time_step_mse: 0.0052 - val_loss: 0.0177 - val_last_time_step_mse: 0.0065\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0167 - last_time_step_mse: 0.0053 - val_loss: 0.0162 - val_last_time_step_mse: 0.0049\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0166 - last_time_step_mse: 0.0052 - val_loss: 0.0164 - val_last_time_step_mse: 0.0049\n"
     ]
    }
   ],
   "source": [
    "# Training will use the rolling window of next 10 at every step.\n",
    "# But we only care about the loss on the future 10.\n",
    "# Use a custom callback function to report loss.\n",
    "# This generates extra numbers in the history below.\n",
    "# The future loss is lower than the cummulative loss.\n",
    "def last_time_step_mse(y_true,y_pred):\n",
    "    return keras.metrics.mean_squared_error(y_true[:,-1],y_pred[:,-1])\n",
    "optimizer = keras.optimizers.Adam(lr=0.005)\n",
    "rnn2.compile(loss=\"mse\", optimizer=optimizer, metrics=[last_time_step_mse])\n",
    "history = rnn2.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
