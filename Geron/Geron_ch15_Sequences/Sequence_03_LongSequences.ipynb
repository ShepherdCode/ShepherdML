{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long sequences\n",
    "By default, RNNs are better at short sequences.\n",
    "They are worse at long sequences because \n",
    "the (unrolled) RNN is very deep.\n",
    "All DNN have unstable gradients and long trainging requirements.\n",
    "RNNs tend to forget the earlier parts of the sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstable gradient problem\n",
    "Addressed this with...\n",
    "* Good parameter initialization.\n",
    "* Faster optimizers.\n",
    "* Dropout.\n",
    "* Different activations. \n",
    "* Use TensorBoard to monitor gradient size.\n",
    "* Gradient Clipping.\n",
    "* Layer Normalization (2016).\n",
    "\n",
    "The default activation is tanh().\n",
    "Avoid ReLU with RNN\n",
    "ReLU is better for DNN because it does not saturate \n",
    "i.e. it keeps increasing.\n",
    "In RNN, weights are shared across time steps,\n",
    "and ReLU tends to increase the weight at every time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer normalization\n",
    "Batch normalization does not work well on RNN. \n",
    "On DNN, it is applied across instances of a batch.\n",
    "But RNN has time steps within an instance.\n",
    "Even when configured to work on RNN, it does not do so hot.\n",
    "\n",
    "Layer normalization computes stats per layer, per instance, \n",
    "across all the features or units of that layer.\n",
    "Layer normalization works the same way for training and testing.\n",
    "Best applied just after input and before activation.\n",
    "\n",
    "Geron writes his own subclass of Keras Layer\n",
    "using Keras SimpleRNNCell.\n",
    "Now, Keras has a subclass of Layer called [LayerNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization).\n",
    "There is a simple demo at [Keras](https://keras.io/api/layers/normalization_layers/layer_normalization/).\n",
    "Maybe this didn't exist when the book came out.\n",
    "(Layer normalization was published in 2016. The book came out in 2018).\n",
    "\n",
    "LayerNormalization has lots of options. \n",
    "Its parameters can be set or learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from pathlib import Path\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "def generate_time_series (batch_size, n_steps):\n",
    "    freq1, freq2, offset1, offset2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offset1) * (freq1 * 10 + 10))\n",
    "    series += 0.2 * np.sin((time - offset2) * (freq2 * 20 + 20))\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "series = generate_time_series(10000,n_steps+10)\n",
    "X_train,y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
    "X_valid,y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
    "X_test,y_test = series[9000:, :n_steps], series[9000:, -10:, 0]\n",
    "\n",
    "Y = np.empty((10000,n_steps,10))\n",
    "for step_ahead in range(1,10+1):\n",
    "    Y[:,:,step_ahead-1] = series[:,step_ahead:step_ahead+n_steps,0]\n",
    "y_train = Y[:7000]\n",
    "y_valid = Y[7000:9000]\n",
    "y_test  = Y[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "219/219 [==============================] - 6s 29ms/step - loss: 0.0742 - val_loss: 0.0472\n",
      "Epoch 2/5\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0441 - val_loss: 0.0411\n",
      "Epoch 3/5\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0397 - val_loss: 0.0407\n",
      "Epoch 4/5\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0375 - val_loss: 0.0360\n",
      "Epoch 5/5\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0357 - val_loss: 0.0348\n"
     ]
    }
   ],
   "source": [
    "# First, repeat this run with no normalization.\n",
    "rnn1 = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20,return_sequences=True,input_shape=[None,1]),\n",
    "    keras.layers.SimpleRNN(20,return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "rnn1.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = rnn1.fit(X_train, y_train, epochs=5,\n",
    "                    validation_data=(X_valid, y_valid))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "219/219 [==============================] - 7s 31ms/step - loss: 0.0701 - val_loss: 0.0454\n",
      "Epoch 2/5\n",
      "219/219 [==============================] - 6s 27ms/step - loss: 0.0431 - val_loss: 0.0405\n",
      "Epoch 3/5\n",
      "219/219 [==============================] - 7s 30ms/step - loss: 0.0396 - val_loss: 0.0378\n",
      "Epoch 4/5\n",
      "219/219 [==============================] - 7s 32ms/step - loss: 0.0371 - val_loss: 0.0362\n",
      "Epoch 5/5\n",
      "219/219 [==============================] - 6s 26ms/step - loss: 0.0355 - val_loss: 0.0342\n"
     ]
    }
   ],
   "source": [
    "# Second, repeat the compute with LayerNormalization.\n",
    "rnn2 = keras.models.Sequential([\n",
    "    keras.layers.LayerNormalization(axis=1),\n",
    "    keras.layers.SimpleRNN(20,return_sequences=True,input_shape=[None,1]),\n",
    "    keras.layers.SimpleRNN(20,return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "rnn2.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = rnn2.fit(X_train, y_train, epochs=5,\n",
    "                    validation_data=(X_valid, y_valid))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer normalization did not help but we didn't expect it to help.\n",
    "It would only help if we were having an exploding gradient.\n",
    "This demo just shows that adding it did no harm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
