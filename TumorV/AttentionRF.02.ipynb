{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Predict CNN attention per nucleus.   \n",
    "Switch from numpy array loadtxt() to pandas dataframe read_csv().   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DEPTH = 8\n",
    "N_ESTIMATORS = 500\n",
    "CHOOSE_EVERY = 10\n",
    "SAVE_MEM = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-11 13:15:12.210602\n",
      "Python 3.9.6\n",
      "sklearn 1.1.1\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "from platform import python_version\n",
    "print('Python',python_version())\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import random\n",
    "import sklearn   # pip install --upgrade scikit-learn\n",
    "print('sklearn',sklearn.__version__)\n",
    "# The model\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch data.\n",
    "# This is the nucleus-level csv file: one row per nucleus from CellProfiler.\n",
    "FILENAME='Process100_Nucleus.csv'\n",
    "# This directory contains one Nucleus.csv file per patient.\n",
    "BASE_PATH='/home/jrm/Adjeroh/Glioma/August_Run/CellProfilerPerPatient/'  # Alien\n",
    "BASE_PATH='D:\\\\Adjeroh\\\\Glioma\\\\August_Run\\\\CellProfilerPerPatient\\\\'   # Windows\n",
    "\n",
    "# Patient data.\n",
    "# Each patch filename indicates patient/case [0:19] and WSI [0:23].\n",
    "# For example: TCGA-06-0129-01Z-00-DX1_5400_5100.png\n",
    "LEN_PATIENT_ID = 19\n",
    "FOLDS_META_FILE = '/home/jrm/Adjeroh/Glioma/August_Run/TrainValidSplits/aug_train_valid_splits.csv'  # Alien\n",
    "FOLDS_META_FILE = 'D:\\\\Adjeroh\\\\Glioma\\\\August_Run\\\\TrainValidSplits\\\\aug_train_valid_splits.csv'  # Windows\n",
    "# Cross validation\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "# These bin thresholds yield nucleus temperature bins of about equal weight.\n",
    "# See AT_PerPatient 01 notebook.\n",
    "BIN_MAX = [0.40,0.65,0.75,0.85,1.0]\n",
    "BIN_LABEL = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA = pd.read_csv(FOLDS_META_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patient_attention(fold,is_train,patient):\n",
    "    cls = 'mcls'\n",
    "    filename = f\"Attention_{cls}_fold{fold}.csv\"\n",
    "    filepath = BASE_PATH + patient + '/' + filename\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_one_fold(fold,is_train):\n",
    "    df = METADATA.loc[METADATA['Fold']==fold]\n",
    "    if is_train:\n",
    "        df = df.loc[df['Split']=='Train']\n",
    "    else:\n",
    "        df = df.loc[df['Split']=='Valid']\n",
    "    patients = df['Case'].tolist()\n",
    "    labels = df['Class'].tolist()\n",
    "    return patients,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a patient name, load the Nucleus.csv file.\n",
    "# Expect a string that matches a directory name, like 'TCGA-S9-A6UB-01Z-00'\n",
    "# Returns a numpy array.\n",
    "def load_one_patient_data(filepath):\n",
    "    # Reading csv file, skip row 1 = column headers\n",
    "    ary = np.loadtxt(filepath,skiprows=1,delimiter=',')\n",
    "    return ary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_one_patient_attention(nuc_path,fold,multiclass,patient):\n",
    "    cls='bcls'\n",
    "    if multiclass:\n",
    "        cls='mcls'\n",
    "    att_filename = f\"Attention_{cls}_fold{fold}.csv\"\n",
    "    att_path = BASE_PATH + patient + '/' + att_filename\n",
    "    #nuc_df = pd.read_csv(nuc_path)\n",
    "    #num_vals = len(nuc_df)\n",
    "    #Y = [0 for i in range(num_vals)]  # temporary placeholder\n",
    "    Y = [1,2,3]\n",
    "    return Y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    bad_cols = ['ImageNumber','ObjectNumber','Number_Object_Number',\n",
    "                'AreaShape_Orientation','Children_Cells_Count']\n",
    "    df = df.drop(columns=bad_cols)\n",
    "    bad_cols = [c for c in df.columns \n",
    "                if c.startswith('Location_') \n",
    "                or c.startswith('AreaShape_BoundingBoxM')\n",
    "                or c.startswith('AreaShape_Center')\n",
    "                or c.startswith('Neighbors_')]\n",
    "    df = df.drop(columns=bad_cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_patients_data(fold,patients,save_mem=False):\n",
    "    X = None  # dataframe with 600 feature columns, one row per nucleus\n",
    "    y = None  # list of labels = attention for each row\n",
    "    z = None  # list of patient name for each row\n",
    "    is_multiclass = True\n",
    "    count=0\n",
    "    for patient in patients:\n",
    "        print(patient)\n",
    "        count += 1\n",
    "        directory = patient+'/'\n",
    "        filepath=BASE_PATH+directory+FILENAME\n",
    "        #Xall = load_one_patient_data(filepath)\n",
    "        Xall = pd.read_csv(filepath)\n",
    "        yall = load_one_patient_attention(filepath,fold,is_multiclass,patient)\n",
    "        if (save_mem):\n",
    "            Xi = Xall.iloc[0::CHOOSE_EVERY] # e.g. choose every tenth patch\n",
    "            yi = yall[0::CHOOSE_EVERY].copy()  # e.g. choose every tenth patch\n",
    "        else:\n",
    "            Xi = Xall\n",
    "            yi = yall\n",
    "        zi = [patient]*len(Xi)  # same ID for all patches from one patient\n",
    "        if X is None:\n",
    "            X = Xi\n",
    "            y = yi\n",
    "            z = zi\n",
    "        else:\n",
    "            X = pd.concat( (X, Xi) )\n",
    "            y = np.concatenate( (y, yi) )\n",
    "            z = np.concatenate( (z, zi) )\n",
    "    # X combines all nuclei of all WSI for all patients requested.\n",
    "    # For debugging, we'll add column headers to the dataframe.\n",
    "    #X = pd.DataFrame(X)\n",
    "    #directory = patients[0] + '/'   # first patient is as good as any\n",
    "    #filepath=BASE_PATH+directory+FILENAME\n",
    "    #with open(filepath) as infile:\n",
    "    #    rows = csv.reader(infile)\n",
    "    #    for row in rows:\n",
    "    #        header = row    # first row has column headers\n",
    "    #        break\n",
    "    #X.columns = header\n",
    "    X = drop_columns(X)\n",
    "    return X,y,z   # patch data, patch labels, patch patient names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is slow, probably due to concatenating dataframes.\n",
    "# Consider a rewrite that concatenates csv before constructing a dataframe.\n",
    "def load_train_valid_set(fold):\n",
    "    is_train = True # training set\n",
    "    patients,cancers = get_metadata_one_fold(fold,is_train) \n",
    "    X_train,y_train,z_train = load_all_patients_data(fold,patients,SAVE_MEM) \n",
    "    print('Train lengths X,y,z:',X_train.shape,len(y_train),len(z_train))\n",
    "    is_train = False # validation set\n",
    "    patients,cancers = get_metadata_one_fold(fold,is_train) \n",
    "    X_valid,y_valid,z_valid = load_all_patients_data(fold,patients,SAVE_MEM) # reduces validation size\n",
    "    print('Valid lengths X,y,z:',X_valid.shape,len(y_valid),len(z_valid))\n",
    "    return X_train,y_train,z_train,X_valid,y_valid,z_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each patch votes on the class for its WSI.\n",
    "# Later, weight each patch label by the confidence i.e. score\n",
    "def aggregate_accuracy(y_pred,y_valid,z_valid):\n",
    "    L = len(y_pred)\n",
    "    if L != len(y_valid) or L != len(z_valid):\n",
    "        raise Exception('Lengths do not match')\n",
    "    correct = {}\n",
    "    incorrect = {}\n",
    "    patients = np.unique(z_valid)\n",
    "    for patient in patients:\n",
    "        correct[patient]=0  # accumulate correct votes on patches\n",
    "        incorrect[patient]=0  # accumulate incorrect votes\n",
    "    for i in range(L):\n",
    "        patient = z_valid[i]\n",
    "        label =   y_valid[i]\n",
    "        pred =    y_pred[i]\n",
    "        if pred == label:\n",
    "            correct[patient] += 1\n",
    "        else:\n",
    "            incorrect[patient] += 1\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for patient in patients:\n",
    "        denominator += 1\n",
    "        if correct[patient]>incorrect[patient]:\n",
    "            numerator += 1\n",
    "    accuracy = float(0)\n",
    "    if denominator>0:\n",
    "        accuracy = 100.0*numerator/denominator \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop: Load, Classify, Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_features(model):\n",
    "    # Prereqs: fit().\n",
    "    names = model.feature_names_in_\n",
    "    importances = model.feature_importances_\n",
    "    pairs = np.column_stack( (names,importances) )\n",
    "    top_array = sorted(pairs, key = lambda e:e[1], reverse=True)\n",
    "    # There must be a way to do this witout a loop!\n",
    "    top_list = []\n",
    "    for i in top_array:\n",
    "         top_list.append((i[1],i[0]))  # 0=feature_name, 1=importance\n",
    "    top_df = pd.DataFrame(top_list)\n",
    "    return top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-08-11 13:15:15.524826 Fold 0 Loading...\n",
      "TCGA-02-0004-01Z-00\n",
      "TCGA-02-0010-01Z-00\n",
      "TCGA-14-0789-01Z-00\n",
      "TCGA-02-0025-01Z-00\n",
      "TCGA-02-0033-01Z-00\n",
      "TCGA-02-0338-01Z-00\n",
      "TCGA-02-0439-01Z-00\n",
      "TCGA-02-0446-01Z-00\n",
      "TCGA-06-0129-01Z-00\n",
      "TCGA-06-0185-01Z-00\n",
      "TCGA-06-0189-01Z-00\n",
      "TCGA-06-0190-01Z-00\n",
      "TCGA-06-0201-01Z-00\n",
      "TCGA-06-0209-01Z-00\n",
      "TCGA-06-0211-01Z-00\n",
      "TCGA-06-0241-01Z-00\n",
      "TCGA-06-0879-01Z-00\n",
      "TCGA-06-1084-01Z-00\n",
      "TCGA-06-1801-01Z-00\n",
      "TCGA-08-0518-01Z-00\n",
      "TCGA-14-1401-01Z-00\n",
      "TCGA-14-1452-01Z-00\n",
      "TCGA-14-1453-01Z-00\n",
      "TCGA-14-1795-01Z-00\n",
      "TCGA-14-1829-01Z-00\n",
      "TCGA-15-1447-01Z-00\n",
      "TCGA-26-5133-01Z-00\n",
      "TCGA-26-5134-01Z-00\n",
      "TCGA-26-5139-01Z-00\n",
      "TCGA-27-1831-01Z-00\n",
      "TCGA-41-5651-01Z-00\n",
      "TCGA-DU-5874-01Z-00\n",
      "TCGA-DU-7009-01Z-00\n",
      "TCGA-DU-7014-01Z-00\n",
      "TCGA-HT-7603-01Z-00\n",
      "TCGA-HT-7693-01Z-00\n",
      "TCGA-HT-7881-01Z-00\n",
      "TCGA-HW-7487-01Z-00\n",
      "TCGA-HW-7491-01Z-00\n",
      "TCGA-HW-7495-01Z-00\n",
      "TCGA-QH-A65Z-01Z-00\n",
      "TCGA-S9-A6UB-01Z-00\n",
      "TCGA-S9-A7J1-01Z-00\n",
      "TCGA-HT-7483-01Z-00\n",
      "TCGA-HT-7482-01Z-00\n",
      "TCGA-FG-7643-01Z-00\n",
      "TCGA-DU-8167-01Z-00\n",
      "TCGA-DB-5274-01Z-00\n",
      "TCGA-DB-5270-01Z-00\n",
      "TCGA-HT-7873-01Z-00\n",
      "TCGA-HT-7902-01Z-00\n",
      "TCGA-QH-A6CZ-01Z-00\n",
      "TCGA-QH-A6XA-01Z-00\n",
      "TCGA-S9-A6WI-01Z-00\n",
      "TCGA-S9-A7IQ-01Z-00\n",
      "TCGA-CS-4943-01Z-00\n",
      "TCGA-DU-5854-01Z-00\n",
      "TCGA-DU-6402-01Z-00\n",
      "TCGA-FG-5963-01Z-00\n",
      "TCGA-FG-A60L-01Z-00\n",
      "TCGA-HT-7854-01Z-00\n",
      "TCGA-HT-8104-01Z-00\n",
      "TCGA-DB-A4XF-01Z-00\n",
      "TCGA-DH-A66B-01Z-00\n",
      "TCGA-S9-A6U6-01Z-00\n",
      "TCGA-HT-A5RC-01Z-00\n",
      "TCGA-QH-A6CS-01Z-00\n",
      "TCGA-QH-A6XC-01Z-00\n",
      "TCGA-S9-A7IS-01Z-00\n",
      "TCGA-S9-A7IZ-01Z-00\n",
      "TCGA-S9-A7R2-01Z-00\n",
      "TCGA-DB-A64P-01Z-00\n",
      "TCGA-DH-A669-01Z-00\n",
      "TCGA-HW-A5KJ-01Z-00\n",
      "TCGA-QH-A65R-01Z-00\n",
      "TCGA-QH-A6X8-01Z-00\n",
      "TCGA-S9-A6TW-01Z-00\n",
      "TCGA-S9-A7J2-01Z-00\n",
      "TCGA-S9-A7J3-01Z-00\n",
      "TCGA-02-0430-01Z-00\n",
      "TCGA-06-5412-01Z-00\n",
      "TCGA-14-0787-01Z-00\n",
      "TCGA-28-1746-01Z-00\n",
      "TCGA-28-1749-01Z-00\n",
      "TCGA-DB-A64L-01Z-00\n",
      "TCGA-DU-7015-01Z-00\n",
      "TCGA-DU-8164-01Z-00\n",
      "TCGA-HT-7605-01Z-00\n",
      "TCGA-HT-7676-01Z-00\n",
      "TCGA-HT-7681-01Z-00\n",
      "TCGA-DU-6404-01Z-00\n",
      "TCGA-CS-5397-01Z-00\n",
      "TCGA-DU-7299-01Z-00\n",
      "TCGA-HT-7606-01Z-00\n",
      "TCGA-HT-7884-01Z-00\n",
      "TCGA-TM-A7CF-01Z-00\n",
      "TCGA-HT-A5RA-01Z-00\n",
      "TCGA-HT-A618-01Z-00\n",
      "TCGA-S9-A6U1-01Z-00\n",
      "TCGA-HT-A5R9-01Z-00\n",
      "TCGA-QH-A6CU-01Z-00\n",
      "Train lengths X,y,z: (148324, 623) 101 148324\n",
      "TCGA-02-0285-01Z-00\n",
      "TCGA-06-0125-01Z-00\n",
      "TCGA-06-0214-01Z-00\n",
      "TCGA-06-0216-01Z-00\n",
      "TCGA-06-0645-01Z-00\n",
      "TCGA-06-0878-01Z-00\n",
      "TCGA-08-0520-01Z-00\n",
      "TCGA-14-0786-01Z-00\n",
      "TCGA-15-1446-01Z-00\n",
      "TCGA-DU-8165-01Z-00\n",
      "TCGA-HT-7616-01Z-00\n",
      "TCGA-HT-A617-01Z-00\n",
      "TCGA-S9-A6WE-01Z-00\n",
      "TCGA-DU-6408-01Z-00\n",
      "TCGA-DB-A4XE-01Z-00\n",
      "TCGA-FG-7636-01Z-00\n",
      "TCGA-S9-A6WO-01Z-00\n",
      "TCGA-CS-4942-01Z-00\n",
      "TCGA-S9-A6U9-01Z-00\n",
      "TCGA-S9-A6WL-01Z-00\n",
      "TCGA-TM-A7C3-01Z-00\n",
      "TCGA-S9-A6TX-01Z-00\n",
      "TCGA-08-0517-01Z-00\n",
      "TCGA-S9-A7QY-01Z-00\n",
      "TCGA-DB-A4XG-01Z-00\n",
      "TCGA-DH-A66G-01Z-00\n",
      "Valid lengths X,y,z: (36559, 623) 26 36559\n",
      "2022-08-11 13:18:42.895706 Train...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [148324, 101]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14084/182442007.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# min_samples_leaf=1 (default) led to overfitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mrfc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_DEPTH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mN_ESTIMATORS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mrfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Ranked feature imporances...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jmill02\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    332\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         )\n",
      "\u001b[1;32mc:\\users\\jmill02\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    594\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    597\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jmill02\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1090\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1092\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1093\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1094\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jmill02\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    385\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    388\u001b[0m             \u001b[1;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[1;33m%\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [148324, 101]"
     ]
    }
   ],
   "source": [
    "patch_accuracies = []   # summary statistics\n",
    "patient_accuracies = [] # summary statistics\n",
    "\n",
    "for fold in range(NUM_FOLDS):\n",
    "    print()\n",
    "    print(datetime.datetime.now(),'Fold',fold,'Loading...')\n",
    "    X_train,y_train,z_train,X_valid,y_valid,z_valid = load_train_valid_set(fold)    \n",
    "\n",
    "    # The shuffle is not strictly necessary with RF.\n",
    "    # CNN models are sensitive to train set order but RF models are not. \n",
    "    # print(datetime.datetime.now(),'Shuffle...')\n",
    "    # X_train,y_train = sklearn.shuffle(X_train,y_train)\n",
    "\n",
    "    print(datetime.datetime.now(),'Train...')\n",
    "    # min_samples_leaf=1 (default) led to overfitting\n",
    "    rfc = RandomForestClassifier(max_depth=MAX_DEPTH,n_estimators=N_ESTIMATORS)\n",
    "    rfc.fit(X_train,y_train)  # slow\n",
    "    \n",
    "    print(datetime.datetime.now(),'Ranked feature imporances...')\n",
    "    top = important_features(rfc)\n",
    "    #pd.set_option('display.max_rows', None)\n",
    "    print(top.loc[:10])\n",
    "    \n",
    "    print(datetime.datetime.now(),'Evaluate...')\n",
    "    y_pred = rfc.predict(X_train)\n",
    "    matches = np.count_nonzero(y_train==y_pred)\n",
    "    accuracy_t = 100.0 * matches / len(y_pred)\n",
    "    print('Fold',fold,'Nucleus-level Training Accuracy:',accuracy_t)\n",
    "    \n",
    "    print(datetime.datetime.now(),'Validate...')\n",
    "    y_pred = rfc.predict(X_valid)\n",
    "    matches = np.count_nonzero(y_valid==y_pred)\n",
    "    accuracy_v = 100.0 * matches / len(y_pred)\n",
    "    print('Fold',fold,'Nucleus-level Validation Accuracy:',accuracy_v)\n",
    "    patch_accuracies.append(accuracy_v)\n",
    "    \n",
    "    accuracy_p = aggregate_accuracy(y_pred,y_valid,z_valid)\n",
    "    patient_accuracies.append(accuracy_p)\n",
    "    print('Fold',fold,'Patient-level Validation Accuracy:',accuracy_p)\n",
    "    \n",
    "    # This shouldn't be necessary but it seems to reduce memory footprint.\n",
    "    X_train=None\n",
    "    X_valid=None\n",
    "    y_train=None\n",
    "    y_valid=None\n",
    "    z_train=None\n",
    "    z_valid=None\n",
    "    y_pred = None\n",
    "    matches = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "if SAVE_MEM:\n",
    "    print('Sampling every',CHOOSE_EVERY,'th record.')\n",
    "print('Cross validation nucleus-level accuracy:',patch_accuracies)\n",
    "print('mean:',np.mean(patch_accuracies),'pop std:',np.std(patch_accuracies,ddof=1))\n",
    "print('Cross validation patient-level accuracy:',patient_accuracies)\n",
    "print('mean:',np.mean(patient_accuracies),'pop std:',np.std(patient_accuracies,ddof=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
