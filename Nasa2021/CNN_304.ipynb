{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojm_6E9f9Kcf"
   },
   "source": [
    "# CNN 304\n",
    "Start with CNN 303 which demonstrated the use of Conv1D layers.\n",
    "\n",
    "Clean up the code e.g. move onehot() to our tools library.\n",
    "\n",
    "Since the CNN was overfitting, try again here but with no padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hh6XplUvC0j0",
    "outputId": "be038be8-416b-4fec-f47e-c7f66a12127e"
   },
   "outputs": [],
   "source": [
    "NC_FILENAME='ncRNA.tiny50.fasta'\n",
    "PC_FILENAME='pcRNA.tiny50.fasta'\n",
    "#NC_FILENAME='ncRNA.gc34.processed.fasta'\n",
    "#PC_FILENAME='pcRNA.gc34.processed.fasta'\n",
    "\n",
    "MODEL_FILE='CNN304'  \n",
    "DATAPATH=''\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    PATH='/content/drive/'\n",
    "    drive.mount(PATH)\n",
    "    DATAPATH=PATH+'My Drive/data/'  # must end in \"/\"\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    DATAPATH='data/'  # must end in \"/\"\n",
    "NC_FILENAME = DATAPATH+NC_FILENAME\n",
    "PC_FILENAME = DATAPATH+PC_FILENAME\n",
    "MODEL_FILE=DATAPATH+MODEL_FILE\n",
    "\n",
    "EPOCHS=5\n",
    "SPLITS=1\n",
    "K=1\n",
    "VOCABULARY_SIZE=4**K+1   # e.g. K=3 => 64 DNA K-mers + 'NNN'\n",
    "EMBED_DIMEN=2\n",
    "FILTERS=16\n",
    "KERNEL=3\n",
    "NEURONS=16\n",
    "DROP=0.25\n",
    "ACT=\"tanh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9TY3HK9ZklE",
    "outputId": "a706bf58-e2ff-4146-cf40-3ba28b2217e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yahoo!\n"
     ]
    }
   ],
   "source": [
    "# Load our own tools\n",
    "GITHUB = True\n",
    "if GITHUB:\n",
    "    #!pip install requests  # Uncomment this if necessary. Seems to be pre-installed.\n",
    "    import requests\n",
    "    r = requests.get('https://raw.githubusercontent.com/ShepherdCode/ShepherdML/master/Strings/tools_fasta.py')\n",
    "    with open('tools_fasta.py', 'w') as f:\n",
    "        f.write(r.text)\n",
    "    # TO DO: delete the file after import\n",
    "import tools_fasta as tools\n",
    "tools.yahoo()  # If this prints \"Yahoo!\" the the import was successful.\n",
    "\n",
    "TOOLS_CHANGED = False   # set to True to re-run with a new version of tools\n",
    "if TOOLS_CHANGED:\n",
    "  from importlib import reload \n",
    "  tools=reload(tools)\n",
    "  print(dir(tools))   # run this to see EVERYTHING in the tools module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VQY7aTj29Kch"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LayerNormalization\n",
    "import time\n",
    "dt='float32'\n",
    "tf.keras.backend.set_floatx(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7jcg6Wl9Kc2"
   },
   "source": [
    "Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qLFNO1Xa9Kc3"
   },
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    adam_default_learn_rate = 0.001\n",
    "    schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate = adam_default_learn_rate*10,\n",
    "        #decay_steps=100000, decay_rate=0.96, staircase=True)\n",
    "        decay_steps=10000, decay_rate=0.99, staircase=True)\n",
    "    # learn rate = initial_learning_rate * decay_rate ^ (step / decay_steps)\n",
    "    alrd = tf.keras.optimizers.Adam(learning_rate=schedule)\n",
    "    bc=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    print(\"COMPILE...\")\n",
    "    #model.compile(loss=bc, optimizer=alrd, metrics=[\"accuracy\"])\n",
    "    model.compile(loss=bc, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    print(\"...COMPILED\")\n",
    "    return model\n",
    "\n",
    "def build_model():\n",
    "    #embed_layer  = keras.layers.Embedding(\n",
    "    #    input_dim=VOCABULARY_SIZE, output_dim=4, mask_zero=True)\n",
    "    SHAPE=(1000,5)  # 1000 time steps, 5 features\n",
    "    clayer1 = keras.layers.Conv1D(FILTERS,KERNEL,activation=ACT,padding=\"same\",\n",
    "                                 input_shape=SHAPE)\n",
    "    clayer2 = keras.layers.Conv1D(FILTERS,KERNEL,activation=ACT,padding=\"same\")\n",
    "    clayer3 = keras.layers.MaxPooling1D(2)\n",
    "    clayer4 = keras.layers.Conv1D(FILTERS,KERNEL,activation=ACT,padding=\"same\")\n",
    "    clayer5 = keras.layers.Conv1D(FILTERS,KERNEL,activation=ACT,padding=\"same\")\n",
    "    clayer6 = keras.layers.MaxPooling1D(2)\n",
    "    clayer7 = keras.layers.Flatten()\n",
    "\n",
    "    dlayer1 = keras.layers.Dense(NEURONS, activation=ACT,dtype=dt, input_shape=[1000])\n",
    "    #dlayer1 = keras.layers.Dense(NEURONS, activation=ACT,dtype=dt)\n",
    "    dlayer2 = keras.layers.Dropout(DROP)\n",
    "    dlayer3 = keras.layers.Dense(NEURONS, activation=ACT,dtype=dt)\n",
    "    dlayer4 = keras.layers.Dropout(DROP)\n",
    "    output_layer = keras.layers.Dense(1, activation=\"sigmoid\", dtype=dt)\n",
    "\n",
    "    cnn = keras.models.Sequential()\n",
    "    #cnn.add(embed_layer)\n",
    "    cnn.add(clayer1)\n",
    "    cnn.add(clayer2)\n",
    "    cnn.add(clayer3)\n",
    "    #cnn.add(clayer4)\n",
    "    #cnn.add(clayer5)\n",
    "    #cnn.add(clayer6)\n",
    "    cnn.add(clayer7)\n",
    "    cnn.add(dlayer1)\n",
    "    #cnn.add(dlayer2)\n",
    "    cnn.add(dlayer3)\n",
    "    #cnn.add(dlayer4)\n",
    "    cnn.add(output_layer)\n",
    "    mlpc = compile_model(cnn)\n",
    "    return mlpc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdIS2utq9Kc9"
   },
   "source": [
    "Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BVo4tbB_9Kc-"
   },
   "outputs": [],
   "source": [
    "def do_cross_validation(X,y,given_model):\n",
    "    cv_scores = []\n",
    "    fold=0\n",
    "    splitter = ShuffleSplit(n_splits=SPLITS, test_size=0.1, random_state=37863)\n",
    "    for train_index,valid_index in splitter.split(X):\n",
    "        fold += 1\n",
    "        X_train=X[train_index] # use iloc[] for dataframe\n",
    "        y_train=y[train_index]\n",
    "        X_valid=X[valid_index]\n",
    "        y_valid=y[valid_index]        \n",
    "        # Avoid continually improving the same model.\n",
    "        model = compile_model(keras.models.clone_model(given_model))\n",
    "        bestname=MODEL_FILE+\".cv.\"+str(fold)+\".best\"\n",
    "        mycallbacks = [keras.callbacks.ModelCheckpoint(\n",
    "            filepath=bestname, save_best_only=True, \n",
    "            monitor='val_accuracy', mode='max')]   \n",
    "        print(\"FIT\")\n",
    "        start_time=time.time()\n",
    "        history=model.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
    "                epochs=EPOCHS, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
    "                callbacks=mycallbacks,\n",
    "                validation_data=(X_valid,y_valid) )\n",
    "        end_time=time.time()\n",
    "        elapsed_time=(end_time-start_time)                        \n",
    "        print(\"Fold %d, %d epochs, %d sec\"%(fold,EPOCHS,elapsed_time))\n",
    "        pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "        plt.grid(True)\n",
    "        plt.gca().set_ylim(0,1)\n",
    "        plt.show()\n",
    "        best_model=keras.models.load_model(bestname)\n",
    "        scores = best_model.evaluate(X_valid, y_valid, verbose=0)\n",
    "        print(\"%s: %.2f%%\" % (best_model.metrics_names[1], scores[1]*100))\n",
    "        cv_scores.append(scores[1] * 100)  \n",
    "    print()\n",
    "    print(\"%d-way Cross Validation mean %.2f%% (+/- %.2f%%)\" % (fold, np.mean(cv_scores), np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd3Wj_vI9KdP"
   },
   "source": [
    "## Train on RNA lengths 200-1Kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYstxXr6RQHB"
   },
   "source": [
    "## to do:\n",
    "Stop calling make_kmers() then onehot().\n",
    "We're only using make_kmers convert ACGT to 1234.\n",
    "Instead, write an RNA_to_onehot() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8fNo6sn9KdH",
    "outputId": "b51a0449-5878-4612-a347-7adb91a48313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from files.\n",
      "Ready: train_set\n",
      "Data reshape\n",
      "First sequence as ints\n",
      "[4 2 1 4 2 1 3 4 2 2 1 1 1 3 4 2 2 1 3 2 1 3 4 4 3 4 2 2 2 4 2 2 4 3 3 1 1\n",
      " 4 2 2 3 4 4 3 3 2 4 4 3 2 2 4 2 2 3 3 2 1 4 4 4 4 4 3 3 2 2 2 4 4 3 2 2 4\n",
      " 4 4 4 1 3 3 3 4 4 3 2 2 1 3 1 4 4 1 1 1 1 3 1 2 1 3 3 1 4 3 2 2 2 1 3 2 4\n",
      " 1 3 4 4 4 3 1 1 4 4 4 4 1 3 1 4 1 1 1 2 1 1 2 3 1 1 4 1 1 4 4 4 2 3 4 1 3\n",
      " 2 1 4 1 1 1 4 1 4 3 4 2 2 2 1 1 3 2 4 4 1 3 4 4 4 3 3 3 1 2 1 4 1 2 4 4 1\n",
      " 4 3 2 4 1 1 1 1 1 1 2 1 4 4 1 4 4 3 3 4 4 3 4 4 4 1 4 2 4 3 1 3 1 4 4 2 1\n",
      " 3 1 1 4 4 1 1 3 2 1 4 4 4 4 1 4 1 4 4 4 4 1 4 4 4 3 2 4 3 2 2 4 2 4 3 3 2\n",
      " 2 1 2 2 2 4 1 2 4 2 4 2 4 4 2 2 4 1 1 2 1 2 4 2 4 2 4 2 2 2 4 2 4 2 2 2 1\n",
      " 3 4 4 4 4 3 4 2 2 3 2 2 4 4 2 2 2 4 3 2 2 4 2 2 4 2 4 4 2 4 3 3 3 3 3 1 3\n",
      " 4 4 1 3 1 4 2 3 1 3 4 4 3 4 1 1 2 1 1 3 1 1 2 1 4 3 2 2 1 2 4 3 4 2 4 2 3\n",
      " 2 4 3 3 2 4 3 2 1 3 2 3 4 3 4 3 3 4 2 2 2 2 4 4 1 2 2 1 3 1 3 4 3 1 3 3 1\n",
      " 4 3 2 3 1 1 3 1 3 1 1 3 3 4 3 3 2 4 3 4 2 4 3 2 1 1 1 2 2 1 3 3 1 1 3 1 3\n",
      " 1 3 2 2 2 4 2 1 2 2 3 3 3 1 1 2 2 2 3 4 2 2 1 3 2 4 3 2 2 1 2 2 4 4 3 1 1\n",
      " 2 4 4 3 3 1 2 4 4 2 2 1 1 3 2 2 4 2 2 1 3 1 1 2 4 3 4 3 1 3 3 3 1 4 1 1 1\n",
      " 4 3 4 1 4 3 1 4 4 4 4 1 1 1 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-cb9c503f1320>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"First sequence as ints\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monehot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"First sequence as onehot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Source/ShepherdML/Nasa2021/tools_fasta.py\u001b[0m in \u001b[0;36monehot\u001b[0;34m(seqs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 3 = G\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 4 = T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0mletters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "MINLEN=200\n",
    "MAXLEN=1000\n",
    "print(\"Load data from files.\")\n",
    "nc_seq=tools.load_fasta(NC_FILENAME,0)\n",
    "pc_seq=tools.load_fasta(PC_FILENAME,1)\n",
    "train_set=pd.concat((nc_seq,pc_seq),axis=0)\n",
    "nc_seq=None\n",
    "pc_seq=None\n",
    "print(\"Ready: train_set\")\n",
    "#train_set\n",
    "subset=tools.make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
    "print (\"Data reshape\")\n",
    "(X_train,y_train)=tools.make_kmers(K,MAXLEN,subset)\n",
    "# print(X_train.shape)\n",
    "print(\"First sequence as ints\")\n",
    "print(X_train[0])  \n",
    "X_train=tools.onehot(X_train)\n",
    "print(\"First sequence as onehot\")\n",
    "print(X_train[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G1HuSs8ZbeL4",
    "outputId": "fdf6e30d-525b-410c-af44-88f3b0f93364"
   },
   "outputs": [],
   "source": [
    "print (\"Compile the model\")\n",
    "model=build_model()\n",
    "print (\"Summarize the model\")\n",
    "print(model.summary())  # Print this only once\n",
    "model.save(MODEL_FILE+'.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mQ8eW5Rg9KdQ",
    "outputId": "36d37405-b632-4c09-c21a-5b45f256954c"
   },
   "outputs": [],
   "source": [
    "print (\"Cross valiation\")\n",
    "do_cross_validation(X_train,y_train,model)  \n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4fh2GI8beMQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_303.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
