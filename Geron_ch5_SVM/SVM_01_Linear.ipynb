{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "# SVM provides large-margin separation.\n",
    "# SVM was traditionally implemented with math, not neural nets.\n",
    "# The math is called QP, Quadratic Programming.\n",
    "\n",
    "# SVM math does not scale to deep learning.\n",
    "# SVM good for linear and multiclass classification.\n",
    "# SVM also good for linear regression in high dimensions.\n",
    "# SVM sensitive to outliers. \n",
    "# SVM inverse detects outliers. In SKLearn, use OneClassSVM.\n",
    "\n",
    "# \"Support vectors\" are the data examples closest to the middle. \n",
    "# The support vectors define the margins.\n",
    "# SVM chooses a decision boundary that maximizes margins between support vectors.\n",
    "\n",
    "# Important to scale all features prior to SVM.\n",
    "# Otherwise SVM will focus on feature with largest scale.\n",
    "\n",
    "# Hard-margin mode: allow no data in margin. Possibly no solution.\n",
    "# Soft-margin mode: minimize data in margin.\n",
    "\n",
    "# Hyperparameter C for regularization. Inverse of usual. \n",
    "# Large C => small margin, overfits to get every last point.\n",
    "# Small C => large margin, generalizes.\n",
    "\n",
    "# Hinge loss defines the SVM. \n",
    "# It penalizes data in the margin.\n",
    "\n",
    "# Book says solving the \"dual problem\" instead of \"primary problem\"\n",
    "# enables use of the kernel trick and is faster.\n",
    "# Comments online say use dual only when #features and #samples are balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear SVM\n",
    "\n",
    "# Three ways to run linear support vector machine:\n",
    "# 1) sklearn.svm.SVC(kernel='linear',C=1)  # for small datasets. Uses OneVsOne for multiclass.\n",
    "# 2) sklearn.svm.LinearSVC                 # reimplemented for more flexibility, larger data sets.\n",
    "# 3) sklearn.linear_model.SGDClassifier(loss='hinge',alpha=1/(m*C))  # gradient descent, best for huge data sets.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC  \n",
    "# Note SKLearn.svm has SVC for classification and SVR for regression.\n",
    "\n",
    "# Flower data again\n",
    "\n",
    "iris = datasets.load_iris() ]\n",
    "X=iris['data'][:,(2,3)]  # 2=petal length 3=petal width\n",
    "y=(iris['target']==2).astype(np.float64)  # binary: is species==2 (out of 3)\n",
    "svm=LinearSVC(C=1, loss='hinge')\n",
    "pipe1 = Pipeline([\n",
    "    (\"scaler\",StandardScaler()),\n",
    "    (\"svm\",svm)\n",
    "])\n",
    "\n",
    "pipe1.fit(X,y)\n",
    "example_0 = [5.5,1.7]\n",
    "pipe1.predict([example_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.50518051]), array([[1.63330754, 2.38788385]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the SVM learned two weights for two features\n",
    "svm.intercept_, svm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nonlinear SVM\n",
    "\n",
    "# Add polynomial features (like x^2 instead of just x).\n",
    "# Then apply linear SVM.\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pipe2 = Pipeline([\n",
    "    ('poly',PolynomialFeatures(degree=3)),  # features include x^2 and x^3\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('svm',svm)\n",
    "])\n",
    "pipe2.fit(X,y)\n",
    "pipe2.predict([example_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.27607675]),\n",
       " array([[ 0.        , -0.10241346,  0.01394803,  0.33257005,  0.36240083,\n",
       "          0.60407585,  0.73263181,  0.67631331,  0.79776263,  0.99750406]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the SVM learned lots of features but most had tiny weights.\n",
    "svm.intercept_, svm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
