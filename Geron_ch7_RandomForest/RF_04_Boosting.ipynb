{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis boosting.\n",
    "\n",
    "## AdaBoost or Gradient Boost.\n",
    "These ensemble methods train each classifier using the one before it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def compare_accuracy(classifiers):\n",
    "    for classifier in classifiers:\n",
    "        #classifier.fit(X_train,y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        print(classifier.__class__.__name__,accuracy_score(y_test,y_pred))\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "X,y = make_moons(n_samples=2000, noise=0.20)\n",
    "# This is a wrapper for ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "Training is serial, not parallel.\n",
    "\n",
    "Loop: train model, evaluate model, assign model weight.\n",
    "Add model weight to each misclassified instance.\n",
    "(This upweights instances that were misclassified by good models.)\n",
    "Train new model on weighted instances.\n",
    "Finally, use the ensemble of all models.\n",
    "\n",
    "Prediction uses majority weighted voting (best models count more).\n",
    "\n",
    "SciKit-Learn has a multi-class AdaBoost.\n",
    "Algorithm is SAMME for classification, SAMME.R for probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Increasing max_depth improves the descision tree but degrades the ada boost.\n",
    "dtc = DecisionTreeClassifier(max_depth=1)\n",
    "dtc.fit(X_train,y_train)  # only for comparison; adaboost retrains it\n",
    "abc = AdaBoostClassifier(\n",
    "    dtc,\n",
    "    n_estimators=200,\n",
    "    algorithm='SAMME.R',\n",
    "    learning_rate=0.5\n",
    ")\n",
    "abc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier 0.805\n",
      "AdaBoostClassifier 0.9566666666666667\n"
     ]
    }
   ],
   "source": [
    "compare_accuracy([dtc,abc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boost\n",
    "Gradient boosted regression trees (GBRT).\n",
    "Each model trains on the residuals of the predecessor.\n",
    "Whereas ERROR is deviation from unknown true population mean,\n",
    "RESIDUAL is deviation from observed sample average.\n",
    "\n",
    "### First, we implement gradient boost manually.\n",
    "Use regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09996956, 0.99275727, 0.09996956, 0.09996956, 1.11986277])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "#\n",
    "dtr1 = DecisionTreeRegressor(max_depth=2)\n",
    "dtr1.fit(X_train,y_train)\n",
    "residual1 = y_train - dtr1.predict(X_train)\n",
    "#\n",
    "dtr2 = DecisionTreeRegressor(max_depth=2)\n",
    "dtr2.fit(X_train,residual1)\n",
    "residual2 = residual1 - dtr2.predict(X_train)\n",
    "#\n",
    "dtr3 = DecisionTreeRegressor(max_depth=2)\n",
    "dtr3.fit(X_train,residual1)\n",
    "residual3 = residual2 - dtr3.predict(X_train)\n",
    "# \n",
    "# THE FINAL PREDICTION IS THE SUM OF THE CLASSIFIER PREDICTIONS!\n",
    "# This is an additive model.\n",
    "# No individual classier except the first would work on its own.\n",
    "#\n",
    "def ensemble_predict (X,classifiers):\n",
    "    return sum(TREE.predict(X) for TREE in classifiers)\n",
    "trees=[dtr1,dtr2,dtr3]\n",
    "y_pred=ensemble_predict(X_test,trees)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, we implement gradient boost the right way.\n",
    "\n",
    "Shrinkage.\n",
    "Lower the learning rate but increase # trees.\n",
    "\n",
    "We'll start with few trees, high rate.\n",
    "Note that too many trees will overfit.\n",
    "Use early stopping to optimize tree number.\n",
    "Use warm_start=True to stop and restart training.\n",
    "Use the staged_predict() method to measure error per successive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor(\n",
    "    max_depth=2,\n",
    "    n_estimators=3,\n",
    "    learning_rate=1.0   \n",
    ")\n",
    "gbr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08095442, 0.94173762, 0.08095442, 0.08095442, 1.00529036])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=gbr.predict(X_test)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third, an even better way.\n",
    "This is mentioned in the book but it is not part of sklearn.\n",
    "XGBoost, part of DMLC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Book stops here.\n",
    "# We will try again as classification (not regression).\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(\n",
    "    max_depth=2,\n",
    "    n_estimators=3,\n",
    "    learning_rate=1.0   \n",
    ")\n",
    "gbc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=gbc.predict(X_test)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
