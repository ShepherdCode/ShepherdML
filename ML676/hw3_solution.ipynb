{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3aQWTz7YjMP"
   },
   "source": [
    "# HW3 Solution\n",
    "## Student Name: Jason Miller\n",
    "\n",
    "---\n",
    "\n",
    "**NOTE:**\n",
    "Change the notebook filename in this way:\n",
    "```\n",
    "hw2_solution_lastname_firstname.ipynb\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "As you can see, you can write an inline equation in this way: $P(\\theta)$.\n",
    "\n",
    "Or you can write a block equation in this way\n",
    "$$\n",
    "\\mathcal{N}(\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\n",
    "$$\n",
    "\n",
    "This is a [Markdown overview](https://colab.research.google.com/notebooks/markdown_guide.ipynb) if you are not familiar with this text editing formalism.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHG2qF-jZhmy"
   },
   "outputs": [],
   "source": [
    "# You can only import these libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlLcVkQuYw1r"
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "By considering the determinant of a 2×2 Gram matrix, \n",
    "show that a positive definite kernel function k(x, x′) \n",
    "satisfies the Cauchy-Schwartz inequality\n",
    "k(x1,x2)2 ≤k(x1,x1)k(x2,x2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1\n",
    "Our textbook (Bishop, page 293) defines Gram matrix K \n",
    "whose elements are the dot products  \n",
    "$K_{ij} = $  \n",
    "$ = k(x_i,x_j)$  \n",
    "$ = \\langle \\phi(x_i),\\phi(x_j) \\rangle$  \n",
    "\n",
    "Our textbook (Murphy, page 481) \n",
    "defines positive definite kernel k as having the property\n",
    "that K is a positive definite matrix. \n",
    "K positive definite implies K preserves vector orientation under multiplication,\n",
    "which implies the determinant of K is positive. \n",
    "\n",
    "So far we have...\n",
    "\n",
    "$K = \\left( \\begin{smallmatrix} k(x_1,x_1)&k(x_1,x_2) \\\\ \n",
    "k(x_2,x_1)&k(x_2,x_2) \\end{smallmatrix} \\right)$\n",
    "\n",
    "$det(K) = ad - bc = k(x_1,x_1)k(x_2,x_2)-k(x_1,x_2)k(x_2,x_1)$  \n",
    "\n",
    "$det(K)>0$  \n",
    "implies  \n",
    "$k(x_1,x_1)k(x_2,x_2) > k(x_1,x_2)k(x_2,x_1)$  \n",
    "\n",
    "The possibility that  \n",
    "$x_1=x_2$  \n",
    "implies the possibility that  \n",
    "$det(K)=0$  \n",
    "so,  \n",
    "$k(x_1,x_1)k(x_2,x_2) \\geq k(x_1,x_2)k(x_2,x_1)$  \n",
    "\n",
    "Our slides \n",
    "(deck 16, \"SVM\", page 33, \"Computational Efficiency\")\n",
    "define a kernal function as a symmetric function.   \n",
    "$k(x_1,x_2) = k(x_2,x_1)$    \n",
    "\n",
    "So  \n",
    "$k(x_1,x_1)k(x_2,x_2) \\geq [k(x_1,x_2)]^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0uRrmF8Zq6R"
   },
   "source": [
    "## Problem 2 Solution\n",
    "### 1 = C  \n",
    "In fig C, the decision boundary is linear, \n",
    "matching the identity kernel in eq 1. \n",
    "In blue, 3 of 4 SVs are inside the margin.\n",
    "In red, at least 2 of 4 SVs are inside the margin. \n",
    "This indicates high tolerance for slack,\n",
    "possibly matching the C=0.1 parameter in eq 1.\n",
    "\n",
    "### 2 = B  \n",
    "In fig B, the decision boundary is linear,\n",
    "matching the identity kernel in eq 1 and 2.\n",
    "There is only one (blue) SV in the margin.\n",
    "This indicates less tolerance for slack,\n",
    "possibly matching the C=1 parameter in eq 2.\n",
    "\n",
    "### 3 = F\n",
    "Eq 3 uses a 2nd order polynomial kernel.\n",
    "We expect to see a quadratic decision boundary, as in fig D.\n",
    "The Fig F decision boundary looks linear but\n",
    "but that could be the solution to the polynomial, \n",
    "especially since the only SVs are one red and one blue.\n",
    "Fig F has no points in the margin,\n",
    "consistent with the lack of slack variables in eq 3.\n",
    "\n",
    "### 4 = A\n",
    "Eq 4 and 5 use a Gaussian kernel.\n",
    "This kernel is capable of the high-degree decision boundary \n",
    "in fig A and E.\n",
    "For eq 4, fig A seems the better choice by the argument below.\n",
    "\n",
    "### 5 = E\n",
    "Eq 5 is similar to eq 4 and fig E is similar to fig A.\n",
    "Compared to fig A, fig E shows more signs of overfitting:\n",
    "the decision boundary is more tailored to the red points\n",
    "and it uses nearly all the blue points as SVs. \n",
    "Where eq 4 has a 2 in the denominator, eq 5 has a 1.\n",
    "The denominator is proportional to the variance of the Gaussian kernel.\n",
    "Since eq 5 uses a smaller variance and a narrower Gaussian,\n",
    "eq 5 is predisposed to overfit more than eq 4.\n",
    "\n",
    "### None = D\n",
    "The fig D decision boundary is consistent with a 2nd order polynomial kernel, but we assigned fig F to the only polynomial kernel.\n",
    "Furthermore, fig D has problems:\n",
    "the blue SV at the top right is an outlier;\n",
    "the boundary should curve a bit more to incorporate \n",
    "one to two more red SVs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFMdYc-wbYCp"
   },
   "source": [
    "## Problem 3\n",
    "\n",
    "### 3 Background.\n",
    "The parameter C is a regularizer (Bishop, page 332)\n",
    "that affects the cost of including support vectors inside the margin.\n",
    "\n",
    "The slides presented in class \n",
    "show four simulations of increasing C (deck 16, \"SVM\", slides 15-29).\n",
    "In no example does the decision boundary move as C increases\n",
    "(except one move that is so slight it could be a drawing artifact).\n",
    "\n",
    "However, it seems that the decision boundary should move sometimes.\n",
    "As C increases, the margin decreases, \n",
    "and that could disqualify some instances as support vectors.\n",
    "As the set of support vectors decreases,\n",
    "it would seem that the decision bounary could get rotated and translated.\n",
    "\n",
    "Indeed, this is shown in one of our textbooks \n",
    "(James, Intro to Statistical Learning).\n",
    "Unfortunately, the C in James has inverse meaning.\n",
    "In class, C is a coefficient of slack, so large C discourages slack.\n",
    "In James, C is a cap on slack, so large C encourages more slack.\n",
    "Nevertheless, James shows an example \n",
    "where each change in C leads to a narrower magin \n",
    "and different support vectors, including dramatic rotations of the decision boundary\n",
    "(Figure 9.7 on page 348)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Problems and Solutions\n",
    "Linear SVM with slack penalties (eq 0.5):  \n",
    "$min \\frac{1}{2} w \\cdot w + C \\sum_i^n{\\xi_i}$  \n",
    "such that:  \n",
    "$\\xi_i \\geq 0$  \n",
    "and:  \n",
    "$(w \\cdot x_i + b) y_i - (1-\\xi_i) \\geq 0$\n",
    "for all i.  \n",
    "\n",
    "#### 3.1 As C increases, b will not increase.\n",
    "POSSIBLE\n",
    "\n",
    "Consider the case where the SMV already separated the data with no slack.\n",
    "Then $\\xi_i=0$ for all i and the second term of eq 0.5 is zero regardless of C. \n",
    "So, increased C will not increase b (or anything else) in this case.\n",
    "\n",
    "Consider the case where the SVM did incorporate slack.\n",
    "Then $\\xi_i>0$ for some i.\n",
    "Increased C encourages less total slack and narrower margin.\n",
    "The decreased margin could induce different support vectors.\n",
    "The different support vectors could induce a different a decision boundary.\n",
    "The new boundary might have different w and b.\n",
    "Thus, increased C could increase b in this case.\n",
    "\n",
    "#### 3.2 As C increases, more points will be misclassified. \n",
    "POSSIBLE\n",
    "\n",
    "Consider the SVM that leaves several points within the margin. \n",
    "Increased C increases the cost of the points within the margin.\n",
    "By eq 0.5, the new SVM must have less total slack and a narrower margin. \n",
    "But the new SVM could have fewer support vectors, \n",
    "and it could increase the slack of one point,\n",
    "and that point could even be on the wrong side of the new decision bounary.\n",
    "Thus, the misclassified count could increase, decrease, \n",
    "or remain the same.\n",
    "\n",
    "#### 3.3 As C increases, the margin will not increase.\n",
    "TRUE\n",
    "\n",
    "Consider the SVM that separated the data with no slack.\n",
    "Increasing C will have no effect on b or w.\n",
    "The margin will not increase (or decrease) in this case.\n",
    "\n",
    "Alternately, suppose $\\xi_i>0$ for some i.\n",
    "Thus, the SVM already minimized the margin using slack.\n",
    "Increasing C could push the SVM to a new minimum,\n",
    "in which total slack goes down but w is greater.\n",
    "Greater w means smaller margin (margin=2/||w||). \n",
    "Thus, the margin will not increase in this case either.\n",
    "\n",
    "Thus, the margin will not increase in all cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tcAa3_RCYgr"
   },
   "source": [
    "## Problem 4\n",
    "\n",
    "Consider the kernel:  \n",
    "$k(u,v) = u · v + 4(u · v)^2$  \n",
    "where the vectors u and v are 2-dimensional. \n",
    "This kernel is equal to an inner product φ(u) · φ(v)\n",
    "for some definition of φ. What is the function φ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a Toward the Solution\n",
    "Expand the given equation:  \n",
    "$k(u,v) $  \n",
    "$= u_1 v_1 + u_2 v_2 + 4(u_1 v_1)^2 + 8 u_1 v_1 u_2 v_2 + 4(u_2 v_2)^2$  \n",
    "\n",
    "Our slides (deck 16, \"SVM\", page 31, \"Quadratic Features\"),\n",
    "give an example of mapping 2D vectors to 3D space:  \n",
    "$\\phi(x) = (x_{1}^{2}, \\sqrt{2} x_1 x_2, x_{2}^{2})$  \n",
    "which has the convenient property that   \n",
    "$k(u,v) = \\langle \\phi{(u)},\\phi{(v)} \\rangle = \\langle u,v \\rangle^2$  \n",
    "\n",
    "Either formulation works out to:  \n",
    "$k(u,v) $  \n",
    "$ = (u_1 v_1)^2 + 2 u_1 v_1 u_2 v_2 + (u_2 v_2)^2$  \n",
    "which is close to the desired result.\n",
    "\n",
    "The desired result is achieved using a coefficient of 2 \n",
    "and two extra dimenstions. Thus, the feature extraction function \n",
    "phi maps 2D to 5D, as shown below.\n",
    "\n",
    "### 4b Solution\n",
    "$\\phi(x) $  \n",
    "$ = \\phi(x_1,x_2)$  \n",
    "$ = (2 x_{1}^{2}, 2 \\sqrt{2} x_1 x_2, 2 x_{2}^{2}, x_1, x_2)$  \n",
    "\n",
    "### 4c Check\n",
    "$k(u,v) $  \n",
    "$ = \\langle \\phi{(u)},\\phi{(v)} \\rangle $  \n",
    "$ = [2 u_1^2, 2 \\sqrt{2} u_1 u_2, 2 u_2^2, u^1, u^2]  · $\n",
    "$   [2 v_1^2, 2 \\sqrt{2} v_1 v_2, 2 v_2^2, v^1, v^2]$  \n",
    "$ = 4 (u_1 v_1)^2 + 8 u_1 u_2 v_1 v_2 + 4 (u_2, v_2)^2 + u_1 v_1 + u_2 v_2$  \n",
    "$ = u · v + 4(u · v)^2$  \n",
    "Correct!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw3_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
