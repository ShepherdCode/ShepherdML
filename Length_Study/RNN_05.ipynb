{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN cross validation\n",
    "Classify pc vs nc RNA.\n",
    "Use K=2,3,4.\n",
    "\n",
    "Set aside the 20% test set, stratified by length.\n",
    "On the remaining 80%,\n",
    "perform 5-fold cross validation.\n",
    "\n",
    "Test subsets of the data with RNN.\n",
    "To do: Read sequences (not K-mers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# For the manual cross validation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume file was preprocessed to contain one line per seq.\n",
    "# Prefer Pandas dataframe but df does not support append.\n",
    "# For conversion to tensor, must avoid python lists.\n",
    "def load_fasta(filename,label):\n",
    "    DEFLINE='>'\n",
    "    labels=[]\n",
    "    seqs=[]\n",
    "    lens=[]\n",
    "    nums=[]\n",
    "    num=0\n",
    "    with open (filename,'r') as infile:\n",
    "        for line in infile:\n",
    "            if line[0]!=DEFLINE:\n",
    "                seq=line.rstrip()\n",
    "                num += 1   # first seqnum is 1\n",
    "                seqlen=len(seq)\n",
    "                nums.append(num)\n",
    "                labels.append(label)\n",
    "                seqs.append(seq)\n",
    "                lens.append(seqlen)\n",
    "    df1=pd.DataFrame(nums,columns=['seqnum'])\n",
    "    df2=pd.DataFrame(labels,columns=['class'])\n",
    "    df3=pd.DataFrame(seqs,columns=['sequence'])\n",
    "    df4=pd.DataFrame(lens,columns=['seqlen'])\n",
    "    df=pd.concat((df1,df2,df3,df4),axis=1)\n",
    "    return df\n",
    "\n",
    "# Split into train/test stratified by sequence length.\n",
    "def sizebin(df):\n",
    "    return pd.cut(df[\"seqlen\"],\n",
    "                              bins=[0,1000,2000,4000,8000,16000,np.inf],\n",
    "                              labels=[0,1,2,3,4,5])\n",
    "def make_train_test(data):\n",
    "    bin_labels= sizebin(data)\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=37863)\n",
    "    # split(x,y) expects that y is the labels. \n",
    "    # Trick: Instead of y, give it it the bin labels that we generated.\n",
    "    for train_index,test_index in splitter.split(data,bin_labels):\n",
    "        train_set = data.iloc[train_index]\n",
    "        test_set = data.iloc[test_index]\n",
    "    return (train_set,test_set)\n",
    "\n",
    "def separate_X_and_y(data):\n",
    "    y=   data[['class']].copy()\n",
    "    X=   data.drop(columns=['class','seqnum','seqlen'])\n",
    "    return (X,y)\n",
    "\n",
    "def subset(data_set,min_len,max_len):\n",
    "    print(\"original \"+str(data_set.shape))\n",
    "    too_short = data_set[ data_set['seqlen'] < min_len ].index\n",
    "    no_short=data_set.drop(too_short)\n",
    "    print(\"no short \"+str(no_short.shape))\n",
    "    too_long = no_short[ no_short['seqlen'] >= max_len ].index\n",
    "    no_long_no_short=no_short.drop(too_long)\n",
    "    print(\"no long, no short \"+str(no_long_no_short.shape))\n",
    "    return no_long_no_short\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cross_validation(X,y,K):\n",
    "    cv_scores = []\n",
    "    act=\"sigmoid\"\n",
    "    dt='float32'\n",
    "    fold=0\n",
    "    eps=100\n",
    "    splitter = ShuffleSplit(n_splits=5, test_size=0.2, random_state=37863)\n",
    "    for train_index,valid_index in splitter.split(X):\n",
    "        X_train=X.iloc[train_index]\n",
    "        y_train=y.iloc[train_index]\n",
    "        X_valid=X.iloc[valid_index]\n",
    "        y_valid=y.iloc[valid_index]\n",
    "        mlp = keras.models.Sequential([\n",
    "            keras.layers.LayerNormalization(trainable=False),\n",
    "            keras.layers.Dense(32, activation=act,dtype=dt),\n",
    "            keras.layers.Dense(32, activation=act,dtype=dt),\n",
    "            keras.layers.Dense(1,  activation=act,dtype=dt)\n",
    "        ])\n",
    "        seq_len=None  # none indicates variable length\n",
    "        input_features=4**K   # 64 DNA K-mers at K=3\n",
    "        rnn2 = keras.models.Sequential([\n",
    "            keras.layers.SimpleRNN(16, return_sequences=True, \n",
    "                                   input_shape=[seq_len,input_features]),\n",
    "            keras.layers.SimpleRNN(16, return_sequences=True),\n",
    "            keras.layers.SimpleRNN(16, return_sequences=True),\n",
    "            keras.layers.SimpleRNN(1),\n",
    "        ])\n",
    "\n",
    "        bc=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        rnn2.compile(loss=bc, optimizer=\"Adam\",metrics=[\"accuracy\"])\n",
    "        history=rnn2.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
    "                epochs=eps, verbose=0,  # verbose=1 for ascii art, verbose=0 for none\n",
    "                validation_data=(X_valid,y_valid) )\n",
    "                        \n",
    "        fold += 1\n",
    "        print(\"Fold %d, %d epochs\"%(fold,eps))\n",
    "\n",
    "        pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "        plt.grid(True)\n",
    "        plt.gca().set_ylim(0,1)\n",
    "        plt.show()\n",
    "\n",
    "        scores = mlp.evaluate(X_valid, y_valid, verbose=0)\n",
    "        print(\"%s: %.2f%%\" % (mlp.metrics_names[1], scores[1]*100))\n",
    "        cv_scores.append(scores[1] * 100)\n",
    "    print()\n",
    "    print(\"Validation core mean %.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this works for K=2, generalize for all K\n",
    "def generate_encoding_set(K,uniform_len):\n",
    "    if K!=2:\n",
    "        print(\"ONLY CODED FOR K=2 !!!!!\")\n",
    "    all_possible=['AA','AC','AG','AT','CA','CC','CG','CT','GA','GC','GG','GT','TA','TC','TG','TT']\n",
    "    i=16\n",
    "    pad_kmer='NN'\n",
    "    while i < uniform_len:\n",
    "        all_possible.append(pad_kmer)\n",
    "        i += 1\n",
    "    return all_possible\n",
    "\n",
    "def generate_all_kmers(K):\n",
    "    shorter_kmers=['']\n",
    "    for i in range(K):\n",
    "        longer_kmers=[]\n",
    "        for mer in shorter_kmers:\n",
    "            longer_kmers.append(mer+'A')\n",
    "            longer_kmers.append(mer+'C')\n",
    "            longer_kmers.append(mer+'G')\n",
    "            longer_kmers.append(mer+'T')\n",
    "        shorter_kmers = longer_kmers\n",
    "    return shorter_kmers\n",
    "\n",
    "def train_encoder(kmers):\n",
    "    narray = np.array(kmers)\n",
    "    array2d = narray.reshape(-1, 1)\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
    "    encoder.fit(array2d)\n",
    "    return encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_to_kmers(data,K,uniform_len):\n",
    "    all_seqs=[]\n",
    "    pad_char='N'\n",
    "    pad_kmer=pad_char*K\n",
    "    # pad_kmer=None      # Prefer this to the above. Will it work? No. Encoder balks.\n",
    "    for seq in data['sequence']:\n",
    "        i=0\n",
    "        seqlen=len(seq)\n",
    "        kmers=[]\n",
    "        while i < seqlen-K+1:\n",
    "            kmer=seq[i:i+K]\n",
    "            kmers.append(kmer)\n",
    "            i += 1\n",
    "        while i < uniform_len:\n",
    "            kmers.append(pad_kmer)\n",
    "            i += 1\n",
    "        all_seqs.append(kmers)\n",
    "    pd2d=pd.DataFrame(all_seqs)\n",
    "    return pd2d   # return 2D dataframe, uniform dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and partition sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINLEN=1000\n",
    "MAXLEN=2000\n",
    "nc_seq=load_fasta('ncRNA.fasta',0)\n",
    "pc_seq=load_fasta('pcRNA.fasta',1)\n",
    "all_seq=pd.concat((nc_seq,pc_seq),axis=0)\n",
    "\n",
    "(train_set,test_set)=make_train_test(all_seq)\n",
    "(X_test,y_test)=separate_X_and_y(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original (30290, 4)\n",
      "no short (9273, 4)\n",
      "no long, no short (3368, 4)\n"
     ]
    }
   ],
   "source": [
    "# Extract subset by length\n",
    "train_set=subset(train_set,MINLEN,MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(3368, 1)\n",
      "sequence    GGCGGGGTCGACTGACGGTAACGGGGCAGAGAGGCTGTTCGCAGAG...\n",
      "Name: 12641, dtype: object\n",
      "1338\n"
     ]
    }
   ],
   "source": [
    "# One array to two: X and y\n",
    "(X_train_all,y_train_all)=separate_X_and_y(train_set)\n",
    "# The returned values are Pandas dataframes.\n",
    "# print(X_train_all.shape,y_train_all.shape)\n",
    "# (X_train_all,y_train_all)\n",
    "# y: Pandas dataframe to Python list.\n",
    "# y_train_all=y_train_all.values.tolist()\n",
    "# The sequences lengths are bounded but not uniform.\n",
    "X_train_all\n",
    "print(type(X_train_all))\n",
    "print(X_train_all.shape)\n",
    "print(X_train_all.iloc[0])\n",
    "print(len(X_train_all.iloc[0]['sequence']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-mer sequence, K=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3368, 2000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=2\n",
    "\n",
    "# X: List of string to List of uniform-length ordered lists of K-mers.\n",
    "X_train_kmers=strings_to_kmers(X_train_all,K,MAXLEN)\n",
    "# X: true 2D array (no more lists)\n",
    "X_train_kmers.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "[array(['AA', 'AC', 'AG', 'AT', 'CA', 'CC', 'CG', 'CT', 'GA', 'GC', 'GG',\n",
      "       'GT', 'NN', 'TA', 'TC', 'TG', 'TT'], dtype='<U2')]\n",
      "transform...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
    "# This takes a long time and may not be complete.\n",
    "# We should fit to sequences that absolutely contain every possible K-mer.\n",
    "sample_data=generate_encoding_set(K,MAXLEN)\n",
    "sample=np.array(sample_data).reshape(-1, 1)\n",
    "print(\"fit...\")\n",
    "encoder.fit(sample)\n",
    "print(str(encoder.categories_))\n",
    "\n",
    "#X_train_reshape=X_train_numpy.reshape(-1, 1)\n",
    "#X_train_encoded=encoder.fit(X_train_numpy)\n",
    "print(\"transform...\")\n",
    "testchar=np.array(['AT']).reshape(-1, 1)\n",
    "encoder.transform(testchar)\n",
    "\n",
    "testary=np.array([['AT','CT'],['AT','CT']]).reshape(-1, 1)\n",
    "encoder.transform(testary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(3368, 2000, 17)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# From pandas dataframe to numpy to list to numpy\n",
    "print(type(X_train_kmers))\n",
    "num_seqs=len(X_train_kmers)\n",
    "tmp_seqs=[]\n",
    "for i in range(num_seqs):\n",
    "    kmer_sequence=X_train_kmers.iloc[i]\n",
    "    X1=encoder.transform(kmer_sequence.to_numpy().reshape(-1, 1))\n",
    "    tmp_seqs.append(X1)\n",
    "encoded_seqs=np.array(tmp_seqs)\n",
    "print(type(encoded_seqs))\n",
    "print(encoded_seqs.shape)\n",
    "#X_train_encoded=encoder.transform(X_train_kmers.to_numpy().reshape(-1, 1))  # FLATTENS\n",
    "#X_train_encoded  # LONG TIME!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-mer sequence, K=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-mer sequence, K=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
