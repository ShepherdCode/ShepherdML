{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatives to Linear Regression\n",
    "\n",
    "# Polynomial Regression.\n",
    "# Idea: do linear regression on combinations of features.\n",
    "# For y=W*X, add a feature X^2, y=W1*X + W2*X^2.\n",
    "# For y=W1*X1+W2*X2, add features for X1^2 and X2^2 and X1*X2.\n",
    "\n",
    "# Linear Regression risk: underfit. \n",
    "# Indicator of underfit: as train set size increases, train error remains high.\n",
    "# Polynomial Regression risk: overfit.\n",
    "# Indicator of overfit: validation error always higher than train error.\n",
    "# How to analyze:\n",
    "# Plot learning curves i.e. (train loss, test loss) vs growing test size.\n",
    "\n",
    "# Reproduce data from previous notebook.\n",
    "m = 100   # training set size\n",
    "import numpy as np\n",
    "X = 2 * np.random.rand(m,1)  \n",
    "y = 4 + 3 * X + np.random.randn(m,1)\n",
    "biasX = np.c_[np.ones((m,1)), X] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.53385405]), array([[2.06226898, 0.40245008]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "Xpoly = poly.fit_transform(X)  # for this simple case, returns array of X and X^2\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(Xpoly,y)\n",
    "linreg.intercept_, linreg.coef_\n",
    "# For linear data, we get close: y = 4.5 + 2.1x + 0.4x^2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.48818983]), array([[2.0536459 , 1.34175627]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = 2 * np.random.rand(m,1)  \n",
    "y = 5 + 3 * X + 1 * pow(X,2) + np.random.randn(m,1)\n",
    "biasX = np.c_[np.ones((m,1)), X] \n",
    "Xpoly = poly.fit_transform(X)  # for this simple case, returns array of X and X^2\n",
    "linreg.fit(Xpoly,y)\n",
    "linreg.intercept_, linreg.coef_\n",
    "# For quadratic data, we get close: y = 5.5 + 2.1x + 1.3x^2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources of modeling error.\n",
    "# 1) Bias = wrong assumptons. (not related to bias node)\n",
    "#    Model complexity reduces bias e.g. assumption of linearity.\n",
    "# 2) Variance = sensitivity to training data. \n",
    "#    Model complexity increases overfitting and variance. \n",
    "# 3) Irreducible error = noise.\n",
    "#    Must preprocess to denoise the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization.\n",
    "# Increase generalization and decrease overfitting.\n",
    "# These types: \n",
    "# 1) Ridge, \n",
    "# 2) Lasso, \n",
    "# 3) Elastic Net,\n",
    "# 4) Early stopping.\n",
    "# Important to scale the data first (so all weights can get small)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression.\n",
    "# Linear regression + ridge regularization.\n",
    "# aka Tikhonov regularization.\n",
    "# Uses the L2 norm i.e. sum of squares of weights.\n",
    "\n",
    "# During training (but not testing), penalize large weights.\n",
    "# Choose regularization parameters alpha:\n",
    "#    zero for no regularization, large number for smooth predictors.\n",
    "# Cost = MSE(theta) + (alpha/2) * sum(theta^2).\n",
    "# Note sum over theta 1..n so x0=bias it not penalized.\n",
    "# The added cost is (1/2)*L2norm(W)^2 for weights W excluding bias.\n",
    "# For gradient descent, the added cost is alpha*w.\n",
    "\n",
    "# Closed form solution with identity matrix I\n",
    "# theta = inverse(XT*X + alpha*I) * XT * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.65848192]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge Regression in SKLearn.\n",
    "# Closed form with Cholesky matrix factorization.\n",
    "from sklearn.linear_model import Ridge\n",
    "reg = Ridge(alpha=1, solver=\"cholesky\")\n",
    "reg.fit(X,y)\n",
    "value_0 = 1.5    # predict y for this arbitrary x\n",
    "reg.predict([[value_0]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.78141339])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge Regression in SKLearn.\n",
    "# Gradient Descent.\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd = SGDRegressor(penalty=\"l2\")   # L2 norm amounts using Ridge regression\n",
    "sgd.fit(X,y.ravel())\n",
    "sgd.predict([[value_0]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.77004])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lasso Regression.\n",
    "# Linear regression + lasso regularization.\n",
    "# LASSO = Least Absolute Shrinkage and Selection Operator.\n",
    "# Uses the L1 norm i.e. sum of abs(weight).\n",
    "# Lasso is a form of feature selection.\n",
    "# Lasso tends to produce a sparse model i.e. to zero out many weights.\n",
    "# Lasso gradient descent bounces around, needs learning schedule that reduces alpha.\n",
    "# Cost function is not differentiable.\n",
    "# Work-around uses a subgradient vector.\n",
    "\n",
    "# SKLearn has two ways to do it.\n",
    "sgd = SGDRegressor(penalty=\"l2\")   # L1 norm amounts using Lasso regression\n",
    "sgd.fit(X,y.ravel())\n",
    "sgd.predict([[value_0]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.5894603])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "las = Lasso(alpha=0.1)\n",
    "las.fit(X,y)\n",
    "las.predict([[value_0]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.39566937])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Elastic Net.\n",
    "# Mix of Ridge and Lasso.\n",
    "# Parameter r: r=0 => Ridge L2 norm, r=1 => Lasso L1 norm.\n",
    "from sklearn.linear_model import ElasticNet\n",
    "net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "net.fit(X,y)\n",
    "net.predict([[value_0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping.\n",
    "# Stop gradient descent a minimum validation error\n",
    "# i.e. between underfit and overfit.\n",
    "\n",
    "# No easy solution in sklearn?\n",
    "# Book shows code to break X into X_train and X_validate,\n",
    "# then loop over epochs (nice - no need to start again each time),\n",
    "# and retain clone of the model with min val error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
