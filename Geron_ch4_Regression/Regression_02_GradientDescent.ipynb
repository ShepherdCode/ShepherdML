{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent.\n",
    "\n",
    "# MSE cost function for Linear Regression is...\n",
    "# + convex i.e. no local minima\n",
    "# + continuous i.e. everywhere differentiable.\n",
    "\n",
    "# Gradient descent is optimal if all features are scaled equivalently.\n",
    "# Otherwise, it follows the feature with smallest scale first.\n",
    "\n",
    "# Need derivative of cost function.\n",
    "# Use partial derivative along each feature.\n",
    "# Use DEL for upside-down DELTA.\n",
    "# DEL = vector of partial derivatives = gradient vector.\n",
    "# DEL * MSE = (2/m)*XT*(X*theta-y).\n",
    "# Application to all training examples = one batch.\n",
    "\n",
    "# The gradient points toward max so subtract it to go toward min.\n",
    "# Learning rate eta: too small => long time, too big => no convergence.\n",
    "# Each parameter update is: theta - eta * DEL * MSE * theta.\n",
    "\n",
    "# Reproduce steps from previous notebook.\n",
    "m = 100   # training set size\n",
    "import numpy as np\n",
    "X = 2 * np.random.rand(m,1)  \n",
    "y = 4 + 3 * X + np.random.randn(m,1)\n",
    "biasX = np.c_[np.ones((m,1)), X] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.10210131],\n",
       "       [2.97381822]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch Gradient Descent\n",
    "# From one initial spot, iterate over all examples.\n",
    "# Optimal solution if convex, otherwise may find local minima.\n",
    "# High memory requirement.\n",
    "# Slow: #iterations * #examples.\n",
    "\n",
    "iterations = 1000   # how much patience we have\n",
    "eta = 0.1  # learn rate\n",
    "theta = np.random.randn(2,1)  # random initializer where seach begins\n",
    "for i in range(iterations):\n",
    "    # This iterates over all (x,y) training examples.\n",
    "    gradientMSE = (2/m)*(biasX.T).dot(biasX.dot(theta)-y)\n",
    "    # Each one update is based on all training examples.\n",
    "    theta = theta - eta * gradientMSE\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.12694867],\n",
       "       [3.03293732]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stochastic Gradient Descent.\n",
    "# From one initial spot, iterate over random set of examples.\n",
    "# Lower memory requirement. Possibility of faster convergence.\n",
    "# Approximation, not optimal solution, for convex.\n",
    "# Optimal if you choose optimal parameters e.g. learning schedule.\n",
    "# More chance to hop out of local minima.\n",
    "# Simulated annealing: gradually decresing learn rate.\n",
    "# Convention: use m iterations and remove m from denominator.\n",
    "# If the training data is sorted, shuffle it first.\n",
    "\n",
    "theta = np.random.randn(2,1)  # random initializer where seach begins\n",
    "t0, t1 = 5,50  # learn schedule parameters\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    for i in range(m):\n",
    "        index = np.random.randint(m)\n",
    "        (xi,yi) = ( biasX[index:index+1] , y[index:index+1] )\n",
    "        gradientMSE = 2*(xi.T).dot(xi.dot(theta)-yi)\n",
    "        eta=learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradientMSE\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.12380719]), array([3.06738848]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Of course, you can do it in one line with SKLearn.\n",
    "# Stochastic Gradient Descent.\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=m, tol=1e-3,penalty=None,eta0=0.1)\n",
    "sgd_reg.fit(X,y.ravel())\n",
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-Batch Gradient Descent.\n",
    "# Combine Batch and Stochastic i.e. small batches = random subsets.\n",
    "# May be faster on GPU or hardware matrix optimization.\n",
    "\n",
    "# SKLearn did not have it and book does not show an example. \n",
    "# SKLearn decided to add it but I cannot find where! \n",
    "# Disccusion https://github.com/scikit-learn/scikit-learn/issues/14468\n",
    "# Work ticket https://github.com/mozilla/CANOSP-2019/issues/30\n",
    "\n",
    "# Author/reader discussion suggests you can roll your own.\n",
    "# https://github.com/ageron/handson-ml/issues/226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
