{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the Random Forest model.\n",
    "\n",
    "# Resurrect the three models.\n",
    "#import joblib\n",
    "#Lmodel = joblib.load(\"housing_linear_regression_model.pkl\")\n",
    "#Dmodel = joblib.load(\"housing_decision_tree_model.pkl\")\n",
    "#Rmodel = joblib.load(\"housing_random_forest_model.pkl\")\n",
    "\n",
    "# First, resurrect the data.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "datapath=\"/Users/jasonmiller/Source/MachineLearning/datasets/housing/housing.csv\"\n",
    "all_data=pd.read_csv(datapath)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(all_data,test_size=0.2,random_state=42)\n",
    "train_predictors = train_set.drop([\"median_house_value\"],axis=1)\n",
    "train_labels = train_set[\"median_house_value\"].copy()\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3,4,5,6  # hard coded index\n",
    "class AddFeatures (BaseEstimator, TransformerMixin):\n",
    "    def fit(self,X,y=None):\n",
    "        return self   # required by base class\n",
    "    def transform(self,X,y=None):\n",
    "        rooms_per_household = X[:,rooms_ix]/X[:,households_ix]\n",
    "        population_per_household = X[:,population_ix]/X[:,households_ix]\n",
    "        bedrooms_per_room = X[:,bedrooms_ix]/X[:,rooms_ix]\n",
    "        # numpy shorthand for a column-wise concatenation\n",
    "        return np.c_[X,rooms_per_household,population_per_household,bedrooms_per_room]\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer',SimpleImputer(strategy='median')),\n",
    "    ('feater_adder',AddFeatures()),\n",
    "    ('scaler',StandardScaler())\n",
    "])\n",
    "categoric_features=['ocean_proximity']\n",
    "numeric_features = list(train_predictors)\n",
    "numeric_features.remove(categoric_features[0])\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", numeric_pipeline, numeric_features),  # dense matrix\n",
    "    (\"cat\", OneHotEncoder(), categoric_features)  # sparse matrix\n",
    "])\n",
    "#\n",
    "prepared_train_predictors = full_pipeline.fit_transform(train_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestRegressor(),\n",
       "             param_grid=[{'bootstrap': [True], 'max_features': [2, 4, 6, 8],\n",
       "                          'n_estimators': [3, 10, 30]},\n",
       "                         {'bootstrap': [False], 'max_features': [2, 4, 6, 8],\n",
       "                          'n_estimators': [3, 10, 30]}],\n",
       "             return_train_score=True, scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use tool for testing parameters.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Explore hyper parameters for Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "Rmodel = RandomForestRegressor()\n",
    "parameter_grid = [\n",
    "    {'bootstrap':[True],  'n_estimators':[3,10,30], 'max_features':[2,4,6,8]},\n",
    "    {'bootstrap':[False], 'n_estimators':[3,10,30], 'max_features':[2,4,6,8]}\n",
    "]\n",
    "# Select cv=5 for five-way cross-validation.\n",
    "grid_search = GridSearchCV(Rmodel,\n",
    "                          parameter_grid,\n",
    "                          cv=5, \n",
    "                          scoring='neg_mean_squared_error',\n",
    "                          return_train_score=True)\n",
    "grid_search.fit(prepared_train_predictors,train_labels)\n",
    "# This takes a long time (3 minutes?) at 99% CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False, 'max_features': 6, 'n_estimators': 30}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This gives the score for each parameter combination.\n",
    "grid_search.cv_results_\n",
    "# These desribes the best so far.\n",
    "grid_search.best_estimator_ \n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestRegressor(),\n",
       "             param_grid=[{'bootstrap': [False], 'max_features': [4, 6, 8],\n",
       "                          'n_estimators': [30, 50, 100]}],\n",
       "             return_train_score=True, scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_grid_2 = [\n",
    "    {'bootstrap':[False], 'n_estimators':[30,50,100], 'max_features':[4,6,8]}\n",
    "]\n",
    "grid_search_2 = GridSearchCV(Rmodel,\n",
    "                          parameter_grid_2,\n",
    "                          cv=5,\n",
    "                          scoring='neg_mean_squared_error',\n",
    "                          return_train_score=True)\n",
    "grid_search_2.fit(prepared_train_predictors,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False, 'max_features': 6, 'n_estimators': 30}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The book has a script to extract rmse from this rather large data structure.\n",
    "# Our second search did not improve on the first.\n",
    "grid_search_2.cv_results_\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestRegressor(),\n",
       "             param_grid=[{'bootstrap': [False], 'max_features': [4, 6, 8],\n",
       "                          'n_estimators': [30, 50, 100]}],\n",
       "             return_train_score=True, scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This tool tests random parameter combinations instead of all of them.\n",
    "# It can sample from given features according to given distributions, not just randomly.\n",
    "# We will try it with default parameters.\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "grid_search_3 = RandomizedSearchCV(Rmodel,list(prepared_train_predictors))\n",
    "grid_search_2.fit(prepared_train_predictors,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False, 'max_features': 6, 'n_estimators': 30}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.ensemble._forest.RandomForestRegressor"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The random search also did not improve.\n",
    "# Let's go with the first one.\n",
    "type(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.62768520e-02, 6.82048196e-02, 4.33951215e-02, 1.70349677e-02,\n",
       "       1.57122728e-02, 1.65873394e-02, 1.56688070e-02, 3.38621010e-01,\n",
       "       5.67202726e-02, 1.04354580e-01, 7.93161886e-02, 1.25611692e-02,\n",
       "       1.49445593e-01, 1.89113773e-04, 2.32228739e-03, 3.58960563e-03])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the random forest with best hyper parameters.\n",
    "best_model = grid_search.best_estimator_\n",
    "# Save it.\n",
    "import joblib\n",
    "joblib.dump(Rmodel,\"housing_tuned_random_forest_model.pkl\")\n",
    "# Ask the model for relative feature importance.\n",
    "# Book gives a convoluted way to figure out which is which, unfortunately.\n",
    "# Our #1 feature (median_income) has importance 33.86%.\n",
    "best_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48579.92482709519"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, if we are done tuning, run the model on the test data.\n",
    "# This has to be the final step! There is no more data to test on.\n",
    "test_predictors = test_set.drop([\"median_house_value\"],axis=1)\n",
    "test_labels = test_set[\"median_house_value\"].copy()\n",
    "# Model should be read only but it is not. Avoid retraining it.\n",
    "prepared_test_predictors = full_pipeline.transform(test_predictors) # do not call fit_transform() !!!\n",
    "final_predictions = best_model.predict(prepared_test_predictors)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "final_mse = mean_squared_error(test_labels,final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46394.14306684, 50671.50759062])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model is off by under $50K. Better than before.\n",
    "# Report confidence interval for this estimate.\n",
    "from scipy import stats\n",
    "confidence = 0.95\n",
    "squared_errors = (final_predictions - test_labels) ** 2\n",
    "np.sqrt(stats.t.interval(confidence,\n",
    "                        len(squared_errors)-1,\n",
    "                        loc=squared_errors.mean(),\n",
    "                        scale=stats.sem(squared_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95% confidence interval is $46.4K to $50.7K."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
