{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GRU_208e.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojm_6E9f9Kcf"
      },
      "source": [
        "# GRU 208\n",
        "* Operate on 16000 GenCode 34 seqs.\n",
        "* 5-way cross validation. Save best model per CV.\n",
        "* Report mean accuracy from final re-validation with best 5.\n",
        "* Use Adam with a learn rate decay schdule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh6XplUvC0j0",
        "outputId": "da440a41-a4a0-4809-e2e7-e4a04f0f825c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "NC_FILENAME='ncRNA.gc34.processed.fasta'\n",
        "PC_FILENAME='pcRNA.gc34.processed.fasta'\n",
        "DATAPATH=\"\"\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    PATH='/content/drive/'\n",
        "    drive.mount(PATH)\n",
        "    DATAPATH=PATH+'My Drive/data/'  # must end in \"/\"\n",
        "    NC_FILENAME = DATAPATH+NC_FILENAME\n",
        "    PC_FILENAME = DATAPATH+PC_FILENAME\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    DATAPATH=\"\" \n",
        "\n",
        "EPOCHS=200\n",
        "SPLITS=1\n",
        "K=3\n",
        "VOCABULARY_SIZE=4**K+1   # e.g. K=3 => 64 DNA K-mers + 'NNN'\n",
        "EMBED_DIMEN=16\n",
        "FILENAME='GRU208'\n",
        "NEURONS=64\n",
        "DROP=0.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQY7aTj29Kch"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LayerNormalization\n",
        "import time\n",
        "dt='float32'\n",
        "tf.keras.backend.set_floatx(dt)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7jcg6Wl9Kc2"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLFNO1Xa9Kc3"
      },
      "source": [
        "def compile_model(model):\n",
        "    adam_default_learn_rate = 0.001\n",
        "    schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate = adam_default_learn_rate*10,\n",
        "        #decay_steps=100000, decay_rate=0.96, staircase=True)\n",
        "        decay_steps=10000, decay_rate=0.99, staircase=True)\n",
        "    # learn rate = initial_learning_rate * decay_rate ^ (step / decay_steps)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=schedule)\n",
        "    bc=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    print(\"COMPILE...\")\n",
        "    model.compile(loss=bc, optimizer=opt, metrics=[\"accuracy\"])\n",
        "    #model.compile(loss=bc, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    print(\"...COMPILED\")\n",
        "    return model\n",
        "\n",
        "def build_model():\n",
        "    act=\"tanh\"\n",
        "    embed_layer  = keras.layers.Embedding(\n",
        "        #VOCABULARY_SIZE, EMBED_DIMEN, input_length=1000, input_length=1000, mask_zero=True)\n",
        "        #input_dim=[None,VOCABULARY_SIZE], output_dim=EMBED_DIMEN, mask_zero=True)\n",
        "        input_dim=VOCABULARY_SIZE, output_dim=EMBED_DIMEN, mask_zero=True)\n",
        "    rnn1_layer = keras.layers.Bidirectional(\n",
        "      keras.layers.GRU(NEURONS, return_sequences=True, \n",
        "          input_shape=[1000,EMBED_DIMEN], activation=act, dropout=DROP))\n",
        "    rnn2_layer = keras.layers.Bidirectional(\n",
        "      keras.layers.GRU(NEURONS, return_sequences=False, \n",
        "        activation=act, dropout=DROP))\n",
        "    dense1_layer = keras.layers.Dense(NEURONS, activation=act,dtype=dt)\n",
        "    drop1_layer = keras.layers.Dropout(DROP)\n",
        "    dense2_layer = keras.layers.Dense(NEURONS, activation=act,dtype=dt)\n",
        "    drop2_layer = keras.layers.Dropout(DROP)\n",
        "    output_layer = keras.layers.Dense(1, activation=\"sigmoid\", dtype=dt)\n",
        "    mlp = keras.models.Sequential()\n",
        "    mlp.add(embed_layer)\n",
        "    mlp.add(rnn1_layer)\n",
        "    mlp.add(rnn2_layer)\n",
        "    mlp.add(dense1_layer)\n",
        "    mlp.add(drop1_layer)\n",
        "    mlp.add(dense2_layer)\n",
        "    mlp.add(drop2_layer)\n",
        "    mlp.add(output_layer)\n",
        "    mlpc = compile_model(mlp)\n",
        "    return mlpc"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV6k-xOm9Kcn"
      },
      "source": [
        "## Load and partition sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I-O_qzw9Kco"
      },
      "source": [
        "# Assume file was preprocessed to contain one line per seq.\n",
        "# Prefer Pandas dataframe but df does not support append.\n",
        "# For conversion to tensor, must avoid python lists.\n",
        "def load_fasta(filename,label):\n",
        "    DEFLINE='>'\n",
        "    labels=[]\n",
        "    seqs=[]\n",
        "    lens=[]\n",
        "    nums=[]\n",
        "    num=0\n",
        "    with open (filename,'r') as infile:\n",
        "        for line in infile:\n",
        "            if line[0]!=DEFLINE:\n",
        "                seq=line.rstrip()\n",
        "                num += 1   # first seqnum is 1\n",
        "                seqlen=len(seq)\n",
        "                nums.append(num)\n",
        "                labels.append(label)\n",
        "                seqs.append(seq)\n",
        "                lens.append(seqlen)\n",
        "    df1=pd.DataFrame(nums,columns=['seqnum'])\n",
        "    df2=pd.DataFrame(labels,columns=['class'])\n",
        "    df3=pd.DataFrame(seqs,columns=['sequence'])\n",
        "    df4=pd.DataFrame(lens,columns=['seqlen'])\n",
        "    df=pd.concat((df1,df2,df3,df4),axis=1)\n",
        "    return df\n",
        "\n",
        "def separate_X_and_y(data):\n",
        "    y=   data[['class']].copy()\n",
        "    X=   data.drop(columns=['class','seqnum','seqlen'])\n",
        "    return (X,y)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRAaO9jP9Kcr"
      },
      "source": [
        "## Make K-mers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8xcZ4Mr9Kcs"
      },
      "source": [
        "def make_kmer_table(K):\n",
        "    npad='N'*K\n",
        "    shorter_kmers=['']\n",
        "    for i in range(K):\n",
        "        longer_kmers=[]\n",
        "        for mer in shorter_kmers:\n",
        "            longer_kmers.append(mer+'A')\n",
        "            longer_kmers.append(mer+'C')\n",
        "            longer_kmers.append(mer+'G')\n",
        "            longer_kmers.append(mer+'T')\n",
        "        shorter_kmers = longer_kmers\n",
        "    all_kmers = shorter_kmers\n",
        "    kmer_dict = {}\n",
        "    kmer_dict[npad]=0\n",
        "    value=1\n",
        "    for mer in all_kmers:\n",
        "        kmer_dict[mer]=value\n",
        "        value += 1\n",
        "    return kmer_dict\n",
        "\n",
        "KMER_TABLE=make_kmer_table(K)\n",
        "\n",
        "def strings_to_vectors(data,uniform_len):\n",
        "    all_seqs=[]\n",
        "    for seq in data['sequence']:\n",
        "        i=0\n",
        "        seqlen=len(seq)\n",
        "        kmers=[]\n",
        "        while i < seqlen-K+1 -1:  # stop at minus one for spaced seed\n",
        "            #kmer=seq[i:i+2]+seq[i+3:i+5]    # SPACED SEED 2/1/2 for K=4\n",
        "            kmer=seq[i:i+K]  \n",
        "            i += 1\n",
        "            value=KMER_TABLE[kmer]\n",
        "            kmers.append(value)\n",
        "        pad_val=0\n",
        "        while i < uniform_len:\n",
        "            kmers.append(pad_val)\n",
        "            i += 1\n",
        "        all_seqs.append(kmers)\n",
        "    pd2d=pd.DataFrame(all_seqs)\n",
        "    return pd2d   # return 2D dataframe, uniform dimensions"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEtA0xiV9Kcv"
      },
      "source": [
        "def make_kmers(MAXLEN,train_set):\n",
        "    (X_train_all,y_train_all)=separate_X_and_y(train_set)\n",
        "    X_train_kmers=strings_to_vectors(X_train_all,MAXLEN)\n",
        "    # From pandas dataframe to numpy to list to numpy\n",
        "    num_seqs=len(X_train_kmers)\n",
        "    tmp_seqs=[]\n",
        "    for i in range(num_seqs):\n",
        "        kmer_sequence=X_train_kmers.iloc[i]\n",
        "        tmp_seqs.append(kmer_sequence)\n",
        "    X_train_kmers=np.array(tmp_seqs)\n",
        "    tmp_seqs=None\n",
        "    labels=y_train_all.to_numpy()\n",
        "    return (X_train_kmers,labels)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaXyySyO9Kcz"
      },
      "source": [
        "def make_frequencies(Xin):\n",
        "    Xout=[]\n",
        "    VOCABULARY_SIZE= 4**K + 1  # plus one for 'NNN'\n",
        "    for seq in Xin:\n",
        "        freqs =[0] * VOCABULARY_SIZE\n",
        "        total = 0\n",
        "        for kmerval in seq:\n",
        "            freqs[kmerval] += 1\n",
        "            total += 1\n",
        "        for c in range(VOCABULARY_SIZE):\n",
        "            freqs[c] = freqs[c]/total\n",
        "        Xout.append(freqs)\n",
        "    Xnum = np.asarray(Xout)\n",
        "    return (Xnum)\n",
        "def make_slice(data_set,min_len,max_len):\n",
        "    slice = data_set.query('seqlen <= '+str(max_len)+' & seqlen>= '+str(min_len))\n",
        "    return slice"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdIS2utq9Kc9"
      },
      "source": [
        "## Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVo4tbB_9Kc-"
      },
      "source": [
        "def do_cross_validation(X,y,given_model):\n",
        "    cv_scores = []\n",
        "    fold=0\n",
        "    splitter = ShuffleSplit(n_splits=SPLITS, test_size=0.1, random_state=37863)\n",
        "    for train_index,valid_index in splitter.split(X):\n",
        "        fold += 1\n",
        "        X_train=X[train_index] # use iloc[] for dataframe\n",
        "        y_train=y[train_index]\n",
        "        X_valid=X[valid_index]\n",
        "        y_valid=y[valid_index]        \n",
        "        # Avoid continually improving the same model.\n",
        "        model = compile_model(keras.models.clone_model(given_model))\n",
        "        bestname=DATAPATH+FILENAME+\".cv.\"+str(fold)+\".best\"\n",
        "        mycallbacks = [keras.callbacks.ModelCheckpoint(\n",
        "            filepath=bestname, save_best_only=True, \n",
        "            monitor='val_accuracy', mode='max')]   \n",
        "        print(\"FIT\")\n",
        "        start_time=time.time()\n",
        "        history=model.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
        "                epochs=EPOCHS, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
        "                callbacks=mycallbacks,\n",
        "                validation_data=(X_valid,y_valid) )\n",
        "        end_time=time.time()\n",
        "        elapsed_time=(end_time-start_time)                        \n",
        "        print(\"Fold %d, %d epochs, %d sec\"%(fold,EPOCHS,elapsed_time))\n",
        "        pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "        plt.grid(True)\n",
        "        plt.gca().set_ylim(0,1)\n",
        "        plt.show()\n",
        "        best_model=keras.models.load_model(bestname)\n",
        "        scores = best_model.evaluate(X_valid, y_valid, verbose=0)\n",
        "        print(\"%s: %.2f%%\" % (best_model.metrics_names[1], scores[1]*100))\n",
        "        cv_scores.append(scores[1] * 100)  \n",
        "    print()\n",
        "    print(\"%d-way Cross Validation mean %.2f%% (+/- %.2f%%)\" % (fold, np.mean(cv_scores), np.std(cv_scores)))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd3Wj_vI9KdP"
      },
      "source": [
        "## Train on RNA lengths 200-1Kb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8fNo6sn9KdH",
        "outputId": "592ea2be-a672-47fa-dc67-376b164f32dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "MINLEN=200\n",
        "MAXLEN=1000\n",
        "print(\"Load data from files.\")\n",
        "nc_seq=load_fasta(NC_FILENAME,0)\n",
        "pc_seq=load_fasta(PC_FILENAME,1)\n",
        "train_set=pd.concat((nc_seq,pc_seq),axis=0)\n",
        "nc_seq=None\n",
        "pc_seq=None\n",
        "print(\"Ready: train_set\")\n",
        "#train_set\n",
        "subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
        "print (\"Data reshape\")\n",
        "(X_train,y_train)=make_kmers(MAXLEN,subset)\n",
        "#print (\"Data prep\")\n",
        "#X_train=make_frequencies(X_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load data from files.\n",
            "Ready: train_set\n",
            "Data reshape\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1HuSs8ZbeL4",
        "outputId": "f532ef18-90a0-4453-ca30-d5d4d2ea9d70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        }
      },
      "source": [
        "print (\"Compile the model\")\n",
        "model=build_model()\n",
        "print (\"Summarize the model\")\n",
        "print(model.summary())  # Print this only once\n",
        "model.save(DATAPATH+FILENAME+'.model')\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compile the model\n",
            "COMPILE...\n",
            "...COMPILED\n",
            "Summarize the model\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          1040      \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 128)         31488     \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 128)               74496     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 119,505\n",
            "Trainable params: 119,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/data/GRU208.model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ8eW5Rg9KdQ",
        "outputId": "a2d477bd-de90-4598-d2ab-d8854c0b2747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print (\"Cross valiation\")\n",
        "do_cross_validation(X_train,y_train,model)  \n",
        "print (\"Done\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross valiation\n",
            "COMPILE...\n",
            "...COMPILED\n",
            "FIT\n",
            "Epoch 1/200\n",
            "453/453 [==============================] - ETA: 0s - loss: 0.6692 - accuracy: 0.6198INFO:tensorflow:Assets written to: /content/drive/My Drive/data/GRU208.cv.1.best/assets\n",
            "453/453 [==============================] - 110s 244ms/step - loss: 0.6692 - accuracy: 0.6198 - val_loss: 0.6410 - val_accuracy: 0.6530\n",
            "Epoch 2/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6618 - accuracy: 0.6260 - val_loss: 0.6443 - val_accuracy: 0.6530\n",
            "Epoch 3/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6599 - accuracy: 0.6283 - val_loss: 0.6366 - val_accuracy: 0.6530\n",
            "Epoch 4/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6601 - accuracy: 0.6268 - val_loss: 0.6766 - val_accuracy: 0.6530\n",
            "Epoch 5/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6636 - accuracy: 0.6266 - val_loss: 0.6389 - val_accuracy: 0.6530\n",
            "Epoch 6/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6628 - accuracy: 0.6242 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 7/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6609 - accuracy: 0.6298 - val_loss: 0.6391 - val_accuracy: 0.6530\n",
            "Epoch 8/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6615 - accuracy: 0.6281 - val_loss: 0.6417 - val_accuracy: 0.6530\n",
            "Epoch 9/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6634 - accuracy: 0.6200 - val_loss: 0.6409 - val_accuracy: 0.6530\n",
            "Epoch 10/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6601 - accuracy: 0.6266 - val_loss: 0.6390 - val_accuracy: 0.6530\n",
            "Epoch 11/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6626 - accuracy: 0.6237 - val_loss: 0.6678 - val_accuracy: 0.4736\n",
            "Epoch 12/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6619 - accuracy: 0.6275 - val_loss: 0.6899 - val_accuracy: 0.4730\n",
            "Epoch 13/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6610 - accuracy: 0.6269 - val_loss: 0.6400 - val_accuracy: 0.6530\n",
            "Epoch 14/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6608 - accuracy: 0.6296 - val_loss: 0.6424 - val_accuracy: 0.6530\n",
            "Epoch 15/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6610 - accuracy: 0.6284 - val_loss: 0.6409 - val_accuracy: 0.6530\n",
            "Epoch 16/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6614 - accuracy: 0.6262 - val_loss: 0.7243 - val_accuracy: 0.6530\n",
            "Epoch 17/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6668 - accuracy: 0.6186 - val_loss: 0.6438 - val_accuracy: 0.6530\n",
            "Epoch 18/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6614 - accuracy: 0.6235 - val_loss: 0.6418 - val_accuracy: 0.6530\n",
            "Epoch 19/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6594 - accuracy: 0.6285 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 20/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6617 - accuracy: 0.6263 - val_loss: 0.6476 - val_accuracy: 0.6530\n",
            "Epoch 21/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6602 - accuracy: 0.6300 - val_loss: 0.6384 - val_accuracy: 0.6530\n",
            "Epoch 22/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6608 - accuracy: 0.6275 - val_loss: 0.6389 - val_accuracy: 0.6530\n",
            "Epoch 23/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6611 - accuracy: 0.6300 - val_loss: 0.6449 - val_accuracy: 0.6530\n",
            "Epoch 24/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6631 - accuracy: 0.6228 - val_loss: 0.6531 - val_accuracy: 0.6530\n",
            "Epoch 25/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6617 - accuracy: 0.6241 - val_loss: 0.6937 - val_accuracy: 0.4761\n",
            "Epoch 26/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6638 - accuracy: 0.6219 - val_loss: 0.6391 - val_accuracy: 0.6530\n",
            "Epoch 27/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6626 - accuracy: 0.6272 - val_loss: 0.6448 - val_accuracy: 0.6530\n",
            "Epoch 28/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6624 - accuracy: 0.6289 - val_loss: 0.6663 - val_accuracy: 0.4624\n",
            "Epoch 29/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6626 - accuracy: 0.6224 - val_loss: 0.6407 - val_accuracy: 0.6530\n",
            "Epoch 30/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6617 - accuracy: 0.6307 - val_loss: 0.6396 - val_accuracy: 0.6530\n",
            "Epoch 31/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6617 - accuracy: 0.6302 - val_loss: 0.6902 - val_accuracy: 0.6530\n",
            "Epoch 32/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6603 - accuracy: 0.6320 - val_loss: 0.6840 - val_accuracy: 0.4606\n",
            "Epoch 33/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6609 - accuracy: 0.6317 - val_loss: 0.6598 - val_accuracy: 0.6530\n",
            "Epoch 34/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6622 - accuracy: 0.6327 - val_loss: 0.6743 - val_accuracy: 0.6530\n",
            "Epoch 35/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6682 - accuracy: 0.6272 - val_loss: 0.7223 - val_accuracy: 0.3470\n",
            "Epoch 36/200\n",
            "453/453 [==============================] - 69s 151ms/step - loss: 0.6631 - accuracy: 0.6344 - val_loss: 0.6624 - val_accuracy: 0.6530\n",
            "Epoch 37/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6677 - accuracy: 0.6289 - val_loss: 0.6479 - val_accuracy: 0.6530\n",
            "Epoch 38/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6623 - accuracy: 0.6324 - val_loss: 0.6473 - val_accuracy: 0.6530\n",
            "Epoch 39/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6643 - accuracy: 0.6313 - val_loss: 0.7136 - val_accuracy: 0.3470\n",
            "Epoch 40/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6673 - accuracy: 0.6257 - val_loss: 0.6556 - val_accuracy: 0.6530\n",
            "Epoch 41/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6633 - accuracy: 0.6320 - val_loss: 0.6853 - val_accuracy: 0.6530\n",
            "Epoch 42/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6659 - accuracy: 0.6311 - val_loss: 0.6539 - val_accuracy: 0.6530\n",
            "Epoch 43/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6674 - accuracy: 0.6253 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 44/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6623 - accuracy: 0.6353 - val_loss: 0.7326 - val_accuracy: 0.6530\n",
            "Epoch 45/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6647 - accuracy: 0.6307 - val_loss: 0.6477 - val_accuracy: 0.6530\n",
            "Epoch 46/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6618 - accuracy: 0.6382 - val_loss: 0.6772 - val_accuracy: 0.6530\n",
            "Epoch 47/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6705 - accuracy: 0.6191 - val_loss: 0.6481 - val_accuracy: 0.6530\n",
            "Epoch 48/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6681 - accuracy: 0.6275 - val_loss: 0.6469 - val_accuracy: 0.6530\n",
            "Epoch 49/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6620 - accuracy: 0.6352 - val_loss: 0.6476 - val_accuracy: 0.6530\n",
            "Epoch 50/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6680 - accuracy: 0.6258 - val_loss: 0.6968 - val_accuracy: 0.3470\n",
            "Epoch 51/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6650 - accuracy: 0.6327 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 52/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6618 - accuracy: 0.6331 - val_loss: 0.6516 - val_accuracy: 0.6530\n",
            "Epoch 53/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6669 - accuracy: 0.6258 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 54/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6649 - accuracy: 0.6318 - val_loss: 0.6602 - val_accuracy: 0.6530\n",
            "Epoch 55/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6675 - accuracy: 0.6278 - val_loss: 0.6866 - val_accuracy: 0.6530\n",
            "Epoch 56/200\n",
            "453/453 [==============================] - 66s 147ms/step - loss: 0.6621 - accuracy: 0.6363 - val_loss: 0.7047 - val_accuracy: 0.3470\n",
            "Epoch 57/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6723 - accuracy: 0.6224 - val_loss: 0.6623 - val_accuracy: 0.6530\n",
            "Epoch 58/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6654 - accuracy: 0.6319 - val_loss: 0.7310 - val_accuracy: 0.6530\n",
            "Epoch 59/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6633 - accuracy: 0.6387 - val_loss: 0.6470 - val_accuracy: 0.6530\n",
            "Epoch 60/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6669 - accuracy: 0.6289 - val_loss: 0.6522 - val_accuracy: 0.6530\n",
            "Epoch 61/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6651 - accuracy: 0.6317 - val_loss: 0.6495 - val_accuracy: 0.6530\n",
            "Epoch 62/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6629 - accuracy: 0.6342 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 63/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6652 - accuracy: 0.6344 - val_loss: 0.6609 - val_accuracy: 0.6530\n",
            "Epoch 64/200\n",
            "453/453 [==============================] - 68s 151ms/step - loss: 0.6646 - accuracy: 0.6351 - val_loss: 0.6556 - val_accuracy: 0.6530\n",
            "Epoch 65/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6677 - accuracy: 0.6259 - val_loss: 0.6692 - val_accuracy: 0.6530\n",
            "Epoch 66/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6643 - accuracy: 0.6347 - val_loss: 0.6495 - val_accuracy: 0.6530\n",
            "Epoch 67/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6645 - accuracy: 0.6306 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 68/200\n",
            "453/453 [==============================] - 69s 151ms/step - loss: 0.6674 - accuracy: 0.6291 - val_loss: 0.6702 - val_accuracy: 0.6530\n",
            "Epoch 69/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6639 - accuracy: 0.6342 - val_loss: 0.6550 - val_accuracy: 0.6530\n",
            "Epoch 70/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6671 - accuracy: 0.6269 - val_loss: 0.6587 - val_accuracy: 0.6530\n",
            "Epoch 71/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6678 - accuracy: 0.6253 - val_loss: 0.6577 - val_accuracy: 0.6530\n",
            "Epoch 72/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6667 - accuracy: 0.6286 - val_loss: 0.6782 - val_accuracy: 0.6530\n",
            "Epoch 73/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6622 - accuracy: 0.6324 - val_loss: 0.6598 - val_accuracy: 0.6530\n",
            "Epoch 74/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6667 - accuracy: 0.6312 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 75/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6652 - accuracy: 0.6299 - val_loss: 0.6619 - val_accuracy: 0.6530\n",
            "Epoch 76/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6650 - accuracy: 0.6314 - val_loss: 0.6848 - val_accuracy: 0.6530\n",
            "Epoch 77/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6713 - accuracy: 0.6265 - val_loss: 0.6510 - val_accuracy: 0.6530\n",
            "Epoch 78/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6667 - accuracy: 0.6258 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 79/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6637 - accuracy: 0.6344 - val_loss: 0.6515 - val_accuracy: 0.6530\n",
            "Epoch 80/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6654 - accuracy: 0.6325 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 81/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6675 - accuracy: 0.6287 - val_loss: 0.6741 - val_accuracy: 0.6530\n",
            "Epoch 82/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6644 - accuracy: 0.6276 - val_loss: 0.6590 - val_accuracy: 0.6530\n",
            "Epoch 83/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6627 - accuracy: 0.6367 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 84/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6603 - accuracy: 0.6384 - val_loss: 0.6526 - val_accuracy: 0.6530\n",
            "Epoch 85/200\n",
            "453/453 [==============================] - 68s 151ms/step - loss: 0.6676 - accuracy: 0.6315 - val_loss: 0.6637 - val_accuracy: 0.6530\n",
            "Epoch 86/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6631 - accuracy: 0.6331 - val_loss: 0.6478 - val_accuracy: 0.6530\n",
            "Epoch 87/200\n",
            "453/453 [==============================] - 68s 151ms/step - loss: 0.6673 - accuracy: 0.6260 - val_loss: 0.6506 - val_accuracy: 0.6530\n",
            "Epoch 88/200\n",
            "453/453 [==============================] - 68s 151ms/step - loss: 0.6668 - accuracy: 0.6308 - val_loss: 0.6502 - val_accuracy: 0.6530\n",
            "Epoch 89/200\n",
            "453/453 [==============================] - 68s 151ms/step - loss: 0.6705 - accuracy: 0.6186 - val_loss: 0.6557 - val_accuracy: 0.6530\n",
            "Epoch 90/200\n",
            "453/453 [==============================] - 68s 151ms/step - loss: 0.6638 - accuracy: 0.6320 - val_loss: 0.6914 - val_accuracy: 0.6530\n",
            "Epoch 91/200\n",
            "453/453 [==============================] - 69s 152ms/step - loss: 0.6680 - accuracy: 0.6248 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 92/200\n",
            "453/453 [==============================] - 68s 151ms/step - loss: 0.6636 - accuracy: 0.6325 - val_loss: 0.6483 - val_accuracy: 0.6530\n",
            "Epoch 93/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6666 - accuracy: 0.6298 - val_loss: 0.6845 - val_accuracy: 0.6530\n",
            "Epoch 94/200\n",
            "453/453 [==============================] - 68s 151ms/step - loss: 0.6682 - accuracy: 0.6284 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 95/200\n",
            "453/453 [==============================] - 69s 152ms/step - loss: 0.6667 - accuracy: 0.6258 - val_loss: 0.6549 - val_accuracy: 0.6530\n",
            "Epoch 96/200\n",
            "453/453 [==============================] - 68s 151ms/step - loss: 0.6656 - accuracy: 0.6315 - val_loss: 0.6479 - val_accuracy: 0.6530\n",
            "Epoch 97/200\n",
            "453/453 [==============================] - 69s 151ms/step - loss: 0.6684 - accuracy: 0.6255 - val_loss: 0.6625 - val_accuracy: 0.6530\n",
            "Epoch 98/200\n",
            "453/453 [==============================] - 69s 152ms/step - loss: 0.6664 - accuracy: 0.6258 - val_loss: 0.6994 - val_accuracy: 0.6530\n",
            "Epoch 99/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6645 - accuracy: 0.6346 - val_loss: 0.6476 - val_accuracy: 0.6530\n",
            "Epoch 100/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6684 - accuracy: 0.6274 - val_loss: 0.6500 - val_accuracy: 0.6530\n",
            "Epoch 101/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6699 - accuracy: 0.6242 - val_loss: 0.6851 - val_accuracy: 0.6530\n",
            "Epoch 102/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6669 - accuracy: 0.6336 - val_loss: 0.6486 - val_accuracy: 0.6530\n",
            "Epoch 103/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6657 - accuracy: 0.6354 - val_loss: 0.6484 - val_accuracy: 0.6530\n",
            "Epoch 104/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6652 - accuracy: 0.6338 - val_loss: 0.6500 - val_accuracy: 0.6530\n",
            "Epoch 105/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6626 - accuracy: 0.6330 - val_loss: 0.6582 - val_accuracy: 0.6530\n",
            "Epoch 106/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6597 - accuracy: 0.6366 - val_loss: 0.6903 - val_accuracy: 0.6530\n",
            "Epoch 107/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6641 - accuracy: 0.6310 - val_loss: 0.6989 - val_accuracy: 0.3470\n",
            "Epoch 108/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6687 - accuracy: 0.6242 - val_loss: 0.6525 - val_accuracy: 0.6530\n",
            "Epoch 109/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6680 - accuracy: 0.6269 - val_loss: 0.6521 - val_accuracy: 0.6530\n",
            "Epoch 110/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6647 - accuracy: 0.6307 - val_loss: 0.6549 - val_accuracy: 0.6530\n",
            "Epoch 111/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6646 - accuracy: 0.6340 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 112/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6658 - accuracy: 0.6282 - val_loss: 0.6505 - val_accuracy: 0.6530\n",
            "Epoch 113/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6623 - accuracy: 0.6353 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 114/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6661 - accuracy: 0.6314 - val_loss: 0.6470 - val_accuracy: 0.6530\n",
            "Epoch 115/200\n",
            "453/453 [==============================] - 69s 152ms/step - loss: 0.6690 - accuracy: 0.6316 - val_loss: 0.6752 - val_accuracy: 0.6530\n",
            "Epoch 116/200\n",
            "453/453 [==============================] - 69s 151ms/step - loss: 0.6675 - accuracy: 0.6300 - val_loss: 0.6831 - val_accuracy: 0.6530\n",
            "Epoch 117/200\n",
            "453/453 [==============================] - 69s 151ms/step - loss: 0.6653 - accuracy: 0.6322 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
            "Epoch 118/200\n",
            "453/453 [==============================] - 69s 152ms/step - loss: 0.6636 - accuracy: 0.6319 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 119/200\n",
            "453/453 [==============================] - 69s 152ms/step - loss: 0.6665 - accuracy: 0.6262 - val_loss: 0.6715 - val_accuracy: 0.6530\n",
            "Epoch 120/200\n",
            "453/453 [==============================] - 68s 151ms/step - loss: 0.6668 - accuracy: 0.6286 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 121/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6639 - accuracy: 0.6338 - val_loss: 0.6469 - val_accuracy: 0.6530\n",
            "Epoch 122/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6625 - accuracy: 0.6340 - val_loss: 0.6614 - val_accuracy: 0.6530\n",
            "Epoch 123/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6632 - accuracy: 0.6329 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 124/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6649 - accuracy: 0.6342 - val_loss: 0.6566 - val_accuracy: 0.6530\n",
            "Epoch 125/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6686 - accuracy: 0.6251 - val_loss: 0.6976 - val_accuracy: 0.6530\n",
            "Epoch 126/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6640 - accuracy: 0.6319 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 127/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6649 - accuracy: 0.6308 - val_loss: 0.6552 - val_accuracy: 0.6530\n",
            "Epoch 128/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6670 - accuracy: 0.6256 - val_loss: 0.6589 - val_accuracy: 0.6530\n",
            "Epoch 129/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6642 - accuracy: 0.6318 - val_loss: 0.6510 - val_accuracy: 0.6530\n",
            "Epoch 130/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6674 - accuracy: 0.6266 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 131/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6667 - accuracy: 0.6310 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 132/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6654 - accuracy: 0.6320 - val_loss: 0.6483 - val_accuracy: 0.6530\n",
            "Epoch 133/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6621 - accuracy: 0.6382 - val_loss: 0.6497 - val_accuracy: 0.6530\n",
            "Epoch 134/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6658 - accuracy: 0.6311 - val_loss: 0.6612 - val_accuracy: 0.6530\n",
            "Epoch 135/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6671 - accuracy: 0.6265 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 136/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6639 - accuracy: 0.6289 - val_loss: 0.6596 - val_accuracy: 0.6530\n",
            "Epoch 137/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6645 - accuracy: 0.6323 - val_loss: 0.6488 - val_accuracy: 0.6530\n",
            "Epoch 138/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6660 - accuracy: 0.6329 - val_loss: 0.6591 - val_accuracy: 0.6530\n",
            "Epoch 139/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6658 - accuracy: 0.6262 - val_loss: 0.6475 - val_accuracy: 0.6530\n",
            "Epoch 140/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6660 - accuracy: 0.6289 - val_loss: 0.6532 - val_accuracy: 0.6530\n",
            "Epoch 141/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6661 - accuracy: 0.6298 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 142/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6634 - accuracy: 0.6345 - val_loss: 0.6492 - val_accuracy: 0.6530\n",
            "Epoch 143/200\n",
            "453/453 [==============================] - 68s 151ms/step - loss: 0.6662 - accuracy: 0.6273 - val_loss: 0.6550 - val_accuracy: 0.6530\n",
            "Epoch 144/200\n",
            "453/453 [==============================] - 68s 151ms/step - loss: 0.6653 - accuracy: 0.6281 - val_loss: 0.6513 - val_accuracy: 0.6530\n",
            "Epoch 145/200\n",
            "453/453 [==============================] - 68s 151ms/step - loss: 0.6643 - accuracy: 0.6323 - val_loss: 0.6666 - val_accuracy: 0.6530\n",
            "Epoch 146/200\n",
            "453/453 [==============================] - 69s 151ms/step - loss: 0.6632 - accuracy: 0.6332 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 147/200\n",
            "453/453 [==============================] - 68s 151ms/step - loss: 0.6642 - accuracy: 0.6344 - val_loss: 0.6537 - val_accuracy: 0.6530\n",
            "Epoch 148/200\n",
            "453/453 [==============================] - 68s 151ms/step - loss: 0.6641 - accuracy: 0.6338 - val_loss: 0.6473 - val_accuracy: 0.6530\n",
            "Epoch 149/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6635 - accuracy: 0.6350 - val_loss: 0.6536 - val_accuracy: 0.6530\n",
            "Epoch 150/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6709 - accuracy: 0.6249 - val_loss: 0.6474 - val_accuracy: 0.6530\n",
            "Epoch 151/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6626 - accuracy: 0.6342 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 152/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6630 - accuracy: 0.6334 - val_loss: 0.6470 - val_accuracy: 0.6530\n",
            "Epoch 153/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6690 - accuracy: 0.6269 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
            "Epoch 154/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6634 - accuracy: 0.6354 - val_loss: 0.6551 - val_accuracy: 0.6530\n",
            "Epoch 155/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6638 - accuracy: 0.6353 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 156/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6682 - accuracy: 0.6258 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 157/200\n",
            "453/453 [==============================] - 63s 140ms/step - loss: 0.6647 - accuracy: 0.6335 - val_loss: 0.6498 - val_accuracy: 0.6530\n",
            "Epoch 158/200\n",
            "453/453 [==============================] - 62s 136ms/step - loss: 0.6654 - accuracy: 0.6290 - val_loss: 0.6474 - val_accuracy: 0.6530\n",
            "Epoch 159/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6620 - accuracy: 0.6343 - val_loss: 0.6508 - val_accuracy: 0.6530\n",
            "Epoch 160/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6667 - accuracy: 0.6280 - val_loss: 0.6562 - val_accuracy: 0.6530\n",
            "Epoch 161/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6641 - accuracy: 0.6300 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 162/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6638 - accuracy: 0.6350 - val_loss: 0.6595 - val_accuracy: 0.6530\n",
            "Epoch 163/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6675 - accuracy: 0.6295 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 164/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6636 - accuracy: 0.6344 - val_loss: 0.6512 - val_accuracy: 0.6530\n",
            "Epoch 165/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6629 - accuracy: 0.6325 - val_loss: 0.6682 - val_accuracy: 0.6530\n",
            "Epoch 166/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6652 - accuracy: 0.6322 - val_loss: 0.6500 - val_accuracy: 0.6530\n",
            "Epoch 167/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6626 - accuracy: 0.6353 - val_loss: 0.6485 - val_accuracy: 0.6530\n",
            "Epoch 168/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6634 - accuracy: 0.6349 - val_loss: 0.6492 - val_accuracy: 0.6530\n",
            "Epoch 169/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6656 - accuracy: 0.6324 - val_loss: 0.6664 - val_accuracy: 0.6530\n",
            "Epoch 170/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6636 - accuracy: 0.6364 - val_loss: 0.6490 - val_accuracy: 0.6530\n",
            "Epoch 171/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6637 - accuracy: 0.6342 - val_loss: 0.6571 - val_accuracy: 0.6530\n",
            "Epoch 172/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6655 - accuracy: 0.6323 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 173/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6694 - accuracy: 0.6264 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 174/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6635 - accuracy: 0.6325 - val_loss: 0.6485 - val_accuracy: 0.6530\n",
            "Epoch 175/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6644 - accuracy: 0.6322 - val_loss: 0.6481 - val_accuracy: 0.6530\n",
            "Epoch 176/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6617 - accuracy: 0.6344 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 177/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6618 - accuracy: 0.6358 - val_loss: 0.7355 - val_accuracy: 0.3470\n",
            "Epoch 178/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6650 - accuracy: 0.6287 - val_loss: 0.6478 - val_accuracy: 0.6530\n",
            "Epoch 179/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6652 - accuracy: 0.6337 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 180/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6645 - accuracy: 0.6358 - val_loss: 0.6799 - val_accuracy: 0.6530\n",
            "Epoch 181/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6647 - accuracy: 0.6304 - val_loss: 0.6553 - val_accuracy: 0.6530\n",
            "Epoch 182/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6630 - accuracy: 0.6327 - val_loss: 0.6620 - val_accuracy: 0.6530\n",
            "Epoch 183/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6658 - accuracy: 0.6275 - val_loss: 0.6471 - val_accuracy: 0.6530\n",
            "Epoch 184/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6655 - accuracy: 0.6280 - val_loss: 0.6521 - val_accuracy: 0.6530\n",
            "Epoch 185/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6676 - accuracy: 0.6257 - val_loss: 0.6495 - val_accuracy: 0.6530\n",
            "Epoch 186/200\n",
            "453/453 [==============================] - 60s 134ms/step - loss: 0.6643 - accuracy: 0.6290 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 187/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6610 - accuracy: 0.6385 - val_loss: 0.6514 - val_accuracy: 0.6530\n",
            "Epoch 188/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6669 - accuracy: 0.6270 - val_loss: 0.6522 - val_accuracy: 0.6530\n",
            "Epoch 189/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6655 - accuracy: 0.6327 - val_loss: 0.6552 - val_accuracy: 0.6530\n",
            "Epoch 190/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6662 - accuracy: 0.6338 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 191/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6683 - accuracy: 0.6253 - val_loss: 0.6508 - val_accuracy: 0.6530\n",
            "Epoch 192/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6643 - accuracy: 0.6316 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 193/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6648 - accuracy: 0.6301 - val_loss: 0.6516 - val_accuracy: 0.6530\n",
            "Epoch 194/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6623 - accuracy: 0.6338 - val_loss: 0.6520 - val_accuracy: 0.6530\n",
            "Epoch 195/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6624 - accuracy: 0.6379 - val_loss: 0.6474 - val_accuracy: 0.6530\n",
            "Epoch 196/200\n",
            "453/453 [==============================] - 62s 136ms/step - loss: 0.6623 - accuracy: 0.6362 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 197/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6637 - accuracy: 0.6309 - val_loss: 0.6600 - val_accuracy: 0.6530\n",
            "Epoch 198/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6670 - accuracy: 0.6293 - val_loss: 0.6896 - val_accuracy: 0.6530\n",
            "Epoch 199/200\n",
            "453/453 [==============================] - 62s 137ms/step - loss: 0.6624 - accuracy: 0.6349 - val_loss: 0.6491 - val_accuracy: 0.6530\n",
            "Epoch 200/200\n",
            "453/453 [==============================] - 62s 136ms/step - loss: 0.6651 - accuracy: 0.6322 - val_loss: 0.6512 - val_accuracy: 0.6530\n",
            "Fold 1, 200 epochs, 13320 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 65.30%\n",
            "\n",
            "1-way Cross Validation mean 65.30% (+/- 0.00%)\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4fh2GI8beMQ"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}