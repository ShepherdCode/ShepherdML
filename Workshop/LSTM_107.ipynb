{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM_107.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojm_6E9f9Kcf"
      },
      "source": [
        "# LSTM 107\n",
        "Train on large, evaluate on all"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh6XplUvC0j0",
        "outputId": "a804bf22-6abb-4049-da72-9a14da814c6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "PATH='/content/drive/'\n",
        "drive.mount(PATH)\n",
        "DATAPATH=PATH+'My Drive/data/'\n",
        "PC_FILENAME = DATAPATH+'pcRNA.fasta'\n",
        "NC_FILENAME = DATAPATH+'ncRNA.fasta'\n",
        "#PC_FILENAME = 'pcRNA.fasta'\n",
        "#NC_FILENAME = 'ncRNA.fasta'\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQY7aTj29Kch"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LayerNormalization\n",
        "import time\n",
        "\n",
        "dt='float32'\n",
        "tf.keras.backend.set_floatx(dt)\n",
        "\n",
        "EPOCHS=200\n",
        "SPLITS=1\n",
        "K=4\n",
        "VOCABULARY_SIZE=4**K+1   # e.g. K=3 => 64 DNA K-mers + 'NNN'\n",
        "EMBED_DIMEN=16\n",
        "FILENAME='LSTM107'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV6k-xOm9Kcn"
      },
      "source": [
        "## Load and partition sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I-O_qzw9Kco"
      },
      "source": [
        "# Assume file was preprocessed to contain one line per seq.\n",
        "# Prefer Pandas dataframe but df does not support append.\n",
        "# For conversion to tensor, must avoid python lists.\n",
        "def load_fasta(filename,label):\n",
        "    DEFLINE='>'\n",
        "    labels=[]\n",
        "    seqs=[]\n",
        "    lens=[]\n",
        "    nums=[]\n",
        "    num=0\n",
        "    with open (filename,'r') as infile:\n",
        "        for line in infile:\n",
        "            if line[0]!=DEFLINE:\n",
        "                seq=line.rstrip()\n",
        "                num += 1   # first seqnum is 1\n",
        "                seqlen=len(seq)\n",
        "                nums.append(num)\n",
        "                labels.append(label)\n",
        "                seqs.append(seq)\n",
        "                lens.append(seqlen)\n",
        "    df1=pd.DataFrame(nums,columns=['seqnum'])\n",
        "    df2=pd.DataFrame(labels,columns=['class'])\n",
        "    df3=pd.DataFrame(seqs,columns=['sequence'])\n",
        "    df4=pd.DataFrame(lens,columns=['seqlen'])\n",
        "    df=pd.concat((df1,df2,df3,df4),axis=1)\n",
        "    return df\n",
        "\n",
        "# Split into train/test stratified by sequence length.\n",
        "def sizebin(df):\n",
        "    return pd.cut(df[\"seqlen\"],\n",
        "                              bins=[0,1000,2000,4000,8000,16000,np.inf],\n",
        "                              labels=[0,1,2,3,4,5])\n",
        "def make_train_test(data):\n",
        "    bin_labels= sizebin(data)\n",
        "    from sklearn.model_selection import StratifiedShuffleSplit\n",
        "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=37863)\n",
        "    # split(x,y) expects that y is the labels. \n",
        "    # Trick: Instead of y, give it it the bin labels that we generated.\n",
        "    for train_index,test_index in splitter.split(data,bin_labels):\n",
        "        train_set = data.iloc[train_index]\n",
        "        test_set = data.iloc[test_index]\n",
        "    return (train_set,test_set)\n",
        "\n",
        "def separate_X_and_y(data):\n",
        "    y=   data[['class']].copy()\n",
        "    X=   data.drop(columns=['class','seqnum','seqlen'])\n",
        "    return (X,y)\n",
        "\n",
        "def make_slice(data_set,min_len,max_len):\n",
        "    print(\"original \"+str(data_set.shape))\n",
        "    too_short = data_set[ data_set['seqlen'] < min_len ].index\n",
        "    no_short=data_set.drop(too_short)\n",
        "    print(\"no short \"+str(no_short.shape))\n",
        "    too_long = no_short[ no_short['seqlen'] >= max_len ].index\n",
        "    no_long_no_short=no_short.drop(too_long)\n",
        "    print(\"no long, no short \"+str(no_long_no_short.shape))\n",
        "    return no_long_no_short\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRAaO9jP9Kcr"
      },
      "source": [
        "## Make K-mers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8xcZ4Mr9Kcs"
      },
      "source": [
        "def make_kmer_table(K):\n",
        "    npad='N'*K\n",
        "    shorter_kmers=['']\n",
        "    for i in range(K):\n",
        "        longer_kmers=[]\n",
        "        for mer in shorter_kmers:\n",
        "            longer_kmers.append(mer+'A')\n",
        "            longer_kmers.append(mer+'C')\n",
        "            longer_kmers.append(mer+'G')\n",
        "            longer_kmers.append(mer+'T')\n",
        "        shorter_kmers = longer_kmers\n",
        "    all_kmers = shorter_kmers\n",
        "    kmer_dict = {}\n",
        "    kmer_dict[npad]=0\n",
        "    value=1\n",
        "    for mer in all_kmers:\n",
        "        kmer_dict[mer]=value\n",
        "        value += 1\n",
        "    return kmer_dict\n",
        "\n",
        "KMER_TABLE=make_kmer_table(K)\n",
        "\n",
        "def strings_to_vectors(data,uniform_len):\n",
        "    all_seqs=[]\n",
        "    for seq in data['sequence']:\n",
        "        i=0\n",
        "        seqlen=len(seq)\n",
        "        kmers=[]\n",
        "        while i < seqlen-K+1 -1:  # stop at minus one for spaced seed\n",
        "            #kmer=seq[i:i+2]+seq[i+3:i+5]    # SPACED SEED 2/1/2 for K=4\n",
        "            kmer=seq[i:i+K]  \n",
        "            i += 1\n",
        "            value=KMER_TABLE[kmer]\n",
        "            kmers.append(value)\n",
        "        pad_val=0\n",
        "        while i < uniform_len:\n",
        "            kmers.append(pad_val)\n",
        "            i += 1\n",
        "        all_seqs.append(kmers)\n",
        "    pd2d=pd.DataFrame(all_seqs)\n",
        "    return pd2d   # return 2D dataframe, uniform dimensions"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEtA0xiV9Kcv"
      },
      "source": [
        "def make_kmers(MAXLEN,train_set):\n",
        "    (X_train_all,y_train_all)=separate_X_and_y(train_set)\n",
        "\n",
        "    # The returned values are Pandas dataframes.\n",
        "    # print(X_train_all.shape,y_train_all.shape)\n",
        "    # (X_train_all,y_train_all)\n",
        "    # y: Pandas dataframe to Python list.\n",
        "    # y_train_all=y_train_all.values.tolist()\n",
        "    # The sequences lengths are bounded but not uniform.\n",
        "    X_train_all\n",
        "    print(type(X_train_all))\n",
        "    print(X_train_all.shape)\n",
        "    print(X_train_all.iloc[0])\n",
        "    print(len(X_train_all.iloc[0]['sequence']))\n",
        "\n",
        "    # X: List of string to List of uniform-length ordered lists of K-mers.\n",
        "    X_train_kmers=strings_to_vectors(X_train_all,MAXLEN)\n",
        "    # X: true 2D array (no more lists)\n",
        "    X_train_kmers.shape\n",
        "\n",
        "    print(\"transform...\")\n",
        "    # From pandas dataframe to numpy to list to numpy\n",
        "    print(type(X_train_kmers))\n",
        "    num_seqs=len(X_train_kmers)\n",
        "    tmp_seqs=[]\n",
        "    for i in range(num_seqs):\n",
        "        kmer_sequence=X_train_kmers.iloc[i]\n",
        "        tmp_seqs.append(kmer_sequence)\n",
        "    X_train_kmers=np.array(tmp_seqs)\n",
        "    tmp_seqs=None\n",
        "    print(type(X_train_kmers))\n",
        "    print(X_train_kmers)\n",
        "\n",
        "    labels=y_train_all.to_numpy()\n",
        "    return (X_train_kmers,labels)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaXyySyO9Kcz"
      },
      "source": [
        "def make_frequencies(Xin):\n",
        "    # Input:  numpy X(numseq,seqlen)  list of vectors of kmerval where val0=NNN,val1=AAA,etc. \n",
        "    # Output: numpy X(numseq,65)    list of frequencies of 0,1,etc.\n",
        "    Xout=[]\n",
        "    VOCABULARY_SIZE= 4**K + 1  # plus one for 'NNN'\n",
        "    for seq in Xin:\n",
        "        freqs =[0] * VOCABULARY_SIZE\n",
        "        total = 0\n",
        "        for kmerval in seq:\n",
        "            freqs[kmerval] += 1\n",
        "            total += 1\n",
        "        for c in range(VOCABULARY_SIZE):\n",
        "            freqs[c] = freqs[c]/total\n",
        "        Xout.append(freqs)\n",
        "    Xnum = np.asarray(Xout)\n",
        "    return (Xnum)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7jcg6Wl9Kc2"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLFNO1Xa9Kc3"
      },
      "source": [
        "def build_model(maxlen):\n",
        "    vocabulary_size=4**K+1   # e.g. K=3 => 64 DNA K-mers + 'NNN'\n",
        "    act=\"sigmoid\"\n",
        "    dt='float32'\n",
        "\n",
        "    neurons=16\n",
        "    dot = 0.50\n",
        "    rnn = keras.models.Sequential()\n",
        "    embed_layer = keras.layers.Embedding(\n",
        "        vocabulary_size,EMBED_DIMEN,input_length=maxlen);\n",
        "    rnn1_layer = keras.layers.Bidirectional(\n",
        "        keras.layers.LSTM(neurons, return_sequences=True, dropout=dot, \n",
        "            input_shape=[maxlen,EMBED_DIMEN]))\n",
        "    rnn2_layer = keras.layers.Bidirectional(\n",
        "        keras.layers.LSTM(neurons, dropout=dot, return_sequences=True))\n",
        "    dense1_layer = keras.layers.Dense(neurons,activation=act,dtype=dt)\n",
        "    drop1_layer  = keras.layers.Dropout(dot)\n",
        "    dense2_layer = keras.layers.Dense(neurons,activation=act,dtype=dt)\n",
        "    drop2_layer  = keras.layers.Dropout(dot)\n",
        "    output_layer = keras.layers.Dense(1,activation=act,dtype=dt)\n",
        "\n",
        "    rnn.add(embed_layer)\n",
        "    rnn.add(rnn1_layer)\n",
        "    rnn.add(rnn2_layer)\n",
        "    rnn.add(dense1_layer)\n",
        "    rnn.add(drop1_layer)\n",
        "    rnn.add(dense2_layer)\n",
        "    rnn.add(drop2_layer)\n",
        "    rnn.add(output_layer)\n",
        "\n",
        "    bc=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    print(\"COMPILE\")\n",
        "    rnn.compile(loss=bc, optimizer=\"Adam\",metrics=[\"accuracy\"])\n",
        "    return rnn"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdIS2utq9Kc9"
      },
      "source": [
        "## Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVo4tbB_9Kc-"
      },
      "source": [
        "def do_cross_validation(X,y,maxlen):\n",
        "    model = None\n",
        "    cv_scores = []\n",
        "    fold=0\n",
        "    splitter = ShuffleSplit(n_splits=SPLITS, test_size=0.2, random_state=37863)\n",
        "    for train_index,valid_index in splitter.split(X):\n",
        "        X_train=X[train_index] # use iloc[] for dataframe\n",
        "        y_train=y[train_index]\n",
        "        X_valid=X[valid_index]\n",
        "        y_valid=y[valid_index]\n",
        "\n",
        "        print(\"BUILD MODEL\")\n",
        "        model=build_model(maxlen)\n",
        "\n",
        "        print(\"FIT\")\n",
        "        start_time=time.time()\n",
        "        # this is complaining about string to float\n",
        "        history=model.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
        "                epochs=EPOCHS, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
        "                validation_data=(X_valid,y_valid) )\n",
        "        end_time=time.time()\n",
        "        elapsed_time=(end_time-start_time)\n",
        "                        \n",
        "        fold += 1\n",
        "        print(\"Fold %d, %d epochs, %d sec\"%(fold,EPOCHS,elapsed_time))\n",
        "\n",
        "        pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "        plt.grid(True)\n",
        "        plt.gca().set_ylim(0,1)\n",
        "        plt.show()\n",
        "\n",
        "        scores = model.evaluate(X_valid, y_valid, verbose=0)\n",
        "        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "        # What are the other metrics_names?\n",
        "        # Try this from Geron page 505:\n",
        "        # np.mean(keras.losses.mean_squared_error(y_valid,y_pred))\n",
        "        cv_scores.append(scores[1] * 100)  \n",
        "    print()\n",
        "    print(\"Validation core mean %.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upc3S0eEgYZG"
      },
      "source": [
        "def just_train(model,X_train,y_train,maxlen):\n",
        "    print(\"FIT\")\n",
        "    start_time=time.time()\n",
        "    # this is complaining about string to float\n",
        "    history=model.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
        "            epochs=EPOCHS, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
        "            )  # no validation data\n",
        "    end_time=time.time()\n",
        "    elapsed_time=(end_time-start_time)\n",
        "    print(\"Train %d epochs, %d sec\"%(EPOCHS,elapsed_time))\n",
        "\n",
        "    pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "    plt.grid(True)\n",
        "    plt.gca().set_ylim(0,1)\n",
        "    plt.show()\n",
        "\n",
        "    #scores = model.evaluate(X_valid, y_valid, verbose=0)\n",
        "    #print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "    #cv_scores.append(scores[1] * 100)  \n",
        "    #print()\n",
        "    #print(\"Validation core mean %.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q-PEh7D9KdH"
      },
      "source": [
        "## Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8fNo6sn9KdH",
        "outputId": "92f42564-a12a-48e9-fcef-d325079d085f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "print(\"Load data from files.\")\n",
        "nc_seq=load_fasta(NC_FILENAME,0)\n",
        "pc_seq=load_fasta(PC_FILENAME,1)\n",
        "all_seq=pd.concat((nc_seq,pc_seq),axis=0)\n",
        "\n",
        "print(\"Put aside the test portion.\")\n",
        "(train_set,test_set)=make_train_test(all_seq)\n",
        "# Do this later when using the test data:\n",
        "# (X_test,y_test)=separate_X_and_y(test_set)\n",
        "\n",
        "nc_seq=None\n",
        "pc_seq=None\n",
        "all_seq=None\n",
        "\n",
        "print(\"Ready: train_set\")\n",
        "train_set"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load data from files.\n",
            "Put aside the test portion.\n",
            "Ready: train_set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>seqnum</th>\n",
              "      <th>class</th>\n",
              "      <th>sequence</th>\n",
              "      <th>seqlen</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1280</th>\n",
              "      <td>1281</td>\n",
              "      <td>0</td>\n",
              "      <td>AGTCCCTCCCCAGCCCAGCAGTCCCTCCAGGCTACATCCAGGAGAC...</td>\n",
              "      <td>348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9088</th>\n",
              "      <td>9089</td>\n",
              "      <td>0</td>\n",
              "      <td>CAGCTCCTGGGATGGCCTCACCTGAGGAGACTCTTGGGCCTTGGCA...</td>\n",
              "      <td>534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6069</th>\n",
              "      <td>6070</td>\n",
              "      <td>1</td>\n",
              "      <td>AGATCTAGGGATGGGGATGGGGAGGAGAAGTGGGAATGGGAAATTG...</td>\n",
              "      <td>592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18549</th>\n",
              "      <td>18550</td>\n",
              "      <td>1</td>\n",
              "      <td>GACGTCTCCCGCGGGCGTCGGCAGGGTCGGCGGCGTCGGCAGCAGT...</td>\n",
              "      <td>945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15027</th>\n",
              "      <td>15028</td>\n",
              "      <td>1</td>\n",
              "      <td>GAGCGCGCGAGCCGGGCCCGGAGCGCACGCCGCCGCCGCCACCGCC...</td>\n",
              "      <td>4382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3386</th>\n",
              "      <td>3387</td>\n",
              "      <td>0</td>\n",
              "      <td>TTTATGTGGATTGTCTGTCTCATGCTTGTTTCACCAGGGTAGTTAC...</td>\n",
              "      <td>578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6495</th>\n",
              "      <td>6496</td>\n",
              "      <td>0</td>\n",
              "      <td>ATAATGGGAAACTAAGGGCAAGTTCTCATGTTCCTGGTCCTGGCTT...</td>\n",
              "      <td>562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6409</th>\n",
              "      <td>6410</td>\n",
              "      <td>1</td>\n",
              "      <td>GGGTTTATTACTACTGAAGGAAGAACGTGAGTAGGTTAGGATTTCG...</td>\n",
              "      <td>740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7640</th>\n",
              "      <td>7641</td>\n",
              "      <td>1</td>\n",
              "      <td>ACAGCTGTGTTTGGCTGCAGGGCCAAGAGCGCTGTCAAGAAGACCC...</td>\n",
              "      <td>3156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14108</th>\n",
              "      <td>14109</td>\n",
              "      <td>0</td>\n",
              "      <td>GAAGTGATTGCAAGTTCAGCAGCATGAAACTGCTCTTTATCCGTGC...</td>\n",
              "      <td>466</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30290 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       seqnum  class                                           sequence  seqlen\n",
              "1280     1281      0  AGTCCCTCCCCAGCCCAGCAGTCCCTCCAGGCTACATCCAGGAGAC...     348\n",
              "9088     9089      0  CAGCTCCTGGGATGGCCTCACCTGAGGAGACTCTTGGGCCTTGGCA...     534\n",
              "6069     6070      1  AGATCTAGGGATGGGGATGGGGAGGAGAAGTGGGAATGGGAAATTG...     592\n",
              "18549   18550      1  GACGTCTCCCGCGGGCGTCGGCAGGGTCGGCGGCGTCGGCAGCAGT...     945\n",
              "15027   15028      1  GAGCGCGCGAGCCGGGCCCGGAGCGCACGCCGCCGCCGCCACCGCC...    4382\n",
              "...       ...    ...                                                ...     ...\n",
              "3386     3387      0  TTTATGTGGATTGTCTGTCTCATGCTTGTTTCACCAGGGTAGTTAC...     578\n",
              "6495     6496      0  ATAATGGGAAACTAAGGGCAAGTTCTCATGTTCCTGGTCCTGGCTT...     562\n",
              "6409     6410      1  GGGTTTATTACTACTGAAGGAAGAACGTGAGTAGGTTAGGATTTCG...     740\n",
              "7640     7641      1  ACAGCTGTGTTTGGCTGCAGGGCCAAGAGCGCTGTCAAGAAGACCC...    3156\n",
              "14108   14109      0  GAAGTGATTGCAAGTTCAGCAGCATGAAACTGCTCTTTATCCGTGC...     466\n",
              "\n",
              "[30290 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lI2PjJsxgYZT"
      },
      "source": [
        "# Reuse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKLYbWycgYZU",
        "outputId": "cbd4a9e4-4977-4eeb-9a70-d547b9383a3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#reload=model1.load(FILENAME+'.short.model')\n",
        "#W = model1.get_weights()\n",
        "#scores = model1.evaluate(X_valid, y_valid, verbose=0)\n",
        "#print(\"%s: %.2f%%\" % (model1.metrics_names[1], scores[1]*100))\n",
        "\n",
        "min=2000\n",
        "max=3000\n",
        "print(\"Train on lengths %d to %d\"%(min,max))\n",
        "print(\"slice...\")\n",
        "subset=make_slice(train_set,min,max)\n",
        "print(\"kmers...\")\n",
        "(X_train,y_train)=make_kmers(max,subset)\n",
        "print(\"BUILD MODEL\")\n",
        "model=build_model(max)\n",
        "model=just_train(model,X_train,y_train,max)\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on lengths 2000 to 3000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (3221, 4)\n",
            "no long, no short (1351, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(1351, 1)\n",
            "sequence    GTCATTCTAGCTGCCTGCTGCCTCCGCAGCGTCCCCCCAGCTCTCC...\n",
            "Name: 19713, dtype: object\n",
            "2039\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[181 212  80 ...   0   0   0]\n",
            " [ 10  37 148 ...   0   0   0]\n",
            " [ 48 190 245 ...   0   0   0]\n",
            " ...\n",
            " [ 49 195  10 ...   0   0   0]\n",
            " [153  97 131 ...   0   0   0]\n",
            " [ 36 143  58 ...   0   0   0]]\n",
            "BUILD MODEL\n",
            "COMPILE\n",
            "FIT\n",
            "Epoch 1/200\n",
            "43/43 [==============================] - 13s 310ms/step - loss: 0.6936 - accuracy: 0.5904\n",
            "Epoch 2/200\n",
            "43/43 [==============================] - 13s 314ms/step - loss: 0.6529 - accuracy: 0.6644\n",
            "Epoch 3/200\n",
            "43/43 [==============================] - 14s 315ms/step - loss: 0.6453 - accuracy: 0.6671\n",
            "Epoch 4/200\n",
            "43/43 [==============================] - 14s 316ms/step - loss: 0.6393 - accuracy: 0.6781\n",
            "Epoch 5/200\n",
            "43/43 [==============================] - 14s 314ms/step - loss: 0.6357 - accuracy: 0.6791\n",
            "Epoch 6/200\n",
            "43/43 [==============================] - 13s 312ms/step - loss: 0.6320 - accuracy: 0.6876\n",
            "Epoch 7/200\n",
            "43/43 [==============================] - 14s 315ms/step - loss: 0.6297 - accuracy: 0.6911\n",
            "Epoch 8/200\n",
            "43/43 [==============================] - 14s 321ms/step - loss: 0.6278 - accuracy: 0.6927\n",
            "Epoch 9/200\n",
            "43/43 [==============================] - 14s 315ms/step - loss: 0.6260 - accuracy: 0.6942\n",
            "Epoch 10/200\n",
            "43/43 [==============================] - 14s 318ms/step - loss: 0.6240 - accuracy: 0.6969\n",
            "Epoch 11/200\n",
            "43/43 [==============================] - 13s 312ms/step - loss: 0.6233 - accuracy: 0.6982\n",
            "Epoch 12/200\n",
            "43/43 [==============================] - 14s 317ms/step - loss: 0.6220 - accuracy: 0.6990\n",
            "Epoch 13/200\n",
            "43/43 [==============================] - 13s 314ms/step - loss: 0.6201 - accuracy: 0.6999\n",
            "Epoch 14/200\n",
            "43/43 [==============================] - 14s 315ms/step - loss: 0.6167 - accuracy: 0.7002\n",
            "Epoch 15/200\n",
            "43/43 [==============================] - 14s 314ms/step - loss: 0.6112 - accuracy: 0.7017\n",
            "Epoch 16/200\n",
            "43/43 [==============================] - 14s 316ms/step - loss: 0.6039 - accuracy: 0.7019\n",
            "Epoch 17/200\n",
            "43/43 [==============================] - 14s 316ms/step - loss: 0.5977 - accuracy: 0.7023\n",
            "Epoch 18/200\n",
            "43/43 [==============================] - 14s 321ms/step - loss: 0.5927 - accuracy: 0.7067\n",
            "Epoch 19/200\n",
            "43/43 [==============================] - 14s 318ms/step - loss: 0.5929 - accuracy: 0.7048\n",
            "Epoch 20/200\n",
            "43/43 [==============================] - 14s 316ms/step - loss: 0.5866 - accuracy: 0.7091\n",
            "Epoch 21/200\n",
            "43/43 [==============================] - 14s 316ms/step - loss: 0.5843 - accuracy: 0.7096\n",
            "Epoch 22/200\n",
            "43/43 [==============================] - 14s 316ms/step - loss: 0.5892 - accuracy: 0.7152\n",
            "Epoch 23/200\n",
            "43/43 [==============================] - 14s 319ms/step - loss: 0.5904 - accuracy: 0.6965\n",
            "Epoch 24/200\n",
            "43/43 [==============================] - 14s 317ms/step - loss: 0.6111 - accuracy: 0.6872\n",
            "Epoch 25/200\n",
            "43/43 [==============================] - 14s 320ms/step - loss: 0.5987 - accuracy: 0.7007\n",
            "Epoch 26/200\n",
            "43/43 [==============================] - 14s 323ms/step - loss: 0.5935 - accuracy: 0.6964\n",
            "Epoch 27/200\n",
            "43/43 [==============================] - 14s 319ms/step - loss: 0.5747 - accuracy: 0.7036\n",
            "Epoch 28/200\n",
            "43/43 [==============================] - 13s 314ms/step - loss: 0.5702 - accuracy: 0.7077\n",
            "Epoch 29/200\n",
            "43/43 [==============================] - 14s 317ms/step - loss: 0.5759 - accuracy: 0.7010\n",
            "Epoch 30/200\n",
            "43/43 [==============================] - 13s 314ms/step - loss: 0.5827 - accuracy: 0.6992\n",
            "Epoch 31/200\n",
            "43/43 [==============================] - 14s 325ms/step - loss: 0.5798 - accuracy: 0.7072\n",
            "Epoch 32/200\n",
            "43/43 [==============================] - 14s 323ms/step - loss: 0.5744 - accuracy: 0.7080\n",
            "Epoch 33/200\n",
            "43/43 [==============================] - 14s 322ms/step - loss: 0.5718 - accuracy: 0.7108\n",
            "Epoch 34/200\n",
            "43/43 [==============================] - 14s 315ms/step - loss: 0.5759 - accuracy: 0.7044\n",
            "Epoch 35/200\n",
            "43/43 [==============================] - 14s 322ms/step - loss: 0.5684 - accuracy: 0.7109\n",
            "Epoch 36/200\n",
            "43/43 [==============================] - 14s 318ms/step - loss: 0.5607 - accuracy: 0.7162\n",
            "Epoch 37/200\n",
            "43/43 [==============================] - 14s 318ms/step - loss: 0.5601 - accuracy: 0.7177\n",
            "Epoch 38/200\n",
            "43/43 [==============================] - 14s 317ms/step - loss: 0.5574 - accuracy: 0.7171\n",
            "Epoch 39/200\n",
            "43/43 [==============================] - 14s 323ms/step - loss: 0.5520 - accuracy: 0.7198\n",
            "Epoch 40/200\n",
            "43/43 [==============================] - 14s 321ms/step - loss: 0.5474 - accuracy: 0.7251\n",
            "Epoch 41/200\n",
            "43/43 [==============================] - 14s 318ms/step - loss: 0.5532 - accuracy: 0.7181\n",
            "Epoch 42/200\n",
            "43/43 [==============================] - 14s 317ms/step - loss: 0.5301 - accuracy: 0.7295\n",
            "Epoch 43/200\n",
            "43/43 [==============================] - 14s 318ms/step - loss: 0.5246 - accuracy: 0.7335\n",
            "Epoch 44/200\n",
            "43/43 [==============================] - 14s 320ms/step - loss: 0.5190 - accuracy: 0.7385\n",
            "Epoch 45/200\n",
            "43/43 [==============================] - 14s 324ms/step - loss: 0.5415 - accuracy: 0.7313\n",
            "Epoch 46/200\n",
            "43/43 [==============================] - 14s 325ms/step - loss: 0.5171 - accuracy: 0.7432\n",
            "Epoch 47/200\n",
            "43/43 [==============================] - 14s 328ms/step - loss: 0.4920 - accuracy: 0.7608\n",
            "Epoch 48/200\n",
            "43/43 [==============================] - 14s 327ms/step - loss: 0.4795 - accuracy: 0.7645\n",
            "Epoch 49/200\n",
            "43/43 [==============================] - 14s 324ms/step - loss: 0.4465 - accuracy: 0.7902\n",
            "Epoch 50/200\n",
            "43/43 [==============================] - 14s 326ms/step - loss: 0.4447 - accuracy: 0.7982\n",
            "Epoch 51/200\n",
            "43/43 [==============================] - 14s 322ms/step - loss: 0.4620 - accuracy: 0.7997\n",
            "Epoch 52/200\n",
            "43/43 [==============================] - 14s 322ms/step - loss: 0.4634 - accuracy: 0.7816\n",
            "Epoch 53/200\n",
            "43/43 [==============================] - 14s 327ms/step - loss: 0.5491 - accuracy: 0.6979\n",
            "Epoch 54/200\n",
            "43/43 [==============================] - 14s 327ms/step - loss: 0.5913 - accuracy: 0.6896\n",
            "Epoch 55/200\n",
            "43/43 [==============================] - 14s 325ms/step - loss: 0.4756 - accuracy: 0.7518\n",
            "Epoch 56/200\n",
            "43/43 [==============================] - 14s 327ms/step - loss: 0.4429 - accuracy: 0.7888\n",
            "Epoch 57/200\n",
            "43/43 [==============================] - 14s 331ms/step - loss: 0.4258 - accuracy: 0.8041\n",
            "Epoch 58/200\n",
            "43/43 [==============================] - 14s 320ms/step - loss: 0.4169 - accuracy: 0.8158\n",
            "Epoch 59/200\n",
            "43/43 [==============================] - 14s 320ms/step - loss: 0.4206 - accuracy: 0.8122\n",
            "Epoch 60/200\n",
            "43/43 [==============================] - 14s 328ms/step - loss: 0.4078 - accuracy: 0.8284\n",
            "Epoch 61/200\n",
            "43/43 [==============================] - 14s 328ms/step - loss: 0.4009 - accuracy: 0.8377\n",
            "Epoch 62/200\n",
            "43/43 [==============================] - 14s 318ms/step - loss: 0.7164 - accuracy: 0.7255\n",
            "Epoch 63/200\n",
            "43/43 [==============================] - 14s 320ms/step - loss: 0.6240 - accuracy: 0.7005\n",
            "Epoch 64/200\n",
            "43/43 [==============================] - 14s 331ms/step - loss: 0.6198 - accuracy: 0.6998\n",
            "Epoch 65/200\n",
            "43/43 [==============================] - 14s 328ms/step - loss: 0.6191 - accuracy: 0.6996\n",
            "Epoch 66/200\n",
            "43/43 [==============================] - 14s 328ms/step - loss: 0.6186 - accuracy: 0.7004\n",
            "Epoch 67/200\n",
            "43/43 [==============================] - 14s 330ms/step - loss: 0.6179 - accuracy: 0.7006\n",
            "Epoch 68/200\n",
            "43/43 [==============================] - 14s 324ms/step - loss: 0.6174 - accuracy: 0.7012\n",
            "Epoch 69/200\n",
            "43/43 [==============================] - 14s 323ms/step - loss: 0.6172 - accuracy: 0.7011\n",
            "Epoch 70/200\n",
            "43/43 [==============================] - 14s 327ms/step - loss: 0.6172 - accuracy: 0.7014\n",
            "Epoch 71/200\n",
            "43/43 [==============================] - 14s 325ms/step - loss: 0.6166 - accuracy: 0.7011\n",
            "Epoch 72/200\n",
            "43/43 [==============================] - 14s 326ms/step - loss: 0.6161 - accuracy: 0.7015\n",
            "Epoch 73/200\n",
            "43/43 [==============================] - 14s 328ms/step - loss: 0.6157 - accuracy: 0.7016\n",
            "Epoch 74/200\n",
            "43/43 [==============================] - 14s 324ms/step - loss: 0.6139 - accuracy: 0.7017\n",
            "Epoch 75/200\n",
            "43/43 [==============================] - 14s 324ms/step - loss: 0.6129 - accuracy: 0.7016\n",
            "Epoch 76/200\n",
            "43/43 [==============================] - 14s 324ms/step - loss: 0.6156 - accuracy: 0.7020\n",
            "Epoch 77/200\n",
            "43/43 [==============================] - 14s 325ms/step - loss: 0.6135 - accuracy: 0.7019\n",
            "Epoch 78/200\n",
            "43/43 [==============================] - 14s 321ms/step - loss: 0.6159 - accuracy: 0.7018\n",
            "Epoch 79/200\n",
            "43/43 [==============================] - 14s 328ms/step - loss: 0.6151 - accuracy: 0.7019\n",
            "Epoch 80/200\n",
            "43/43 [==============================] - 14s 328ms/step - loss: 0.6150 - accuracy: 0.7019\n",
            "Epoch 81/200\n",
            "43/43 [==============================] - 14s 329ms/step - loss: 0.6147 - accuracy: 0.7020\n",
            "Epoch 82/200\n",
            "43/43 [==============================] - 14s 323ms/step - loss: 0.6148 - accuracy: 0.7020\n",
            "Epoch 83/200\n",
            "43/43 [==============================] - 14s 326ms/step - loss: 0.6145 - accuracy: 0.7022\n",
            "Epoch 84/200\n",
            "43/43 [==============================] - 14s 328ms/step - loss: 0.6144 - accuracy: 0.7022\n",
            "Epoch 85/200\n",
            "43/43 [==============================] - 14s 325ms/step - loss: 0.6142 - accuracy: 0.7022\n",
            "Epoch 86/200\n",
            "43/43 [==============================] - 14s 329ms/step - loss: 0.6141 - accuracy: 0.7022\n",
            "Epoch 87/200\n",
            "43/43 [==============================] - 14s 327ms/step - loss: 0.6138 - accuracy: 0.7022\n",
            "Epoch 88/200\n",
            "43/43 [==============================] - 14s 326ms/step - loss: 0.6137 - accuracy: 0.7023\n",
            "Epoch 89/200\n",
            "43/43 [==============================] - 14s 325ms/step - loss: 0.6138 - accuracy: 0.7023\n",
            "Epoch 90/200\n",
            "43/43 [==============================] - 14s 326ms/step - loss: 0.6135 - accuracy: 0.7023\n",
            "Epoch 91/200\n",
            "43/43 [==============================] - 14s 331ms/step - loss: 0.6139 - accuracy: 0.7023\n",
            "Epoch 92/200\n",
            "43/43 [==============================] - 14s 332ms/step - loss: 0.6132 - accuracy: 0.7023\n",
            "Epoch 93/200\n",
            "43/43 [==============================] - 14s 322ms/step - loss: 0.6134 - accuracy: 0.7023\n",
            "Epoch 94/200\n",
            "43/43 [==============================] - 14s 323ms/step - loss: 0.6132 - accuracy: 0.7024\n",
            "Epoch 95/200\n",
            "43/43 [==============================] - 14s 324ms/step - loss: 0.6130 - accuracy: 0.7024\n",
            "Epoch 96/200\n",
            "43/43 [==============================] - 14s 322ms/step - loss: 0.6129 - accuracy: 0.7024\n",
            "Epoch 97/200\n",
            "43/43 [==============================] - 14s 321ms/step - loss: 0.6129 - accuracy: 0.7024\n",
            "Epoch 98/200\n",
            "43/43 [==============================] - 14s 325ms/step - loss: 0.6129 - accuracy: 0.7024\n",
            "Epoch 99/200\n",
            "43/43 [==============================] - 14s 322ms/step - loss: 0.6127 - accuracy: 0.7024\n",
            "Epoch 100/200\n",
            "43/43 [==============================] - 14s 321ms/step - loss: 0.6128 - accuracy: 0.7024\n",
            "Epoch 101/200\n",
            "43/43 [==============================] - 14s 320ms/step - loss: 0.6122 - accuracy: 0.7024\n",
            "Epoch 102/200\n",
            "43/43 [==============================] - 14s 330ms/step - loss: 0.6128 - accuracy: 0.7024\n",
            "Epoch 103/200\n",
            "43/43 [==============================] - 14s 325ms/step - loss: 0.6122 - accuracy: 0.7024\n",
            "Epoch 104/200\n",
            "43/43 [==============================] - 14s 323ms/step - loss: 0.6122 - accuracy: 0.7024\n",
            "Epoch 105/200\n",
            "43/43 [==============================] - 14s 322ms/step - loss: 0.6123 - accuracy: 0.7024\n",
            "Epoch 106/200\n",
            "43/43 [==============================] - 14s 327ms/step - loss: 0.6120 - accuracy: 0.7024\n",
            "Epoch 107/200\n",
            "43/43 [==============================] - 14s 326ms/step - loss: 0.6120 - accuracy: 0.7024\n",
            "Epoch 108/200\n",
            "43/43 [==============================] - 14s 324ms/step - loss: 0.6115 - accuracy: 0.7024\n",
            "Epoch 109/200\n",
            "43/43 [==============================] - 14s 323ms/step - loss: 0.6114 - accuracy: 0.7024\n",
            "Epoch 110/200\n",
            "43/43 [==============================] - 15s 342ms/step - loss: 0.6109 - accuracy: 0.7024\n",
            "Epoch 111/200\n",
            "43/43 [==============================] - 14s 323ms/step - loss: 0.6087 - accuracy: 0.7024\n",
            "Epoch 112/200\n",
            "43/43 [==============================] - 14s 328ms/step - loss: 0.6128 - accuracy: 0.7024\n",
            "Epoch 113/200\n",
            "43/43 [==============================] - 14s 325ms/step - loss: 0.6120 - accuracy: 0.7024\n",
            "Epoch 114/200\n",
            "43/43 [==============================] - 14s 322ms/step - loss: 0.6121 - accuracy: 0.7024\n",
            "Epoch 115/200\n",
            "43/43 [==============================] - 14s 324ms/step - loss: 0.6111 - accuracy: 0.7024\n",
            "Epoch 116/200\n",
            "43/43 [==============================] - 14s 325ms/step - loss: 0.6070 - accuracy: 0.7024\n",
            "Epoch 117/200\n",
            "43/43 [==============================] - 14s 331ms/step - loss: 0.6116 - accuracy: 0.7024\n",
            "Epoch 118/200\n",
            "43/43 [==============================] - 14s 325ms/step - loss: 0.6090 - accuracy: 0.7024\n",
            "Epoch 119/200\n",
            "43/43 [==============================] - 14s 324ms/step - loss: 0.6080 - accuracy: 0.7024\n",
            "Epoch 120/200\n",
            "43/43 [==============================] - 14s 327ms/step - loss: 0.5940 - accuracy: 0.7024\n",
            "Epoch 121/200\n",
            "43/43 [==============================] - 14s 329ms/step - loss: 0.5900 - accuracy: 0.7023\n",
            "Epoch 122/200\n",
            "43/43 [==============================] - 14s 336ms/step - loss: 0.5888 - accuracy: 0.7022\n",
            "Epoch 123/200\n",
            "43/43 [==============================] - 14s 324ms/step - loss: 0.5759 - accuracy: 0.7019\n",
            "Epoch 124/200\n",
            "43/43 [==============================] - 14s 324ms/step - loss: 0.5685 - accuracy: 0.7021\n",
            "Epoch 125/200\n",
            "43/43 [==============================] - 14s 331ms/step - loss: 0.5634 - accuracy: 0.7021\n",
            "Epoch 126/200\n",
            "43/43 [==============================] - 14s 329ms/step - loss: 0.5511 - accuracy: 0.7020\n",
            "Epoch 127/200\n",
            "43/43 [==============================] - 14s 327ms/step - loss: 0.5538 - accuracy: 0.7021\n",
            "Epoch 128/200\n",
            "43/43 [==============================] - 14s 322ms/step - loss: 0.5483 - accuracy: 0.7020\n",
            "Epoch 129/200\n",
            "43/43 [==============================] - 14s 322ms/step - loss: 0.5399 - accuracy: 0.7025\n",
            "Epoch 130/200\n",
            "43/43 [==============================] - 14s 327ms/step - loss: 0.5554 - accuracy: 0.7026\n",
            "Epoch 131/200\n",
            "43/43 [==============================] - 14s 327ms/step - loss: 0.5800 - accuracy: 0.7021\n",
            "Epoch 132/200\n",
            "43/43 [==============================] - 14s 325ms/step - loss: 0.5787 - accuracy: 0.7021\n",
            "Epoch 133/200\n",
            "43/43 [==============================] - 14s 322ms/step - loss: 0.5681 - accuracy: 0.7019\n",
            "Epoch 134/200\n",
            "43/43 [==============================] - 14s 330ms/step - loss: 0.5746 - accuracy: 0.7015\n",
            "Epoch 135/200\n",
            "43/43 [==============================] - 14s 329ms/step - loss: 0.5646 - accuracy: 0.7015\n",
            "Epoch 136/200\n",
            "43/43 [==============================] - 14s 331ms/step - loss: 0.5688 - accuracy: 0.7023\n",
            "Epoch 137/200\n",
            "43/43 [==============================] - 14s 320ms/step - loss: 0.5816 - accuracy: 0.7040\n",
            "Epoch 138/200\n",
            "43/43 [==============================] - 14s 329ms/step - loss: 0.5766 - accuracy: 0.7023\n",
            "Epoch 139/200\n",
            "43/43 [==============================] - 14s 329ms/step - loss: 0.5574 - accuracy: 0.7111\n",
            "Epoch 140/200\n",
            "43/43 [==============================] - 14s 326ms/step - loss: 0.5735 - accuracy: 0.6949\n",
            "Epoch 141/200\n",
            "43/43 [==============================] - 14s 324ms/step - loss: 0.5930 - accuracy: 0.6888\n",
            "Epoch 142/200\n",
            "43/43 [==============================] - 14s 322ms/step - loss: 0.5662 - accuracy: 0.7028\n",
            "Epoch 143/200\n",
            "43/43 [==============================] - 14s 326ms/step - loss: 0.5597 - accuracy: 0.7085\n",
            "Epoch 144/200\n",
            "43/43 [==============================] - 14s 324ms/step - loss: 0.5746 - accuracy: 0.6956\n",
            "Epoch 145/200\n",
            "43/43 [==============================] - 14s 329ms/step - loss: 0.5604 - accuracy: 0.7049\n",
            "Epoch 146/200\n",
            "43/43 [==============================] - 14s 325ms/step - loss: 0.5560 - accuracy: 0.7098\n",
            "Epoch 147/200\n",
            "43/43 [==============================] - 14s 326ms/step - loss: 0.5587 - accuracy: 0.7084\n",
            "Epoch 148/200\n",
            "43/43 [==============================] - 14s 326ms/step - loss: 0.5551 - accuracy: 0.7192\n",
            "Epoch 149/200\n",
            "43/43 [==============================] - 14s 324ms/step - loss: 0.5496 - accuracy: 0.7233\n",
            "Epoch 150/200\n",
            "43/43 [==============================] - 14s 326ms/step - loss: 0.5601 - accuracy: 0.7196\n",
            "Epoch 151/200\n",
            "43/43 [==============================] - 14s 325ms/step - loss: 0.5633 - accuracy: 0.7230\n",
            "Epoch 152/200\n",
            "43/43 [==============================] - 14s 329ms/step - loss: 0.5520 - accuracy: 0.7219\n",
            "Epoch 153/200\n",
            "43/43 [==============================] - 14s 329ms/step - loss: 0.5330 - accuracy: 0.7307\n",
            "Epoch 154/200\n",
            "43/43 [==============================] - 14s 319ms/step - loss: 0.5119 - accuracy: 0.7392\n",
            "Epoch 155/200\n",
            "43/43 [==============================] - 14s 321ms/step - loss: 0.4956 - accuracy: 0.7429\n",
            "Epoch 156/200\n",
            "43/43 [==============================] - 14s 328ms/step - loss: 0.4968 - accuracy: 0.7385\n",
            "Epoch 157/200\n",
            "43/43 [==============================] - 14s 322ms/step - loss: 0.4731 - accuracy: 0.7571\n",
            "Epoch 158/200\n",
            "43/43 [==============================] - 14s 329ms/step - loss: 0.4413 - accuracy: 0.7865\n",
            "Epoch 159/200\n",
            "43/43 [==============================] - 14s 321ms/step - loss: 0.4301 - accuracy: 0.8011\n",
            "Epoch 160/200\n",
            "43/43 [==============================] - 14s 319ms/step - loss: 0.4848 - accuracy: 0.7807\n",
            "Epoch 161/200\n",
            "43/43 [==============================] - 14s 322ms/step - loss: 0.4533 - accuracy: 0.7862\n",
            "Epoch 162/200\n",
            "43/43 [==============================] - 14s 326ms/step - loss: 0.4865 - accuracy: 0.7438\n",
            "Epoch 163/200\n",
            "43/43 [==============================] - 14s 329ms/step - loss: 0.4286 - accuracy: 0.8109\n",
            "Epoch 164/200\n",
            "43/43 [==============================] - 14s 319ms/step - loss: 0.4265 - accuracy: 0.8164\n",
            "Epoch 165/200\n",
            "43/43 [==============================] - 14s 325ms/step - loss: 0.4206 - accuracy: 0.8162\n",
            "Epoch 166/200\n",
            "43/43 [==============================] - 14s 318ms/step - loss: 0.4197 - accuracy: 0.8199\n",
            "Epoch 167/200\n",
            "43/43 [==============================] - 14s 326ms/step - loss: 0.4125 - accuracy: 0.8254\n",
            "Epoch 168/200\n",
            "43/43 [==============================] - 14s 326ms/step - loss: 0.4221 - accuracy: 0.8164\n",
            "Epoch 169/200\n",
            "43/43 [==============================] - 14s 319ms/step - loss: 0.4134 - accuracy: 0.8237\n",
            "Epoch 170/200\n",
            "43/43 [==============================] - 14s 327ms/step - loss: 0.4043 - accuracy: 0.8255\n",
            "Epoch 171/200\n",
            "43/43 [==============================] - 14s 324ms/step - loss: 0.3919 - accuracy: 0.8342\n",
            "Epoch 172/200\n",
            "43/43 [==============================] - 14s 323ms/step - loss: 0.4432 - accuracy: 0.7885\n",
            "Epoch 173/200\n",
            "43/43 [==============================] - 14s 323ms/step - loss: 0.4120 - accuracy: 0.8131\n",
            "Epoch 174/200\n",
            "43/43 [==============================] - 14s 322ms/step - loss: 0.3886 - accuracy: 0.8310\n",
            "Epoch 175/200\n",
            "43/43 [==============================] - 14s 319ms/step - loss: 0.3845 - accuracy: 0.8334\n",
            "Epoch 176/200\n",
            "43/43 [==============================] - 14s 326ms/step - loss: 0.3712 - accuracy: 0.8398\n",
            "Epoch 177/200\n",
            "43/43 [==============================] - 14s 320ms/step - loss: 0.3741 - accuracy: 0.8381\n",
            "Epoch 178/200\n",
            "43/43 [==============================] - 14s 326ms/step - loss: 0.3712 - accuracy: 0.8386\n",
            "Epoch 179/200\n",
            "43/43 [==============================] - 14s 328ms/step - loss: 0.3742 - accuracy: 0.8368\n",
            "Epoch 180/200\n",
            "43/43 [==============================] - 14s 322ms/step - loss: 0.3698 - accuracy: 0.8404\n",
            "Epoch 181/200\n",
            "43/43 [==============================] - 14s 322ms/step - loss: 0.3598 - accuracy: 0.8454\n",
            "Epoch 182/200\n",
            "43/43 [==============================] - 14s 319ms/step - loss: 0.3964 - accuracy: 0.8188\n",
            "Epoch 183/200\n",
            "43/43 [==============================] - 14s 321ms/step - loss: 0.3635 - accuracy: 0.8426\n",
            "Epoch 184/200\n",
            "43/43 [==============================] - 14s 323ms/step - loss: 0.3611 - accuracy: 0.8456\n",
            "Epoch 185/200\n",
            "43/43 [==============================] - 14s 319ms/step - loss: 0.3563 - accuracy: 0.8487\n",
            "Epoch 186/200\n",
            "43/43 [==============================] - 14s 318ms/step - loss: 0.3562 - accuracy: 0.8495\n",
            "Epoch 187/200\n",
            "43/43 [==============================] - 14s 325ms/step - loss: 0.3556 - accuracy: 0.8477\n",
            "Epoch 188/200\n",
            "43/43 [==============================] - 14s 320ms/step - loss: 0.3500 - accuracy: 0.8485\n",
            "Epoch 189/200\n",
            "43/43 [==============================] - 14s 320ms/step - loss: 0.3461 - accuracy: 0.8529\n",
            "Epoch 190/200\n",
            "43/43 [==============================] - 14s 320ms/step - loss: 0.3708 - accuracy: 0.8450\n",
            "Epoch 191/200\n",
            "43/43 [==============================] - 14s 321ms/step - loss: 0.3724 - accuracy: 0.8465\n",
            "Epoch 192/200\n",
            "43/43 [==============================] - 14s 320ms/step - loss: 0.3600 - accuracy: 0.8511\n",
            "Epoch 193/200\n",
            "43/43 [==============================] - 14s 324ms/step - loss: 0.3568 - accuracy: 0.8476\n",
            "Epoch 194/200\n",
            "43/43 [==============================] - 14s 323ms/step - loss: 0.3798 - accuracy: 0.8352\n",
            "Epoch 195/200\n",
            "43/43 [==============================] - 14s 323ms/step - loss: 0.3376 - accuracy: 0.8580\n",
            "Epoch 196/200\n",
            "43/43 [==============================] - 14s 321ms/step - loss: 0.3621 - accuracy: 0.8443\n",
            "Epoch 197/200\n",
            "43/43 [==============================] - 14s 324ms/step - loss: 0.3472 - accuracy: 0.8517\n",
            "Epoch 198/200\n",
            "43/43 [==============================] - 14s 320ms/step - loss: 0.3371 - accuracy: 0.8591\n",
            "Epoch 199/200\n",
            "43/43 [==============================] - 14s 317ms/step - loss: 0.3373 - accuracy: 0.8583\n",
            "Epoch 200/200\n",
            "43/43 [==============================] - 14s 324ms/step - loss: 0.3819 - accuracy: 0.8325\n",
            "Train 200 epochs, 2858 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEmSlrt_gYZZ"
      },
      "source": [
        "def evaluate(model,min,max):\n",
        "    print(\"Evaluate on lengths %d to %d\"%(min,max))\n",
        "    print(\"slice...\")\n",
        "    subset=make_slice(train_set,min,max)\n",
        "    print(\"kmers...\")\n",
        "    (X_valid,y_valid)=make_kmers(max,subset)\n",
        "    print(\"evaluate....\")\n",
        "    scores = model.evaluate(X_valid, y_valid, verbose=1)  # valid = train, expect 100%\n",
        "    print(\"Evaluated on lengths %d to %d\"%(min,max))\n",
        "    print(\"%s: %.2f%%\\n\" % (model.metrics_names[1], scores[1]*100))\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiNABsVNgYZc",
        "outputId": "61f92806-a40b-41c6-f58d-c8373164893c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "evaluate(model,200,1000)\n",
        "evaluate(model,1000,2000)\n",
        "evaluate(model,2000,3000)\n",
        "evaluate(model,3000,5000)\n",
        "evaluate(model,5000,10000)\n",
        "evaluate(model,10000,30000)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate on lengths 200 to 1000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (30290, 4)\n",
            "no long, no short (8879, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(8879, 1)\n",
            "sequence    AGTCCCTCCCCAGCCCAGCAGTCCCTCCAGGCTACATCCAGGAGAC...\n",
            "Name: 1280, dtype: object\n",
            "348\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[ 46 182 214 ...   0   0   0]\n",
            " [ 36 142  56 ...   0   0   0]\n",
            " [135  28 110 ...   0   0   0]\n",
            " ...\n",
            " [147  73  36 ...   0   0   0]\n",
            " [228 143  57 ...   0   0   0]\n",
            " [131  12  47 ...   0   0   0]]\n",
            "evaluate....\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 3000) for input Tensor(\"embedding_input:0\", shape=(None, 3000), dtype=float32), but it was called on an input with incompatible shape (None, 1000).\n",
            "278/278 [==============================] - 17s 63ms/step - loss: 0.6940 - accuracy: 0.6170\n",
            "Evaluated on lengths 200 to 1000\n",
            "accuracy: 61.70%\n",
            "\n",
            "Evaluate on lengths 1000 to 2000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (9273, 4)\n",
            "no long, no short (3368, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(3368, 1)\n",
            "sequence    GGCGGGGTCGACTGACGGTAACGGGGCAGAGAGGCTGTTCGCAGAG...\n",
            "Name: 12641, dtype: object\n",
            "1338\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[167 155 107 ...   0   0   0]\n",
            " [226 133  20 ...   0   0   0]\n",
            " [108 175 186 ...   0   0   0]\n",
            " ...\n",
            " [175 185 225 ...   0   0   0]\n",
            " [ 37 148  78 ...   0   0   0]\n",
            " [188 240 192 ...   0   0   0]]\n",
            "evaluate....\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 3000) for input Tensor(\"embedding_input:0\", shape=(None, 3000), dtype=float32), but it was called on an input with incompatible shape (None, 2000).\n",
            "106/106 [==============================] - 12s 115ms/step - loss: 0.5310 - accuracy: 0.7499\n",
            "Evaluated on lengths 1000 to 2000\n",
            "accuracy: 74.99%\n",
            "\n",
            "Evaluate on lengths 2000 to 3000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (3221, 4)\n",
            "no long, no short (1351, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(1351, 1)\n",
            "sequence    GTCATTCTAGCTGCCTGCTGCCTCCGCAGCGTCCCCCCAGCTCTCC...\n",
            "Name: 19713, dtype: object\n",
            "2039\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[181 212  80 ...   0   0   0]\n",
            " [ 10  37 148 ...   0   0   0]\n",
            " [ 48 190 245 ...   0   0   0]\n",
            " ...\n",
            " [ 49 195  10 ...   0   0   0]\n",
            " [153  97 131 ...   0   0   0]\n",
            " [ 36 143  58 ...   0   0   0]]\n",
            "evaluate....\n",
            "43/43 [==============================] - 7s 168ms/step - loss: 0.3000 - accuracy: 0.8798\n",
            "Evaluated on lengths 2000 to 3000\n",
            "accuracy: 87.98%\n",
            "\n",
            "Evaluate on lengths 3000 to 5000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (1336, 4)\n",
            "no long, no short (895, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(895, 1)\n",
            "sequence    AGGCCGCGTCCGCCCGCGCCCGCTCTGGCCCCCGCGGAGCCGCGCA...\n",
            "Name: 19203, dtype: object\n",
            "3491\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[ 42 166 151 ...   0   0   0]\n",
            " [ 62 245 209 ...   0   0   0]\n",
            " [ 47 185 228 ...   0   0   0]\n",
            " ...\n",
            " [166 151  92 ...   0   0   0]\n",
            " [ 59 233 163 ...   0   0   0]\n",
            " [  3  11  41 ...   0   0   0]]\n",
            "evaluate....\n",
            "28/28 [==============================] - 8s 272ms/step - loss: 0.2972 - accuracy: 0.8808\n",
            "Evaluated on lengths 3000 to 5000\n",
            "accuracy: 88.08%\n",
            "\n",
            "Evaluate on lengths 5000 to 10000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (345, 4)\n",
            "no long, no short (314, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(314, 1)\n",
            "sequence    GGGAGCTCCGTTGTGCGTCGCTTAAGTGAGGGCGGCGGATGGGCGA...\n",
            "Name: 19766, dtype: object\n",
            "6207\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[169 163 138 ...   0   0   0]\n",
            " [167 155 106 ...   0   0   0]\n",
            " [ 19  76  47 ...   0   0   0]\n",
            " ...\n",
            " [ 91 107 169 ...   0   0   0]\n",
            " [ 62 247 218 ...   0   0   0]\n",
            " [229 145  67 ...   0   0   0]]\n",
            "evaluate....\n",
            "10/10 [==============================] - 5s 489ms/step - loss: 0.2397 - accuracy: 0.8928\n",
            "Evaluated on lengths 5000 to 10000\n",
            "accuracy: 89.28%\n",
            "\n",
            "Evaluate on lengths 10000 to 30000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (29, 4)\n",
            "no long, no short (29, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(29, 1)\n",
            "sequence    GCGCTAGCTCCCATGCTGGCCTCGGTGCCACTCGCGCGCCGGCCGC...\n",
            "Name: 7362, dtype: object\n",
            "11322\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[154 104 157 ...   0   0   0]\n",
            " [160 128 254 ...   0   0   0]\n",
            " [ 59 235 169 ...   0   0   0]\n",
            " ...\n",
            " [ 47 187 236 ...   0   0   0]\n",
            " [ 47 188 239 ...   0   0   0]\n",
            " [139  41 163 ...   0   0   0]]\n",
            "evaluate....\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3511 - accuracy: 0.8017\n",
            "Evaluated on lengths 10000 to 30000\n",
            "accuracy: 80.17%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbieSKdOjeaL"
      },
      "source": [
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd3Wj_vI9KdP"
      },
      "source": [
        "## Len 200-1Kb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ8eW5Rg9KdQ"
      },
      "source": [
        "MINLEN=200\n",
        "MAXLEN=1000\n",
        "\n",
        "if False:\n",
        "  print (\"Compile the model\")\n",
        "  model=build_model(MAXLEN)\n",
        "  print (\"Summarize the model\")\n",
        "  print(model.summary())  # Print this only once\n",
        "  print(\"Working on full training set, slice by sequence length.\")\n",
        "  print(\"Slice size range [%d - %d)\"%(MINLEN,MAXLEN))\n",
        "  subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
        "  print (\"Sequence to Kmer\")\n",
        "  (X_train,y_train)=make_kmers(MAXLEN,subset)\n",
        "  X_train\n",
        "  X_train=make_frequencies(X_train)\n",
        "  X_train\n",
        "  print (\"Cross valiation\")\n",
        "  model1 = do_cross_validation(X_train,y_train,MAXLEN)\n",
        "  model1.save(FILENAME+'.short.model')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC68X4zr9KdU"
      },
      "source": [
        "## Len 1Kb-2Kb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nm3oU3h9KdV"
      },
      "source": [
        "MINLEN=1000\n",
        "MAXLEN=2000\n",
        "\n",
        "if False:\n",
        "    print (\"Compile the model\")\n",
        "    model=build_model(MAXLEN)\n",
        "    print (\"Summarize the model\")\n",
        "    print(model.summary())  # Print this only once\n",
        "    print(\"Working on full training set, slice by sequence length.\")\n",
        "    print(\"Slice size range [%d - %d)\"%(MINLEN,MAXLEN))\n",
        "    subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
        "    print (\"Sequence to Kmer\")\n",
        "    (X_train,y_train)=make_kmers(MAXLEN,subset)\n",
        "    X_train\n",
        "    X_train=make_frequencies(X_train)\n",
        "    X_train\n",
        "    print (\"Cross valiation\")\n",
        "    model2 = do_cross_validation(X_train,y_train,MAXLEN)\n",
        "    model2.save(FILENAME+'.medium.model')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyACRnZx9Kde"
      },
      "source": [
        "## Len 2Kb-3Kb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUxmLnQ-9Kde"
      },
      "source": [
        "MINLEN=2000\n",
        "MAXLEN=3000\n",
        "\n",
        "if False:\n",
        "    print (\"Compile the model\")\n",
        "    model=build_model(MAXLEN)\n",
        "    print (\"Summarize the model\")\n",
        "    print(model.summary())  # Print this only once\n",
        "    print(\"Working on full training set, slice by sequence length.\")\n",
        "    print(\"Slice size range [%d - %d)\"%(MINLEN,MAXLEN))\n",
        "    subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
        "    print (\"Sequence to Kmer\")\n",
        "    (X_train,y_train)=make_kmers(MAXLEN,subset)\n",
        "    X_train\n",
        "    X_train=make_frequencies(X_train)\n",
        "    X_train\n",
        "    print (\"Cross valiation\")\n",
        "    model3 = do_cross_validation(X_train,y_train,MAXLEN)\n",
        "    model3.save(FILENAME+'.long.model')"
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}