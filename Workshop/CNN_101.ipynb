{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 101\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "import time\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "EPOCHS=200\n",
    "SPLITS=1\n",
    "K=1\n",
    "EMBED_DIMEN=16\n",
    "FILENAME='CNN101'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and partition sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume file was preprocessed to contain one line per seq.\n",
    "# Prefer Pandas dataframe but df does not support append.\n",
    "# For conversion to tensor, must avoid python lists.\n",
    "def load_fasta(filename,label):\n",
    "    DEFLINE='>'\n",
    "    labels=[]\n",
    "    seqs=[]\n",
    "    lens=[]\n",
    "    nums=[]\n",
    "    num=0\n",
    "    with open (filename,'r') as infile:\n",
    "        for line in infile:\n",
    "            if line[0]!=DEFLINE:\n",
    "                seq=line.rstrip()\n",
    "                num += 1   # first seqnum is 1\n",
    "                seqlen=len(seq)\n",
    "                nums.append(num)\n",
    "                labels.append(label)\n",
    "                seqs.append(seq)\n",
    "                lens.append(seqlen)\n",
    "    df1=pd.DataFrame(nums,columns=['seqnum'])\n",
    "    df2=pd.DataFrame(labels,columns=['class'])\n",
    "    df3=pd.DataFrame(seqs,columns=['sequence'])\n",
    "    df4=pd.DataFrame(lens,columns=['seqlen'])\n",
    "    df=pd.concat((df1,df2,df3,df4),axis=1)\n",
    "    return df\n",
    "\n",
    "# Split into train/test stratified by sequence length.\n",
    "def sizebin(df):\n",
    "    return pd.cut(df[\"seqlen\"],\n",
    "                              bins=[0,1000,2000,4000,8000,16000,np.inf],\n",
    "                              labels=[0,1,2,3,4,5])\n",
    "def make_train_test(data):\n",
    "    bin_labels= sizebin(data)\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=37863)\n",
    "    # split(x,y) expects that y is the labels. \n",
    "    # Trick: Instead of y, give it it the bin labels that we generated.\n",
    "    for train_index,test_index in splitter.split(data,bin_labels):\n",
    "        train_set = data.iloc[train_index]\n",
    "        test_set = data.iloc[test_index]\n",
    "    return (train_set,test_set)\n",
    "\n",
    "def separate_X_and_y(data):\n",
    "    y=   data[['class']].copy()\n",
    "    X=   data.drop(columns=['class','seqnum','seqlen'])\n",
    "    return (X,y)\n",
    "\n",
    "def make_slice(data_set,min_len,max_len):\n",
    "    print(\"original \"+str(data_set.shape))\n",
    "    too_short = data_set[ data_set['seqlen'] < min_len ].index\n",
    "    no_short=data_set.drop(too_short)\n",
    "    print(\"no short \"+str(no_short.shape))\n",
    "    too_long = no_short[ no_short['seqlen'] >= max_len ].index\n",
    "    no_long_no_short=no_short.drop(too_long)\n",
    "    print(\"no long, no short \"+str(no_long_no_short.shape))\n",
    "    return no_long_no_short\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kmer_table(K):\n",
    "    npad='N'*K\n",
    "    shorter_kmers=['']\n",
    "    for i in range(K):\n",
    "        longer_kmers=[]\n",
    "        for mer in shorter_kmers:\n",
    "            longer_kmers.append(mer+'A')\n",
    "            longer_kmers.append(mer+'C')\n",
    "            longer_kmers.append(mer+'G')\n",
    "            longer_kmers.append(mer+'T')\n",
    "        shorter_kmers = longer_kmers\n",
    "    all_kmers = shorter_kmers\n",
    "    kmer_dict = {}\n",
    "    kmer_dict[npad]=0\n",
    "    value=1\n",
    "    for mer in all_kmers:\n",
    "        kmer_dict[mer]=value\n",
    "        value += 1\n",
    "    return kmer_dict\n",
    "\n",
    "KMER_TABLE=make_kmer_table(K)\n",
    "\n",
    "def strings_to_vectors(data,uniform_len):\n",
    "    all_seqs=[]\n",
    "    for seq in data['sequence']:\n",
    "        i=0\n",
    "        seqlen=len(seq)\n",
    "        kmers=[]\n",
    "        while i < seqlen-K+1:\n",
    "            kmer=seq[i:i+K]\n",
    "            i += 1\n",
    "            value=KMER_TABLE[kmer]\n",
    "            kmers.append(value)\n",
    "        pad_val=0\n",
    "        while i < uniform_len:\n",
    "            kmers.append(pad_val)\n",
    "            i += 1\n",
    "        all_seqs.append(kmers)\n",
    "    pd2d=pd.DataFrame(all_seqs)\n",
    "    return pd2d   # return 2D dataframe, uniform dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(maxlen,dimen):\n",
    "    vocabulary_size=4**K+1   # e.g. K=3 => 64 DNA K-mers + 'NNN'\n",
    "    act=\"sigmoid\"\n",
    "    dt='float32'\n",
    "\n",
    "    neurons=32\n",
    "    rnn = keras.models.Sequential()\n",
    "    conv1_layer = keras.layers.Conv1D(filters=64,kernel_size=3,\n",
    "                activation=\"relu\",padding=\"valid\",input_shape=[maxlen,vocabulary_size])\n",
    "    pool1_layer = keras.layers.MaxPooling1D(2)\n",
    "    conv2_layer = keras.layers.Conv1D(neurons,3,activation=\"relu\",padding=\"same\")\n",
    "    conv3_layer = keras.layers.Conv1D(neurons,3,activation=\"relu\",padding=\"same\")\n",
    "    pool2_layer = keras.layers.MaxPooling1D(2)\n",
    "    conv4_layer = keras.layers.Conv1D(neurons,3,activation=\"relu\",padding=\"same\")\n",
    "    conv5_layer = keras.layers.Conv1D(neurons,3,activation=\"relu\",padding=\"same\")\n",
    "    flat1_layer = keras.layers.Flatten()\n",
    "    dense1_layer = keras.layers.Dense(neurons,activation=act,dtype=dt)\n",
    "    drop1_layer = keras.layers.Dropout(0.5)\n",
    "    dense2_layer = keras.layers.Dense(neurons,activation=act,dtype=dt)\n",
    "    drop2_layer = keras.layers.Dropout(0.5)\n",
    "    output_layer = keras.layers.Dense(1,activation=act,dtype=dt)\n",
    "\n",
    "    rnn.add(conv1_layer)\n",
    "    #rnn.add(pool1_layer)\n",
    "    rnn.add(conv2_layer)\n",
    "    #rnn.add(conv3_layer)\n",
    "    rnn.add(pool2_layer)\n",
    "    rnn.add(conv4_layer)\n",
    "    rnn.add(conv5_layer)\n",
    "    rnn.add(flat1_layer)\n",
    "    rnn.add(dense1_layer)\n",
    "    rnn.add(drop1_layer)\n",
    "    rnn.add(dense2_layer)\n",
    "    rnn.add(drop2_layer)\n",
    "    rnn.add(output_layer)\n",
    "\n",
    "    bc=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    opt=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    print(\"COMPILE\")\n",
    "    rnn.compile(loss=bc, optimizer=opt,metrics=[\"accuracy\"])\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cross_validation(X,y,eps,maxlen,dimen):\n",
    "    cv_scores = []\n",
    "    fold=0\n",
    "    splitter = ShuffleSplit(n_splits=SPLITS, test_size=0.2, random_state=37863)\n",
    "    rnn2=None\n",
    "    for train_index,valid_index in splitter.split(X):\n",
    "        X_train=X[train_index] # use iloc[] for dataframe\n",
    "        y_train=y[train_index]\n",
    "        X_valid=X[valid_index]\n",
    "        y_valid=y[valid_index]\n",
    "\n",
    "        print(\"BUILD MODEL\")\n",
    "        rnn2=build_model(maxlen,dimen)\n",
    "\n",
    "        print(\"FIT\")\n",
    "        start_time=time.time()\n",
    "        history=rnn2.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
    "                epochs=eps, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
    "                validation_data=(X_valid,y_valid) )\n",
    "        end_time=time.time()\n",
    "        elapsed_time=(end_time-start_time)\n",
    "                        \n",
    "        fold += 1\n",
    "        print(\"Fold %d, %d epochs, %d sec\"%(fold,eps,elapsed_time))\n",
    "\n",
    "        pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "        plt.grid(True)\n",
    "        plt.gca().set_ylim(0,1)\n",
    "        plt.show()\n",
    "\n",
    "        scores = rnn2.evaluate(X_valid, y_valid, verbose=0)\n",
    "        print(\"%s: %.2f%%\" % (rnn2.metrics_names[1], scores[1]*100))\n",
    "        # What are the other metrics_names?\n",
    "        # Try this from Geron page 505:\n",
    "        # np.mean(keras.losses.mean_squared_error(y_valid,y_pred))\n",
    "        cv_scores.append(scores[1] * 100)\n",
    "    print()\n",
    "    print(\"Validation core mean %.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n",
    "    return rnn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kmers(MINLEN,MAXLEN,train_set):\n",
    "    (X_train_all,y_train_all)=separate_X_and_y(train_set)\n",
    "\n",
    "    # The returned values are Pandas dataframes.\n",
    "    # print(X_train_all.shape,y_train_all.shape)\n",
    "    # (X_train_all,y_train_all)\n",
    "    # y: Pandas dataframe to Python list.\n",
    "    # y_train_all=y_train_all.values.tolist()\n",
    "    # The sequences lengths are bounded but not uniform.\n",
    "    X_train_all\n",
    "    #print(type(X_train_all))\n",
    "    #print(X_train_all.shape)\n",
    "    #print(X_train_all.iloc[0])\n",
    "    #print(len(X_train_all.iloc[0]['sequence']))\n",
    "\n",
    "    # X: List of string to List of uniform-length ordered lists of K-mers.\n",
    "    X_train_kmers=strings_to_vectors(X_train_all,MAXLEN)\n",
    "    # X: true 2D array (no more lists)\n",
    "    X_train_kmers.shape\n",
    "\n",
    "    print(\"transform...\")\n",
    "    # From pandas dataframe to numpy to list to numpy\n",
    "    #print(type(X_train_kmers))\n",
    "    num_seqs=len(X_train_kmers)\n",
    "    tmp_seqs=[]\n",
    "    for i in range(num_seqs):\n",
    "        kmer_sequence=X_train_kmers.iloc[i]\n",
    "        tmp_seqs.append(kmer_sequence)\n",
    "    X_train_kmers=np.array(tmp_seqs)\n",
    "    tmp_seqs=None\n",
    "    #print(type(X_train_kmers))\n",
    "    #print(X_train_kmers)\n",
    "\n",
    "    labels=y_train_all.to_numpy()\n",
    "    return (X_train_kmers,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from files.\n",
      "Put aside the test portion.\n",
      "Ready: train_set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seqnum</th>\n",
       "      <th>class</th>\n",
       "      <th>sequence</th>\n",
       "      <th>seqlen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>1281</td>\n",
       "      <td>0</td>\n",
       "      <td>AGTCCCTCCCCAGCCCAGCAGTCCCTCCAGGCTACATCCAGGAGAC...</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>9089</td>\n",
       "      <td>0</td>\n",
       "      <td>CAGCTCCTGGGATGGCCTCACCTGAGGAGACTCTTGGGCCTTGGCA...</td>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6069</th>\n",
       "      <td>6070</td>\n",
       "      <td>1</td>\n",
       "      <td>AGATCTAGGGATGGGGATGGGGAGGAGAAGTGGGAATGGGAAATTG...</td>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18549</th>\n",
       "      <td>18550</td>\n",
       "      <td>1</td>\n",
       "      <td>GACGTCTCCCGCGGGCGTCGGCAGGGTCGGCGGCGTCGGCAGCAGT...</td>\n",
       "      <td>945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15027</th>\n",
       "      <td>15028</td>\n",
       "      <td>1</td>\n",
       "      <td>GAGCGCGCGAGCCGGGCCCGGAGCGCACGCCGCCGCCGCCACCGCC...</td>\n",
       "      <td>4382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3386</th>\n",
       "      <td>3387</td>\n",
       "      <td>0</td>\n",
       "      <td>TTTATGTGGATTGTCTGTCTCATGCTTGTTTCACCAGGGTAGTTAC...</td>\n",
       "      <td>578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>6496</td>\n",
       "      <td>0</td>\n",
       "      <td>ATAATGGGAAACTAAGGGCAAGTTCTCATGTTCCTGGTCCTGGCTT...</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6409</th>\n",
       "      <td>6410</td>\n",
       "      <td>1</td>\n",
       "      <td>GGGTTTATTACTACTGAAGGAAGAACGTGAGTAGGTTAGGATTTCG...</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7640</th>\n",
       "      <td>7641</td>\n",
       "      <td>1</td>\n",
       "      <td>ACAGCTGTGTTTGGCTGCAGGGCCAAGAGCGCTGTCAAGAAGACCC...</td>\n",
       "      <td>3156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14108</th>\n",
       "      <td>14109</td>\n",
       "      <td>0</td>\n",
       "      <td>GAAGTGATTGCAAGTTCAGCAGCATGAAACTGCTCTTTATCCGTGC...</td>\n",
       "      <td>466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30290 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       seqnum  class                                           sequence  \\\n",
       "1280     1281      0  AGTCCCTCCCCAGCCCAGCAGTCCCTCCAGGCTACATCCAGGAGAC...   \n",
       "9088     9089      0  CAGCTCCTGGGATGGCCTCACCTGAGGAGACTCTTGGGCCTTGGCA...   \n",
       "6069     6070      1  AGATCTAGGGATGGGGATGGGGAGGAGAAGTGGGAATGGGAAATTG...   \n",
       "18549   18550      1  GACGTCTCCCGCGGGCGTCGGCAGGGTCGGCGGCGTCGGCAGCAGT...   \n",
       "15027   15028      1  GAGCGCGCGAGCCGGGCCCGGAGCGCACGCCGCCGCCGCCACCGCC...   \n",
       "...       ...    ...                                                ...   \n",
       "3386     3387      0  TTTATGTGGATTGTCTGTCTCATGCTTGTTTCACCAGGGTAGTTAC...   \n",
       "6495     6496      0  ATAATGGGAAACTAAGGGCAAGTTCTCATGTTCCTGGTCCTGGCTT...   \n",
       "6409     6410      1  GGGTTTATTACTACTGAAGGAAGAACGTGAGTAGGTTAGGATTTCG...   \n",
       "7640     7641      1  ACAGCTGTGTTTGGCTGCAGGGCCAAGAGCGCTGTCAAGAAGACCC...   \n",
       "14108   14109      0  GAAGTGATTGCAAGTTCAGCAGCATGAAACTGCTCTTTATCCGTGC...   \n",
       "\n",
       "       seqlen  \n",
       "1280      348  \n",
       "9088      534  \n",
       "6069      592  \n",
       "18549     945  \n",
       "15027    4382  \n",
       "...       ...  \n",
       "3386      578  \n",
       "6495      562  \n",
       "6409      740  \n",
       "7640     3156  \n",
       "14108     466  \n",
       "\n",
       "[30290 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Load data from files.\")\n",
    "nc_seq=load_fasta('ncRNA.fasta',0)\n",
    "pc_seq=load_fasta('pcRNA.fasta',1)\n",
    "all_seq=pd.concat((nc_seq,pc_seq),axis=0)\n",
    "\n",
    "print(\"Put aside the test portion.\")\n",
    "(train_set,test_set)=make_train_test(all_seq)\n",
    "# Do this later when using the test data:\n",
    "# (X_test,y_test)=separate_X_and_y(test_set)\n",
    "\n",
    "nc_seq=None\n",
    "pc_seq=None\n",
    "all_seq=None\n",
    "\n",
    "print(\"Ready: train_set\")\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(seqs):\n",
    "    newX = []\n",
    "    vectors=[]\n",
    "    vectors.append([0,0,0,0,1])\n",
    "    vectors.append([1,0,0,0,0])\n",
    "    vectors.append([0,1,0,0,0])\n",
    "    vectors.append([0,0,1,0,0])\n",
    "    vectors.append([0,0,0,1,0])\n",
    "    for seq in X_train:\n",
    "        letters=[]\n",
    "        for num in seq:\n",
    "            hot = vectors[num]\n",
    "            letters.append(hot)\n",
    "        newX.append(letters)\n",
    "    return np.asarray(newX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Len 200-1Kb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on full training set, slice by sequence length.\n",
      "Slice size range [200 - 1000)\n",
      "original (30290, 4)\n",
      "no short (30290, 4)\n",
      "no long, no short (8879, 4)\n",
      "Sequence to Kmer\n",
      "transform...\n",
      "Length of list:\n",
      "8879\n",
      "Length of first element:\n",
      "1000\n",
      "First element:\n",
      "[[1 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " ...\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 1]]\n",
      "Compile the model\n",
      "COMPILE\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 998, 64)           1024      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 998, 32)           6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 499, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 499, 32)           3104      \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 499, 32)           3104      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 15968)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                511008    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 525,505\n",
      "Trainable params: 525,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Cross valiation\n",
      "BUILD MODEL\n",
      "COMPILE\n",
      "FIT\n",
      "Epoch 1/200\n",
      "222/222 [==============================] - 22s 97ms/step - loss: 0.7019 - accuracy: 0.5015 - val_loss: 0.6944 - val_accuracy: 0.5011\n",
      "Epoch 2/200\n",
      "222/222 [==============================] - 21s 95ms/step - loss: 0.6942 - accuracy: 0.4987 - val_loss: 0.6936 - val_accuracy: 0.5011\n",
      "Epoch 3/200\n",
      "222/222 [==============================] - 22s 98ms/step - loss: 0.6944 - accuracy: 0.4964 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 4/200\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.6937 - accuracy: 0.5091 - val_loss: 0.6936 - val_accuracy: 0.5011\n",
      "Epoch 5/200\n",
      "222/222 [==============================] - 21s 97ms/step - loss: 0.6938 - accuracy: 0.5089 - val_loss: 0.6940 - val_accuracy: 0.5011\n",
      "Epoch 6/200\n",
      "222/222 [==============================] - 22s 97ms/step - loss: 0.6935 - accuracy: 0.5046 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 7/200\n",
      "222/222 [==============================] - 21s 95ms/step - loss: 0.6934 - accuracy: 0.5019 - val_loss: 0.6938 - val_accuracy: 0.5011\n",
      "Epoch 8/200\n",
      "222/222 [==============================] - 22s 97ms/step - loss: 0.6938 - accuracy: 0.5022 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 9/200\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.6930 - accuracy: 0.5099 - val_loss: 0.6940 - val_accuracy: 0.4989\n",
      "Epoch 10/200\n",
      "222/222 [==============================] - 22s 97ms/step - loss: 0.6930 - accuracy: 0.5120 - val_loss: 0.6948 - val_accuracy: 0.5011\n",
      "Epoch 11/200\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.6936 - accuracy: 0.4999 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 12/200\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.6935 - accuracy: 0.5012 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 13/200\n",
      "222/222 [==============================] - 21s 95ms/step - loss: 0.6935 - accuracy: 0.5032 - val_loss: 0.6932 - val_accuracy: 0.4989\n",
      "Epoch 14/200\n",
      "222/222 [==============================] - 21s 94ms/step - loss: 0.6933 - accuracy: 0.5012 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 15/200\n",
      "222/222 [==============================] - 22s 97ms/step - loss: 0.6935 - accuracy: 0.4985 - val_loss: 0.6934 - val_accuracy: 0.5011\n",
      "Epoch 16/200\n",
      "222/222 [==============================] - 21s 97ms/step - loss: 0.6934 - accuracy: 0.5051 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 17/200\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.6933 - accuracy: 0.5091 - val_loss: 0.6932 - val_accuracy: 0.4989\n",
      "Epoch 18/200\n",
      "222/222 [==============================] - 22s 98ms/step - loss: 0.6933 - accuracy: 0.5013 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 19/200\n",
      "222/222 [==============================] - 22s 98ms/step - loss: 0.6934 - accuracy: 0.5071 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 20/200\n",
      "222/222 [==============================] - 22s 99ms/step - loss: 0.6936 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 21/200\n",
      "222/222 [==============================] - 23s 102ms/step - loss: 0.6934 - accuracy: 0.5015 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 22/200\n",
      "222/222 [==============================] - 22s 100ms/step - loss: 0.6934 - accuracy: 0.5056 - val_loss: 0.6931 - val_accuracy: 0.5011\n",
      "Epoch 23/200\n",
      "222/222 [==============================] - 21s 95ms/step - loss: 0.6934 - accuracy: 0.5005 - val_loss: 0.6934 - val_accuracy: 0.5011\n",
      "Epoch 24/200\n",
      "222/222 [==============================] - 21s 94ms/step - loss: 0.6933 - accuracy: 0.5011 - val_loss: 0.6932 - val_accuracy: 0.4989\n",
      "Epoch 25/200\n",
      "222/222 [==============================] - 21s 97ms/step - loss: 0.6936 - accuracy: 0.5095 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 26/200\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.6932 - accuracy: 0.5016 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 27/200\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.6935 - accuracy: 0.4994 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 28/200\n",
      "222/222 [==============================] - 22s 97ms/step - loss: 0.6935 - accuracy: 0.4980 - val_loss: 0.6938 - val_accuracy: 0.5011\n",
      "Epoch 29/200\n",
      "222/222 [==============================] - 21s 95ms/step - loss: 0.6936 - accuracy: 0.4984 - val_loss: 0.6932 - val_accuracy: 0.4989\n",
      "Epoch 30/200\n",
      "222/222 [==============================] - 21s 97ms/step - loss: 0.6934 - accuracy: 0.5033 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 31/200\n",
      "222/222 [==============================] - 22s 98ms/step - loss: 0.6938 - accuracy: 0.5008 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 32/200\n",
      "222/222 [==============================] - 22s 98ms/step - loss: 0.6933 - accuracy: 0.5001 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 33/200\n",
      "222/222 [==============================] - 22s 97ms/step - loss: 0.6933 - accuracy: 0.5020 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 34/200\n",
      "222/222 [==============================] - 21s 97ms/step - loss: 0.6936 - accuracy: 0.4968 - val_loss: 0.6938 - val_accuracy: 0.5011\n",
      "Epoch 35/200\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.6934 - accuracy: 0.5023 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 36/200\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.6935 - accuracy: 0.5050 - val_loss: 0.6935 - val_accuracy: 0.5011\n",
      "Epoch 37/200\n",
      "222/222 [==============================] - 21s 94ms/step - loss: 0.6933 - accuracy: 0.5047 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 38/200\n",
      "222/222 [==============================] - 22s 98ms/step - loss: 0.6931 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.5011\n",
      "Epoch 39/200\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.6935 - accuracy: 0.5018 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 40/200\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 0.6933 - accuracy: 0.5042 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 41/200\n",
      "222/222 [==============================] - 22s 97ms/step - loss: 0.6935 - accuracy: 0.5005 - val_loss: 0.6934 - val_accuracy: 0.5011\n",
      "Epoch 42/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222/222 [==============================] - 21s 95ms/step - loss: 0.6936 - accuracy: 0.5046 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 43/200\n",
      "222/222 [==============================] - 21s 95ms/step - loss: 0.6933 - accuracy: 0.5064 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 44/200\n",
      "222/222 [==============================] - 21s 94ms/step - loss: 0.6934 - accuracy: 0.5011 - val_loss: 0.6931 - val_accuracy: 0.5011\n",
      "Epoch 45/200\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 0.6935 - accuracy: 0.4996 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 46/200\n",
      "222/222 [==============================] - 21s 94ms/step - loss: 0.6934 - accuracy: 0.4995 - val_loss: 0.6935 - val_accuracy: 0.5011\n",
      "Epoch 47/200\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.6934 - accuracy: 0.5036 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 48/200\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.6933 - accuracy: 0.4977 - val_loss: 0.6942 - val_accuracy: 0.5011\n",
      "Epoch 49/200\n",
      "222/222 [==============================] - 21s 94ms/step - loss: 0.6934 - accuracy: 0.5036 - val_loss: 0.6940 - val_accuracy: 0.5011\n",
      "Epoch 50/200\n",
      "222/222 [==============================] - 21s 94ms/step - loss: 0.6933 - accuracy: 0.5065 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 51/200\n",
      "222/222 [==============================] - 23s 104ms/step - loss: 0.6934 - accuracy: 0.5084 - val_loss: 0.6932 - val_accuracy: 0.4989\n",
      "Epoch 52/200\n",
      "222/222 [==============================] - 23s 104ms/step - loss: 0.6932 - accuracy: 0.5053 - val_loss: 0.6941 - val_accuracy: 0.5011\n",
      "Epoch 53/200\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.6935 - accuracy: 0.5057 - val_loss: 0.6935 - val_accuracy: 0.5011\n",
      "Epoch 54/200\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.6933 - accuracy: 0.5040 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 55/200\n",
      "222/222 [==============================] - 22s 97ms/step - loss: 0.6932 - accuracy: 0.5016 - val_loss: 0.6932 - val_accuracy: 0.4989\n",
      "Epoch 56/200\n",
      "222/222 [==============================] - 21s 94ms/step - loss: 0.6933 - accuracy: 0.5061 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 57/200\n",
      "222/222 [==============================] - 21s 95ms/step - loss: 0.6935 - accuracy: 0.5033 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 58/200\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.6933 - accuracy: 0.5032 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 59/200\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.6935 - accuracy: 0.4987 - val_loss: 0.6937 - val_accuracy: 0.5011\n",
      "Epoch 60/200\n",
      "222/222 [==============================] - 21s 97ms/step - loss: 0.6934 - accuracy: 0.4963 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 61/200\n",
      "222/222 [==============================] - 22s 97ms/step - loss: 0.6932 - accuracy: 0.5075 - val_loss: 0.6932 - val_accuracy: 0.4989\n",
      "Epoch 62/200\n",
      "222/222 [==============================] - 21s 97ms/step - loss: 0.6934 - accuracy: 0.5011 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 63/200\n",
      "222/222 [==============================] - 22s 97ms/step - loss: 0.6935 - accuracy: 0.4966 - val_loss: 0.6935 - val_accuracy: 0.5011\n",
      "Epoch 64/200\n",
      "222/222 [==============================] - 22s 97ms/step - loss: 0.6931 - accuracy: 0.5098 - val_loss: 0.6934 - val_accuracy: 0.5011\n",
      "Epoch 65/200\n",
      "222/222 [==============================] - 22s 99ms/step - loss: 0.6935 - accuracy: 0.5034 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 66/200\n",
      "222/222 [==============================] - 22s 100ms/step - loss: 0.6933 - accuracy: 0.4999 - val_loss: 0.6934 - val_accuracy: 0.5011\n",
      "Epoch 67/200\n",
      "222/222 [==============================] - 22s 98ms/step - loss: 0.6935 - accuracy: 0.5044 - val_loss: 0.6945 - val_accuracy: 0.5011\n",
      "Epoch 68/200\n",
      "222/222 [==============================] - 28s 125ms/step - loss: 0.6934 - accuracy: 0.5022 - val_loss: 0.6934 - val_accuracy: 0.5011\n",
      "Epoch 69/200\n",
      "222/222 [==============================] - 23s 103ms/step - loss: 0.6932 - accuracy: 0.5042 - val_loss: 0.6932 - val_accuracy: 0.4989\n",
      "Epoch 70/200\n",
      "222/222 [==============================] - 23s 105ms/step - loss: 0.6937 - accuracy: 0.4996 - val_loss: 0.6931 - val_accuracy: 0.4989\n",
      "Epoch 71/200\n",
      "222/222 [==============================] - 28s 125ms/step - loss: 0.6933 - accuracy: 0.5013 - val_loss: 0.6938 - val_accuracy: 0.5011\n",
      "Epoch 72/200\n",
      "222/222 [==============================] - 26s 116ms/step - loss: 0.6933 - accuracy: 0.4987 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 73/200\n",
      "222/222 [==============================] - 26s 116ms/step - loss: 0.6933 - accuracy: 0.5005 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 74/200\n",
      "222/222 [==============================] - 26s 116ms/step - loss: 0.6937 - accuracy: 0.5013 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 75/200\n",
      "222/222 [==============================] - 28s 125ms/step - loss: 0.6935 - accuracy: 0.5070 - val_loss: 0.6938 - val_accuracy: 0.5011\n",
      "Epoch 76/200\n",
      "222/222 [==============================] - 24s 110ms/step - loss: 0.6937 - accuracy: 0.5018 - val_loss: 0.6936 - val_accuracy: 0.5011\n",
      "Epoch 77/200\n",
      "222/222 [==============================] - 24s 109ms/step - loss: 0.6932 - accuracy: 0.5075 - val_loss: 0.6937 - val_accuracy: 0.5011\n",
      "Epoch 78/200\n",
      "222/222 [==============================] - 24s 106ms/step - loss: 0.6934 - accuracy: 0.5053 - val_loss: 0.6932 - val_accuracy: 0.4989\n",
      "Epoch 79/200\n",
      "222/222 [==============================] - 23s 102ms/step - loss: 0.6935 - accuracy: 0.4981 - val_loss: 0.6932 - val_accuracy: 0.4989\n",
      "Epoch 80/200\n",
      "222/222 [==============================] - 24s 106ms/step - loss: 0.6936 - accuracy: 0.4929 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 81/200\n",
      "222/222 [==============================] - 23s 104ms/step - loss: 0.6938 - accuracy: 0.4957 - val_loss: 0.6935 - val_accuracy: 0.5011\n",
      "Epoch 82/200\n",
      "222/222 [==============================] - 22s 101ms/step - loss: 0.6933 - accuracy: 0.5029 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 83/200\n",
      "222/222 [==============================] - 23s 102ms/step - loss: 0.6934 - accuracy: 0.5065 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 84/200\n",
      "222/222 [==============================] - 23s 104ms/step - loss: 0.6937 - accuracy: 0.4977 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 85/200\n",
      "222/222 [==============================] - 23s 104ms/step - loss: 0.6932 - accuracy: 0.5023 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 86/200\n",
      "222/222 [==============================] - 24s 110ms/step - loss: 0.6934 - accuracy: 0.4981 - val_loss: 0.6934 - val_accuracy: 0.5011\n",
      "Epoch 87/200\n",
      "222/222 [==============================] - 23s 105ms/step - loss: 0.6934 - accuracy: 0.5011 - val_loss: 0.6937 - val_accuracy: 0.5011\n",
      "Epoch 88/200\n",
      "222/222 [==============================] - 23s 102ms/step - loss: 0.6934 - accuracy: 0.5018 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 89/200\n",
      "222/222 [==============================] - 23s 104ms/step - loss: 0.6932 - accuracy: 0.4967 - val_loss: 0.6931 - val_accuracy: 0.5011\n",
      "Epoch 90/200\n",
      "222/222 [==============================] - 23s 105ms/step - loss: 0.6932 - accuracy: 0.5049 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 91/200\n",
      "222/222 [==============================] - 24s 108ms/step - loss: 0.6942 - accuracy: 0.5044 - val_loss: 0.6939 - val_accuracy: 0.5011\n",
      "Epoch 92/200\n",
      "222/222 [==============================] - 24s 107ms/step - loss: 0.6937 - accuracy: 0.5016 - val_loss: 0.6936 - val_accuracy: 0.5011\n",
      "Epoch 93/200\n",
      "222/222 [==============================] - 24s 109ms/step - loss: 0.6935 - accuracy: 0.5008 - val_loss: 0.6935 - val_accuracy: 0.5011\n",
      "Epoch 94/200\n",
      "222/222 [==============================] - 24s 110ms/step - loss: 0.6935 - accuracy: 0.5002 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 95/200\n",
      "222/222 [==============================] - 24s 110ms/step - loss: 0.6934 - accuracy: 0.5068 - val_loss: 0.6931 - val_accuracy: 0.5011\n",
      "Epoch 96/200\n",
      "222/222 [==============================] - 24s 106ms/step - loss: 0.6936 - accuracy: 0.4994 - val_loss: 0.6931 - val_accuracy: 0.4989\n",
      "Epoch 97/200\n",
      "222/222 [==============================] - 24s 110ms/step - loss: 0.6936 - accuracy: 0.4974 - val_loss: 0.6932 - val_accuracy: 0.4989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/200\n",
      "222/222 [==============================] - 23s 104ms/step - loss: 0.6933 - accuracy: 0.4999 - val_loss: 0.6936 - val_accuracy: 0.5011\n",
      "Epoch 99/200\n",
      "222/222 [==============================] - 23s 105ms/step - loss: 0.6936 - accuracy: 0.4922 - val_loss: 0.6934 - val_accuracy: 0.5011\n",
      "Epoch 100/200\n",
      "222/222 [==============================] - 26s 115ms/step - loss: 0.6934 - accuracy: 0.5013 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 101/200\n",
      "222/222 [==============================] - 26s 117ms/step - loss: 0.6934 - accuracy: 0.5053 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 102/200\n",
      "222/222 [==============================] - 23s 102ms/step - loss: 0.6934 - accuracy: 0.5036 - val_loss: 0.6931 - val_accuracy: 0.5011\n",
      "Epoch 103/200\n",
      "222/222 [==============================] - 23s 106ms/step - loss: 0.6934 - accuracy: 0.5005 - val_loss: 0.6932 - val_accuracy: 0.4989\n",
      "Epoch 104/200\n",
      "222/222 [==============================] - 23s 105ms/step - loss: 0.6936 - accuracy: 0.5053 - val_loss: 0.6931 - val_accuracy: 0.5011\n",
      "Epoch 105/200\n",
      "222/222 [==============================] - 24s 108ms/step - loss: 0.6933 - accuracy: 0.5036 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 106/200\n",
      "222/222 [==============================] - 24s 108ms/step - loss: 0.6932 - accuracy: 0.4975 - val_loss: 0.6934 - val_accuracy: 0.4989\n",
      "Epoch 107/200\n",
      "222/222 [==============================] - 25s 111ms/step - loss: 0.6938 - accuracy: 0.4971 - val_loss: 0.6931 - val_accuracy: 0.5011\n",
      "Epoch 108/200\n",
      "222/222 [==============================] - 23s 105ms/step - loss: 0.6934 - accuracy: 0.4987 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 109/200\n",
      "222/222 [==============================] - 24s 109ms/step - loss: 0.6933 - accuracy: 0.5042 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 110/200\n",
      "222/222 [==============================] - 24s 106ms/step - loss: 0.6936 - accuracy: 0.4946 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 111/200\n",
      "222/222 [==============================] - 23s 105ms/step - loss: 0.6933 - accuracy: 0.5009 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 112/200\n",
      "222/222 [==============================] - 26s 119ms/step - loss: 0.6929 - accuracy: 0.5127 - val_loss: 0.6935 - val_accuracy: 0.4989\n",
      "Epoch 113/200\n",
      "222/222 [==============================] - 23s 102ms/step - loss: 0.6940 - accuracy: 0.4891 - val_loss: 0.6931 - val_accuracy: 0.5011\n",
      "Epoch 114/200\n",
      "222/222 [==============================] - 23s 103ms/step - loss: 0.6933 - accuracy: 0.5043 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 115/200\n",
      "222/222 [==============================] - 23s 104ms/step - loss: 0.6932 - accuracy: 0.5053 - val_loss: 0.6941 - val_accuracy: 0.5011\n",
      "Epoch 116/200\n",
      "222/222 [==============================] - 26s 115ms/step - loss: 0.6934 - accuracy: 0.5019 - val_loss: 0.6943 - val_accuracy: 0.5011\n",
      "Epoch 117/200\n",
      "222/222 [==============================] - 25s 114ms/step - loss: 0.6933 - accuracy: 0.5056 - val_loss: 0.6940 - val_accuracy: 0.5011\n",
      "Epoch 118/200\n",
      "222/222 [==============================] - 27s 123ms/step - loss: 0.6934 - accuracy: 0.5036 - val_loss: 0.6932 - val_accuracy: 0.4989\n",
      "Epoch 119/200\n",
      "222/222 [==============================] - 27s 124ms/step - loss: 0.6936 - accuracy: 0.4912 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 120/200\n",
      "222/222 [==============================] - 22s 101ms/step - loss: 0.6937 - accuracy: 0.4957 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 121/200\n",
      "222/222 [==============================] - 22s 101ms/step - loss: 0.6933 - accuracy: 0.5056 - val_loss: 0.6935 - val_accuracy: 0.5011\n",
      "Epoch 122/200\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.6932 - accuracy: 0.5106 - val_loss: 0.6936 - val_accuracy: 0.5011\n",
      "Epoch 123/200\n",
      "222/222 [==============================] - 22s 98ms/step - loss: 0.6933 - accuracy: 0.5078 - val_loss: 0.6931 - val_accuracy: 0.5011\n",
      "Epoch 124/200\n",
      "222/222 [==============================] - 22s 97ms/step - loss: 0.6933 - accuracy: 0.5137 - val_loss: 0.6935 - val_accuracy: 0.4989\n",
      "Epoch 125/200\n",
      "222/222 [==============================] - 23s 104ms/step - loss: 0.6936 - accuracy: 0.4987 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 126/200\n",
      "222/222 [==============================] - 22s 101ms/step - loss: 0.6934 - accuracy: 0.5032 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 127/200\n",
      "222/222 [==============================] - 22s 99ms/step - loss: 0.6933 - accuracy: 0.5018 - val_loss: 0.6931 - val_accuracy: 0.5011\n",
      "Epoch 128/200\n",
      "222/222 [==============================] - 22s 97ms/step - loss: 0.6937 - accuracy: 0.5016 - val_loss: 0.6931 - val_accuracy: 0.5011\n",
      "Epoch 129/200\n",
      "222/222 [==============================] - 23s 102ms/step - loss: 0.6934 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5011\n",
      "Epoch 130/200\n",
      "222/222 [==============================] - 25s 113ms/step - loss: 0.6936 - accuracy: 0.4996 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 131/200\n",
      "222/222 [==============================] - 25s 113ms/step - loss: 0.6937 - accuracy: 0.5006 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 132/200\n",
      "222/222 [==============================] - 23s 104ms/step - loss: 0.6934 - accuracy: 0.4944 - val_loss: 0.6937 - val_accuracy: 0.5011\n",
      "Epoch 133/200\n",
      "222/222 [==============================] - 22s 97ms/step - loss: 0.6933 - accuracy: 0.5016 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 134/200\n",
      "222/222 [==============================] - 24s 107ms/step - loss: 0.6933 - accuracy: 0.4992 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 135/200\n",
      "222/222 [==============================] - 22s 97ms/step - loss: 0.6933 - accuracy: 0.5029 - val_loss: 0.6935 - val_accuracy: 0.5011\n",
      "Epoch 136/200\n",
      "222/222 [==============================] - 24s 109ms/step - loss: 0.6935 - accuracy: 0.5030 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 137/200\n",
      "222/222 [==============================] - 27s 121ms/step - loss: 0.6933 - accuracy: 0.5039 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 138/200\n",
      "222/222 [==============================] - 23s 104ms/step - loss: 0.6934 - accuracy: 0.5036 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 139/200\n",
      "222/222 [==============================] - 22s 100ms/step - loss: 0.6933 - accuracy: 0.5004 - val_loss: 0.6932 - val_accuracy: 0.4989\n",
      "Epoch 140/200\n",
      "222/222 [==============================] - 21s 97ms/step - loss: 0.6934 - accuracy: 0.5011 - val_loss: 0.6931 - val_accuracy: 0.5011\n",
      "Epoch 141/200\n",
      "222/222 [==============================] - 22s 99ms/step - loss: 0.6936 - accuracy: 0.4930 - val_loss: 0.6939 - val_accuracy: 0.5011\n",
      "Epoch 142/200\n",
      "222/222 [==============================] - 22s 100ms/step - loss: 0.6933 - accuracy: 0.5054 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 143/200\n",
      "222/222 [==============================] - 25s 113ms/step - loss: 0.6932 - accuracy: 0.5089 - val_loss: 0.6932 - val_accuracy: 0.4989\n",
      "Epoch 144/200\n",
      "222/222 [==============================] - 25s 111ms/step - loss: 0.6935 - accuracy: 0.5004 - val_loss: 0.6932 - val_accuracy: 0.4989\n",
      "Epoch 145/200\n",
      "222/222 [==============================] - 23s 103ms/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6938 - val_accuracy: 0.4989\n",
      "Epoch 146/200\n",
      "222/222 [==============================] - 22s 99ms/step - loss: 0.6933 - accuracy: 0.5013 - val_loss: 0.6935 - val_accuracy: 0.5011\n",
      "Epoch 147/200\n",
      "222/222 [==============================] - 22s 98ms/step - loss: 0.6934 - accuracy: 0.4947 - val_loss: 0.6931 - val_accuracy: 0.5011\n",
      "Epoch 148/200\n",
      "222/222 [==============================] - 22s 98ms/step - loss: 0.6929 - accuracy: 0.5122 - val_loss: 0.6934 - val_accuracy: 0.4989\n",
      "Epoch 149/200\n",
      "222/222 [==============================] - 22s 98ms/step - loss: 0.6933 - accuracy: 0.5018 - val_loss: 0.6933 - val_accuracy: 0.4989\n",
      "Epoch 150/200\n",
      "222/222 [==============================] - 22s 97ms/step - loss: 0.6933 - accuracy: 0.4981 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 151/200\n",
      "222/222 [==============================] - 22s 97ms/step - loss: 0.6932 - accuracy: 0.5057 - val_loss: 0.6931 - val_accuracy: 0.4989\n",
      "Epoch 152/200\n",
      "222/222 [==============================] - 23s 102ms/step - loss: 0.6935 - accuracy: 0.4908 - val_loss: 0.6935 - val_accuracy: 0.5011\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222/222 [==============================] - 27s 122ms/step - loss: 0.6934 - accuracy: 0.5074 - val_loss: 0.6937 - val_accuracy: 0.5011\n",
      "Epoch 154/200\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.6933 - accuracy: 0.5004 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 155/200\n",
      "222/222 [==============================] - 21s 97ms/step - loss: 0.6933 - accuracy: 0.5057 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 156/200\n",
      "222/222 [==============================] - 24s 108ms/step - loss: 0.6936 - accuracy: 0.5018 - val_loss: 0.6934 - val_accuracy: 0.5011\n",
      "Epoch 157/200\n",
      "222/222 [==============================] - 23s 102ms/step - loss: 0.6933 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 158/200\n",
      "222/222 [==============================] - 24s 108ms/step - loss: 0.6935 - accuracy: 0.4909 - val_loss: 0.6932 - val_accuracy: 0.4989\n",
      "Epoch 159/200\n",
      "222/222 [==============================] - 23s 103ms/step - loss: 0.6935 - accuracy: 0.4984 - val_loss: 0.6932 - val_accuracy: 0.4989\n",
      "Epoch 160/200\n",
      "222/222 [==============================] - 26s 119ms/step - loss: 0.6933 - accuracy: 0.4987 - val_loss: 0.6934 - val_accuracy: 0.5011\n",
      "Epoch 161/200\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.6936 - accuracy: 0.4963 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 162/200\n",
      "222/222 [==============================] - 25s 112ms/step - loss: 0.6934 - accuracy: 0.4981 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 163/200\n",
      "222/222 [==============================] - 25s 114ms/step - loss: 0.6933 - accuracy: 0.5053 - val_loss: 0.6931 - val_accuracy: 0.5011\n",
      "Epoch 164/200\n",
      "222/222 [==============================] - 28s 124ms/step - loss: 0.6935 - accuracy: 0.4987 - val_loss: 0.6938 - val_accuracy: 0.5011\n",
      "Epoch 165/200\n",
      "222/222 [==============================] - 25s 111ms/step - loss: 0.6932 - accuracy: 0.5046 - val_loss: 0.6933 - val_accuracy: 0.4989\n",
      "Epoch 166/200\n",
      "222/222 [==============================] - 24s 110ms/step - loss: 0.6939 - accuracy: 0.4974 - val_loss: 0.6938 - val_accuracy: 0.5011\n",
      "Epoch 167/200\n",
      "222/222 [==============================] - 26s 117ms/step - loss: 0.6936 - accuracy: 0.5026 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 168/200\n",
      "222/222 [==============================] - 23s 102ms/step - loss: 0.6934 - accuracy: 0.5067 - val_loss: 0.6933 - val_accuracy: 0.4989\n",
      "Epoch 169/200\n",
      "222/222 [==============================] - 24s 108ms/step - loss: 0.6933 - accuracy: 0.4985 - val_loss: 0.6934 - val_accuracy: 0.5011\n",
      "Epoch 170/200\n",
      "222/222 [==============================] - 24s 108ms/step - loss: 0.6936 - accuracy: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 171/200\n",
      "222/222 [==============================] - 24s 110ms/step - loss: 0.6933 - accuracy: 0.5051 - val_loss: 0.6940 - val_accuracy: 0.5011\n",
      "Epoch 172/200\n",
      "222/222 [==============================] - 26s 116ms/step - loss: 0.6936 - accuracy: 0.5042 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 173/200\n",
      "222/222 [==============================] - 25s 111ms/step - loss: 0.6936 - accuracy: 0.4956 - val_loss: 0.6931 - val_accuracy: 0.4989\n",
      "Epoch 174/200\n",
      "222/222 [==============================] - 26s 116ms/step - loss: 0.6929 - accuracy: 0.5125 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 175/200\n",
      "222/222 [==============================] - 23s 104ms/step - loss: 0.6938 - accuracy: 0.4973 - val_loss: 0.6940 - val_accuracy: 0.5011\n",
      "Epoch 176/200\n",
      "222/222 [==============================] - 24s 107ms/step - loss: 0.6936 - accuracy: 0.5044 - val_loss: 0.6931 - val_accuracy: 0.5011\n",
      "Epoch 177/200\n",
      "222/222 [==============================] - 28s 125ms/step - loss: 0.6934 - accuracy: 0.4958 - val_loss: 0.6934 - val_accuracy: 0.5011\n",
      "Epoch 178/200\n",
      "222/222 [==============================] - 26s 117ms/step - loss: 0.6933 - accuracy: 0.5075 - val_loss: 0.6932 - val_accuracy: 0.4989\n",
      "Epoch 179/200\n",
      "222/222 [==============================] - 27s 122ms/step - loss: 0.6933 - accuracy: 0.4946 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 180/200\n",
      "222/222 [==============================] - 24s 110ms/step - loss: 0.6936 - accuracy: 0.5046 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 181/200\n",
      "222/222 [==============================] - 30s 136ms/step - loss: 0.6935 - accuracy: 0.4973 - val_loss: 0.6934 - val_accuracy: 0.5011\n",
      "Epoch 182/200\n",
      "222/222 [==============================] - 28s 125ms/step - loss: 0.6935 - accuracy: 0.4975 - val_loss: 0.6937 - val_accuracy: 0.5011\n",
      "Epoch 183/200\n",
      "222/222 [==============================] - 27s 122ms/step - loss: 0.6933 - accuracy: 0.5001 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 184/200\n",
      "222/222 [==============================] - 25s 114ms/step - loss: 0.6933 - accuracy: 0.5039 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 185/200\n",
      "222/222 [==============================] - 26s 119ms/step - loss: 0.6935 - accuracy: 0.5043 - val_loss: 0.6938 - val_accuracy: 0.5011\n",
      "Epoch 186/200\n",
      "222/222 [==============================] - 26s 115ms/step - loss: 0.6934 - accuracy: 0.4973 - val_loss: 0.6935 - val_accuracy: 0.5011\n",
      "Epoch 187/200\n",
      "222/222 [==============================] - 26s 116ms/step - loss: 0.6935 - accuracy: 0.4970 - val_loss: 0.6934 - val_accuracy: 0.5011\n",
      "Epoch 188/200\n",
      "222/222 [==============================] - 31s 142ms/step - loss: 0.6932 - accuracy: 0.5053 - val_loss: 0.6931 - val_accuracy: 0.5011\n",
      "Epoch 189/200\n",
      "222/222 [==============================] - 27s 121ms/step - loss: 0.6934 - accuracy: 0.5026 - val_loss: 0.6938 - val_accuracy: 0.5011\n",
      "Epoch 190/200\n",
      "222/222 [==============================] - 26s 119ms/step - loss: 0.6933 - accuracy: 0.5026 - val_loss: 0.6935 - val_accuracy: 0.5011\n",
      "Epoch 191/200\n",
      "222/222 [==============================] - 27s 122ms/step - loss: 0.6935 - accuracy: 0.5034 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
      "Epoch 192/200\n",
      "222/222 [==============================] - 28s 126ms/step - loss: 0.6937 - accuracy: 0.4974 - val_loss: 0.6937 - val_accuracy: 0.5011\n",
      "Epoch 193/200\n",
      "222/222 [==============================] - 29s 130ms/step - loss: 0.6936 - accuracy: 0.5011 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 194/200\n",
      "222/222 [==============================] - 26s 118ms/step - loss: 0.6935 - accuracy: 0.5018 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 195/200\n",
      "222/222 [==============================] - 29s 131ms/step - loss: 0.6934 - accuracy: 0.5002 - val_loss: 0.6937 - val_accuracy: 0.5011\n",
      "Epoch 196/200\n",
      "222/222 [==============================] - 31s 139ms/step - loss: 0.6931 - accuracy: 0.5085 - val_loss: 0.6932 - val_accuracy: 0.4989\n",
      "Epoch 197/200\n",
      "222/222 [==============================] - 29s 130ms/step - loss: 0.6934 - accuracy: 0.5030 - val_loss: 0.6932 - val_accuracy: 0.5011\n",
      "Epoch 198/200\n",
      "222/222 [==============================] - 28s 128ms/step - loss: 0.6933 - accuracy: 0.5013 - val_loss: 0.6931 - val_accuracy: 0.5011\n",
      "Epoch 199/200\n",
      "222/222 [==============================] - 25s 113ms/step - loss: 0.6934 - accuracy: 0.4977 - val_loss: 0.6936 - val_accuracy: 0.5011\n",
      "Epoch 200/200\n",
      "222/222 [==============================] - 27s 122ms/step - loss: 0.6932 - accuracy: 0.5073 - val_loss: 0.6933 - val_accuracy: 0.4989\n",
      "Fold 1, 200 epochs, 4708 sec\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 49.89%\n",
      "\n",
      "Validation core mean 49.89% (+/- 0.00%)\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: CNN101.short.model/assets\n"
     ]
    }
   ],
   "source": [
    "MINLEN=200\n",
    "MAXLEN=1000\n",
    "\n",
    "print(\"Working on full training set, slice by sequence length.\")\n",
    "print(\"Slice size range [%d - %d)\"%(MINLEN,MAXLEN))\n",
    "subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
    "\n",
    "print (\"Sequence to Kmer\")\n",
    "(X_train,y_train)=make_kmers(MINLEN,MAXLEN,subset)\n",
    "X_train=onehot(X_train)\n",
    "print(\"Length of list:\")\n",
    "print(len(X_train))\n",
    "print(\"Length of first element:\")\n",
    "print(len(X_train[0]))\n",
    "print(\"First element:\")\n",
    "print(X_train[0])\n",
    "print (\"Compile the model\")\n",
    "model=build_model(MAXLEN,EMBED_DIMEN)\n",
    "print(model.summary())  # Print this only once\n",
    "print (\"Cross valiation\")\n",
    "model1=do_cross_validation(X_train,y_train,EPOCHS,MAXLEN,EMBED_DIMEN)\n",
    "model1.save(FILENAME+'.short.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Len 1K-2Kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on full training set, slice by sequence length.\n",
      "Slice size range [1000 - 2000)\n",
      "original (30290, 4)\n",
      "no short (9273, 4)\n",
      "no long, no short (3368, 4)\n",
      "Sequence to Kmer\n",
      "transform...\n",
      "Compile the model\n",
      "COMPILE\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 1998, 64)          1024      \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 1998, 32)          6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 999, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 999, 32)           3104      \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 999, 32)           3104      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 31968)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                1023008   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,037,505\n",
      "Trainable params: 1,037,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Cross valiation\n",
      "BUILD MODEL\n",
      "COMPILE\n",
      "FIT\n",
      "Epoch 1/200\n",
      "85/85 [==============================] - 19s 225ms/step - loss: 0.6901 - accuracy: 0.5883 - val_loss: 0.6744 - val_accuracy: 0.6039\n",
      "Epoch 2/200\n",
      "85/85 [==============================] - 18s 207ms/step - loss: 0.6694 - accuracy: 0.6177 - val_loss: 0.6714 - val_accuracy: 0.6039\n",
      "Epoch 3/200\n",
      "85/85 [==============================] - 18s 209ms/step - loss: 0.6660 - accuracy: 0.6214 - val_loss: 0.6759 - val_accuracy: 0.6039\n",
      "Epoch 4/200\n",
      "85/85 [==============================] - 17s 203ms/step - loss: 0.6651 - accuracy: 0.6221 - val_loss: 0.6738 - val_accuracy: 0.6039\n",
      "Epoch 5/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6647 - accuracy: 0.6221 - val_loss: 0.6736 - val_accuracy: 0.6039\n",
      "Epoch 6/200\n",
      "85/85 [==============================] - 17s 202ms/step - loss: 0.6660 - accuracy: 0.6218 - val_loss: 0.6714 - val_accuracy: 0.6039\n",
      "Epoch 7/200\n",
      "85/85 [==============================] - 17s 198ms/step - loss: 0.6640 - accuracy: 0.6218 - val_loss: 0.6730 - val_accuracy: 0.6039\n",
      "Epoch 8/200\n",
      "85/85 [==============================] - 17s 198ms/step - loss: 0.6637 - accuracy: 0.6221 - val_loss: 0.6716 - val_accuracy: 0.6039\n",
      "Epoch 9/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6649 - accuracy: 0.6221 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 10/200\n",
      "85/85 [==============================] - 20s 240ms/step - loss: 0.6643 - accuracy: 0.6221 - val_loss: 0.6715 - val_accuracy: 0.6039\n",
      "Epoch 11/200\n",
      "85/85 [==============================] - 23s 269ms/step - loss: 0.6643 - accuracy: 0.6221 - val_loss: 0.6728 - val_accuracy: 0.6039\n",
      "Epoch 12/200\n",
      "85/85 [==============================] - 19s 229ms/step - loss: 0.6631 - accuracy: 0.6221 - val_loss: 0.6717 - val_accuracy: 0.6039\n",
      "Epoch 13/200\n",
      "85/85 [==============================] - 22s 257ms/step - loss: 0.6637 - accuracy: 0.6221 - val_loss: 0.6727 - val_accuracy: 0.6039\n",
      "Epoch 14/200\n",
      "85/85 [==============================] - 19s 228ms/step - loss: 0.6639 - accuracy: 0.6221 - val_loss: 0.6730 - val_accuracy: 0.6039\n",
      "Epoch 15/200\n",
      "85/85 [==============================] - 20s 241ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6716 - val_accuracy: 0.6039\n",
      "Epoch 16/200\n",
      "85/85 [==============================] - 18s 216ms/step - loss: 0.6638 - accuracy: 0.6221 - val_loss: 0.6716 - val_accuracy: 0.6039\n",
      "Epoch 17/200\n",
      "85/85 [==============================] - 19s 219ms/step - loss: 0.6636 - accuracy: 0.6221 - val_loss: 0.6725 - val_accuracy: 0.6039\n",
      "Epoch 18/200\n",
      "85/85 [==============================] - 18s 213ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6727 - val_accuracy: 0.6039\n",
      "Epoch 19/200\n",
      "85/85 [==============================] - 19s 223ms/step - loss: 0.6631 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 20/200\n",
      "85/85 [==============================] - 21s 245ms/step - loss: 0.6642 - accuracy: 0.6221 - val_loss: 0.6723 - val_accuracy: 0.6039\n",
      "Epoch 21/200\n",
      "85/85 [==============================] - 18s 217ms/step - loss: 0.6630 - accuracy: 0.6221 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 22/200\n",
      "85/85 [==============================] - 20s 234ms/step - loss: 0.6639 - accuracy: 0.6221 - val_loss: 0.6715 - val_accuracy: 0.6039\n",
      "Epoch 23/200\n",
      "85/85 [==============================] - 17s 202ms/step - loss: 0.6638 - accuracy: 0.6221 - val_loss: 0.6724 - val_accuracy: 0.6039\n",
      "Epoch 24/200\n",
      "85/85 [==============================] - 20s 239ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6724 - val_accuracy: 0.6039\n",
      "Epoch 25/200\n",
      "85/85 [==============================] - 19s 220ms/step - loss: 0.6638 - accuracy: 0.6221 - val_loss: 0.6719 - val_accuracy: 0.6039\n",
      "Epoch 26/200\n",
      "85/85 [==============================] - 17s 202ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6721 - val_accuracy: 0.6039\n",
      "Epoch 27/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6728 - val_accuracy: 0.6039\n",
      "Epoch 28/200\n",
      "85/85 [==============================] - 19s 223ms/step - loss: 0.6628 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 29/200\n",
      "85/85 [==============================] - 18s 215ms/step - loss: 0.6644 - accuracy: 0.6218 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 30/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6717 - val_accuracy: 0.6039\n",
      "Epoch 31/200\n",
      "85/85 [==============================] - 18s 216ms/step - loss: 0.6637 - accuracy: 0.6221 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 32/200\n",
      "85/85 [==============================] - 22s 257ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6715 - val_accuracy: 0.6039\n",
      "Epoch 33/200\n",
      "85/85 [==============================] - 21s 245ms/step - loss: 0.6641 - accuracy: 0.6221 - val_loss: 0.6716 - val_accuracy: 0.6039\n",
      "Epoch 34/200\n",
      "85/85 [==============================] - 18s 213ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6723 - val_accuracy: 0.6039\n",
      "Epoch 35/200\n",
      "85/85 [==============================] - 17s 203ms/step - loss: 0.6631 - accuracy: 0.6221 - val_loss: 0.6718 - val_accuracy: 0.6039\n",
      "Epoch 36/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6636 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 37/200\n",
      "85/85 [==============================] - 17s 201ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6719 - val_accuracy: 0.6039\n",
      "Epoch 38/200\n",
      "85/85 [==============================] - 17s 202ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 39/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6734 - val_accuracy: 0.6039\n",
      "Epoch 40/200\n",
      "85/85 [==============================] - 17s 200ms/step - loss: 0.6638 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 41/200\n",
      "85/85 [==============================] - 17s 200ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 42/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6631 - accuracy: 0.6221 - val_loss: 0.6724 - val_accuracy: 0.6039\n",
      "Epoch 43/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 17s 203ms/step - loss: 0.6637 - accuracy: 0.6221 - val_loss: 0.6716 - val_accuracy: 0.6039\n",
      "Epoch 44/200\n",
      "85/85 [==============================] - 17s 195ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6726 - val_accuracy: 0.6039\n",
      "Epoch 45/200\n",
      "85/85 [==============================] - 17s 198ms/step - loss: 0.6637 - accuracy: 0.6221 - val_loss: 0.6723 - val_accuracy: 0.6039\n",
      "Epoch 46/200\n",
      "85/85 [==============================] - 16s 193ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6725 - val_accuracy: 0.6039\n",
      "Epoch 47/200\n",
      "85/85 [==============================] - 17s 195ms/step - loss: 0.6630 - accuracy: 0.6221 - val_loss: 0.6718 - val_accuracy: 0.6039\n",
      "Epoch 48/200\n",
      "85/85 [==============================] - 17s 200ms/step - loss: 0.6629 - accuracy: 0.6221 - val_loss: 0.6724 - val_accuracy: 0.6039\n",
      "Epoch 49/200\n",
      "85/85 [==============================] - 17s 198ms/step - loss: 0.6640 - accuracy: 0.6221 - val_loss: 0.6721 - val_accuracy: 0.6039\n",
      "Epoch 50/200\n",
      "85/85 [==============================] - 17s 196ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6716 - val_accuracy: 0.6039\n",
      "Epoch 51/200\n",
      "85/85 [==============================] - 17s 195ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6715 - val_accuracy: 0.6039\n",
      "Epoch 52/200\n",
      "85/85 [==============================] - 17s 196ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 53/200\n",
      "85/85 [==============================] - 17s 201ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6729 - val_accuracy: 0.6039\n",
      "Epoch 54/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6630 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 55/200\n",
      "85/85 [==============================] - 17s 198ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6715 - val_accuracy: 0.6039\n",
      "Epoch 56/200\n",
      "85/85 [==============================] - 16s 194ms/step - loss: 0.6640 - accuracy: 0.6221 - val_loss: 0.6716 - val_accuracy: 0.6039\n",
      "Epoch 57/200\n",
      "85/85 [==============================] - 17s 195ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 58/200\n",
      "85/85 [==============================] - 17s 196ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6723 - val_accuracy: 0.6039\n",
      "Epoch 59/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6633 - accuracy: 0.6218 - val_loss: 0.6729 - val_accuracy: 0.6039\n",
      "Epoch 60/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6634 - accuracy: 0.6218 - val_loss: 0.6725 - val_accuracy: 0.6039\n",
      "Epoch 61/200\n",
      "85/85 [==============================] - 18s 212ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6724 - val_accuracy: 0.6039\n",
      "Epoch 62/200\n",
      "85/85 [==============================] - 17s 206ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6717 - val_accuracy: 0.6039\n",
      "Epoch 63/200\n",
      "85/85 [==============================] - 17s 196ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 64/200\n",
      "85/85 [==============================] - 17s 198ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6721 - val_accuracy: 0.6039\n",
      "Epoch 65/200\n",
      "85/85 [==============================] - 17s 198ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 66/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6721 - val_accuracy: 0.6039\n",
      "Epoch 67/200\n",
      "85/85 [==============================] - 17s 201ms/step - loss: 0.6639 - accuracy: 0.6221 - val_loss: 0.6723 - val_accuracy: 0.6039\n",
      "Epoch 68/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6717 - val_accuracy: 0.6039\n",
      "Epoch 69/200\n",
      "85/85 [==============================] - 17s 201ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6718 - val_accuracy: 0.6039\n",
      "Epoch 70/200\n",
      "85/85 [==============================] - 17s 204ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 71/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6624 - accuracy: 0.6232 - val_loss: 0.6715 - val_accuracy: 0.6039\n",
      "Epoch 72/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6640 - accuracy: 0.6214 - val_loss: 0.6725 - val_accuracy: 0.6039\n",
      "Epoch 73/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6631 - accuracy: 0.6218 - val_loss: 0.6727 - val_accuracy: 0.6039\n",
      "Epoch 74/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6634 - accuracy: 0.6236 - val_loss: 0.6725 - val_accuracy: 0.6039\n",
      "Epoch 75/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6634 - accuracy: 0.6199 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 76/200\n",
      "85/85 [==============================] - 17s 196ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6723 - val_accuracy: 0.6039\n",
      "Epoch 77/200\n",
      "85/85 [==============================] - 17s 196ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6731 - val_accuracy: 0.6039\n",
      "Epoch 78/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 79/200\n",
      "85/85 [==============================] - 17s 202ms/step - loss: 0.6638 - accuracy: 0.6221 - val_loss: 0.6728 - val_accuracy: 0.6039\n",
      "Epoch 80/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6724 - val_accuracy: 0.6039\n",
      "Epoch 81/200\n",
      "85/85 [==============================] - 17s 200ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6718 - val_accuracy: 0.6039\n",
      "Epoch 82/200\n",
      "85/85 [==============================] - 17s 198ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6723 - val_accuracy: 0.6039\n",
      "Epoch 83/200\n",
      "85/85 [==============================] - 17s 198ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6717 - val_accuracy: 0.6039\n",
      "Epoch 84/200\n",
      "85/85 [==============================] - 17s 196ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6723 - val_accuracy: 0.6039\n",
      "Epoch 85/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6719 - val_accuracy: 0.6039\n",
      "Epoch 86/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6637 - accuracy: 0.6221 - val_loss: 0.6719 - val_accuracy: 0.6039\n",
      "Epoch 87/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 88/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6716 - val_accuracy: 0.6039\n",
      "Epoch 89/200\n",
      "85/85 [==============================] - 17s 200ms/step - loss: 0.6636 - accuracy: 0.6221 - val_loss: 0.6719 - val_accuracy: 0.6039\n",
      "Epoch 90/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6719 - val_accuracy: 0.6039\n",
      "Epoch 91/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6715 - val_accuracy: 0.6039\n",
      "Epoch 92/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6636 - accuracy: 0.6221 - val_loss: 0.6715 - val_accuracy: 0.6039\n",
      "Epoch 93/200\n",
      "85/85 [==============================] - 17s 195ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6719 - val_accuracy: 0.6039\n",
      "Epoch 94/200\n",
      "85/85 [==============================] - 17s 203ms/step - loss: 0.6636 - accuracy: 0.6221 - val_loss: 0.6728 - val_accuracy: 0.6039\n",
      "Epoch 95/200\n",
      "85/85 [==============================] - 17s 195ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6723 - val_accuracy: 0.6039\n",
      "Epoch 96/200\n",
      "85/85 [==============================] - 17s 201ms/step - loss: 0.6630 - accuracy: 0.6221 - val_loss: 0.6718 - val_accuracy: 0.6039\n",
      "Epoch 97/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6634 - accuracy: 0.6214 - val_loss: 0.6721 - val_accuracy: 0.6039\n",
      "Epoch 98/200\n",
      "85/85 [==============================] - 18s 208ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6721 - val_accuracy: 0.6039\n",
      "Epoch 99/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 18s 211ms/step - loss: 0.6636 - accuracy: 0.6221 - val_loss: 0.6721 - val_accuracy: 0.6039\n",
      "Epoch 100/200\n",
      "85/85 [==============================] - 17s 201ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6726 - val_accuracy: 0.6039\n",
      "Epoch 101/200\n",
      "85/85 [==============================] - 20s 240ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 102/200\n",
      "85/85 [==============================] - 23s 267ms/step - loss: 0.6634 - accuracy: 0.6225 - val_loss: 0.6719 - val_accuracy: 0.6039\n",
      "Epoch 103/200\n",
      "85/85 [==============================] - 19s 227ms/step - loss: 0.6637 - accuracy: 0.6221 - val_loss: 0.6719 - val_accuracy: 0.6039\n",
      "Epoch 104/200\n",
      "85/85 [==============================] - 23s 269ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6725 - val_accuracy: 0.6039\n",
      "Epoch 105/200\n",
      "85/85 [==============================] - 18s 214ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6724 - val_accuracy: 0.6039\n",
      "Epoch 106/200\n",
      "85/85 [==============================] - 20s 234ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 107/200\n",
      "85/85 [==============================] - 20s 234ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6733 - val_accuracy: 0.6039\n",
      "Epoch 108/200\n",
      "85/85 [==============================] - 18s 211ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6725 - val_accuracy: 0.6039\n",
      "Epoch 109/200\n",
      "85/85 [==============================] - 19s 218ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6723 - val_accuracy: 0.6039\n",
      "Epoch 110/200\n",
      "85/85 [==============================] - 16s 192ms/step - loss: 0.6638 - accuracy: 0.6221 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 111/200\n",
      "85/85 [==============================] - 17s 198ms/step - loss: 0.6636 - accuracy: 0.6218 - val_loss: 0.6721 - val_accuracy: 0.6039\n",
      "Epoch 112/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6717 - val_accuracy: 0.6039\n",
      "Epoch 113/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 114/200\n",
      "85/85 [==============================] - 17s 200ms/step - loss: 0.6630 - accuracy: 0.6221 - val_loss: 0.6721 - val_accuracy: 0.6039\n",
      "Epoch 115/200\n",
      "85/85 [==============================] - 17s 206ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6716 - val_accuracy: 0.6039\n",
      "Epoch 116/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6636 - accuracy: 0.6221 - val_loss: 0.6725 - val_accuracy: 0.6039\n",
      "Epoch 117/200\n",
      "85/85 [==============================] - 18s 212ms/step - loss: 0.6636 - accuracy: 0.6221 - val_loss: 0.6719 - val_accuracy: 0.6039\n",
      "Epoch 118/200\n",
      "85/85 [==============================] - 21s 253ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 119/200\n",
      "85/85 [==============================] - 18s 214ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6723 - val_accuracy: 0.6039\n",
      "Epoch 120/200\n",
      "85/85 [==============================] - 17s 200ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6719 - val_accuracy: 0.6039\n",
      "Epoch 121/200\n",
      "85/85 [==============================] - 17s 200ms/step - loss: 0.6637 - accuracy: 0.6221 - val_loss: 0.6717 - val_accuracy: 0.6039\n",
      "Epoch 122/200\n",
      "85/85 [==============================] - 18s 215ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 123/200\n",
      "85/85 [==============================] - 18s 216ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6716 - val_accuracy: 0.6039\n",
      "Epoch 124/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6716 - val_accuracy: 0.6039\n",
      "Epoch 125/200\n",
      "85/85 [==============================] - 17s 198ms/step - loss: 0.6633 - accuracy: 0.6218 - val_loss: 0.6726 - val_accuracy: 0.6039\n",
      "Epoch 126/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6631 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 127/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6726 - val_accuracy: 0.6039\n",
      "Epoch 128/200\n",
      "85/85 [==============================] - 17s 198ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 129/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6716 - val_accuracy: 0.6039\n",
      "Epoch 130/200\n",
      "85/85 [==============================] - 17s 202ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6718 - val_accuracy: 0.6039\n",
      "Epoch 131/200\n",
      "85/85 [==============================] - 16s 193ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 132/200\n",
      "85/85 [==============================] - 17s 196ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6729 - val_accuracy: 0.6039\n",
      "Epoch 133/200\n",
      "85/85 [==============================] - 17s 200ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6724 - val_accuracy: 0.6039\n",
      "Epoch 134/200\n",
      "85/85 [==============================] - 17s 196ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6719 - val_accuracy: 0.6039\n",
      "Epoch 135/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6723 - val_accuracy: 0.6039\n",
      "Epoch 136/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6723 - val_accuracy: 0.6039\n",
      "Epoch 137/200\n",
      "85/85 [==============================] - 17s 198ms/step - loss: 0.6631 - accuracy: 0.6221 - val_loss: 0.6725 - val_accuracy: 0.6039\n",
      "Epoch 138/200\n",
      "85/85 [==============================] - 18s 215ms/step - loss: 0.6631 - accuracy: 0.6221 - val_loss: 0.6719 - val_accuracy: 0.6039\n",
      "Epoch 139/200\n",
      "85/85 [==============================] - 19s 223ms/step - loss: 0.6631 - accuracy: 0.6221 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 140/200\n",
      "85/85 [==============================] - 17s 200ms/step - loss: 0.6630 - accuracy: 0.6221 - val_loss: 0.6726 - val_accuracy: 0.6039\n",
      "Epoch 141/200\n",
      "85/85 [==============================] - 17s 200ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6725 - val_accuracy: 0.6039\n",
      "Epoch 142/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 143/200\n",
      "85/85 [==============================] - 19s 226ms/step - loss: 0.6623 - accuracy: 0.6229 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 144/200\n",
      "85/85 [==============================] - 21s 251ms/step - loss: 0.6658 - accuracy: 0.6180 - val_loss: 0.6732 - val_accuracy: 0.6039\n",
      "Epoch 145/200\n",
      "85/85 [==============================] - 24s 284ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6718 - val_accuracy: 0.6039\n",
      "Epoch 146/200\n",
      "85/85 [==============================] - 21s 252ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6719 - val_accuracy: 0.6039\n",
      "Epoch 147/200\n",
      "85/85 [==============================] - 18s 213ms/step - loss: 0.6631 - accuracy: 0.6229 - val_loss: 0.6733 - val_accuracy: 0.6039\n",
      "Epoch 148/200\n",
      "85/85 [==============================] - 17s 201ms/step - loss: 0.6639 - accuracy: 0.6221 - val_loss: 0.6730 - val_accuracy: 0.6039\n",
      "Epoch 149/200\n",
      "85/85 [==============================] - 18s 206ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 150/200\n",
      "85/85 [==============================] - 20s 238ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 151/200\n",
      "85/85 [==============================] - 21s 250ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6725 - val_accuracy: 0.6039\n",
      "Epoch 152/200\n",
      "85/85 [==============================] - 22s 264ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6725 - val_accuracy: 0.6039\n",
      "Epoch 153/200\n",
      "85/85 [==============================] - 22s 256ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 154/200\n",
      "85/85 [==============================] - 18s 209ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6719 - val_accuracy: 0.6039\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 22s 255ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6729 - val_accuracy: 0.6039\n",
      "Epoch 156/200\n",
      "85/85 [==============================] - 20s 232ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6721 - val_accuracy: 0.6039\n",
      "Epoch 157/200\n",
      "85/85 [==============================] - 17s 204ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6718 - val_accuracy: 0.6039\n",
      "Epoch 158/200\n",
      "85/85 [==============================] - 21s 244ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6724 - val_accuracy: 0.6039\n",
      "Epoch 159/200\n",
      "85/85 [==============================] - 20s 232ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 160/200\n",
      "85/85 [==============================] - 22s 254ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 161/200\n",
      "85/85 [==============================] - 21s 250ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 162/200\n",
      "85/85 [==============================] - 22s 256ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6718 - val_accuracy: 0.6039\n",
      "Epoch 163/200\n",
      "85/85 [==============================] - 19s 218ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 164/200\n",
      "85/85 [==============================] - 17s 206ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6724 - val_accuracy: 0.6039\n",
      "Epoch 165/200\n",
      "85/85 [==============================] - 19s 225ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6721 - val_accuracy: 0.6039\n",
      "Epoch 166/200\n",
      "85/85 [==============================] - 18s 213ms/step - loss: 0.6637 - accuracy: 0.6221 - val_loss: 0.6718 - val_accuracy: 0.6039\n",
      "Epoch 167/200\n",
      "85/85 [==============================] - 17s 203ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6721 - val_accuracy: 0.6039\n",
      "Epoch 168/200\n",
      "85/85 [==============================] - 18s 210ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6719 - val_accuracy: 0.6039\n",
      "Epoch 169/200\n",
      "85/85 [==============================] - 18s 210ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6727 - val_accuracy: 0.6039\n",
      "Epoch 170/200\n",
      "85/85 [==============================] - 17s 200ms/step - loss: 0.6636 - accuracy: 0.6221 - val_loss: 0.6725 - val_accuracy: 0.6039\n",
      "Epoch 171/200\n",
      "85/85 [==============================] - 19s 218ms/step - loss: 0.6636 - accuracy: 0.6221 - val_loss: 0.6717 - val_accuracy: 0.6039\n",
      "Epoch 172/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6715 - val_accuracy: 0.6039\n",
      "Epoch 173/200\n",
      "85/85 [==============================] - 17s 202ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 174/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6719 - val_accuracy: 0.6039\n",
      "Epoch 175/200\n",
      "85/85 [==============================] - 17s 200ms/step - loss: 0.6636 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 176/200\n",
      "85/85 [==============================] - 17s 204ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6716 - val_accuracy: 0.6039\n",
      "Epoch 177/200\n",
      "85/85 [==============================] - 20s 231ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6721 - val_accuracy: 0.6039\n",
      "Epoch 178/200\n",
      "85/85 [==============================] - 23s 268ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6718 - val_accuracy: 0.6039\n",
      "Epoch 179/200\n",
      "85/85 [==============================] - 21s 248ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6721 - val_accuracy: 0.6039\n",
      "Epoch 180/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 181/200\n",
      "85/85 [==============================] - 20s 235ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 182/200\n",
      "85/85 [==============================] - 22s 255ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6724 - val_accuracy: 0.6039\n",
      "Epoch 183/200\n",
      "85/85 [==============================] - 23s 272ms/step - loss: 0.6636 - accuracy: 0.6221 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 184/200\n",
      "85/85 [==============================] - 17s 204ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 185/200\n",
      "85/85 [==============================] - 18s 214ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6718 - val_accuracy: 0.6039\n",
      "Epoch 186/200\n",
      "85/85 [==============================] - 17s 204ms/step - loss: 0.6636 - accuracy: 0.6221 - val_loss: 0.6719 - val_accuracy: 0.6039\n",
      "Epoch 187/200\n",
      "85/85 [==============================] - 17s 201ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6723 - val_accuracy: 0.6039\n",
      "Epoch 188/200\n",
      "85/85 [==============================] - 17s 201ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6722 - val_accuracy: 0.6039\n",
      "Epoch 189/200\n",
      "85/85 [==============================] - 17s 203ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6718 - val_accuracy: 0.6039\n",
      "Epoch 190/200\n",
      "85/85 [==============================] - 17s 203ms/step - loss: 0.6635 - accuracy: 0.6221 - val_loss: 0.6724 - val_accuracy: 0.6039\n",
      "Epoch 191/200\n",
      "85/85 [==============================] - 17s 195ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6720 - val_accuracy: 0.6039\n",
      "Epoch 192/200\n",
      "85/85 [==============================] - 18s 210ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6721 - val_accuracy: 0.6039\n",
      "Epoch 193/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6631 - accuracy: 0.6221 - val_loss: 0.6724 - val_accuracy: 0.6039\n",
      "Epoch 194/200\n",
      "85/85 [==============================] - 17s 202ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6721 - val_accuracy: 0.6039\n",
      "Epoch 195/200\n",
      "85/85 [==============================] - 17s 197ms/step - loss: 0.6633 - accuracy: 0.6218 - val_loss: 0.6726 - val_accuracy: 0.6039\n",
      "Epoch 196/200\n",
      "85/85 [==============================] - 20s 233ms/step - loss: 0.6634 - accuracy: 0.6218 - val_loss: 0.6724 - val_accuracy: 0.6039\n",
      "Epoch 197/200\n",
      "85/85 [==============================] - 17s 202ms/step - loss: 0.6633 - accuracy: 0.6221 - val_loss: 0.6724 - val_accuracy: 0.6039\n",
      "Epoch 198/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6721 - val_accuracy: 0.6039\n",
      "Epoch 199/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6719 - val_accuracy: 0.6039\n",
      "Epoch 200/200\n",
      "85/85 [==============================] - 17s 199ms/step - loss: 0.6634 - accuracy: 0.6221 - val_loss: 0.6717 - val_accuracy: 0.6039\n",
      "Fold 1, 200 epochs, 3637 sec\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 60.39%\n",
      "\n",
      "Validation core mean 60.39% (+/- 0.00%)\n",
      "INFO:tensorflow:Assets written to: CNN101.medium.model/assets\n"
     ]
    }
   ],
   "source": [
    "MINLEN=1000\n",
    "MAXLEN=2000\n",
    "\n",
    "print(\"Working on full training set, slice by sequence length.\")\n",
    "print(\"Slice size range [%d - %d)\"%(MINLEN,MAXLEN))\n",
    "subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
    "\n",
    "print (\"Sequence to Kmer\")\n",
    "(X_train,y_train)=make_kmers(MINLEN,MAXLEN,subset)\n",
    "X_train=onehot(X_train)\n",
    "print (\"Compile the model\")\n",
    "model=build_model(MAXLEN,EMBED_DIMEN)\n",
    "print(model.summary())  # Print this only once\n",
    "print (\"Cross valiation\")\n",
    "model2=do_cross_validation(X_train,y_train,EPOCHS,MAXLEN,EMBED_DIMEN)\n",
    "model2.save(FILENAME+'.medium.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Len 2K-3Kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on full training set, slice by sequence length.\n",
      "Slice size range [2000 - 3000)\n",
      "original (30290, 4)\n",
      "no short (3221, 4)\n",
      "no long, no short (1351, 4)\n",
      "Sequence to Kmer\n",
      "transform...\n",
      "Compile the model\n",
      "COMPILE\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 2998, 64)          1024      \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 2998, 32)          6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 1499, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 1499, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1499, 32)          3104      \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 47968)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                1535008   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,549,505\n",
      "Trainable params: 1,549,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Cross valiation\n",
      "BUILD MODEL\n",
      "COMPILE\n",
      "FIT\n",
      "Epoch 1/200\n",
      "34/34 [==============================] - 10s 280ms/step - loss: 0.6381 - accuracy: 0.6917 - val_loss: 0.6308 - val_accuracy: 0.6790\n",
      "Epoch 2/200\n",
      "34/34 [==============================] - 10s 284ms/step - loss: 0.6289 - accuracy: 0.6861 - val_loss: 0.6285 - val_accuracy: 0.6790\n",
      "Epoch 3/200\n",
      "34/34 [==============================] - 10s 289ms/step - loss: 0.6222 - accuracy: 0.7046 - val_loss: 0.6291 - val_accuracy: 0.6790\n",
      "Epoch 4/200\n",
      "34/34 [==============================] - 10s 280ms/step - loss: 0.6111 - accuracy: 0.7083 - val_loss: 0.6280 - val_accuracy: 0.6790\n",
      "Epoch 5/200\n",
      "34/34 [==============================] - 9s 277ms/step - loss: 0.6144 - accuracy: 0.7083 - val_loss: 0.6282 - val_accuracy: 0.6790\n",
      "Epoch 6/200\n",
      "34/34 [==============================] - 9s 279ms/step - loss: 0.6139 - accuracy: 0.7083 - val_loss: 0.6292 - val_accuracy: 0.6790\n",
      "Epoch 7/200\n",
      "34/34 [==============================] - 9s 276ms/step - loss: 0.6097 - accuracy: 0.7083 - val_loss: 0.6304 - val_accuracy: 0.6790\n",
      "Epoch 8/200\n",
      "34/34 [==============================] - 10s 284ms/step - loss: 0.6070 - accuracy: 0.7083 - val_loss: 0.6292 - val_accuracy: 0.6790\n",
      "Epoch 9/200\n",
      "34/34 [==============================] - 9s 275ms/step - loss: 0.6073 - accuracy: 0.7083 - val_loss: 0.6278 - val_accuracy: 0.6790\n",
      "Epoch 10/200\n",
      "34/34 [==============================] - 9s 272ms/step - loss: 0.6082 - accuracy: 0.7083 - val_loss: 0.6299 - val_accuracy: 0.6790\n",
      "Epoch 11/200\n",
      "34/34 [==============================] - 9s 275ms/step - loss: 0.6058 - accuracy: 0.7083 - val_loss: 0.6276 - val_accuracy: 0.6790\n",
      "Epoch 12/200\n",
      "34/34 [==============================] - 9s 268ms/step - loss: 0.6101 - accuracy: 0.7083 - val_loss: 0.6277 - val_accuracy: 0.6790\n",
      "Epoch 13/200\n",
      "34/34 [==============================] - 9s 276ms/step - loss: 0.6065 - accuracy: 0.7083 - val_loss: 0.6339 - val_accuracy: 0.6790\n",
      "Epoch 14/200\n",
      "34/34 [==============================] - 10s 280ms/step - loss: 0.6059 - accuracy: 0.7083 - val_loss: 0.6281 - val_accuracy: 0.6790\n",
      "Epoch 15/200\n",
      "34/34 [==============================] - 9s 278ms/step - loss: 0.6066 - accuracy: 0.7083 - val_loss: 0.6282 - val_accuracy: 0.6790\n",
      "Epoch 16/200\n",
      "34/34 [==============================] - 9s 274ms/step - loss: 0.6067 - accuracy: 0.7083 - val_loss: 0.6291 - val_accuracy: 0.6790\n",
      "Epoch 17/200\n",
      "34/34 [==============================] - 10s 281ms/step - loss: 0.6065 - accuracy: 0.7083 - val_loss: 0.6280 - val_accuracy: 0.6790\n",
      "Epoch 18/200\n",
      "34/34 [==============================] - 9s 279ms/step - loss: 0.6041 - accuracy: 0.7083 - val_loss: 0.6293 - val_accuracy: 0.6790\n",
      "Epoch 19/200\n",
      "34/34 [==============================] - 9s 275ms/step - loss: 0.6052 - accuracy: 0.7083 - val_loss: 0.6291 - val_accuracy: 0.6790\n",
      "Epoch 20/200\n",
      "34/34 [==============================] - 9s 277ms/step - loss: 0.6049 - accuracy: 0.7083 - val_loss: 0.6289 - val_accuracy: 0.6790\n",
      "Epoch 21/200\n",
      "34/34 [==============================] - 9s 276ms/step - loss: 0.6032 - accuracy: 0.7083 - val_loss: 0.6294 - val_accuracy: 0.6790\n",
      "Epoch 22/200\n",
      "34/34 [==============================] - 9s 275ms/step - loss: 0.6060 - accuracy: 0.7083 - val_loss: 0.6293 - val_accuracy: 0.6790\n",
      "Epoch 23/200\n",
      "34/34 [==============================] - 9s 272ms/step - loss: 0.6045 - accuracy: 0.7083 - val_loss: 0.6283 - val_accuracy: 0.6790\n",
      "Epoch 24/200\n",
      "34/34 [==============================] - 9s 277ms/step - loss: 0.6057 - accuracy: 0.7083 - val_loss: 0.6290 - val_accuracy: 0.6790\n",
      "Epoch 25/200\n",
      "34/34 [==============================] - 10s 284ms/step - loss: 0.6044 - accuracy: 0.7083 - val_loss: 0.6287 - val_accuracy: 0.6790\n",
      "Epoch 26/200\n",
      "34/34 [==============================] - 9s 275ms/step - loss: 0.6066 - accuracy: 0.7083 - val_loss: 0.6278 - val_accuracy: 0.6790\n",
      "Epoch 27/200\n",
      "34/34 [==============================] - 9s 274ms/step - loss: 0.6057 - accuracy: 0.7083 - val_loss: 0.6290 - val_accuracy: 0.6790\n",
      "Epoch 28/200\n",
      "34/34 [==============================] - 9s 273ms/step - loss: 0.6037 - accuracy: 0.7083 - val_loss: 0.6303 - val_accuracy: 0.6790\n",
      "Epoch 29/200\n",
      "34/34 [==============================] - 9s 267ms/step - loss: 0.6080 - accuracy: 0.7083 - val_loss: 0.6290 - val_accuracy: 0.6790\n",
      "Epoch 30/200\n",
      "34/34 [==============================] - 11s 310ms/step - loss: 0.6054 - accuracy: 0.7083 - val_loss: 0.6297 - val_accuracy: 0.6790\n",
      "Epoch 31/200\n",
      "34/34 [==============================] - 10s 285ms/step - loss: 0.6057 - accuracy: 0.7083 - val_loss: 0.6290 - val_accuracy: 0.6790\n",
      "Epoch 32/200\n",
      "34/34 [==============================] - 10s 294ms/step - loss: 0.6045 - accuracy: 0.7083 - val_loss: 0.6292 - val_accuracy: 0.6790\n",
      "Epoch 33/200\n",
      "34/34 [==============================] - 10s 304ms/step - loss: 0.6048 - accuracy: 0.7083 - val_loss: 0.6301 - val_accuracy: 0.6790\n",
      "Epoch 34/200\n",
      "34/34 [==============================] - 10s 299ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6290 - val_accuracy: 0.6790\n",
      "Epoch 35/200\n",
      "34/34 [==============================] - 10s 288ms/step - loss: 0.6047 - accuracy: 0.7083 - val_loss: 0.6287 - val_accuracy: 0.6790\n",
      "Epoch 36/200\n",
      "34/34 [==============================] - 10s 299ms/step - loss: 0.6044 - accuracy: 0.7083 - val_loss: 0.6299 - val_accuracy: 0.6790\n",
      "Epoch 37/200\n",
      "34/34 [==============================] - 10s 293ms/step - loss: 0.6034 - accuracy: 0.7083 - val_loss: 0.6300 - val_accuracy: 0.6790\n",
      "Epoch 38/200\n",
      "34/34 [==============================] - 10s 298ms/step - loss: 0.6043 - accuracy: 0.7083 - val_loss: 0.6301 - val_accuracy: 0.6790\n",
      "Epoch 39/200\n",
      "34/34 [==============================] - 10s 305ms/step - loss: 0.6050 - accuracy: 0.7083 - val_loss: 0.6296 - val_accuracy: 0.6790\n",
      "Epoch 40/200\n",
      "34/34 [==============================] - 9s 273ms/step - loss: 0.6034 - accuracy: 0.7083 - val_loss: 0.6302 - val_accuracy: 0.6790\n",
      "Epoch 41/200\n",
      "34/34 [==============================] - 10s 286ms/step - loss: 0.6048 - accuracy: 0.7083 - val_loss: 0.6299 - val_accuracy: 0.6790\n",
      "Epoch 42/200\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.6034 - accuracy: 0.7083 - val_loss: 0.6290 - val_accuracy: 0.6790\n",
      "Epoch 43/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 8s 245ms/step - loss: 0.6045 - accuracy: 0.7083 - val_loss: 0.6297 - val_accuracy: 0.6790\n",
      "Epoch 44/200\n",
      "34/34 [==============================] - 9s 255ms/step - loss: 0.6044 - accuracy: 0.7083 - val_loss: 0.6299 - val_accuracy: 0.6790\n",
      "Epoch 45/200\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.6037 - accuracy: 0.7083 - val_loss: 0.6296 - val_accuracy: 0.6790\n",
      "Epoch 46/200\n",
      "34/34 [==============================] - 10s 280ms/step - loss: 0.6032 - accuracy: 0.7083 - val_loss: 0.6302 - val_accuracy: 0.6790\n",
      "Epoch 47/200\n",
      "34/34 [==============================] - 9s 256ms/step - loss: 0.6041 - accuracy: 0.7083 - val_loss: 0.6290 - val_accuracy: 0.6790\n",
      "Epoch 48/200\n",
      "34/34 [==============================] - 9s 262ms/step - loss: 0.6041 - accuracy: 0.7083 - val_loss: 0.6292 - val_accuracy: 0.6790\n",
      "Epoch 49/200\n",
      "34/34 [==============================] - 9s 263ms/step - loss: 0.6042 - accuracy: 0.7083 - val_loss: 0.6295 - val_accuracy: 0.6790\n",
      "Epoch 50/200\n",
      "34/34 [==============================] - 9s 277ms/step - loss: 0.6052 - accuracy: 0.7083 - val_loss: 0.6302 - val_accuracy: 0.6790\n",
      "Epoch 51/200\n",
      "34/34 [==============================] - 10s 283ms/step - loss: 0.6052 - accuracy: 0.7083 - val_loss: 0.6293 - val_accuracy: 0.6790\n",
      "Epoch 52/200\n",
      "34/34 [==============================] - 10s 285ms/step - loss: 0.6044 - accuracy: 0.7083 - val_loss: 0.6294 - val_accuracy: 0.6790\n",
      "Epoch 53/200\n",
      "34/34 [==============================] - 10s 294ms/step - loss: 0.6025 - accuracy: 0.7083 - val_loss: 0.6281 - val_accuracy: 0.6790\n",
      "Epoch 54/200\n",
      "34/34 [==============================] - 10s 291ms/step - loss: 0.6054 - accuracy: 0.7083 - val_loss: 0.6311 - val_accuracy: 0.6790\n",
      "Epoch 55/200\n",
      "34/34 [==============================] - 10s 281ms/step - loss: 0.6043 - accuracy: 0.7083 - val_loss: 0.6301 - val_accuracy: 0.6790\n",
      "Epoch 56/200\n",
      "34/34 [==============================] - 10s 308ms/step - loss: 0.6041 - accuracy: 0.7083 - val_loss: 0.6300 - val_accuracy: 0.6790\n",
      "Epoch 57/200\n",
      "34/34 [==============================] - 10s 289ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6295 - val_accuracy: 0.6790\n",
      "Epoch 58/200\n",
      "34/34 [==============================] - 10s 306ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6305 - val_accuracy: 0.6790\n",
      "Epoch 59/200\n",
      "34/34 [==============================] - 10s 306ms/step - loss: 0.6042 - accuracy: 0.7083 - val_loss: 0.6293 - val_accuracy: 0.6790\n",
      "Epoch 60/200\n",
      "34/34 [==============================] - 10s 299ms/step - loss: 0.6040 - accuracy: 0.7083 - val_loss: 0.6291 - val_accuracy: 0.6790\n",
      "Epoch 61/200\n",
      "34/34 [==============================] - 10s 305ms/step - loss: 0.6043 - accuracy: 0.7083 - val_loss: 0.6300 - val_accuracy: 0.6790\n",
      "Epoch 62/200\n",
      "34/34 [==============================] - 10s 285ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6292 - val_accuracy: 0.6790\n",
      "Epoch 63/200\n",
      "34/34 [==============================] - 10s 282ms/step - loss: 0.6050 - accuracy: 0.7083 - val_loss: 0.6287 - val_accuracy: 0.6790\n",
      "Epoch 64/200\n",
      "34/34 [==============================] - 10s 289ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6294 - val_accuracy: 0.6790\n",
      "Epoch 65/200\n",
      "34/34 [==============================] - 10s 306ms/step - loss: 0.6041 - accuracy: 0.7083 - val_loss: 0.6294 - val_accuracy: 0.6790\n",
      "Epoch 66/200\n",
      "34/34 [==============================] - 10s 293ms/step - loss: 0.6033 - accuracy: 0.7083 - val_loss: 0.6299 - val_accuracy: 0.6790\n",
      "Epoch 67/200\n",
      "34/34 [==============================] - 10s 287ms/step - loss: 0.6040 - accuracy: 0.7083 - val_loss: 0.6293 - val_accuracy: 0.6790\n",
      "Epoch 68/200\n",
      "34/34 [==============================] - 10s 297ms/step - loss: 0.6046 - accuracy: 0.7083 - val_loss: 0.6294 - val_accuracy: 0.6790\n",
      "Epoch 69/200\n",
      "34/34 [==============================] - 10s 300ms/step - loss: 0.6035 - accuracy: 0.7083 - val_loss: 0.6296 - val_accuracy: 0.6790\n",
      "Epoch 70/200\n",
      "34/34 [==============================] - 10s 300ms/step - loss: 0.6046 - accuracy: 0.7083 - val_loss: 0.6294 - val_accuracy: 0.6790\n",
      "Epoch 71/200\n",
      "34/34 [==============================] - 8s 241ms/step - loss: 0.6042 - accuracy: 0.7083 - val_loss: 0.6293 - val_accuracy: 0.6790\n",
      "Epoch 72/200\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.6043 - accuracy: 0.7083 - val_loss: 0.6291 - val_accuracy: 0.6790\n",
      "Epoch 73/200\n",
      "34/34 [==============================] - 9s 265ms/step - loss: 0.6045 - accuracy: 0.7083 - val_loss: 0.6295 - val_accuracy: 0.6790\n",
      "Epoch 74/200\n",
      "34/34 [==============================] - 10s 287ms/step - loss: 0.6037 - accuracy: 0.7083 - val_loss: 0.6303 - val_accuracy: 0.6790\n",
      "Epoch 75/200\n",
      "34/34 [==============================] - 10s 285ms/step - loss: 0.6048 - accuracy: 0.7083 - val_loss: 0.6293 - val_accuracy: 0.6790\n",
      "Epoch 76/200\n",
      "34/34 [==============================] - 10s 296ms/step - loss: 0.6045 - accuracy: 0.7083 - val_loss: 0.6307 - val_accuracy: 0.6790\n",
      "Epoch 77/200\n",
      "34/34 [==============================] - 10s 306ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6291 - val_accuracy: 0.6790\n",
      "Epoch 78/200\n",
      "34/34 [==============================] - 10s 300ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6297 - val_accuracy: 0.6790\n",
      "Epoch 79/200\n",
      "34/34 [==============================] - 10s 287ms/step - loss: 0.6043 - accuracy: 0.7083 - val_loss: 0.6297 - val_accuracy: 0.6790\n",
      "Epoch 80/200\n",
      "34/34 [==============================] - 10s 301ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6295 - val_accuracy: 0.6790\n",
      "Epoch 81/200\n",
      "34/34 [==============================] - 10s 301ms/step - loss: 0.6037 - accuracy: 0.7083 - val_loss: 0.6294 - val_accuracy: 0.6790\n",
      "Epoch 82/200\n",
      "34/34 [==============================] - 10s 289ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6300 - val_accuracy: 0.6790\n",
      "Epoch 83/200\n",
      "34/34 [==============================] - 10s 293ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6302 - val_accuracy: 0.6790\n",
      "Epoch 84/200\n",
      "34/34 [==============================] - 10s 282ms/step - loss: 0.6041 - accuracy: 0.7083 - val_loss: 0.6301 - val_accuracy: 0.6790\n",
      "Epoch 85/200\n",
      "34/34 [==============================] - 10s 292ms/step - loss: 0.6037 - accuracy: 0.7083 - val_loss: 0.6303 - val_accuracy: 0.6790\n",
      "Epoch 86/200\n",
      "34/34 [==============================] - 10s 296ms/step - loss: 0.6041 - accuracy: 0.7083 - val_loss: 0.6296 - val_accuracy: 0.6790\n",
      "Epoch 87/200\n",
      "34/34 [==============================] - 10s 292ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6301 - val_accuracy: 0.6790\n",
      "Epoch 88/200\n",
      "34/34 [==============================] - 10s 294ms/step - loss: 0.6044 - accuracy: 0.7083 - val_loss: 0.6297 - val_accuracy: 0.6790\n",
      "Epoch 89/200\n",
      "34/34 [==============================] - 10s 294ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6301 - val_accuracy: 0.6790\n",
      "Epoch 90/200\n",
      "34/34 [==============================] - 10s 301ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6301 - val_accuracy: 0.6790\n",
      "Epoch 91/200\n",
      "34/34 [==============================] - 10s 290ms/step - loss: 0.6037 - accuracy: 0.7083 - val_loss: 0.6296 - val_accuracy: 0.6790\n",
      "Epoch 92/200\n",
      "34/34 [==============================] - 10s 298ms/step - loss: 0.6042 - accuracy: 0.7083 - val_loss: 0.6298 - val_accuracy: 0.6790\n",
      "Epoch 93/200\n",
      "34/34 [==============================] - 10s 299ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6296 - val_accuracy: 0.6790\n",
      "Epoch 94/200\n",
      "34/34 [==============================] - 10s 291ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6300 - val_accuracy: 0.6790\n",
      "Epoch 95/200\n",
      "34/34 [==============================] - 10s 290ms/step - loss: 0.6036 - accuracy: 0.7083 - val_loss: 0.6295 - val_accuracy: 0.6790\n",
      "Epoch 96/200\n",
      "34/34 [==============================] - 10s 294ms/step - loss: 0.6043 - accuracy: 0.7083 - val_loss: 0.6299 - val_accuracy: 0.6790\n",
      "Epoch 97/200\n",
      "34/34 [==============================] - 10s 287ms/step - loss: 0.6043 - accuracy: 0.7083 - val_loss: 0.6302 - val_accuracy: 0.6790\n",
      "Epoch 98/200\n",
      "34/34 [==============================] - 10s 282ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6290 - val_accuracy: 0.6790\n",
      "Epoch 99/200\n",
      "34/34 [==============================] - 10s 296ms/step - loss: 0.6041 - accuracy: 0.7083 - val_loss: 0.6305 - val_accuracy: 0.6790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200\n",
      "34/34 [==============================] - 10s 297ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6298 - val_accuracy: 0.6790\n",
      "Epoch 101/200\n",
      "34/34 [==============================] - 10s 301ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6300 - val_accuracy: 0.6790\n",
      "Epoch 102/200\n",
      "34/34 [==============================] - 10s 282ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6303 - val_accuracy: 0.6790\n",
      "Epoch 103/200\n",
      "34/34 [==============================] - 10s 283ms/step - loss: 0.6041 - accuracy: 0.7083 - val_loss: 0.6300 - val_accuracy: 0.6790\n",
      "Epoch 104/200\n",
      "34/34 [==============================] - 10s 285ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6295 - val_accuracy: 0.6790\n",
      "Epoch 105/200\n",
      "34/34 [==============================] - 11s 328ms/step - loss: 0.6043 - accuracy: 0.7083 - val_loss: 0.6307 - val_accuracy: 0.6790\n",
      "Epoch 106/200\n",
      "34/34 [==============================] - 10s 294ms/step - loss: 0.6041 - accuracy: 0.7083 - val_loss: 0.6294 - val_accuracy: 0.6790\n",
      "Epoch 107/200\n",
      "34/34 [==============================] - 9s 277ms/step - loss: 0.6041 - accuracy: 0.7083 - val_loss: 0.6299 - val_accuracy: 0.6790\n",
      "Epoch 108/200\n",
      "34/34 [==============================] - 10s 280ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6290 - val_accuracy: 0.6790\n",
      "Epoch 109/200\n",
      "34/34 [==============================] - 9s 276ms/step - loss: 0.6041 - accuracy: 0.7083 - val_loss: 0.6295 - val_accuracy: 0.6790\n",
      "Epoch 110/200\n",
      "34/34 [==============================] - 10s 280ms/step - loss: 0.6041 - accuracy: 0.7083 - val_loss: 0.6293 - val_accuracy: 0.6790\n",
      "Epoch 111/200\n",
      "34/34 [==============================] - 10s 298ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6299 - val_accuracy: 0.6790\n",
      "Epoch 112/200\n",
      "34/34 [==============================] - 10s 293ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6296 - val_accuracy: 0.6790\n",
      "Epoch 113/200\n",
      "34/34 [==============================] - 10s 290ms/step - loss: 0.6041 - accuracy: 0.7083 - val_loss: 0.6293 - val_accuracy: 0.6790\n",
      "Epoch 114/200\n",
      "34/34 [==============================] - 10s 296ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6301 - val_accuracy: 0.6790\n",
      "Epoch 115/200\n",
      "34/34 [==============================] - 10s 300ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6295 - val_accuracy: 0.6790\n",
      "Epoch 116/200\n",
      "34/34 [==============================] - 10s 301ms/step - loss: 0.6040 - accuracy: 0.7083 - val_loss: 0.6293 - val_accuracy: 0.6790\n",
      "Epoch 117/200\n",
      "34/34 [==============================] - 10s 296ms/step - loss: 0.6040 - accuracy: 0.7083 - val_loss: 0.6298 - val_accuracy: 0.6790\n",
      "Epoch 118/200\n",
      "34/34 [==============================] - 10s 286ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6299 - val_accuracy: 0.6790\n",
      "Epoch 119/200\n",
      "34/34 [==============================] - 10s 298ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6301 - val_accuracy: 0.6790\n",
      "Epoch 120/200\n",
      "34/34 [==============================] - 10s 283ms/step - loss: 0.6030 - accuracy: 0.7083 - val_loss: 0.6294 - val_accuracy: 0.6790\n",
      "Epoch 121/200\n",
      "34/34 [==============================] - 11s 311ms/step - loss: 0.6046 - accuracy: 0.7083 - val_loss: 0.6294 - val_accuracy: 0.6790\n",
      "Epoch 122/200\n",
      "34/34 [==============================] - 8s 240ms/step - loss: 0.6037 - accuracy: 0.7083 - val_loss: 0.6296 - val_accuracy: 0.6790\n",
      "Epoch 123/200\n",
      "34/34 [==============================] - 8s 239ms/step - loss: 0.6033 - accuracy: 0.7083 - val_loss: 0.6295 - val_accuracy: 0.6790\n",
      "Epoch 124/200\n",
      "34/34 [==============================] - 8s 238ms/step - loss: 0.6035 - accuracy: 0.7083 - val_loss: 0.6293 - val_accuracy: 0.6790\n",
      "Epoch 125/200\n",
      "34/34 [==============================] - 8s 235ms/step - loss: 0.6051 - accuracy: 0.7083 - val_loss: 0.6299 - val_accuracy: 0.6790\n",
      "Epoch 126/200\n",
      "34/34 [==============================] - 10s 279ms/step - loss: 0.6035 - accuracy: 0.7083 - val_loss: 0.6296 - val_accuracy: 0.6790\n",
      "Epoch 127/200\n",
      "34/34 [==============================] - 10s 284ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6295 - val_accuracy: 0.6790\n",
      "Epoch 128/200\n",
      "34/34 [==============================] - 10s 283ms/step - loss: 0.6042 - accuracy: 0.7083 - val_loss: 0.6304 - val_accuracy: 0.6790\n",
      "Epoch 129/200\n",
      "34/34 [==============================] - 9s 275ms/step - loss: 0.6042 - accuracy: 0.7083 - val_loss: 0.6300 - val_accuracy: 0.6790\n",
      "Epoch 130/200\n",
      "34/34 [==============================] - 10s 282ms/step - loss: 0.6036 - accuracy: 0.7083 - val_loss: 0.6298 - val_accuracy: 0.6790\n",
      "Epoch 131/200\n",
      "34/34 [==============================] - 10s 285ms/step - loss: 0.6056 - accuracy: 0.7083 - val_loss: 0.6303 - val_accuracy: 0.6790\n",
      "Epoch 132/200\n",
      "34/34 [==============================] - 10s 302ms/step - loss: 0.6040 - accuracy: 0.7083 - val_loss: 0.6288 - val_accuracy: 0.6790\n",
      "Epoch 133/200\n",
      "34/34 [==============================] - 10s 296ms/step - loss: 0.6040 - accuracy: 0.7083 - val_loss: 0.6295 - val_accuracy: 0.6790\n",
      "Epoch 134/200\n",
      "34/34 [==============================] - 10s 304ms/step - loss: 0.6042 - accuracy: 0.7083 - val_loss: 0.6297 - val_accuracy: 0.6790\n",
      "Epoch 135/200\n",
      "34/34 [==============================] - 10s 293ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6301 - val_accuracy: 0.6790\n",
      "Epoch 136/200\n",
      "34/34 [==============================] - 10s 302ms/step - loss: 0.6041 - accuracy: 0.7083 - val_loss: 0.6299 - val_accuracy: 0.6790\n",
      "Epoch 137/200\n",
      "34/34 [==============================] - 9s 277ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6298 - val_accuracy: 0.6790\n",
      "Epoch 138/200\n",
      "34/34 [==============================] - 10s 305ms/step - loss: 0.6035 - accuracy: 0.7083 - val_loss: 0.6296 - val_accuracy: 0.6790\n",
      "Epoch 139/200\n",
      "34/34 [==============================] - 10s 301ms/step - loss: 0.6041 - accuracy: 0.7083 - val_loss: 0.6291 - val_accuracy: 0.6790\n",
      "Epoch 140/200\n",
      "34/34 [==============================] - 10s 289ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6292 - val_accuracy: 0.6790\n",
      "Epoch 141/200\n",
      "34/34 [==============================] - 10s 296ms/step - loss: 0.6040 - accuracy: 0.7083 - val_loss: 0.6297 - val_accuracy: 0.6790\n",
      "Epoch 142/200\n",
      "34/34 [==============================] - 10s 292ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6297 - val_accuracy: 0.6790\n",
      "Epoch 143/200\n",
      "34/34 [==============================] - 10s 291ms/step - loss: 0.6040 - accuracy: 0.7083 - val_loss: 0.6301 - val_accuracy: 0.6790\n",
      "Epoch 144/200\n",
      "34/34 [==============================] - 9s 278ms/step - loss: 0.6036 - accuracy: 0.7083 - val_loss: 0.6302 - val_accuracy: 0.6790\n",
      "Epoch 145/200\n",
      "34/34 [==============================] - 10s 279ms/step - loss: 0.6044 - accuracy: 0.7083 - val_loss: 0.6297 - val_accuracy: 0.6790\n",
      "Epoch 146/200\n",
      "34/34 [==============================] - 9s 277ms/step - loss: 0.6040 - accuracy: 0.7083 - val_loss: 0.6301 - val_accuracy: 0.6790\n",
      "Epoch 147/200\n",
      "34/34 [==============================] - 10s 281ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6297 - val_accuracy: 0.6790\n",
      "Epoch 148/200\n",
      "34/34 [==============================] - 10s 298ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6298 - val_accuracy: 0.6790\n",
      "Epoch 149/200\n",
      "34/34 [==============================] - 10s 293ms/step - loss: 0.6043 - accuracy: 0.7083 - val_loss: 0.6298 - val_accuracy: 0.6790\n",
      "Epoch 150/200\n",
      "34/34 [==============================] - 13s 369ms/step - loss: 0.6044 - accuracy: 0.7083 - val_loss: 0.6293 - val_accuracy: 0.6790\n",
      "Epoch 151/200\n",
      "34/34 [==============================] - 14s 405ms/step - loss: 0.6037 - accuracy: 0.7083 - val_loss: 0.6303 - val_accuracy: 0.6790\n",
      "Epoch 152/200\n",
      "34/34 [==============================] - 10s 306ms/step - loss: 0.6035 - accuracy: 0.7083 - val_loss: 0.6299 - val_accuracy: 0.6790\n",
      "Epoch 153/200\n",
      "34/34 [==============================] - 9s 273ms/step - loss: 0.6045 - accuracy: 0.7083 - val_loss: 0.6303 - val_accuracy: 0.6790\n",
      "Epoch 154/200\n",
      "34/34 [==============================] - 10s 282ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6298 - val_accuracy: 0.6790\n",
      "Epoch 155/200\n",
      "34/34 [==============================] - 9s 275ms/step - loss: 0.6037 - accuracy: 0.7083 - val_loss: 0.6294 - val_accuracy: 0.6790\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 9s 271ms/step - loss: 0.6036 - accuracy: 0.7083 - val_loss: 0.6299 - val_accuracy: 0.6790\n",
      "Epoch 157/200\n",
      "34/34 [==============================] - 9s 272ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6301 - val_accuracy: 0.6790\n",
      "Epoch 158/200\n",
      "34/34 [==============================] - 9s 275ms/step - loss: 0.6043 - accuracy: 0.7083 - val_loss: 0.6291 - val_accuracy: 0.6790\n",
      "Epoch 159/200\n",
      "34/34 [==============================] - 9s 273ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6296 - val_accuracy: 0.6790\n",
      "Epoch 160/200\n",
      "34/34 [==============================] - 9s 275ms/step - loss: 0.6040 - accuracy: 0.7083 - val_loss: 0.6303 - val_accuracy: 0.6790\n",
      "Epoch 161/200\n",
      "34/34 [==============================] - 9s 272ms/step - loss: 0.6042 - accuracy: 0.7083 - val_loss: 0.6297 - val_accuracy: 0.6790\n",
      "Epoch 162/200\n",
      "34/34 [==============================] - 10s 284ms/step - loss: 0.6042 - accuracy: 0.7083 - val_loss: 0.6301 - val_accuracy: 0.6790\n",
      "Epoch 163/200\n",
      "34/34 [==============================] - 9s 271ms/step - loss: 0.6036 - accuracy: 0.7083 - val_loss: 0.6298 - val_accuracy: 0.6790\n",
      "Epoch 164/200\n",
      "34/34 [==============================] - 9s 275ms/step - loss: 0.6040 - accuracy: 0.7083 - val_loss: 0.6301 - val_accuracy: 0.6790\n",
      "Epoch 165/200\n",
      "34/34 [==============================] - 9s 275ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6299 - val_accuracy: 0.6790\n",
      "Epoch 166/200\n",
      "34/34 [==============================] - 9s 271ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6294 - val_accuracy: 0.6790\n",
      "Epoch 167/200\n",
      "34/34 [==============================] - 11s 324ms/step - loss: 0.6052 - accuracy: 0.7083 - val_loss: 0.6307 - val_accuracy: 0.6790\n",
      "Epoch 168/200\n",
      "34/34 [==============================] - 10s 283ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6291 - val_accuracy: 0.6790\n",
      "Epoch 169/200\n",
      "34/34 [==============================] - 10s 288ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6293 - val_accuracy: 0.6790\n",
      "Epoch 170/200\n",
      "34/34 [==============================] - 10s 298ms/step - loss: 0.6037 - accuracy: 0.7083 - val_loss: 0.6297 - val_accuracy: 0.6790\n",
      "Epoch 171/200\n",
      "34/34 [==============================] - 10s 300ms/step - loss: 0.6037 - accuracy: 0.7083 - val_loss: 0.6296 - val_accuracy: 0.6790\n",
      "Epoch 172/200\n",
      "34/34 [==============================] - 10s 287ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6294 - val_accuracy: 0.6790\n",
      "Epoch 173/200\n",
      "34/34 [==============================] - 10s 294ms/step - loss: 0.6042 - accuracy: 0.7083 - val_loss: 0.6292 - val_accuracy: 0.6790\n",
      "Epoch 174/200\n",
      "34/34 [==============================] - 10s 306ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6301 - val_accuracy: 0.6790\n",
      "Epoch 175/200\n",
      "34/34 [==============================] - 9s 277ms/step - loss: 0.6037 - accuracy: 0.7083 - val_loss: 0.6293 - val_accuracy: 0.6790\n",
      "Epoch 176/200\n",
      "34/34 [==============================] - 10s 292ms/step - loss: 0.6040 - accuracy: 0.7083 - val_loss: 0.6298 - val_accuracy: 0.6790\n",
      "Epoch 177/200\n",
      "34/34 [==============================] - 11s 314ms/step - loss: 0.6043 - accuracy: 0.7083 - val_loss: 0.6296 - val_accuracy: 0.6790\n",
      "Epoch 178/200\n",
      "34/34 [==============================] - 10s 286ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6296 - val_accuracy: 0.6790\n",
      "Epoch 179/200\n",
      "34/34 [==============================] - 10s 293ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6294 - val_accuracy: 0.6790\n",
      "Epoch 180/200\n",
      "34/34 [==============================] - 9s 276ms/step - loss: 0.6045 - accuracy: 0.7083 - val_loss: 0.6295 - val_accuracy: 0.6790\n",
      "Epoch 181/200\n",
      "34/34 [==============================] - 9s 267ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6302 - val_accuracy: 0.6790\n",
      "Epoch 182/200\n",
      "34/34 [==============================] - 9s 260ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6298 - val_accuracy: 0.6790\n",
      "Epoch 183/200\n",
      "34/34 [==============================] - 10s 300ms/step - loss: 0.6035 - accuracy: 0.7083 - val_loss: 0.6295 - val_accuracy: 0.6790\n",
      "Epoch 184/200\n",
      "34/34 [==============================] - 10s 300ms/step - loss: 0.6046 - accuracy: 0.7083 - val_loss: 0.6289 - val_accuracy: 0.6790\n",
      "Epoch 185/200\n",
      "34/34 [==============================] - 10s 298ms/step - loss: 0.6041 - accuracy: 0.7083 - val_loss: 0.6302 - val_accuracy: 0.6790\n",
      "Epoch 186/200\n",
      "34/34 [==============================] - 9s 279ms/step - loss: 0.6040 - accuracy: 0.7083 - val_loss: 0.6297 - val_accuracy: 0.6790\n",
      "Epoch 187/200\n",
      "34/34 [==============================] - 10s 297ms/step - loss: 0.6040 - accuracy: 0.7083 - val_loss: 0.6296 - val_accuracy: 0.6790\n",
      "Epoch 188/200\n",
      "34/34 [==============================] - 10s 295ms/step - loss: 0.6042 - accuracy: 0.7083 - val_loss: 0.6295 - val_accuracy: 0.6790\n",
      "Epoch 189/200\n",
      "34/34 [==============================] - 10s 299ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6300 - val_accuracy: 0.6790\n",
      "Epoch 190/200\n",
      "34/34 [==============================] - 10s 296ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6297 - val_accuracy: 0.6790\n",
      "Epoch 191/200\n",
      "34/34 [==============================] - 10s 300ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6299 - val_accuracy: 0.6790\n",
      "Epoch 192/200\n",
      "34/34 [==============================] - 10s 287ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6294 - val_accuracy: 0.6790\n",
      "Epoch 193/200\n",
      "34/34 [==============================] - 9s 278ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6293 - val_accuracy: 0.6790\n",
      "Epoch 194/200\n",
      "34/34 [==============================] - 10s 291ms/step - loss: 0.6037 - accuracy: 0.7083 - val_loss: 0.6298 - val_accuracy: 0.6790\n",
      "Epoch 195/200\n",
      "34/34 [==============================] - 9s 274ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6301 - val_accuracy: 0.6790\n",
      "Epoch 196/200\n",
      "34/34 [==============================] - 10s 302ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6297 - val_accuracy: 0.6790\n",
      "Epoch 197/200\n",
      "34/34 [==============================] - 10s 301ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6299 - val_accuracy: 0.6790\n",
      "Epoch 198/200\n",
      "34/34 [==============================] - 10s 287ms/step - loss: 0.6040 - accuracy: 0.7083 - val_loss: 0.6291 - val_accuracy: 0.6790\n",
      "Epoch 199/200\n",
      "34/34 [==============================] - 10s 296ms/step - loss: 0.6038 - accuracy: 0.7083 - val_loss: 0.6295 - val_accuracy: 0.6790\n",
      "Epoch 200/200\n",
      "34/34 [==============================] - 10s 290ms/step - loss: 0.6039 - accuracy: 0.7083 - val_loss: 0.6302 - val_accuracy: 0.6790\n",
      "Fold 1, 200 epochs, 2009 sec\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 67.90%\n",
      "\n",
      "Validation core mean 67.90% (+/- 0.00%)\n",
      "INFO:tensorflow:Assets written to: CNN101.long.model/assets\n"
     ]
    }
   ],
   "source": [
    "MINLEN=2000\n",
    "MAXLEN=3000\n",
    "\n",
    "print(\"Working on full training set, slice by sequence length.\")\n",
    "print(\"Slice size range [%d - %d)\"%(MINLEN,MAXLEN))\n",
    "subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
    "\n",
    "print (\"Sequence to Kmer\")\n",
    "(X_train,y_train)=make_kmers(MINLEN,MAXLEN,subset)\n",
    "X_train=onehot(X_train)\n",
    "print (\"Compile the model\")\n",
    "model=build_model(MAXLEN,EMBED_DIMEN)\n",
    "print(model.summary())  # Print this only once\n",
    "print (\"Cross valiation\")\n",
    "model3=do_cross_validation(X_train,y_train,EPOCHS,MAXLEN,EMBED_DIMEN)\n",
    "model3.save(FILENAME+'.long.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
