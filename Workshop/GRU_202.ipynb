{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "GRU_202.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojm_6E9f9Kcf"
      },
      "source": [
        "# GRU 202\n",
        "* Operate on 16000 GenCode 34 seqs.\n",
        "* 5-way cross validation. Save best model per CV.\n",
        "* Report mean accuracy from final re-validation with best 5.\n",
        "* Use Adam with a learn rate decay schdule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh6XplUvC0j0",
        "outputId": "25c45f56-d412-46e4-c2e3-69fb345aff8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "NC_FILENAME='ncRNA.gc34.processed.fasta'\n",
        "PC_FILENAME='pcRNA.gc34.processed.fasta'\n",
        "DATAPATH=\"\"\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    PATH='/content/drive/'\n",
        "    drive.mount(PATH)\n",
        "    DATAPATH=PATH+'My Drive/data/'  # must end in \"/\"\n",
        "    NC_FILENAME = DATAPATH+NC_FILENAME\n",
        "    PC_FILENAME = DATAPATH+PC_FILENAME\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    DATAPATH=\"\" \n",
        "\n",
        "EPOCHS=200\n",
        "SPLITS=5\n",
        "K=3\n",
        "VOCABULARY_SIZE=4**K+1   # e.g. K=3 => 64 DNA K-mers + 'NNN'\n",
        "EMBED_DIMEN=16\n",
        "FILENAME='GRU202'\n",
        "NEURONS=16"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQY7aTj29Kch"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LayerNormalization\n",
        "import time\n",
        "dt='float32'\n",
        "tf.keras.backend.set_floatx(dt)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7jcg6Wl9Kc2"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLFNO1Xa9Kc3"
      },
      "source": [
        "def compile_model(model):\n",
        "    adam_default_learn_rate = 0.001\n",
        "    schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate = adam_default_learn_rate*10,\n",
        "        #decay_steps=100000, decay_rate=0.96, staircase=True)\n",
        "        decay_steps=10000, decay_rate=0.99, staircase=True)\n",
        "    # learn rate = initial_learning_rate * decay_rate ^ (step / decay_steps)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=schedule)\n",
        "    bc=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    print(\"COMPILE...\")\n",
        "    model.compile(loss=bc, optimizer=opt, metrics=[\"accuracy\"])\n",
        "    print(\"...COMPILED\")\n",
        "    return model\n",
        "\n",
        "def build_model():\n",
        "    act=\"elu\"\n",
        "    embed_layer  = keras.layers.Embedding(mask_zero=True,\n",
        "        input_dim=VOCABULARY_SIZE, output_dim=EMBED_DIMEN)\n",
        "    rnn1_layer = keras.layers.Bidirectional(\n",
        "        keras.layers.GRU(NEURONS, return_sequences=True, \n",
        "                         dropout=0.00, activation=act))\n",
        "    rnn2_layer = keras.layers.Bidirectional(\n",
        "        keras.layers.GRU(NEURONS, dropout=0.00, return_sequences=False, activation=act)) \n",
        "    dense1_layer = keras.layers.Dense(NEURONS, activation=act,dtype=dt)\n",
        "    dense2_layer = keras.layers.Dense(NEURONS, activation=act,dtype=dt)\n",
        "    output_layer = keras.layers.Dense(1,  activation=\"sigmoid\",dtype=dt)\n",
        "    mlp = keras.models.Sequential()\n",
        "    mlp.add(embed_layer)\n",
        "    mlp.add(rnn1_layer)\n",
        "    mlp.add(rnn2_layer)\n",
        "    mlp.add(dense1_layer)\n",
        "    mlp.add(dense2_layer)\n",
        "    mlp.add(output_layer)\n",
        "    mlpc = compile_model(mlp)\n",
        "    return mlpc"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV6k-xOm9Kcn"
      },
      "source": [
        "## Load and partition sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I-O_qzw9Kco"
      },
      "source": [
        "# Assume file was preprocessed to contain one line per seq.\n",
        "# Prefer Pandas dataframe but df does not support append.\n",
        "# For conversion to tensor, must avoid python lists.\n",
        "def load_fasta(filename,label):\n",
        "    DEFLINE='>'\n",
        "    labels=[]\n",
        "    seqs=[]\n",
        "    lens=[]\n",
        "    nums=[]\n",
        "    num=0\n",
        "    with open (filename,'r') as infile:\n",
        "        for line in infile:\n",
        "            if line[0]!=DEFLINE:\n",
        "                seq=line.rstrip()\n",
        "                num += 1   # first seqnum is 1\n",
        "                seqlen=len(seq)\n",
        "                nums.append(num)\n",
        "                labels.append(label)\n",
        "                seqs.append(seq)\n",
        "                lens.append(seqlen)\n",
        "    df1=pd.DataFrame(nums,columns=['seqnum'])\n",
        "    df2=pd.DataFrame(labels,columns=['class'])\n",
        "    df3=pd.DataFrame(seqs,columns=['sequence'])\n",
        "    df4=pd.DataFrame(lens,columns=['seqlen'])\n",
        "    df=pd.concat((df1,df2,df3,df4),axis=1)\n",
        "    return df\n",
        "\n",
        "def separate_X_and_y(data):\n",
        "    y=   data[['class']].copy()\n",
        "    X=   data.drop(columns=['class','seqnum','seqlen'])\n",
        "    return (X,y)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRAaO9jP9Kcr"
      },
      "source": [
        "## Make K-mers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8xcZ4Mr9Kcs"
      },
      "source": [
        "def make_kmer_table(K):\n",
        "    npad='N'*K\n",
        "    shorter_kmers=['']\n",
        "    for i in range(K):\n",
        "        longer_kmers=[]\n",
        "        for mer in shorter_kmers:\n",
        "            longer_kmers.append(mer+'A')\n",
        "            longer_kmers.append(mer+'C')\n",
        "            longer_kmers.append(mer+'G')\n",
        "            longer_kmers.append(mer+'T')\n",
        "        shorter_kmers = longer_kmers\n",
        "    all_kmers = shorter_kmers\n",
        "    kmer_dict = {}\n",
        "    kmer_dict[npad]=0\n",
        "    value=1\n",
        "    for mer in all_kmers:\n",
        "        kmer_dict[mer]=value\n",
        "        value += 1\n",
        "    return kmer_dict\n",
        "\n",
        "KMER_TABLE=make_kmer_table(K)\n",
        "\n",
        "def strings_to_vectors(data,uniform_len):\n",
        "    all_seqs=[]\n",
        "    for seq in data['sequence']:\n",
        "        i=0\n",
        "        seqlen=len(seq)\n",
        "        kmers=[]\n",
        "        while i < seqlen-K+1 -1:  # stop at minus one for spaced seed\n",
        "            #kmer=seq[i:i+2]+seq[i+3:i+5]    # SPACED SEED 2/1/2 for K=4\n",
        "            kmer=seq[i:i+K]  \n",
        "            i += 1\n",
        "            value=KMER_TABLE[kmer]\n",
        "            kmers.append(value)\n",
        "        pad_val=0\n",
        "        while i < uniform_len:\n",
        "            kmers.append(pad_val)\n",
        "            i += 1\n",
        "        all_seqs.append(kmers)\n",
        "    pd2d=pd.DataFrame(all_seqs)\n",
        "    return pd2d   # return 2D dataframe, uniform dimensions"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEtA0xiV9Kcv"
      },
      "source": [
        "def make_kmers(MAXLEN,train_set):\n",
        "    (X_train_all,y_train_all)=separate_X_and_y(train_set)\n",
        "    X_train_kmers=strings_to_vectors(X_train_all,MAXLEN)\n",
        "    # From pandas dataframe to numpy to list to numpy\n",
        "    num_seqs=len(X_train_kmers)\n",
        "    tmp_seqs=[]\n",
        "    for i in range(num_seqs):\n",
        "        kmer_sequence=X_train_kmers.iloc[i]\n",
        "        tmp_seqs.append(kmer_sequence)\n",
        "    X_train_kmers=np.array(tmp_seqs)\n",
        "    tmp_seqs=None\n",
        "    labels=y_train_all.to_numpy()\n",
        "    return (X_train_kmers,labels)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaXyySyO9Kcz"
      },
      "source": [
        "def make_frequencies(Xin):\n",
        "    Xout=[]\n",
        "    VOCABULARY_SIZE= 4**K + 1  # plus one for 'NNN'\n",
        "    for seq in Xin:\n",
        "        freqs =[0] * VOCABULARY_SIZE\n",
        "        total = 0\n",
        "        for kmerval in seq:\n",
        "            freqs[kmerval] += 1\n",
        "            total += 1\n",
        "        for c in range(VOCABULARY_SIZE):\n",
        "            freqs[c] = freqs[c]/total\n",
        "        Xout.append(freqs)\n",
        "    Xnum = np.asarray(Xout)\n",
        "    return (Xnum)\n",
        "def make_slice(data_set,min_len,max_len):\n",
        "    slice = data_set.query('seqlen <= '+str(max_len)+' & seqlen>= '+str(min_len))\n",
        "    return slice"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdIS2utq9Kc9"
      },
      "source": [
        "## Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVo4tbB_9Kc-"
      },
      "source": [
        "def do_cross_validation(X,y,given_model):\n",
        "    cv_scores = []\n",
        "    fold=0\n",
        "    splitter = ShuffleSplit(n_splits=SPLITS, test_size=0.1, random_state=37863)\n",
        "    for train_index,valid_index in splitter.split(X):\n",
        "        fold += 1\n",
        "        X_train=X[train_index] # use iloc[] for dataframe\n",
        "        y_train=y[train_index]\n",
        "        X_valid=X[valid_index]\n",
        "        y_valid=y[valid_index]        \n",
        "        # Avoid continually improving the same model.\n",
        "        model = compile_model(keras.models.clone_model(given_model))\n",
        "        bestname=DATAPATH+FILENAME+\".cv.\"+str(fold)+\".best\"\n",
        "        mycallbacks = [keras.callbacks.ModelCheckpoint(\n",
        "            filepath=bestname, save_best_only=True, \n",
        "            monitor='val_accuracy', mode='max')]   \n",
        "        print(\"FIT\")\n",
        "        start_time=time.time()\n",
        "        history=model.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
        "                epochs=EPOCHS, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
        "                callbacks=mycallbacks,\n",
        "                validation_data=(X_valid,y_valid) )\n",
        "        end_time=time.time()\n",
        "        elapsed_time=(end_time-start_time)                        \n",
        "        print(\"Fold %d, %d epochs, %d sec\"%(fold,EPOCHS,elapsed_time))\n",
        "        pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "        plt.grid(True)\n",
        "        plt.gca().set_ylim(0,1)\n",
        "        plt.show()\n",
        "        best_model=keras.models.load_model(bestname)\n",
        "        scores = best_model.evaluate(X_valid, y_valid, verbose=0)\n",
        "        print(\"%s: %.2f%%\" % (best_model.metrics_names[1], scores[1]*100))\n",
        "        cv_scores.append(scores[1] * 100)  \n",
        "    print()\n",
        "    print(\"%d-way Cross Validation mean %.2f%% (+/- %.2f%%)\" % (fold, np.mean(cv_scores), np.std(cv_scores)))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd3Wj_vI9KdP"
      },
      "source": [
        "## Train on RNA lengths 200-1Kb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8fNo6sn9KdH",
        "outputId": "36791683-2f8e-4693-b990-42dedbef0647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        }
      },
      "source": [
        "MINLEN=200\n",
        "MAXLEN=1000\n",
        "print(\"Load data from files.\")\n",
        "nc_seq=load_fasta(NC_FILENAME,0)\n",
        "pc_seq=load_fasta(PC_FILENAME,1)\n",
        "train_set=pd.concat((nc_seq,pc_seq),axis=0)\n",
        "nc_seq=None\n",
        "pc_seq=None\n",
        "print(\"Ready: train_set\")\n",
        "#train_set\n",
        "print (\"Compile the model\")\n",
        "model=build_model()\n",
        "print (\"Summarize the model\")\n",
        "print(model.summary())  # Print this only once\n",
        "model.save(DATAPATH+FILENAME+'.model')\n",
        "print (\"Data prep\")\n",
        "subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
        "print (\"Data reshape\")\n",
        "(X_train,y_train)=make_kmers(MAXLEN,subset)\n",
        "X_train=make_frequencies(X_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load data from files.\n",
            "Ready: train_set\n",
            "Compile the model\n",
            "COMPILE...\n",
            "...COMPILED\n",
            "Summarize the model\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 16)          1040      \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, None, 32)          3264      \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 32)                4800      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 9,921\n",
            "Trainable params: 9,921\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/data/GRU202.model/assets\n",
            "Data prep\n",
            "Data reshape\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ8eW5Rg9KdQ",
        "outputId": "232c5ac0-8ce7-4721-a183-1daea6f0a094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print (\"Cross valiation\")\n",
        "do_cross_validation(X_train,y_train,model)  \n",
        "print (\"Done\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross valiation\n",
            "COMPILE...\n",
            "...COMPILED\n",
            "FIT\n",
            "Epoch 1/200\n",
            "453/453 [==============================] - ETA: 0s - loss: 0.6557 - accuracy: 0.6384INFO:tensorflow:Assets written to: /content/drive/My Drive/data/GRU202.cv.1.best/assets\n",
            "453/453 [==============================] - 37s 82ms/step - loss: 0.6557 - accuracy: 0.6384 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 2/200\n",
            "453/453 [==============================] - 30s 65ms/step - loss: 0.6549 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 3/200\n",
            "453/453 [==============================] - 30s 65ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 4/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6544 - accuracy: 0.6397 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
            "Epoch 5/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6543 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 6/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 7/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6487 - val_accuracy: 0.6530\n",
            "Epoch 8/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 9/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 10/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6543 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 11/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6479 - val_accuracy: 0.6530\n",
            "Epoch 12/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6468 - val_accuracy: 0.6530\n",
            "Epoch 13/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
            "Epoch 14/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6601 - accuracy: 0.6333 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 15/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6597 - accuracy: 0.6397 - val_loss: 0.6594 - val_accuracy: 0.6530\n",
            "Epoch 16/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6578 - accuracy: 0.6397 - val_loss: 0.6532 - val_accuracy: 0.6530\n",
            "Epoch 17/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6580 - accuracy: 0.6397 - val_loss: 0.6532 - val_accuracy: 0.6530\n",
            "Epoch 18/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6563 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 19/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6563 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 20/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6579 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 21/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6562 - accuracy: 0.6397 - val_loss: 0.6488 - val_accuracy: 0.6530\n",
            "Epoch 22/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6563 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 23/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6573 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 24/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6565 - accuracy: 0.6397 - val_loss: 0.6473 - val_accuracy: 0.6530\n",
            "Epoch 25/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6555 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 26/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6568 - accuracy: 0.6397 - val_loss: 0.6535 - val_accuracy: 0.6530\n",
            "Epoch 27/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6555 - accuracy: 0.6397 - val_loss: 0.6494 - val_accuracy: 0.6530\n",
            "Epoch 28/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6585 - accuracy: 0.6397 - val_loss: 0.6548 - val_accuracy: 0.6530\n",
            "Epoch 29/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6568 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 30/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6580 - accuracy: 0.6397 - val_loss: 0.6491 - val_accuracy: 0.6530\n",
            "Epoch 31/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6565 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 32/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6570 - accuracy: 0.6397 - val_loss: 0.6533 - val_accuracy: 0.6530\n",
            "Epoch 33/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6565 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 34/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6576 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 35/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6567 - accuracy: 0.6397 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
            "Epoch 36/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6589 - accuracy: 0.6365 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 37/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6554 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 38/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6567 - accuracy: 0.6397 - val_loss: 0.6498 - val_accuracy: 0.6530\n",
            "Epoch 39/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6562 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 40/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6576 - accuracy: 0.6397 - val_loss: 0.6688 - val_accuracy: 0.6530\n",
            "Epoch 41/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6571 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 42/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6566 - accuracy: 0.6397 - val_loss: 0.6496 - val_accuracy: 0.6530\n",
            "Epoch 43/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6561 - accuracy: 0.6397 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 44/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6581 - accuracy: 0.6397 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 45/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6563 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 46/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6581 - accuracy: 0.6397 - val_loss: 0.6486 - val_accuracy: 0.6530\n",
            "Epoch 47/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6568 - accuracy: 0.6397 - val_loss: 0.6505 - val_accuracy: 0.6530\n",
            "Epoch 48/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6564 - accuracy: 0.6397 - val_loss: 0.6483 - val_accuracy: 0.6530\n",
            "Epoch 49/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6558 - accuracy: 0.6397 - val_loss: 0.6491 - val_accuracy: 0.6530\n",
            "Epoch 50/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6557 - accuracy: 0.6397 - val_loss: 0.6523 - val_accuracy: 0.6530\n",
            "Epoch 51/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6569 - accuracy: 0.6397 - val_loss: 0.6625 - val_accuracy: 0.6530\n",
            "Epoch 52/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6561 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 53/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6585 - accuracy: 0.6397 - val_loss: 0.6505 - val_accuracy: 0.6530\n",
            "Epoch 54/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6573 - accuracy: 0.6397 - val_loss: 0.6540 - val_accuracy: 0.6530\n",
            "Epoch 55/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6562 - accuracy: 0.6397 - val_loss: 0.6484 - val_accuracy: 0.6530\n",
            "Epoch 56/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6570 - accuracy: 0.6397 - val_loss: 0.6569 - val_accuracy: 0.6530\n",
            "Epoch 57/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6564 - accuracy: 0.6397 - val_loss: 0.6479 - val_accuracy: 0.6530\n",
            "Epoch 58/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6576 - accuracy: 0.6397 - val_loss: 0.6499 - val_accuracy: 0.6530\n",
            "Epoch 59/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6571 - accuracy: 0.6397 - val_loss: 0.6510 - val_accuracy: 0.6530\n",
            "Epoch 60/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6561 - accuracy: 0.6397 - val_loss: 0.6604 - val_accuracy: 0.6530\n",
            "Epoch 61/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6573 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 62/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6561 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 63/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6575 - accuracy: 0.6397 - val_loss: 0.6468 - val_accuracy: 0.6530\n",
            "Epoch 64/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6577 - accuracy: 0.6397 - val_loss: 0.6477 - val_accuracy: 0.6530\n",
            "Epoch 65/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6571 - accuracy: 0.6397 - val_loss: 0.6481 - val_accuracy: 0.6530\n",
            "Epoch 66/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6567 - accuracy: 0.6397 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
            "Epoch 67/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6566 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 68/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6571 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 69/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6570 - accuracy: 0.6397 - val_loss: 0.6476 - val_accuracy: 0.6530\n",
            "Epoch 70/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6555 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 71/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6569 - accuracy: 0.6397 - val_loss: 0.6472 - val_accuracy: 0.6530\n",
            "Epoch 72/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6560 - accuracy: 0.6397 - val_loss: 0.6606 - val_accuracy: 0.6530\n",
            "Epoch 73/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6576 - accuracy: 0.6397 - val_loss: 0.6475 - val_accuracy: 0.6530\n",
            "Epoch 74/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6570 - accuracy: 0.6397 - val_loss: 0.6470 - val_accuracy: 0.6530\n",
            "Epoch 75/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6570 - accuracy: 0.6397 - val_loss: 0.6471 - val_accuracy: 0.6530\n",
            "Epoch 76/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6587 - accuracy: 0.6397 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 77/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6568 - accuracy: 0.6397 - val_loss: 0.6574 - val_accuracy: 0.6530\n",
            "Epoch 78/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6563 - accuracy: 0.6397 - val_loss: 0.6562 - val_accuracy: 0.6530\n",
            "Epoch 79/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6559 - accuracy: 0.6397 - val_loss: 0.6588 - val_accuracy: 0.6530\n",
            "Epoch 80/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6562 - accuracy: 0.6397 - val_loss: 0.6480 - val_accuracy: 0.6530\n",
            "Epoch 81/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6574 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 82/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6558 - accuracy: 0.6397 - val_loss: 0.6476 - val_accuracy: 0.6530\n",
            "Epoch 83/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6563 - accuracy: 0.6397 - val_loss: 0.6480 - val_accuracy: 0.6530\n",
            "Epoch 84/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6575 - accuracy: 0.6397 - val_loss: 0.6491 - val_accuracy: 0.6530\n",
            "Epoch 85/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6573 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 86/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6560 - accuracy: 0.6397 - val_loss: 0.6489 - val_accuracy: 0.6530\n",
            "Epoch 87/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6570 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 88/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6567 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 89/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6584 - accuracy: 0.6397 - val_loss: 0.6501 - val_accuracy: 0.6530\n",
            "Epoch 90/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6560 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 91/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6587 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 92/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6577 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 93/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6563 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 94/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6557 - accuracy: 0.6397 - val_loss: 0.6470 - val_accuracy: 0.6530\n",
            "Epoch 95/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6563 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 96/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6569 - accuracy: 0.6397 - val_loss: 0.6499 - val_accuracy: 0.6530\n",
            "Epoch 97/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6564 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 98/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6568 - accuracy: 0.6397 - val_loss: 0.6468 - val_accuracy: 0.6530\n",
            "Epoch 99/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6584 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 100/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6581 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 101/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6576 - accuracy: 0.6397 - val_loss: 0.6547 - val_accuracy: 0.6530\n",
            "Epoch 102/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6564 - accuracy: 0.6397 - val_loss: 0.6486 - val_accuracy: 0.6530\n",
            "Epoch 103/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6563 - accuracy: 0.6397 - val_loss: 0.6504 - val_accuracy: 0.6530\n",
            "Epoch 104/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6569 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 105/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6576 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 106/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6567 - accuracy: 0.6397 - val_loss: 0.6712 - val_accuracy: 0.6530\n",
            "Epoch 107/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6574 - accuracy: 0.6397 - val_loss: 0.6479 - val_accuracy: 0.6530\n",
            "Epoch 108/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6571 - accuracy: 0.6397 - val_loss: 0.6636 - val_accuracy: 0.6530\n",
            "Epoch 109/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6572 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 110/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6567 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 111/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6563 - accuracy: 0.6397 - val_loss: 0.6498 - val_accuracy: 0.6530\n",
            "Epoch 112/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6575 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 113/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6572 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 114/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6571 - accuracy: 0.6397 - val_loss: 0.6520 - val_accuracy: 0.6530\n",
            "Epoch 115/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6560 - accuracy: 0.6397 - val_loss: 0.6501 - val_accuracy: 0.6530\n",
            "Epoch 116/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6571 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 117/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6564 - accuracy: 0.6397 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
            "Epoch 118/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6576 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 119/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6571 - accuracy: 0.6397 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 120/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6569 - accuracy: 0.6397 - val_loss: 0.6508 - val_accuracy: 0.6530\n",
            "Epoch 121/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6567 - accuracy: 0.6397 - val_loss: 0.6549 - val_accuracy: 0.6530\n",
            "Epoch 122/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6597 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 123/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6559 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 124/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6562 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 125/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6557 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 126/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6574 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 127/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6554 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 128/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6586 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 129/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6570 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 130/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6561 - accuracy: 0.6397 - val_loss: 0.6520 - val_accuracy: 0.6530\n",
            "Epoch 131/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6577 - accuracy: 0.6397 - val_loss: 0.6555 - val_accuracy: 0.6530\n",
            "Epoch 132/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6552 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 133/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6561 - accuracy: 0.6397 - val_loss: 0.6648 - val_accuracy: 0.6530\n",
            "Epoch 134/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6574 - accuracy: 0.6397 - val_loss: 0.6693 - val_accuracy: 0.6530\n",
            "Epoch 135/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6570 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 136/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6570 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 137/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6562 - accuracy: 0.6397 - val_loss: 0.6479 - val_accuracy: 0.6530\n",
            "Epoch 138/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6589 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 139/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6569 - accuracy: 0.6397 - val_loss: 0.6476 - val_accuracy: 0.6530\n",
            "Epoch 140/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6573 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 141/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6558 - accuracy: 0.6397 - val_loss: 0.6472 - val_accuracy: 0.6530\n",
            "Epoch 142/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6569 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 143/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6583 - accuracy: 0.6397 - val_loss: 0.6531 - val_accuracy: 0.6530\n",
            "Epoch 144/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6563 - accuracy: 0.6397 - val_loss: 0.6543 - val_accuracy: 0.6530\n",
            "Epoch 145/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6561 - accuracy: 0.6397 - val_loss: 0.6492 - val_accuracy: 0.6530\n",
            "Epoch 146/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6579 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 147/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6564 - accuracy: 0.6397 - val_loss: 0.6689 - val_accuracy: 0.6530\n",
            "Epoch 148/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6574 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 149/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6567 - accuracy: 0.6397 - val_loss: 0.6529 - val_accuracy: 0.6530\n",
            "Epoch 150/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6568 - accuracy: 0.6397 - val_loss: 0.6607 - val_accuracy: 0.6530\n",
            "Epoch 151/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6581 - accuracy: 0.6397 - val_loss: 0.6500 - val_accuracy: 0.6530\n",
            "Epoch 152/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6556 - accuracy: 0.6397 - val_loss: 0.6494 - val_accuracy: 0.6530\n",
            "Epoch 153/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6577 - accuracy: 0.6397 - val_loss: 0.6482 - val_accuracy: 0.6530\n",
            "Epoch 154/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6570 - accuracy: 0.6397 - val_loss: 0.6484 - val_accuracy: 0.6530\n",
            "Epoch 155/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6568 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 156/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6560 - accuracy: 0.6397 - val_loss: 0.6474 - val_accuracy: 0.6530\n",
            "Epoch 157/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6559 - accuracy: 0.6397 - val_loss: 0.6473 - val_accuracy: 0.6530\n",
            "Epoch 158/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6572 - accuracy: 0.6397 - val_loss: 0.6470 - val_accuracy: 0.6530\n",
            "Epoch 159/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6560 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 160/200\n",
            "453/453 [==============================] - 32s 71ms/step - loss: 0.6571 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 161/200\n",
            "453/453 [==============================] - 32s 71ms/step - loss: 0.6569 - accuracy: 0.6397 - val_loss: 0.6474 - val_accuracy: 0.6530\n",
            "Epoch 162/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6568 - accuracy: 0.6397 - val_loss: 0.6469 - val_accuracy: 0.6530\n",
            "Epoch 163/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6569 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 164/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6566 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 165/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6578 - accuracy: 0.6397 - val_loss: 0.6514 - val_accuracy: 0.6530\n",
            "Epoch 166/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6564 - accuracy: 0.6397 - val_loss: 0.6735 - val_accuracy: 0.6530\n",
            "Epoch 167/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6587 - accuracy: 0.6397 - val_loss: 0.6478 - val_accuracy: 0.6530\n",
            "Epoch 168/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6571 - accuracy: 0.6397 - val_loss: 0.6482 - val_accuracy: 0.6530\n",
            "Epoch 169/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6572 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 170/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6575 - accuracy: 0.6397 - val_loss: 0.6473 - val_accuracy: 0.6530\n",
            "Epoch 171/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6576 - accuracy: 0.6397 - val_loss: 0.6475 - val_accuracy: 0.6530\n",
            "Epoch 172/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6557 - accuracy: 0.6397 - val_loss: 0.6497 - val_accuracy: 0.6530\n",
            "Epoch 173/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6560 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 174/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6568 - accuracy: 0.6397 - val_loss: 0.6491 - val_accuracy: 0.6530\n",
            "Epoch 175/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6576 - accuracy: 0.6397 - val_loss: 0.6483 - val_accuracy: 0.6530\n",
            "Epoch 176/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6565 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 177/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6568 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 178/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6569 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 179/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6564 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 180/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6567 - accuracy: 0.6397 - val_loss: 0.6527 - val_accuracy: 0.6530\n",
            "Epoch 181/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6564 - accuracy: 0.6397 - val_loss: 0.6493 - val_accuracy: 0.6530\n",
            "Epoch 182/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6570 - accuracy: 0.6397 - val_loss: 0.6483 - val_accuracy: 0.6530\n",
            "Epoch 183/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6559 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 184/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6567 - accuracy: 0.6397 - val_loss: 0.6480 - val_accuracy: 0.6530\n",
            "Epoch 185/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6570 - accuracy: 0.6397 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
            "Epoch 186/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6562 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 187/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6562 - accuracy: 0.6397 - val_loss: 0.6480 - val_accuracy: 0.6530\n",
            "Epoch 188/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6579 - accuracy: 0.6397 - val_loss: 0.6509 - val_accuracy: 0.6530\n",
            "Epoch 189/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6556 - accuracy: 0.6397 - val_loss: 0.6479 - val_accuracy: 0.6530\n",
            "Epoch 190/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6571 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 191/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6573 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 192/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6559 - accuracy: 0.6397 - val_loss: 0.6492 - val_accuracy: 0.6530\n",
            "Epoch 193/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6563 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 194/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6571 - accuracy: 0.6397 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
            "Epoch 195/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6576 - accuracy: 0.6379 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 196/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6571 - accuracy: 0.6397 - val_loss: 0.6492 - val_accuracy: 0.6530\n",
            "Epoch 197/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6567 - accuracy: 0.6397 - val_loss: 0.6503 - val_accuracy: 0.6530\n",
            "Epoch 198/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6558 - accuracy: 0.6397 - val_loss: 0.6477 - val_accuracy: 0.6530\n",
            "Epoch 199/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6567 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 200/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6583 - accuracy: 0.6397 - val_loss: 0.6493 - val_accuracy: 0.6530\n",
            "Fold 1, 200 epochs, 6155 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 65.30%\n",
            "COMPILE...\n",
            "...COMPILED\n",
            "FIT\n",
            "Epoch 1/200\n",
            "453/453 [==============================] - ETA: 0s - loss: 0.6552 - accuracy: 0.6393INFO:tensorflow:Assets written to: /content/drive/My Drive/data/GRU202.cv.2.best/assets\n",
            "453/453 [==============================] - 39s 86ms/step - loss: 0.6552 - accuracy: 0.6393 - val_loss: 0.6526 - val_accuracy: 0.6449\n",
            "Epoch 2/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6539 - accuracy: 0.6406 - val_loss: 0.6514 - val_accuracy: 0.6449\n",
            "Epoch 3/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6538 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 4/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6540 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 5/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6540 - accuracy: 0.6406 - val_loss: 0.6519 - val_accuracy: 0.6449\n",
            "Epoch 6/200\n",
            "453/453 [==============================] - 29s 65ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6531 - val_accuracy: 0.6449\n",
            "Epoch 7/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 8/200\n",
            "453/453 [==============================] - 30s 65ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
            "Epoch 9/200\n",
            "453/453 [==============================] - 29s 65ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 10/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6553 - val_accuracy: 0.6449\n",
            "Epoch 11/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 12/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 13/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6531 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 14/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6569 - accuracy: 0.6376 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 15/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6580 - accuracy: 0.6405 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 16/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6556 - accuracy: 0.6406 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
            "Epoch 17/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6564 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 18/200\n",
            "453/453 [==============================] - 30s 65ms/step - loss: 0.6548 - accuracy: 0.6406 - val_loss: 0.6534 - val_accuracy: 0.6449\n",
            "Epoch 19/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6552 - accuracy: 0.6388 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 20/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6547 - accuracy: 0.6406 - val_loss: 0.6547 - val_accuracy: 0.6449\n",
            "Epoch 21/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6555 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 22/200\n",
            "453/453 [==============================] - 29s 65ms/step - loss: 0.6519 - accuracy: 0.6387 - val_loss: 0.6496 - val_accuracy: 0.6449\n",
            "Epoch 23/200\n",
            "453/453 [==============================] - 30s 65ms/step - loss: 0.6460 - accuracy: 0.6369 - val_loss: 0.6426 - val_accuracy: 0.6449\n",
            "Epoch 24/200\n",
            "453/453 [==============================] - 30s 65ms/step - loss: 0.6466 - accuracy: 0.6360 - val_loss: 0.6410 - val_accuracy: 0.6449\n",
            "Epoch 25/200\n",
            "453/453 [==============================] - 30s 65ms/step - loss: 0.6456 - accuracy: 0.6391 - val_loss: 0.6482 - val_accuracy: 0.6449\n",
            "Epoch 26/200\n",
            "453/453 [==============================] - 29s 65ms/step - loss: 0.6503 - accuracy: 0.6403 - val_loss: 0.6545 - val_accuracy: 0.6449\n",
            "Epoch 27/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6559 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 28/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6547 - accuracy: 0.6406 - val_loss: 0.6519 - val_accuracy: 0.6449\n",
            "Epoch 29/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6561 - accuracy: 0.6406 - val_loss: 0.6524 - val_accuracy: 0.6449\n",
            "Epoch 30/200\n",
            "453/453 [==============================] - 30s 65ms/step - loss: 0.6553 - accuracy: 0.6388 - val_loss: 0.6521 - val_accuracy: 0.6449\n",
            "Epoch 31/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6555 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 32/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6573 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 33/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6571 - accuracy: 0.6406 - val_loss: 0.6683 - val_accuracy: 0.6449\n",
            "Epoch 34/200\n",
            "453/453 [==============================] - 30s 65ms/step - loss: 0.6566 - accuracy: 0.6406 - val_loss: 0.6560 - val_accuracy: 0.6449\n",
            "Epoch 35/200\n",
            "453/453 [==============================] - 30s 65ms/step - loss: 0.6566 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
            "Epoch 36/200\n",
            "453/453 [==============================] - 29s 65ms/step - loss: 0.6562 - accuracy: 0.6406 - val_loss: 0.6553 - val_accuracy: 0.6449\n",
            "Epoch 37/200\n",
            "453/453 [==============================] - 30s 65ms/step - loss: 0.6563 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 38/200\n",
            "453/453 [==============================] - 29s 65ms/step - loss: 0.6550 - accuracy: 0.6406 - val_loss: 0.6594 - val_accuracy: 0.6449\n",
            "Epoch 39/200\n",
            "453/453 [==============================] - 29s 65ms/step - loss: 0.6553 - accuracy: 0.6406 - val_loss: 0.6530 - val_accuracy: 0.6449\n",
            "Epoch 40/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6547 - accuracy: 0.6406 - val_loss: 0.6544 - val_accuracy: 0.6449\n",
            "Epoch 41/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6552 - accuracy: 0.6406 - val_loss: 0.6531 - val_accuracy: 0.6449\n",
            "Epoch 42/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6570 - accuracy: 0.6406 - val_loss: 0.6550 - val_accuracy: 0.6449\n",
            "Epoch 43/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6571 - accuracy: 0.6406 - val_loss: 0.6521 - val_accuracy: 0.6449\n",
            "Epoch 44/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6561 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 45/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6565 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 46/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6563 - accuracy: 0.6406 - val_loss: 0.6526 - val_accuracy: 0.6449\n",
            "Epoch 47/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6555 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 48/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6558 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 49/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6553 - accuracy: 0.6406 - val_loss: 0.6532 - val_accuracy: 0.6449\n",
            "Epoch 50/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6552 - accuracy: 0.6406 - val_loss: 0.6530 - val_accuracy: 0.6449\n",
            "Epoch 51/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6557 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 52/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6556 - accuracy: 0.6406 - val_loss: 0.6590 - val_accuracy: 0.6449\n",
            "Epoch 53/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6560 - accuracy: 0.6406 - val_loss: 0.6517 - val_accuracy: 0.6449\n",
            "Epoch 54/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6554 - accuracy: 0.6406 - val_loss: 0.6514 - val_accuracy: 0.6449\n",
            "Epoch 55/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6544 - accuracy: 0.6406 - val_loss: 0.6600 - val_accuracy: 0.6449\n",
            "Epoch 56/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6556 - accuracy: 0.6406 - val_loss: 0.6515 - val_accuracy: 0.6449\n",
            "Epoch 57/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6558 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 58/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6551 - accuracy: 0.6406 - val_loss: 0.6529 - val_accuracy: 0.6449\n",
            "Epoch 59/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6551 - accuracy: 0.6406 - val_loss: 0.6513 - val_accuracy: 0.6449\n",
            "Epoch 60/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6550 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 61/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6553 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 62/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6554 - accuracy: 0.6406 - val_loss: 0.6518 - val_accuracy: 0.6449\n",
            "Epoch 63/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6560 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
            "Epoch 64/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6568 - accuracy: 0.6406 - val_loss: 0.6546 - val_accuracy: 0.6449\n",
            "Epoch 65/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6573 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 66/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6553 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 67/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6559 - accuracy: 0.6407 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 68/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6556 - accuracy: 0.6406 - val_loss: 0.6516 - val_accuracy: 0.6449\n",
            "Epoch 69/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6560 - accuracy: 0.6406 - val_loss: 0.6531 - val_accuracy: 0.6449\n",
            "Epoch 70/200\n",
            "453/453 [==============================] - 30s 65ms/step - loss: 0.6562 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 71/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6558 - accuracy: 0.6406 - val_loss: 0.6518 - val_accuracy: 0.6449\n",
            "Epoch 72/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6558 - accuracy: 0.6406 - val_loss: 0.6548 - val_accuracy: 0.6449\n",
            "Epoch 73/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6565 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
            "Epoch 74/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6553 - accuracy: 0.6406 - val_loss: 0.6514 - val_accuracy: 0.6449\n",
            "Epoch 75/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6552 - accuracy: 0.6406 - val_loss: 0.6527 - val_accuracy: 0.6449\n",
            "Epoch 76/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6567 - accuracy: 0.6406 - val_loss: 0.6517 - val_accuracy: 0.6449\n",
            "Epoch 77/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6556 - accuracy: 0.6406 - val_loss: 0.6583 - val_accuracy: 0.6449\n",
            "Epoch 78/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6565 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 79/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6554 - accuracy: 0.6406 - val_loss: 0.6539 - val_accuracy: 0.6449\n",
            "Epoch 80/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6552 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 81/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6557 - accuracy: 0.6406 - val_loss: 0.6525 - val_accuracy: 0.6449\n",
            "Epoch 82/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6563 - accuracy: 0.6406 - val_loss: 0.6513 - val_accuracy: 0.6449\n",
            "Epoch 83/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6554 - accuracy: 0.6406 - val_loss: 0.6522 - val_accuracy: 0.6449\n",
            "Epoch 84/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6561 - accuracy: 0.6406 - val_loss: 0.6610 - val_accuracy: 0.6449\n",
            "Epoch 85/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6553 - accuracy: 0.6406 - val_loss: 0.6523 - val_accuracy: 0.6449\n",
            "Epoch 86/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6561 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 87/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6559 - accuracy: 0.6406 - val_loss: 0.6585 - val_accuracy: 0.6449\n",
            "Epoch 88/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6564 - accuracy: 0.6406 - val_loss: 0.6604 - val_accuracy: 0.6449\n",
            "Epoch 89/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6553 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 90/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6550 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 91/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6556 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 92/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6550 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 93/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6551 - accuracy: 0.6406 - val_loss: 0.6516 - val_accuracy: 0.6449\n",
            "Epoch 94/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6556 - accuracy: 0.6406 - val_loss: 0.6570 - val_accuracy: 0.6449\n",
            "Epoch 95/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6581 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 96/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6552 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 97/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6550 - accuracy: 0.6406 - val_loss: 0.6535 - val_accuracy: 0.6449\n",
            "Epoch 98/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6555 - accuracy: 0.6406 - val_loss: 0.6561 - val_accuracy: 0.6449\n",
            "Epoch 99/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6552 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 100/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6561 - accuracy: 0.6406 - val_loss: 0.6576 - val_accuracy: 0.6449\n",
            "Epoch 101/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6567 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
            "Epoch 102/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6555 - accuracy: 0.6406 - val_loss: 0.6523 - val_accuracy: 0.6449\n",
            "Epoch 103/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6554 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 104/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6549 - accuracy: 0.6406 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
            "Epoch 105/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6549 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 106/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6571 - accuracy: 0.6406 - val_loss: 0.6676 - val_accuracy: 0.6449\n",
            "Epoch 107/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6582 - accuracy: 0.6391 - val_loss: 0.6618 - val_accuracy: 0.6449\n",
            "Epoch 108/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6555 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 109/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6570 - accuracy: 0.6406 - val_loss: 0.6514 - val_accuracy: 0.6449\n",
            "Epoch 110/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6554 - accuracy: 0.6406 - val_loss: 0.6592 - val_accuracy: 0.6449\n",
            "Epoch 111/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6562 - accuracy: 0.6406 - val_loss: 0.6538 - val_accuracy: 0.6449\n",
            "Epoch 112/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6559 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 113/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6568 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
            "Epoch 114/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6560 - accuracy: 0.6406 - val_loss: 0.6545 - val_accuracy: 0.6449\n",
            "Epoch 115/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6564 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 116/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6553 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 117/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6568 - accuracy: 0.6406 - val_loss: 0.6543 - val_accuracy: 0.6449\n",
            "Epoch 118/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6558 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 119/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6559 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 120/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6568 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 121/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6561 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 122/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6555 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 123/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6575 - accuracy: 0.6378 - val_loss: 0.6589 - val_accuracy: 0.6449\n",
            "Epoch 124/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6563 - accuracy: 0.6406 - val_loss: 0.6542 - val_accuracy: 0.6449\n",
            "Epoch 125/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6559 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 126/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6557 - accuracy: 0.6406 - val_loss: 0.6522 - val_accuracy: 0.6449\n",
            "Epoch 127/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6555 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 128/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6552 - accuracy: 0.6406 - val_loss: 0.6537 - val_accuracy: 0.6449\n",
            "Epoch 129/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6565 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 130/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6563 - accuracy: 0.6406 - val_loss: 0.6525 - val_accuracy: 0.6449\n",
            "Epoch 131/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6556 - accuracy: 0.6406 - val_loss: 0.6521 - val_accuracy: 0.6449\n",
            "Epoch 132/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6555 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 133/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6551 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 134/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6563 - accuracy: 0.6406 - val_loss: 0.6529 - val_accuracy: 0.6449\n",
            "Epoch 135/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6574 - accuracy: 0.6406 - val_loss: 0.6553 - val_accuracy: 0.6449\n",
            "Epoch 136/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6566 - accuracy: 0.6406 - val_loss: 0.6567 - val_accuracy: 0.6449\n",
            "Epoch 137/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6562 - accuracy: 0.6406 - val_loss: 0.6533 - val_accuracy: 0.6449\n",
            "Epoch 138/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6561 - accuracy: 0.6399 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 139/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6564 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 140/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6563 - accuracy: 0.6392 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 141/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6561 - accuracy: 0.6406 - val_loss: 0.6537 - val_accuracy: 0.6449\n",
            "Epoch 142/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6562 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 143/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6558 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 144/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6562 - accuracy: 0.6406 - val_loss: 0.6562 - val_accuracy: 0.6449\n",
            "Epoch 145/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6560 - accuracy: 0.6406 - val_loss: 0.6516 - val_accuracy: 0.6449\n",
            "Epoch 146/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6569 - accuracy: 0.6406 - val_loss: 0.6527 - val_accuracy: 0.6449\n",
            "Epoch 147/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6558 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 148/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6564 - accuracy: 0.6406 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
            "Epoch 149/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6554 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 150/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6562 - accuracy: 0.6391 - val_loss: 0.6536 - val_accuracy: 0.6449\n",
            "Epoch 151/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6572 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 152/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6568 - accuracy: 0.6406 - val_loss: 0.6537 - val_accuracy: 0.6449\n",
            "Epoch 153/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6562 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 154/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6551 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 155/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6557 - accuracy: 0.6406 - val_loss: 0.6567 - val_accuracy: 0.6449\n",
            "Epoch 156/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6574 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
            "Epoch 157/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6563 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 158/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6555 - accuracy: 0.6406 - val_loss: 0.6567 - val_accuracy: 0.6449\n",
            "Epoch 159/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6558 - accuracy: 0.6406 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
            "Epoch 160/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6569 - accuracy: 0.6406 - val_loss: 0.6521 - val_accuracy: 0.6449\n",
            "Epoch 161/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6567 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 162/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6560 - accuracy: 0.6406 - val_loss: 0.6524 - val_accuracy: 0.6449\n",
            "Epoch 163/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6557 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 164/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6554 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 165/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6553 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 166/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6575 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 167/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6571 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 168/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6550 - accuracy: 0.6406 - val_loss: 0.6519 - val_accuracy: 0.6449\n",
            "Epoch 169/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6554 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 170/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6572 - accuracy: 0.6406 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
            "Epoch 171/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6561 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 172/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6558 - accuracy: 0.6381 - val_loss: 0.6513 - val_accuracy: 0.6449\n",
            "Epoch 173/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6561 - accuracy: 0.6406 - val_loss: 0.6539 - val_accuracy: 0.6449\n",
            "Epoch 174/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6559 - accuracy: 0.6406 - val_loss: 0.6523 - val_accuracy: 0.6449\n",
            "Epoch 175/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6565 - accuracy: 0.6406 - val_loss: 0.6586 - val_accuracy: 0.6449\n",
            "Epoch 176/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6563 - accuracy: 0.6406 - val_loss: 0.6513 - val_accuracy: 0.6449\n",
            "Epoch 177/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6566 - accuracy: 0.6406 - val_loss: 0.6579 - val_accuracy: 0.6449\n",
            "Epoch 178/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6559 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 179/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6579 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 180/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6556 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 181/200\n",
            "453/453 [==============================] - 31s 70ms/step - loss: 0.6565 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 182/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6557 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 183/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6552 - accuracy: 0.6406 - val_loss: 0.6515 - val_accuracy: 0.6449\n",
            "Epoch 184/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6565 - accuracy: 0.6406 - val_loss: 0.6517 - val_accuracy: 0.6449\n",
            "Epoch 185/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6565 - accuracy: 0.6406 - val_loss: 0.6528 - val_accuracy: 0.6449\n",
            "Epoch 186/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6566 - accuracy: 0.6406 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
            "Epoch 187/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6561 - accuracy: 0.6406 - val_loss: 0.6563 - val_accuracy: 0.6449\n",
            "Epoch 188/200\n",
            "453/453 [==============================] - 30s 66ms/step - loss: 0.6557 - accuracy: 0.6406 - val_loss: 0.6525 - val_accuracy: 0.6449\n",
            "Epoch 189/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6572 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 190/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6553 - accuracy: 0.6406 - val_loss: 0.6520 - val_accuracy: 0.6449\n",
            "Epoch 191/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6558 - accuracy: 0.6406 - val_loss: 0.6531 - val_accuracy: 0.6449\n",
            "Epoch 192/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6566 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 193/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6559 - accuracy: 0.6406 - val_loss: 0.6542 - val_accuracy: 0.6449\n",
            "Epoch 194/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6563 - accuracy: 0.6406 - val_loss: 0.6515 - val_accuracy: 0.6449\n",
            "Epoch 195/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6558 - accuracy: 0.6406 - val_loss: 0.6550 - val_accuracy: 0.6449\n",
            "Epoch 196/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6573 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 197/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6566 - accuracy: 0.6406 - val_loss: 0.6533 - val_accuracy: 0.6449\n",
            "Epoch 198/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6560 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 199/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6556 - accuracy: 0.6406 - val_loss: 0.6560 - val_accuracy: 0.6449\n",
            "Epoch 200/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6563 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Fold 2, 200 epochs, 6115 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 64.49%\n",
            "COMPILE...\n",
            "...COMPILED\n",
            "FIT\n",
            "Epoch 1/200\n",
            "453/453 [==============================] - ETA: 0s - loss: 0.6542 - accuracy: 0.6427INFO:tensorflow:Assets written to: /content/drive/My Drive/data/GRU202.cv.3.best/assets\n",
            "453/453 [==============================] - 39s 85ms/step - loss: 0.6542 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 2/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6525 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 3/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
            "Epoch 4/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6527 - accuracy: 0.6427 - val_loss: 0.6624 - val_accuracy: 0.6263\n",
            "Epoch 5/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6525 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
            "Epoch 6/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6638 - val_accuracy: 0.6263\n",
            "Epoch 7/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6621 - val_accuracy: 0.6263\n",
            "Epoch 8/200\n",
            "453/453 [==============================] - 32s 71ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
            "Epoch 9/200\n",
            "453/453 [==============================] - 32s 71ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 10/200\n",
            "453/453 [==============================] - 32s 71ms/step - loss: 0.6541 - accuracy: 0.6421 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 11/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6532 - accuracy: 0.6410 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 12/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6518 - accuracy: 0.6418 - val_loss: 0.6574 - val_accuracy: 0.6263\n",
            "Epoch 13/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6506 - accuracy: 0.6418 - val_loss: 0.6574 - val_accuracy: 0.6263\n",
            "Epoch 14/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6496 - accuracy: 0.6427 - val_loss: 0.6625 - val_accuracy: 0.6263\n",
            "Epoch 15/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6500 - accuracy: 0.6427 - val_loss: 0.6590 - val_accuracy: 0.6263\n",
            "Epoch 16/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6630 - val_accuracy: 0.6263\n",
            "Epoch 17/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6513 - accuracy: 0.6427 - val_loss: 0.6601 - val_accuracy: 0.6263\n",
            "Epoch 18/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6498 - accuracy: 0.6427 - val_loss: 0.6578 - val_accuracy: 0.6263\n",
            "Epoch 19/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6498 - accuracy: 0.6427 - val_loss: 0.6581 - val_accuracy: 0.6263\n",
            "Epoch 20/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6506 - accuracy: 0.6427 - val_loss: 0.6591 - val_accuracy: 0.6263\n",
            "Epoch 21/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6491 - accuracy: 0.6427 - val_loss: 0.6580 - val_accuracy: 0.6263\n",
            "Epoch 22/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6501 - accuracy: 0.6427 - val_loss: 0.6582 - val_accuracy: 0.6263\n",
            "Epoch 23/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6498 - accuracy: 0.6427 - val_loss: 0.6578 - val_accuracy: 0.6263\n",
            "Epoch 24/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6492 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 25/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6527 - accuracy: 0.6426 - val_loss: 0.6602 - val_accuracy: 0.6263\n",
            "Epoch 26/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6500 - accuracy: 0.6422 - val_loss: 0.6605 - val_accuracy: 0.6263\n",
            "Epoch 27/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6494 - accuracy: 0.6427 - val_loss: 0.6586 - val_accuracy: 0.6263\n",
            "Epoch 28/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6495 - accuracy: 0.6427 - val_loss: 0.6587 - val_accuracy: 0.6263\n",
            "Epoch 29/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6502 - accuracy: 0.6427 - val_loss: 0.6586 - val_accuracy: 0.6263\n",
            "Epoch 30/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6492 - accuracy: 0.6427 - val_loss: 0.6575 - val_accuracy: 0.6263\n",
            "Epoch 31/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6527 - accuracy: 0.6427 - val_loss: 0.6616 - val_accuracy: 0.6263\n",
            "Epoch 32/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6537 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 33/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6538 - accuracy: 0.6427 - val_loss: 0.6622 - val_accuracy: 0.6263\n",
            "Epoch 34/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6525 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 35/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6534 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 36/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6537 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 37/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6540 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
            "Epoch 38/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6546 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 39/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6544 - accuracy: 0.6427 - val_loss: 0.6616 - val_accuracy: 0.6263\n",
            "Epoch 40/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6535 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 41/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6544 - accuracy: 0.6427 - val_loss: 0.6633 - val_accuracy: 0.6263\n",
            "Epoch 42/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6542 - accuracy: 0.6415 - val_loss: 0.6627 - val_accuracy: 0.6263\n",
            "Epoch 43/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6549 - accuracy: 0.6422 - val_loss: 0.6649 - val_accuracy: 0.6263\n",
            "Epoch 44/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6517 - accuracy: 0.6427 - val_loss: 0.6632 - val_accuracy: 0.6263\n",
            "Epoch 45/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6503 - accuracy: 0.6418 - val_loss: 0.6622 - val_accuracy: 0.6263\n",
            "Epoch 46/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6506 - accuracy: 0.6427 - val_loss: 0.6602 - val_accuracy: 0.6263\n",
            "Epoch 47/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6517 - accuracy: 0.6421 - val_loss: 0.6588 - val_accuracy: 0.6263\n",
            "Epoch 48/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6505 - accuracy: 0.6427 - val_loss: 0.6593 - val_accuracy: 0.6263\n",
            "Epoch 49/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6506 - accuracy: 0.6427 - val_loss: 0.6582 - val_accuracy: 0.6263\n",
            "Epoch 50/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6503 - accuracy: 0.6427 - val_loss: 0.6599 - val_accuracy: 0.6263\n",
            "Epoch 51/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6501 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
            "Epoch 52/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6500 - accuracy: 0.6427 - val_loss: 0.6574 - val_accuracy: 0.6263\n",
            "Epoch 53/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6495 - accuracy: 0.6427 - val_loss: 0.6575 - val_accuracy: 0.6263\n",
            "Epoch 54/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6503 - accuracy: 0.6427 - val_loss: 0.6577 - val_accuracy: 0.6263\n",
            "Epoch 55/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6506 - accuracy: 0.6427 - val_loss: 0.6595 - val_accuracy: 0.6263\n",
            "Epoch 56/200\n",
            "453/453 [==============================] - 32s 71ms/step - loss: 0.6508 - accuracy: 0.6427 - val_loss: 0.6581 - val_accuracy: 0.6263\n",
            "Epoch 57/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6509 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 58/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6516 - accuracy: 0.6418 - val_loss: 0.6599 - val_accuracy: 0.6263\n",
            "Epoch 59/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6551 - accuracy: 0.6427 - val_loss: 0.6658 - val_accuracy: 0.6263\n",
            "Epoch 60/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6533 - accuracy: 0.6427 - val_loss: 0.6661 - val_accuracy: 0.6263\n",
            "Epoch 61/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6528 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 62/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6532 - accuracy: 0.6427 - val_loss: 0.6726 - val_accuracy: 0.6263\n",
            "Epoch 63/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6561 - accuracy: 0.6427 - val_loss: 0.6628 - val_accuracy: 0.6263\n",
            "Epoch 64/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6550 - accuracy: 0.6427 - val_loss: 0.6579 - val_accuracy: 0.6263\n",
            "Epoch 65/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6527 - accuracy: 0.6427 - val_loss: 0.6591 - val_accuracy: 0.6263\n",
            "Epoch 66/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
            "Epoch 67/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6515 - accuracy: 0.6427 - val_loss: 0.6629 - val_accuracy: 0.6263\n",
            "Epoch 68/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6663 - val_accuracy: 0.6263\n",
            "Epoch 69/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6519 - accuracy: 0.6427 - val_loss: 0.6578 - val_accuracy: 0.6263\n",
            "Epoch 70/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6518 - accuracy: 0.6417 - val_loss: 0.6710 - val_accuracy: 0.6263\n",
            "Epoch 71/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6508 - accuracy: 0.6411 - val_loss: 0.6578 - val_accuracy: 0.6263\n",
            "Epoch 72/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6502 - accuracy: 0.6427 - val_loss: 0.6580 - val_accuracy: 0.6263\n",
            "Epoch 73/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6511 - accuracy: 0.6427 - val_loss: 0.6585 - val_accuracy: 0.6263\n",
            "Epoch 74/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6508 - accuracy: 0.6413 - val_loss: 0.6624 - val_accuracy: 0.6263\n",
            "Epoch 75/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6513 - accuracy: 0.6427 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
            "Epoch 76/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6510 - accuracy: 0.6427 - val_loss: 0.6597 - val_accuracy: 0.6263\n",
            "Epoch 77/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6501 - accuracy: 0.6427 - val_loss: 0.6649 - val_accuracy: 0.6263\n",
            "Epoch 78/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6502 - accuracy: 0.6427 - val_loss: 0.6594 - val_accuracy: 0.6263\n",
            "Epoch 79/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6501 - accuracy: 0.6427 - val_loss: 0.6590 - val_accuracy: 0.6263\n",
            "Epoch 80/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6508 - accuracy: 0.6427 - val_loss: 0.6576 - val_accuracy: 0.6263\n",
            "Epoch 81/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6512 - accuracy: 0.6427 - val_loss: 0.6577 - val_accuracy: 0.6263\n",
            "Epoch 82/200\n",
            "453/453 [==============================] - 32s 71ms/step - loss: 0.6511 - accuracy: 0.6427 - val_loss: 0.6574 - val_accuracy: 0.6263\n",
            "Epoch 83/200\n",
            "453/453 [==============================] - 32s 71ms/step - loss: 0.6510 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
            "Epoch 84/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6498 - accuracy: 0.6427 - val_loss: 0.6671 - val_accuracy: 0.6263\n",
            "Epoch 85/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6498 - accuracy: 0.6427 - val_loss: 0.6576 - val_accuracy: 0.6263\n",
            "Epoch 86/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6504 - accuracy: 0.6427 - val_loss: 0.6627 - val_accuracy: 0.6263\n",
            "Epoch 87/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6508 - accuracy: 0.6427 - val_loss: 0.6574 - val_accuracy: 0.6263\n",
            "Epoch 88/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6509 - accuracy: 0.6427 - val_loss: 0.6581 - val_accuracy: 0.6263\n",
            "Epoch 89/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6514 - accuracy: 0.6427 - val_loss: 0.6585 - val_accuracy: 0.6263\n",
            "Epoch 90/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 6.0273 - accuracy: 0.6369 - val_loss: 263.9094 - val_accuracy: 0.6263\n",
            "Epoch 91/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 2.6515 - accuracy: 0.6298 - val_loss: 0.6707 - val_accuracy: 0.6263\n",
            "Epoch 92/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6555 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 93/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6546 - accuracy: 0.6427 - val_loss: 0.6657 - val_accuracy: 0.6263\n",
            "Epoch 94/200\n",
            "453/453 [==============================] - 32s 71ms/step - loss: 0.6550 - accuracy: 0.6427 - val_loss: 0.6623 - val_accuracy: 0.6263\n",
            "Epoch 95/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6562 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 96/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6566 - accuracy: 0.6391 - val_loss: 0.6762 - val_accuracy: 0.6263\n",
            "Epoch 97/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6559 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 98/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6558 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
            "Epoch 99/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6547 - accuracy: 0.6427 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
            "Epoch 100/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6550 - accuracy: 0.6427 - val_loss: 0.6659 - val_accuracy: 0.6263\n",
            "Epoch 101/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6551 - accuracy: 0.6427 - val_loss: 0.6648 - val_accuracy: 0.6263\n",
            "Epoch 102/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6551 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 103/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6557 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 104/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6556 - accuracy: 0.6427 - val_loss: 0.6746 - val_accuracy: 0.6263\n",
            "Epoch 105/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6546 - accuracy: 0.6427 - val_loss: 0.6629 - val_accuracy: 0.6263\n",
            "Epoch 106/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6540 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 107/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6537 - accuracy: 0.6427 - val_loss: 0.6738 - val_accuracy: 0.6263\n",
            "Epoch 108/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6531 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
            "Epoch 109/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6530 - accuracy: 0.6427 - val_loss: 0.6633 - val_accuracy: 0.6263\n",
            "Epoch 110/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6528 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 111/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6529 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 112/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6525 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
            "Epoch 113/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
            "Epoch 114/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6527 - accuracy: 0.6427 - val_loss: 0.6634 - val_accuracy: 0.6263\n",
            "Epoch 115/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6535 - accuracy: 0.6427 - val_loss: 0.6623 - val_accuracy: 0.6263\n",
            "Epoch 116/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6533 - accuracy: 0.6427 - val_loss: 0.6626 - val_accuracy: 0.6263\n",
            "Epoch 117/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6532 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 118/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6530 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 119/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6533 - accuracy: 0.6427 - val_loss: 0.6621 - val_accuracy: 0.6263\n",
            "Epoch 120/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6540 - accuracy: 0.6399 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 121/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6541 - accuracy: 0.6427 - val_loss: 0.6623 - val_accuracy: 0.6263\n",
            "Epoch 122/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6533 - accuracy: 0.6427 - val_loss: 0.6642 - val_accuracy: 0.6263\n",
            "Epoch 123/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6542 - accuracy: 0.6427 - val_loss: 0.6682 - val_accuracy: 0.6263\n",
            "Epoch 124/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6542 - accuracy: 0.6427 - val_loss: 0.6663 - val_accuracy: 0.6263\n",
            "Epoch 125/200\n",
            "453/453 [==============================] - 32s 70ms/step - loss: 0.6538 - accuracy: 0.6427 - val_loss: 0.6635 - val_accuracy: 0.6263\n",
            "Epoch 126/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6543 - accuracy: 0.6427 - val_loss: 0.6618 - val_accuracy: 0.6263\n",
            "Epoch 127/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6543 - accuracy: 0.6427 - val_loss: 0.6666 - val_accuracy: 0.6263\n",
            "Epoch 128/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6546 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 129/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6545 - accuracy: 0.6427 - val_loss: 0.6618 - val_accuracy: 0.6263\n",
            "Epoch 130/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6547 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 131/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6535 - accuracy: 0.6427 - val_loss: 0.6686 - val_accuracy: 0.6263\n",
            "Epoch 132/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6550 - accuracy: 0.6427 - val_loss: 0.6623 - val_accuracy: 0.6263\n",
            "Epoch 133/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6547 - accuracy: 0.6427 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
            "Epoch 134/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6538 - accuracy: 0.6427 - val_loss: 0.6616 - val_accuracy: 0.6263\n",
            "Epoch 135/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6555 - accuracy: 0.6400 - val_loss: 0.6664 - val_accuracy: 0.6263\n",
            "Epoch 136/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6535 - accuracy: 0.6427 - val_loss: 0.6680 - val_accuracy: 0.6263\n",
            "Epoch 137/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6544 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
            "Epoch 138/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6541 - accuracy: 0.6427 - val_loss: 0.6654 - val_accuracy: 0.6263\n",
            "Epoch 139/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6540 - accuracy: 0.6427 - val_loss: 0.6627 - val_accuracy: 0.6263\n",
            "Epoch 140/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6539 - accuracy: 0.6427 - val_loss: 0.6622 - val_accuracy: 0.6263\n",
            "Epoch 141/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6538 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 142/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6547 - accuracy: 0.6427 - val_loss: 0.6633 - val_accuracy: 0.6263\n",
            "Epoch 143/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6541 - accuracy: 0.6427 - val_loss: 0.6624 - val_accuracy: 0.6263\n",
            "Epoch 144/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6532 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
            "Epoch 145/200\n",
            "453/453 [==============================] - 31s 69ms/step - loss: 0.6543 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 146/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6535 - accuracy: 0.6427 - val_loss: 0.6631 - val_accuracy: 0.6263\n",
            "Epoch 147/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6558 - accuracy: 0.6427 - val_loss: 0.6753 - val_accuracy: 0.6263\n",
            "Epoch 148/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6542 - accuracy: 0.6427 - val_loss: 0.6631 - val_accuracy: 0.6263\n",
            "Epoch 149/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6556 - accuracy: 0.6427 - val_loss: 0.6631 - val_accuracy: 0.6263\n",
            "Epoch 150/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6537 - accuracy: 0.6427 - val_loss: 0.6650 - val_accuracy: 0.6263\n",
            "Epoch 151/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6556 - accuracy: 0.6422 - val_loss: 0.6710 - val_accuracy: 0.6263\n",
            "Epoch 152/200\n",
            "453/453 [==============================] - 30s 67ms/step - loss: 0.6556 - accuracy: 0.6427 - val_loss: 0.6669 - val_accuracy: 0.6263\n",
            "Epoch 153/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6545 - accuracy: 0.6427 - val_loss: 0.6637 - val_accuracy: 0.6263\n",
            "Epoch 154/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6554 - accuracy: 0.6427 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
            "Epoch 155/200\n",
            "453/453 [==============================] - 31s 67ms/step - loss: 0.6556 - accuracy: 0.6427 - val_loss: 0.6640 - val_accuracy: 0.6263\n",
            "Epoch 156/200\n",
            "453/453 [==============================] - 31s 68ms/step - loss: 0.6555 - accuracy: 0.6427 - val_loss: 0.6665 - val_accuracy: 0.6263\n",
            "Epoch 157/200\n",
            " 11/453 [..............................] - ETA: 26s - loss: 0.6437 - accuracy: 0.6591"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-afc406ae5036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Cross valiation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdo_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-ceb8985e65bd>\u001b[0m in \u001b[0;36mdo_cross_validation\u001b[0;34m(X, y, given_model)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# verbose=1 for ascii art, verbose=0 for none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmycallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 validation_data=(X_valid,y_valid) )\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mend_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0melapsed_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN0K-1e2g4WN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E37va5UZg4WQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}