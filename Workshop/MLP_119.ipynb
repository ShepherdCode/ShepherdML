{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_119.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojm_6E9f9Kcf"
      },
      "source": [
        "# MLP 119\n",
        "Train on short, evaluate on all"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh6XplUvC0j0",
        "outputId": "451a87fb-133c-4de1-f06d-c329fd1bb797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "PATH='/content/drive/'\n",
        "drive.mount(PATH)\n",
        "DATAPATH=PATH+'My Drive/data/'\n",
        "PC_FILENAME = DATAPATH+'pcRNA.fasta'\n",
        "NC_FILENAME = DATAPATH+'ncRNA.fasta'\n",
        "#PC_FILENAME = 'pcRNA.fasta'\n",
        "#NC_FILENAME = 'ncRNA.fasta'\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQY7aTj29Kch"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LayerNormalization\n",
        "import time\n",
        "\n",
        "dt='float32'\n",
        "tf.keras.backend.set_floatx(dt)\n",
        "\n",
        "EPOCHS=200\n",
        "SPLITS=1\n",
        "K=4\n",
        "VOCABULARY_SIZE=4**K+1   # e.g. K=3 => 64 DNA K-mers + 'NNN'\n",
        "EMBED_DIMEN=16\n",
        "FILENAME='MLP119'"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV6k-xOm9Kcn"
      },
      "source": [
        "## Load and partition sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I-O_qzw9Kco"
      },
      "source": [
        "# Assume file was preprocessed to contain one line per seq.\n",
        "# Prefer Pandas dataframe but df does not support append.\n",
        "# For conversion to tensor, must avoid python lists.\n",
        "def load_fasta(filename,label):\n",
        "    DEFLINE='>'\n",
        "    labels=[]\n",
        "    seqs=[]\n",
        "    lens=[]\n",
        "    nums=[]\n",
        "    num=0\n",
        "    with open (filename,'r') as infile:\n",
        "        for line in infile:\n",
        "            if line[0]!=DEFLINE:\n",
        "                seq=line.rstrip()\n",
        "                num += 1   # first seqnum is 1\n",
        "                seqlen=len(seq)\n",
        "                nums.append(num)\n",
        "                labels.append(label)\n",
        "                seqs.append(seq)\n",
        "                lens.append(seqlen)\n",
        "    df1=pd.DataFrame(nums,columns=['seqnum'])\n",
        "    df2=pd.DataFrame(labels,columns=['class'])\n",
        "    df3=pd.DataFrame(seqs,columns=['sequence'])\n",
        "    df4=pd.DataFrame(lens,columns=['seqlen'])\n",
        "    df=pd.concat((df1,df2,df3,df4),axis=1)\n",
        "    return df\n",
        "\n",
        "# Split into train/test stratified by sequence length.\n",
        "def sizebin(df):\n",
        "    return pd.cut(df[\"seqlen\"],\n",
        "                              bins=[0,1000,2000,4000,8000,16000,np.inf],\n",
        "                              labels=[0,1,2,3,4,5])\n",
        "def make_train_test(data):\n",
        "    bin_labels= sizebin(data)\n",
        "    from sklearn.model_selection import StratifiedShuffleSplit\n",
        "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=37863)\n",
        "    # split(x,y) expects that y is the labels. \n",
        "    # Trick: Instead of y, give it it the bin labels that we generated.\n",
        "    for train_index,test_index in splitter.split(data,bin_labels):\n",
        "        train_set = data.iloc[train_index]\n",
        "        test_set = data.iloc[test_index]\n",
        "    return (train_set,test_set)\n",
        "\n",
        "def separate_X_and_y(data):\n",
        "    y=   data[['class']].copy()\n",
        "    X=   data.drop(columns=['class','seqnum','seqlen'])\n",
        "    return (X,y)\n",
        "\n",
        "def make_slice(data_set,min_len,max_len):\n",
        "    print(\"original \"+str(data_set.shape))\n",
        "    too_short = data_set[ data_set['seqlen'] < min_len ].index\n",
        "    no_short=data_set.drop(too_short)\n",
        "    print(\"no short \"+str(no_short.shape))\n",
        "    too_long = no_short[ no_short['seqlen'] >= max_len ].index\n",
        "    no_long_no_short=no_short.drop(too_long)\n",
        "    print(\"no long, no short \"+str(no_long_no_short.shape))\n",
        "    return no_long_no_short\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRAaO9jP9Kcr"
      },
      "source": [
        "## Make K-mers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8xcZ4Mr9Kcs"
      },
      "source": [
        "def make_kmer_table(K):\n",
        "    npad='N'*K\n",
        "    shorter_kmers=['']\n",
        "    for i in range(K):\n",
        "        longer_kmers=[]\n",
        "        for mer in shorter_kmers:\n",
        "            longer_kmers.append(mer+'A')\n",
        "            longer_kmers.append(mer+'C')\n",
        "            longer_kmers.append(mer+'G')\n",
        "            longer_kmers.append(mer+'T')\n",
        "        shorter_kmers = longer_kmers\n",
        "    all_kmers = shorter_kmers\n",
        "    kmer_dict = {}\n",
        "    kmer_dict[npad]=0\n",
        "    value=1\n",
        "    for mer in all_kmers:\n",
        "        kmer_dict[mer]=value\n",
        "        value += 1\n",
        "    return kmer_dict\n",
        "\n",
        "KMER_TABLE=make_kmer_table(K)\n",
        "\n",
        "def strings_to_vectors(data,uniform_len):\n",
        "    all_seqs=[]\n",
        "    for seq in data['sequence']:\n",
        "        i=0\n",
        "        seqlen=len(seq)\n",
        "        kmers=[]\n",
        "        while i < seqlen-K+1 -1:  # stop at minus one for spaced seed\n",
        "            #kmer=seq[i:i+2]+seq[i+3:i+5]    # SPACED SEED 2/1/2 for K=4\n",
        "            kmer=seq[i:i+K]  \n",
        "            i += 1\n",
        "            value=KMER_TABLE[kmer]\n",
        "            kmers.append(value)\n",
        "        pad_val=0\n",
        "        while i < uniform_len:\n",
        "            kmers.append(pad_val)\n",
        "            i += 1\n",
        "        all_seqs.append(kmers)\n",
        "    pd2d=pd.DataFrame(all_seqs)\n",
        "    return pd2d   # return 2D dataframe, uniform dimensions"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEtA0xiV9Kcv"
      },
      "source": [
        "def make_kmers(MAXLEN,train_set):\n",
        "    (X_train_all,y_train_all)=separate_X_and_y(train_set)\n",
        "\n",
        "    # The returned values are Pandas dataframes.\n",
        "    # print(X_train_all.shape,y_train_all.shape)\n",
        "    # (X_train_all,y_train_all)\n",
        "    # y: Pandas dataframe to Python list.\n",
        "    # y_train_all=y_train_all.values.tolist()\n",
        "    # The sequences lengths are bounded but not uniform.\n",
        "    X_train_all\n",
        "    print(type(X_train_all))\n",
        "    print(X_train_all.shape)\n",
        "    print(X_train_all.iloc[0])\n",
        "    print(len(X_train_all.iloc[0]['sequence']))\n",
        "\n",
        "    # X: List of string to List of uniform-length ordered lists of K-mers.\n",
        "    X_train_kmers=strings_to_vectors(X_train_all,MAXLEN)\n",
        "    # X: true 2D array (no more lists)\n",
        "    X_train_kmers.shape\n",
        "\n",
        "    print(\"transform...\")\n",
        "    # From pandas dataframe to numpy to list to numpy\n",
        "    print(type(X_train_kmers))\n",
        "    num_seqs=len(X_train_kmers)\n",
        "    tmp_seqs=[]\n",
        "    for i in range(num_seqs):\n",
        "        kmer_sequence=X_train_kmers.iloc[i]\n",
        "        tmp_seqs.append(kmer_sequence)\n",
        "    X_train_kmers=np.array(tmp_seqs)\n",
        "    tmp_seqs=None\n",
        "    print(type(X_train_kmers))\n",
        "    print(X_train_kmers)\n",
        "\n",
        "    labels=y_train_all.to_numpy()\n",
        "    return (X_train_kmers,labels)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaXyySyO9Kcz"
      },
      "source": [
        "def make_frequencies(Xin):\n",
        "    # Input:  numpy X(numseq,seqlen)  list of vectors of kmerval where val0=NNN,val1=AAA,etc. \n",
        "    # Output: numpy X(numseq,65)    list of frequencies of 0,1,etc.\n",
        "    Xout=[]\n",
        "    VOCABULARY_SIZE= 4**K + 1  # plus one for 'NNN'\n",
        "    for seq in Xin:\n",
        "        freqs =[0] * VOCABULARY_SIZE\n",
        "        total = 0\n",
        "        for kmerval in seq:\n",
        "            freqs[kmerval] += 1\n",
        "            total += 1\n",
        "        for c in range(VOCABULARY_SIZE):\n",
        "            freqs[c] = freqs[c]/total\n",
        "        Xout.append(freqs)\n",
        "    Xnum = np.asarray(Xout)\n",
        "    return (Xnum)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7jcg6Wl9Kc2"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLFNO1Xa9Kc3"
      },
      "source": [
        "def build_model(maxlen):\n",
        "    act=\"sigmoid\"\n",
        "\n",
        "    embed_layer  = keras.layers.Embedding(\n",
        "        VOCABULARY_SIZE,EMBED_DIMEN,input_length=maxlen);\n",
        "    \n",
        "    neurons=16\n",
        "    dense1_layer = keras.layers.Dense(neurons, activation=act,dtype=dt,input_dim=VOCABULARY_SIZE)\n",
        "    dense2_layer = keras.layers.Dense(neurons, activation=act,dtype=dt)\n",
        "    dense3_layer = keras.layers.Dense(neurons, activation=act,dtype=dt)\n",
        "    output_layer = keras.layers.Dense(1,  activation=act,dtype=dt)\n",
        "\n",
        "    mlp = keras.models.Sequential()\n",
        "    #mlp.add(embed_layer)\n",
        "    mlp.add(dense1_layer)\n",
        "    mlp.add(dense2_layer)\n",
        "    #mlp.add(dense3_layer)\n",
        "    mlp.add(output_layer)\n",
        "    \n",
        "    bc=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    print(\"COMPILE...\")\n",
        "    mlp.compile(loss=bc, optimizer=\"Adam\",metrics=[\"accuracy\"])\n",
        "    print(\"...COMPILED\")\n",
        "    return mlp"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdIS2utq9Kc9"
      },
      "source": [
        "## Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVo4tbB_9Kc-"
      },
      "source": [
        "def do_cross_validation(X,y,maxlen):\n",
        "    model = None\n",
        "    cv_scores = []\n",
        "    fold=0\n",
        "    splitter = ShuffleSplit(n_splits=SPLITS, test_size=0.2, random_state=37863)\n",
        "    for train_index,valid_index in splitter.split(X):\n",
        "        X_train=X[train_index] # use iloc[] for dataframe\n",
        "        y_train=y[train_index]\n",
        "        X_valid=X[valid_index]\n",
        "        y_valid=y[valid_index]\n",
        "\n",
        "        print(\"BUILD MODEL\")\n",
        "        model=build_model(maxlen)\n",
        "\n",
        "        print(\"FIT\")\n",
        "        start_time=time.time()\n",
        "        # this is complaining about string to float\n",
        "        history=model.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
        "                epochs=EPOCHS, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
        "                validation_data=(X_valid,y_valid) )\n",
        "        end_time=time.time()\n",
        "        elapsed_time=(end_time-start_time)\n",
        "                        \n",
        "        fold += 1\n",
        "        print(\"Fold %d, %d epochs, %d sec\"%(fold,EPOCHS,elapsed_time))\n",
        "\n",
        "        pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "        plt.grid(True)\n",
        "        plt.gca().set_ylim(0,1)\n",
        "        plt.show()\n",
        "\n",
        "        scores = model.evaluate(X_valid, y_valid, verbose=0)\n",
        "        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "        # What are the other metrics_names?\n",
        "        # Try this from Geron page 505:\n",
        "        # np.mean(keras.losses.mean_squared_error(y_valid,y_pred))\n",
        "        cv_scores.append(scores[1] * 100)  \n",
        "    print()\n",
        "    print(\"Validation core mean %.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n",
        "    return model"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upc3S0eEgYZG"
      },
      "source": [
        "def just_train(model,X_train,y_train,maxlen):\n",
        "    print(\"FIT\")\n",
        "    start_time=time.time()\n",
        "    # this is complaining about string to float\n",
        "    history=model.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
        "            epochs=EPOCHS, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
        "            )  # no validation data\n",
        "    end_time=time.time()\n",
        "    elapsed_time=(end_time-start_time)\n",
        "    print(\"Train %d epochs, %d sec\"%(EPOCHS,elapsed_time))\n",
        "\n",
        "    pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "    plt.grid(True)\n",
        "    plt.gca().set_ylim(0,1)\n",
        "    plt.show()\n",
        "\n",
        "    #scores = model.evaluate(X_valid, y_valid, verbose=0)\n",
        "    #print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "    #cv_scores.append(scores[1] * 100)  \n",
        "    #print()\n",
        "    #print(\"Validation core mean %.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n",
        "    return model"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q-PEh7D9KdH"
      },
      "source": [
        "## Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8fNo6sn9KdH",
        "outputId": "53a3752a-a93a-42b8-d30b-177664152dc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "print(\"Load data from files.\")\n",
        "nc_seq=load_fasta(NC_FILENAME,0)\n",
        "pc_seq=load_fasta(PC_FILENAME,1)\n",
        "all_seq=pd.concat((nc_seq,pc_seq),axis=0)\n",
        "\n",
        "print(\"Put aside the test portion.\")\n",
        "(train_set,test_set)=make_train_test(all_seq)\n",
        "# Do this later when using the test data:\n",
        "# (X_test,y_test)=separate_X_and_y(test_set)\n",
        "\n",
        "nc_seq=None\n",
        "pc_seq=None\n",
        "all_seq=None\n",
        "\n",
        "print(\"Ready: train_set\")\n",
        "train_set"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load data from files.\n",
            "Put aside the test portion.\n",
            "Ready: train_set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>seqnum</th>\n",
              "      <th>class</th>\n",
              "      <th>sequence</th>\n",
              "      <th>seqlen</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1280</th>\n",
              "      <td>1281</td>\n",
              "      <td>0</td>\n",
              "      <td>AGTCCCTCCCCAGCCCAGCAGTCCCTCCAGGCTACATCCAGGAGAC...</td>\n",
              "      <td>348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9088</th>\n",
              "      <td>9089</td>\n",
              "      <td>0</td>\n",
              "      <td>CAGCTCCTGGGATGGCCTCACCTGAGGAGACTCTTGGGCCTTGGCA...</td>\n",
              "      <td>534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6069</th>\n",
              "      <td>6070</td>\n",
              "      <td>1</td>\n",
              "      <td>AGATCTAGGGATGGGGATGGGGAGGAGAAGTGGGAATGGGAAATTG...</td>\n",
              "      <td>592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18549</th>\n",
              "      <td>18550</td>\n",
              "      <td>1</td>\n",
              "      <td>GACGTCTCCCGCGGGCGTCGGCAGGGTCGGCGGCGTCGGCAGCAGT...</td>\n",
              "      <td>945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15027</th>\n",
              "      <td>15028</td>\n",
              "      <td>1</td>\n",
              "      <td>GAGCGCGCGAGCCGGGCCCGGAGCGCACGCCGCCGCCGCCACCGCC...</td>\n",
              "      <td>4382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3386</th>\n",
              "      <td>3387</td>\n",
              "      <td>0</td>\n",
              "      <td>TTTATGTGGATTGTCTGTCTCATGCTTGTTTCACCAGGGTAGTTAC...</td>\n",
              "      <td>578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6495</th>\n",
              "      <td>6496</td>\n",
              "      <td>0</td>\n",
              "      <td>ATAATGGGAAACTAAGGGCAAGTTCTCATGTTCCTGGTCCTGGCTT...</td>\n",
              "      <td>562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6409</th>\n",
              "      <td>6410</td>\n",
              "      <td>1</td>\n",
              "      <td>GGGTTTATTACTACTGAAGGAAGAACGTGAGTAGGTTAGGATTTCG...</td>\n",
              "      <td>740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7640</th>\n",
              "      <td>7641</td>\n",
              "      <td>1</td>\n",
              "      <td>ACAGCTGTGTTTGGCTGCAGGGCCAAGAGCGCTGTCAAGAAGACCC...</td>\n",
              "      <td>3156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14108</th>\n",
              "      <td>14109</td>\n",
              "      <td>0</td>\n",
              "      <td>GAAGTGATTGCAAGTTCAGCAGCATGAAACTGCTCTTTATCCGTGC...</td>\n",
              "      <td>466</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30290 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       seqnum  class                                           sequence  seqlen\n",
              "1280     1281      0  AGTCCCTCCCCAGCCCAGCAGTCCCTCCAGGCTACATCCAGGAGAC...     348\n",
              "9088     9089      0  CAGCTCCTGGGATGGCCTCACCTGAGGAGACTCTTGGGCCTTGGCA...     534\n",
              "6069     6070      1  AGATCTAGGGATGGGGATGGGGAGGAGAAGTGGGAATGGGAAATTG...     592\n",
              "18549   18550      1  GACGTCTCCCGCGGGCGTCGGCAGGGTCGGCGGCGTCGGCAGCAGT...     945\n",
              "15027   15028      1  GAGCGCGCGAGCCGGGCCCGGAGCGCACGCCGCCGCCGCCACCGCC...    4382\n",
              "...       ...    ...                                                ...     ...\n",
              "3386     3387      0  TTTATGTGGATTGTCTGTCTCATGCTTGTTTCACCAGGGTAGTTAC...     578\n",
              "6495     6496      0  ATAATGGGAAACTAAGGGCAAGTTCTCATGTTCCTGGTCCTGGCTT...     562\n",
              "6409     6410      1  GGGTTTATTACTACTGAAGGAAGAACGTGAGTAGGTTAGGATTTCG...     740\n",
              "7640     7641      1  ACAGCTGTGTTTGGCTGCAGGGCCAAGAGCGCTGTCAAGAAGACCC...    3156\n",
              "14108   14109      0  GAAGTGATTGCAAGTTCAGCAGCATGAAACTGCTCTTTATCCGTGC...     466\n",
              "\n",
              "[30290 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lI2PjJsxgYZT"
      },
      "source": [
        "# Reuse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKLYbWycgYZU",
        "outputId": "b1b021e9-4d8a-4f95-8644-5d09602d4f0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#reload=model1.load(FILENAME+'.short.model')\n",
        "#W = model1.get_weights()\n",
        "#scores = model1.evaluate(X_valid, y_valid, verbose=0)\n",
        "#print(\"%s: %.2f%%\" % (model1.metrics_names[1], scores[1]*100))\n",
        "\n",
        "min=200\n",
        "max=1000\n",
        "print(\"slice...\")\n",
        "subset=make_slice(train_set,min,max)\n",
        "print(\"kmers...\")\n",
        "(X_train,y_train)=make_kmers(max,subset)\n",
        "print(\"frequencies...\")\n",
        "X_train=make_frequencies(X_train)\n",
        "print(\"BUILD MODEL\")\n",
        "model=build_model(max)\n",
        "model=just_train(model,X_train,y_train,max)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "slice...\n",
            "original (30290, 4)\n",
            "no short (30290, 4)\n",
            "no long, no short (8879, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(8879, 1)\n",
            "sequence    AGTCCCTCCCCAGCCCAGCAGTCCCTCCAGGCTACATCCAGGAGAC...\n",
            "Name: 1280, dtype: object\n",
            "348\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[ 46 182 214 ...   0   0   0]\n",
            " [ 36 142  56 ...   0   0   0]\n",
            " [135  28 110 ...   0   0   0]\n",
            " ...\n",
            " [147  73  36 ...   0   0   0]\n",
            " [228 143  57 ...   0   0   0]\n",
            " [131  12  47 ...   0   0   0]]\n",
            "frequencies...\n",
            "BUILD MODEL\n",
            "COMPILE...\n",
            "...COMPILED\n",
            "FIT\n",
            "Epoch 1/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.6917 - accuracy: 0.5237\n",
            "Epoch 2/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.6848 - accuracy: 0.6143\n",
            "Epoch 3/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.6688 - accuracy: 0.6585\n",
            "Epoch 4/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6806\n",
            "Epoch 5/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.6067 - accuracy: 0.6914\n",
            "Epoch 6/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.5845 - accuracy: 0.6993\n",
            "Epoch 7/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.5714 - accuracy: 0.7048\n",
            "Epoch 8/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.5627 - accuracy: 0.7100\n",
            "Epoch 9/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.5564 - accuracy: 0.7154\n",
            "Epoch 10/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.5500 - accuracy: 0.7195\n",
            "Epoch 11/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.5447 - accuracy: 0.7187\n",
            "Epoch 12/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.5393 - accuracy: 0.7256\n",
            "Epoch 13/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.5347 - accuracy: 0.7282\n",
            "Epoch 14/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.5299 - accuracy: 0.7300\n",
            "Epoch 15/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.5257 - accuracy: 0.7352\n",
            "Epoch 16/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.5202 - accuracy: 0.7393\n",
            "Epoch 17/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.5156 - accuracy: 0.7415\n",
            "Epoch 18/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7450\n",
            "Epoch 19/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.5054 - accuracy: 0.7492\n",
            "Epoch 20/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4998 - accuracy: 0.7543\n",
            "Epoch 21/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4948 - accuracy: 0.7571\n",
            "Epoch 22/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4894 - accuracy: 0.7619\n",
            "Epoch 23/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4837 - accuracy: 0.7668\n",
            "Epoch 24/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4779 - accuracy: 0.7699\n",
            "Epoch 25/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4714 - accuracy: 0.7754\n",
            "Epoch 26/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4652 - accuracy: 0.7780\n",
            "Epoch 27/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4594 - accuracy: 0.7821\n",
            "Epoch 28/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4534 - accuracy: 0.7859\n",
            "Epoch 29/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4473 - accuracy: 0.7871\n",
            "Epoch 30/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4418 - accuracy: 0.7939\n",
            "Epoch 31/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4363 - accuracy: 0.7958\n",
            "Epoch 32/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4315 - accuracy: 0.7973\n",
            "Epoch 33/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4263 - accuracy: 0.8020\n",
            "Epoch 34/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4219 - accuracy: 0.8043\n",
            "Epoch 35/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4181 - accuracy: 0.8055\n",
            "Epoch 36/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4138 - accuracy: 0.8085\n",
            "Epoch 37/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4112 - accuracy: 0.8102\n",
            "Epoch 38/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4086 - accuracy: 0.8119\n",
            "Epoch 39/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4059 - accuracy: 0.8128\n",
            "Epoch 40/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4025 - accuracy: 0.8153\n",
            "Epoch 41/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4002 - accuracy: 0.8162\n",
            "Epoch 42/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3982 - accuracy: 0.8201\n",
            "Epoch 43/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3960 - accuracy: 0.8209\n",
            "Epoch 44/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3959 - accuracy: 0.8195\n",
            "Epoch 45/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3934 - accuracy: 0.8210\n",
            "Epoch 46/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3915 - accuracy: 0.8212\n",
            "Epoch 47/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3902 - accuracy: 0.8250\n",
            "Epoch 48/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3891 - accuracy: 0.8240\n",
            "Epoch 49/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3876 - accuracy: 0.8264\n",
            "Epoch 50/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3867 - accuracy: 0.8242\n",
            "Epoch 51/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3859 - accuracy: 0.8267\n",
            "Epoch 52/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3851 - accuracy: 0.8271\n",
            "Epoch 53/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3845 - accuracy: 0.8268\n",
            "Epoch 54/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3832 - accuracy: 0.8260\n",
            "Epoch 55/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3822 - accuracy: 0.8270\n",
            "Epoch 56/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3813 - accuracy: 0.8255\n",
            "Epoch 57/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3811 - accuracy: 0.8294\n",
            "Epoch 58/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3801 - accuracy: 0.8278\n",
            "Epoch 59/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3793 - accuracy: 0.8269\n",
            "Epoch 60/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3791 - accuracy: 0.8278\n",
            "Epoch 61/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3783 - accuracy: 0.8298\n",
            "Epoch 62/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3776 - accuracy: 0.8286\n",
            "Epoch 63/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3778 - accuracy: 0.8281\n",
            "Epoch 64/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3778 - accuracy: 0.8291\n",
            "Epoch 65/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3762 - accuracy: 0.8288\n",
            "Epoch 66/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3759 - accuracy: 0.8294\n",
            "Epoch 67/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3754 - accuracy: 0.8291\n",
            "Epoch 68/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3750 - accuracy: 0.8290\n",
            "Epoch 69/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3752 - accuracy: 0.8328\n",
            "Epoch 70/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3736 - accuracy: 0.8314\n",
            "Epoch 71/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3747 - accuracy: 0.8293\n",
            "Epoch 72/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3731 - accuracy: 0.8324\n",
            "Epoch 73/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3738 - accuracy: 0.8303\n",
            "Epoch 74/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3728 - accuracy: 0.8299\n",
            "Epoch 75/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3722 - accuracy: 0.8313\n",
            "Epoch 76/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3724 - accuracy: 0.8303\n",
            "Epoch 77/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3717 - accuracy: 0.8317\n",
            "Epoch 78/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3724 - accuracy: 0.8317\n",
            "Epoch 79/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3728 - accuracy: 0.8317\n",
            "Epoch 80/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3711 - accuracy: 0.8324\n",
            "Epoch 81/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3712 - accuracy: 0.8324\n",
            "Epoch 82/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3711 - accuracy: 0.8340\n",
            "Epoch 83/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3720 - accuracy: 0.8296\n",
            "Epoch 84/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3701 - accuracy: 0.8316\n",
            "Epoch 85/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3701 - accuracy: 0.8333\n",
            "Epoch 86/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3699 - accuracy: 0.8338\n",
            "Epoch 87/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3699 - accuracy: 0.8314\n",
            "Epoch 88/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3701 - accuracy: 0.8331\n",
            "Epoch 89/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3692 - accuracy: 0.8306\n",
            "Epoch 90/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3695 - accuracy: 0.8326\n",
            "Epoch 91/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3695 - accuracy: 0.8343\n",
            "Epoch 92/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3689 - accuracy: 0.8331\n",
            "Epoch 93/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3686 - accuracy: 0.8359\n",
            "Epoch 94/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3687 - accuracy: 0.8342\n",
            "Epoch 95/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3684 - accuracy: 0.8365\n",
            "Epoch 96/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3680 - accuracy: 0.8364\n",
            "Epoch 97/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3671 - accuracy: 0.8355\n",
            "Epoch 98/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3675 - accuracy: 0.8348\n",
            "Epoch 99/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3680 - accuracy: 0.8340\n",
            "Epoch 100/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3670 - accuracy: 0.8356\n",
            "Epoch 101/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3682 - accuracy: 0.8323\n",
            "Epoch 102/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3672 - accuracy: 0.8351\n",
            "Epoch 103/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3676 - accuracy: 0.8328\n",
            "Epoch 104/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3671 - accuracy: 0.8340\n",
            "Epoch 105/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3673 - accuracy: 0.8357\n",
            "Epoch 106/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3670 - accuracy: 0.8352\n",
            "Epoch 107/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3679 - accuracy: 0.8338\n",
            "Epoch 108/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3678 - accuracy: 0.8356\n",
            "Epoch 109/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3671 - accuracy: 0.8357\n",
            "Epoch 110/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3675 - accuracy: 0.8343\n",
            "Epoch 111/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3662 - accuracy: 0.8342\n",
            "Epoch 112/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3664 - accuracy: 0.8364\n",
            "Epoch 113/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3659 - accuracy: 0.8376\n",
            "Epoch 114/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3663 - accuracy: 0.8370\n",
            "Epoch 115/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3661 - accuracy: 0.8380\n",
            "Epoch 116/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3663 - accuracy: 0.8368\n",
            "Epoch 117/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3665 - accuracy: 0.8364\n",
            "Epoch 118/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3657 - accuracy: 0.8341\n",
            "Epoch 119/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3656 - accuracy: 0.8379\n",
            "Epoch 120/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3655 - accuracy: 0.8377\n",
            "Epoch 121/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3657 - accuracy: 0.8365\n",
            "Epoch 122/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3652 - accuracy: 0.8364\n",
            "Epoch 123/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3663 - accuracy: 0.8352\n",
            "Epoch 124/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3652 - accuracy: 0.8370\n",
            "Epoch 125/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3648 - accuracy: 0.8346\n",
            "Epoch 126/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3656 - accuracy: 0.8367\n",
            "Epoch 127/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3652 - accuracy: 0.8380\n",
            "Epoch 128/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3643 - accuracy: 0.8358\n",
            "Epoch 129/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3647 - accuracy: 0.8374\n",
            "Epoch 130/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3656 - accuracy: 0.8366\n",
            "Epoch 131/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3648 - accuracy: 0.8366\n",
            "Epoch 132/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3649 - accuracy: 0.8356\n",
            "Epoch 133/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3650 - accuracy: 0.8365\n",
            "Epoch 134/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3647 - accuracy: 0.8373\n",
            "Epoch 135/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3650 - accuracy: 0.8378\n",
            "Epoch 136/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3646 - accuracy: 0.8383\n",
            "Epoch 137/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3647 - accuracy: 0.8362\n",
            "Epoch 138/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3641 - accuracy: 0.8369\n",
            "Epoch 139/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3647 - accuracy: 0.8391\n",
            "Epoch 140/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3645 - accuracy: 0.8376\n",
            "Epoch 141/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3643 - accuracy: 0.8366\n",
            "Epoch 142/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3637 - accuracy: 0.8391\n",
            "Epoch 143/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3647 - accuracy: 0.8344\n",
            "Epoch 144/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3643 - accuracy: 0.8374\n",
            "Epoch 145/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3648 - accuracy: 0.8350\n",
            "Epoch 146/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3650 - accuracy: 0.8353\n",
            "Epoch 147/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3647 - accuracy: 0.8373\n",
            "Epoch 148/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3643 - accuracy: 0.8368\n",
            "Epoch 149/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3646 - accuracy: 0.8384\n",
            "Epoch 150/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3638 - accuracy: 0.8368\n",
            "Epoch 151/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3646 - accuracy: 0.8384\n",
            "Epoch 152/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3646 - accuracy: 0.8377\n",
            "Epoch 153/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3639 - accuracy: 0.8392\n",
            "Epoch 154/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3639 - accuracy: 0.8386\n",
            "Epoch 155/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3636 - accuracy: 0.8364\n",
            "Epoch 156/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3633 - accuracy: 0.8374\n",
            "Epoch 157/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3640 - accuracy: 0.8384\n",
            "Epoch 158/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3639 - accuracy: 0.8380\n",
            "Epoch 159/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3643 - accuracy: 0.8383\n",
            "Epoch 160/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3636 - accuracy: 0.8362\n",
            "Epoch 161/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3646 - accuracy: 0.8376\n",
            "Epoch 162/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3635 - accuracy: 0.8379\n",
            "Epoch 163/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3641 - accuracy: 0.8379\n",
            "Epoch 164/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3643 - accuracy: 0.8368\n",
            "Epoch 165/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3643 - accuracy: 0.8364\n",
            "Epoch 166/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3634 - accuracy: 0.8383\n",
            "Epoch 167/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3636 - accuracy: 0.8362\n",
            "Epoch 168/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3637 - accuracy: 0.8373\n",
            "Epoch 169/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3637 - accuracy: 0.8376\n",
            "Epoch 170/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3638 - accuracy: 0.8375\n",
            "Epoch 171/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3630 - accuracy: 0.8388\n",
            "Epoch 172/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3635 - accuracy: 0.8394\n",
            "Epoch 173/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3640 - accuracy: 0.8387\n",
            "Epoch 174/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3633 - accuracy: 0.8375\n",
            "Epoch 175/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3639 - accuracy: 0.8387\n",
            "Epoch 176/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3631 - accuracy: 0.8387\n",
            "Epoch 177/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3631 - accuracy: 0.8362\n",
            "Epoch 178/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3633 - accuracy: 0.8378\n",
            "Epoch 179/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3635 - accuracy: 0.8398\n",
            "Epoch 180/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3643 - accuracy: 0.8379\n",
            "Epoch 181/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3628 - accuracy: 0.8387\n",
            "Epoch 182/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3632 - accuracy: 0.8373\n",
            "Epoch 183/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3630 - accuracy: 0.8391\n",
            "Epoch 184/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3635 - accuracy: 0.8379\n",
            "Epoch 185/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3633 - accuracy: 0.8379\n",
            "Epoch 186/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3633 - accuracy: 0.8386\n",
            "Epoch 187/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3630 - accuracy: 0.8378\n",
            "Epoch 188/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3631 - accuracy: 0.8376\n",
            "Epoch 189/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3633 - accuracy: 0.8382\n",
            "Epoch 190/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3622 - accuracy: 0.8379\n",
            "Epoch 191/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3630 - accuracy: 0.8388\n",
            "Epoch 192/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3622 - accuracy: 0.8396\n",
            "Epoch 193/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3633 - accuracy: 0.8389\n",
            "Epoch 194/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3634 - accuracy: 0.8394\n",
            "Epoch 195/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3626 - accuracy: 0.8384\n",
            "Epoch 196/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3632 - accuracy: 0.8400\n",
            "Epoch 197/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3629 - accuracy: 0.8394\n",
            "Epoch 198/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3626 - accuracy: 0.8370\n",
            "Epoch 199/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3627 - accuracy: 0.8392\n",
            "Epoch 200/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3632 - accuracy: 0.8384\n",
            "Train 200 epochs, 152 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEmSlrt_gYZZ"
      },
      "source": [
        "def evaluate(model,min,max):\n",
        "    print(\"Evaluate again on lengths %d to %d\"%(min,max))\n",
        "    print(\"slice...\")\n",
        "    subset=make_slice(train_set,min,max)\n",
        "    print(\"kmers...\")\n",
        "    (X_valid,y_valid)=make_kmers(max,subset)\n",
        "    print(\"frequencies...\")\n",
        "    X_valid=make_frequencies(X_valid)\n",
        "    print(\"evaluate....\")\n",
        "    scores = model.evaluate(X_valid, y_valid, verbose=1)  # valid = train, expect 100%\n",
        "    print(\"Evaluated on lengths %d to %d\"%(min,max))\n",
        "    print(\"%s: %.2f%%\\n\" % (model.metrics_names[1], scores[1]*100))\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiNABsVNgYZc",
        "outputId": "64466b9e-ed2d-4ca4-bd28-61c6222116c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "evaluate(model,200,1000)\n",
        "evaluate(model,1000,2000)\n",
        "evaluate(model,2000,3000)\n",
        "# crashed after this\n",
        "#evaluate(model,3000,10000)\n",
        "#evaluate(model,10000,30000)\n",
        "#evaluate(model,200,30000)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate again on lengths 200 to 1000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (30290, 4)\n",
            "no long, no short (8879, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(8879, 1)\n",
            "sequence    AGTCCCTCCCCAGCCCAGCAGTCCCTCCAGGCTACATCCAGGAGAC...\n",
            "Name: 1280, dtype: object\n",
            "348\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[ 46 182 214 ...   0   0   0]\n",
            " [ 36 142  56 ...   0   0   0]\n",
            " [135  28 110 ...   0   0   0]\n",
            " ...\n",
            " [147  73  36 ...   0   0   0]\n",
            " [228 143  57 ...   0   0   0]\n",
            " [131  12  47 ...   0   0   0]]\n",
            "frequencies...\n",
            "evaluate....\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.3614 - accuracy: 0.8387\n",
            "Evaluated on lengths 200 to 1000\n",
            "accuracy: 83.87%\n",
            "\n",
            "Evaluate again on lengths 1000 to 2000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (9273, 4)\n",
            "no long, no short (3368, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(3368, 1)\n",
            "sequence    GGCGGGGTCGACTGACGGTAACGGGGCAGAGAGGCTGTTCGCAGAG...\n",
            "Name: 12641, dtype: object\n",
            "1338\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[167 155 107 ...   0   0   0]\n",
            " [226 133  20 ...   0   0   0]\n",
            " [108 175 186 ...   0   0   0]\n",
            " ...\n",
            " [175 185 225 ...   0   0   0]\n",
            " [ 37 148  78 ...   0   0   0]\n",
            " [188 240 192 ...   0   0   0]]\n",
            "frequencies...\n",
            "evaluate....\n",
            "106/106 [==============================] - 0s 2ms/step - loss: 0.3893 - accuracy: 0.8319\n",
            "Evaluated on lengths 1000 to 2000\n",
            "accuracy: 83.19%\n",
            "\n",
            "Evaluate again on lengths 2000 to 3000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (3221, 4)\n",
            "no long, no short (1351, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(1351, 1)\n",
            "sequence    GTCATTCTAGCTGCCTGCTGCCTCCGCAGCGTCCCCCCAGCTCTCC...\n",
            "Name: 19713, dtype: object\n",
            "2039\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[181 212  80 ...   0   0   0]\n",
            " [ 10  37 148 ...   0   0   0]\n",
            " [ 48 190 245 ...   0   0   0]\n",
            " ...\n",
            " [ 49 195  10 ...   0   0   0]\n",
            " [153  97 131 ...   0   0   0]\n",
            " [ 36 143  58 ...   0   0   0]]\n",
            "frequencies...\n",
            "evaluate....\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 0.3911 - accuracy: 0.8187\n",
            "Evaluated on lengths 2000 to 3000\n",
            "accuracy: 81.87%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbieSKdOjeaL",
        "outputId": "35ee2dfc-e97a-4da0-8eef-f4c77f12c37d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "evaluate(model,3000,5000)\n",
        "evaluate(model,5000,10000)\n",
        "evaluate(model,10000,30000)\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate again on lengths 3000 to 5000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (1336, 4)\n",
            "no long, no short (895, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(895, 1)\n",
            "sequence    AGGCCGCGTCCGCCCGCGCCCGCTCTGGCCCCCGCGGAGCCGCGCA...\n",
            "Name: 19203, dtype: object\n",
            "3491\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[ 42 166 151 ...   0   0   0]\n",
            " [ 62 245 209 ...   0   0   0]\n",
            " [ 47 185 228 ...   0   0   0]\n",
            " ...\n",
            " [166 151  92 ...   0   0   0]\n",
            " [ 59 233 163 ...   0   0   0]\n",
            " [  3  11  41 ...   0   0   0]]\n",
            "frequencies...\n",
            "evaluate....\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.5820 - accuracy: 0.7184\n",
            "Evaluated on lengths 3000 to 5000\n",
            "accuracy: 71.84%\n",
            "\n",
            "Evaluate again on lengths 5000 to 10000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (345, 4)\n",
            "no long, no short (314, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(314, 1)\n",
            "sequence    GGGAGCTCCGTTGTGCGTCGCTTAAGTGAGGGCGGCGGATGGGCGA...\n",
            "Name: 19766, dtype: object\n",
            "6207\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[169 163 138 ...   0   0   0]\n",
            " [167 155 106 ...   0   0   0]\n",
            " [ 19  76  47 ...   0   0   0]\n",
            " ...\n",
            " [ 91 107 169 ...   0   0   0]\n",
            " [ 62 247 218 ...   0   0   0]\n",
            " [229 145  67 ...   0   0   0]]\n",
            "frequencies...\n",
            "evaluate....\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9865 - accuracy: 0.4522\n",
            "Evaluated on lengths 5000 to 10000\n",
            "accuracy: 45.22%\n",
            "\n",
            "Evaluate again on lengths 10000 to 30000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (29, 4)\n",
            "no long, no short (29, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(29, 1)\n",
            "sequence    GCGCTAGCTCCCATGCTGGCCTCGGTGCCACTCGCGCGCCGGCCGC...\n",
            "Name: 7362, dtype: object\n",
            "11322\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[154 104 157 ...   0   0   0]\n",
            " [160 128 254 ...   0   0   0]\n",
            " [ 59 235 169 ...   0   0   0]\n",
            " ...\n",
            " [ 47 187 236 ...   0   0   0]\n",
            " [ 47 188 239 ...   0   0   0]\n",
            " [139  41 163 ...   0   0   0]]\n",
            "frequencies...\n",
            "evaluate....\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6751 - accuracy: 0.2069\n",
            "Evaluated on lengths 10000 to 30000\n",
            "accuracy: 20.69%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd3Wj_vI9KdP"
      },
      "source": [
        "## Len 200-1Kb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ8eW5Rg9KdQ"
      },
      "source": [
        "MINLEN=200\n",
        "MAXLEN=1000\n",
        "\n",
        "if False:\n",
        "  print (\"Compile the model\")\n",
        "  model=build_model(MAXLEN)\n",
        "  print (\"Summarize the model\")\n",
        "  print(model.summary())  # Print this only once\n",
        "  print(\"Working on full training set, slice by sequence length.\")\n",
        "  print(\"Slice size range [%d - %d)\"%(MINLEN,MAXLEN))\n",
        "  subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
        "  print (\"Sequence to Kmer\")\n",
        "  (X_train,y_train)=make_kmers(MAXLEN,subset)\n",
        "  X_train\n",
        "  X_train=make_frequencies(X_train)\n",
        "  X_train\n",
        "  print (\"Cross valiation\")\n",
        "  model1 = do_cross_validation(X_train,y_train,MAXLEN)\n",
        "  model1.save(FILENAME+'.short.model')"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC68X4zr9KdU"
      },
      "source": [
        "## Len 1Kb-2Kb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nm3oU3h9KdV"
      },
      "source": [
        "MINLEN=1000\n",
        "MAXLEN=2000\n",
        "\n",
        "if False:\n",
        "    print (\"Compile the model\")\n",
        "    model=build_model(MAXLEN)\n",
        "    print (\"Summarize the model\")\n",
        "    print(model.summary())  # Print this only once\n",
        "    print(\"Working on full training set, slice by sequence length.\")\n",
        "    print(\"Slice size range [%d - %d)\"%(MINLEN,MAXLEN))\n",
        "    subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
        "    print (\"Sequence to Kmer\")\n",
        "    (X_train,y_train)=make_kmers(MAXLEN,subset)\n",
        "    X_train\n",
        "    X_train=make_frequencies(X_train)\n",
        "    X_train\n",
        "    print (\"Cross valiation\")\n",
        "    model2 = do_cross_validation(X_train,y_train,MAXLEN)\n",
        "    model2.save(FILENAME+'.medium.model')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyACRnZx9Kde"
      },
      "source": [
        "## Len 2Kb-3Kb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUxmLnQ-9Kde"
      },
      "source": [
        "MINLEN=2000\n",
        "MAXLEN=3000\n",
        "\n",
        "if False:\n",
        "    print (\"Compile the model\")\n",
        "    model=build_model(MAXLEN)\n",
        "    print (\"Summarize the model\")\n",
        "    print(model.summary())  # Print this only once\n",
        "    print(\"Working on full training set, slice by sequence length.\")\n",
        "    print(\"Slice size range [%d - %d)\"%(MINLEN,MAXLEN))\n",
        "    subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
        "    print (\"Sequence to Kmer\")\n",
        "    (X_train,y_train)=make_kmers(MAXLEN,subset)\n",
        "    X_train\n",
        "    X_train=make_frequencies(X_train)\n",
        "    X_train\n",
        "    print (\"Cross valiation\")\n",
        "    model3 = do_cross_validation(X_train,y_train,MAXLEN)\n",
        "    model3.save(FILENAME+'.long.model')"
      ],
      "execution_count": 41,
      "outputs": []
    }
  ]
}