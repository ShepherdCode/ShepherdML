{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM_106.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojm_6E9f9Kcf"
      },
      "source": [
        "# LSTM 106\n",
        "Train on medium, evaluate on all"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh6XplUvC0j0",
        "outputId": "1b76df6b-bfa2-4168-d239-e74e92913b7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "PATH='/content/drive/'\n",
        "drive.mount(PATH)\n",
        "DATAPATH=PATH+'My Drive/data/'\n",
        "PC_FILENAME = DATAPATH+'pcRNA.fasta'\n",
        "NC_FILENAME = DATAPATH+'ncRNA.fasta'\n",
        "#PC_FILENAME = 'pcRNA.fasta'\n",
        "#NC_FILENAME = 'ncRNA.fasta'\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQY7aTj29Kch"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LayerNormalization\n",
        "import time\n",
        "\n",
        "dt='float32'\n",
        "tf.keras.backend.set_floatx(dt)\n",
        "\n",
        "EPOCHS=200\n",
        "SPLITS=1\n",
        "K=4\n",
        "VOCABULARY_SIZE=4**K+1   # e.g. K=3 => 64 DNA K-mers + 'NNN'\n",
        "EMBED_DIMEN=16\n",
        "FILENAME='LSTM106'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV6k-xOm9Kcn"
      },
      "source": [
        "## Load and partition sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I-O_qzw9Kco"
      },
      "source": [
        "# Assume file was preprocessed to contain one line per seq.\n",
        "# Prefer Pandas dataframe but df does not support append.\n",
        "# For conversion to tensor, must avoid python lists.\n",
        "def load_fasta(filename,label):\n",
        "    DEFLINE='>'\n",
        "    labels=[]\n",
        "    seqs=[]\n",
        "    lens=[]\n",
        "    nums=[]\n",
        "    num=0\n",
        "    with open (filename,'r') as infile:\n",
        "        for line in infile:\n",
        "            if line[0]!=DEFLINE:\n",
        "                seq=line.rstrip()\n",
        "                num += 1   # first seqnum is 1\n",
        "                seqlen=len(seq)\n",
        "                nums.append(num)\n",
        "                labels.append(label)\n",
        "                seqs.append(seq)\n",
        "                lens.append(seqlen)\n",
        "    df1=pd.DataFrame(nums,columns=['seqnum'])\n",
        "    df2=pd.DataFrame(labels,columns=['class'])\n",
        "    df3=pd.DataFrame(seqs,columns=['sequence'])\n",
        "    df4=pd.DataFrame(lens,columns=['seqlen'])\n",
        "    df=pd.concat((df1,df2,df3,df4),axis=1)\n",
        "    return df\n",
        "\n",
        "# Split into train/test stratified by sequence length.\n",
        "def sizebin(df):\n",
        "    return pd.cut(df[\"seqlen\"],\n",
        "                              bins=[0,1000,2000,4000,8000,16000,np.inf],\n",
        "                              labels=[0,1,2,3,4,5])\n",
        "def make_train_test(data):\n",
        "    bin_labels= sizebin(data)\n",
        "    from sklearn.model_selection import StratifiedShuffleSplit\n",
        "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=37863)\n",
        "    # split(x,y) expects that y is the labels. \n",
        "    # Trick: Instead of y, give it it the bin labels that we generated.\n",
        "    for train_index,test_index in splitter.split(data,bin_labels):\n",
        "        train_set = data.iloc[train_index]\n",
        "        test_set = data.iloc[test_index]\n",
        "    return (train_set,test_set)\n",
        "\n",
        "def separate_X_and_y(data):\n",
        "    y=   data[['class']].copy()\n",
        "    X=   data.drop(columns=['class','seqnum','seqlen'])\n",
        "    return (X,y)\n",
        "\n",
        "def make_slice(data_set,min_len,max_len):\n",
        "    print(\"original \"+str(data_set.shape))\n",
        "    too_short = data_set[ data_set['seqlen'] < min_len ].index\n",
        "    no_short=data_set.drop(too_short)\n",
        "    print(\"no short \"+str(no_short.shape))\n",
        "    too_long = no_short[ no_short['seqlen'] >= max_len ].index\n",
        "    no_long_no_short=no_short.drop(too_long)\n",
        "    print(\"no long, no short \"+str(no_long_no_short.shape))\n",
        "    return no_long_no_short\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRAaO9jP9Kcr"
      },
      "source": [
        "## Make K-mers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8xcZ4Mr9Kcs"
      },
      "source": [
        "def make_kmer_table(K):\n",
        "    npad='N'*K\n",
        "    shorter_kmers=['']\n",
        "    for i in range(K):\n",
        "        longer_kmers=[]\n",
        "        for mer in shorter_kmers:\n",
        "            longer_kmers.append(mer+'A')\n",
        "            longer_kmers.append(mer+'C')\n",
        "            longer_kmers.append(mer+'G')\n",
        "            longer_kmers.append(mer+'T')\n",
        "        shorter_kmers = longer_kmers\n",
        "    all_kmers = shorter_kmers\n",
        "    kmer_dict = {}\n",
        "    kmer_dict[npad]=0\n",
        "    value=1\n",
        "    for mer in all_kmers:\n",
        "        kmer_dict[mer]=value\n",
        "        value += 1\n",
        "    return kmer_dict\n",
        "\n",
        "KMER_TABLE=make_kmer_table(K)\n",
        "\n",
        "def strings_to_vectors(data,uniform_len):\n",
        "    all_seqs=[]\n",
        "    for seq in data['sequence']:\n",
        "        i=0\n",
        "        seqlen=len(seq)\n",
        "        kmers=[]\n",
        "        while i < seqlen-K+1 -1:  # stop at minus one for spaced seed\n",
        "            #kmer=seq[i:i+2]+seq[i+3:i+5]    # SPACED SEED 2/1/2 for K=4\n",
        "            kmer=seq[i:i+K]  \n",
        "            i += 1\n",
        "            value=KMER_TABLE[kmer]\n",
        "            kmers.append(value)\n",
        "        pad_val=0\n",
        "        while i < uniform_len:\n",
        "            kmers.append(pad_val)\n",
        "            i += 1\n",
        "        all_seqs.append(kmers)\n",
        "    pd2d=pd.DataFrame(all_seqs)\n",
        "    return pd2d   # return 2D dataframe, uniform dimensions"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEtA0xiV9Kcv"
      },
      "source": [
        "def make_kmers(MAXLEN,train_set):\n",
        "    (X_train_all,y_train_all)=separate_X_and_y(train_set)\n",
        "\n",
        "    # The returned values are Pandas dataframes.\n",
        "    # print(X_train_all.shape,y_train_all.shape)\n",
        "    # (X_train_all,y_train_all)\n",
        "    # y: Pandas dataframe to Python list.\n",
        "    # y_train_all=y_train_all.values.tolist()\n",
        "    # The sequences lengths are bounded but not uniform.\n",
        "    X_train_all\n",
        "    print(type(X_train_all))\n",
        "    print(X_train_all.shape)\n",
        "    print(X_train_all.iloc[0])\n",
        "    print(len(X_train_all.iloc[0]['sequence']))\n",
        "\n",
        "    # X: List of string to List of uniform-length ordered lists of K-mers.\n",
        "    X_train_kmers=strings_to_vectors(X_train_all,MAXLEN)\n",
        "    # X: true 2D array (no more lists)\n",
        "    X_train_kmers.shape\n",
        "\n",
        "    print(\"transform...\")\n",
        "    # From pandas dataframe to numpy to list to numpy\n",
        "    print(type(X_train_kmers))\n",
        "    num_seqs=len(X_train_kmers)\n",
        "    tmp_seqs=[]\n",
        "    for i in range(num_seqs):\n",
        "        kmer_sequence=X_train_kmers.iloc[i]\n",
        "        tmp_seqs.append(kmer_sequence)\n",
        "    X_train_kmers=np.array(tmp_seqs)\n",
        "    tmp_seqs=None\n",
        "    print(type(X_train_kmers))\n",
        "    print(X_train_kmers)\n",
        "\n",
        "    labels=y_train_all.to_numpy()\n",
        "    return (X_train_kmers,labels)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaXyySyO9Kcz"
      },
      "source": [
        "def make_frequencies(Xin):\n",
        "    # Input:  numpy X(numseq,seqlen)  list of vectors of kmerval where val0=NNN,val1=AAA,etc. \n",
        "    # Output: numpy X(numseq,65)    list of frequencies of 0,1,etc.\n",
        "    Xout=[]\n",
        "    VOCABULARY_SIZE= 4**K + 1  # plus one for 'NNN'\n",
        "    for seq in Xin:\n",
        "        freqs =[0] * VOCABULARY_SIZE\n",
        "        total = 0\n",
        "        for kmerval in seq:\n",
        "            freqs[kmerval] += 1\n",
        "            total += 1\n",
        "        for c in range(VOCABULARY_SIZE):\n",
        "            freqs[c] = freqs[c]/total\n",
        "        Xout.append(freqs)\n",
        "    Xnum = np.asarray(Xout)\n",
        "    return (Xnum)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7jcg6Wl9Kc2"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLFNO1Xa9Kc3"
      },
      "source": [
        "def build_model(maxlen):\n",
        "    vocabulary_size=4**K+1   # e.g. K=3 => 64 DNA K-mers + 'NNN'\n",
        "    act=\"sigmoid\"\n",
        "    dt='float32'\n",
        "\n",
        "    neurons=16\n",
        "    dot = 0.50\n",
        "    rnn = keras.models.Sequential()\n",
        "    embed_layer = keras.layers.Embedding(\n",
        "        vocabulary_size,EMBED_DIMEN,input_length=maxlen);\n",
        "    rnn1_layer = keras.layers.Bidirectional(\n",
        "        keras.layers.LSTM(neurons, return_sequences=True, dropout=dot, \n",
        "            input_shape=[maxlen,EMBED_DIMEN]))\n",
        "    rnn2_layer = keras.layers.Bidirectional(\n",
        "        keras.layers.LSTM(neurons, dropout=dot, return_sequences=True))\n",
        "    dense1_layer = keras.layers.Dense(neurons,activation=act,dtype=dt)\n",
        "    drop1_layer  = keras.layers.Dropout(dot)\n",
        "    dense2_layer = keras.layers.Dense(neurons,activation=act,dtype=dt)\n",
        "    drop2_layer  = keras.layers.Dropout(dot)\n",
        "    output_layer = keras.layers.Dense(1,activation=act,dtype=dt)\n",
        "\n",
        "    rnn.add(embed_layer)\n",
        "    rnn.add(rnn1_layer)\n",
        "    rnn.add(rnn2_layer)\n",
        "    rnn.add(dense1_layer)\n",
        "    rnn.add(drop1_layer)\n",
        "    rnn.add(dense2_layer)\n",
        "    rnn.add(drop2_layer)\n",
        "    rnn.add(output_layer)\n",
        "\n",
        "    bc=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    print(\"COMPILE\")\n",
        "    rnn.compile(loss=bc, optimizer=\"Adam\",metrics=[\"accuracy\"])\n",
        "    return rnn"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdIS2utq9Kc9"
      },
      "source": [
        "## Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVo4tbB_9Kc-"
      },
      "source": [
        "def do_cross_validation(X,y,maxlen):\n",
        "    model = None\n",
        "    cv_scores = []\n",
        "    fold=0\n",
        "    splitter = ShuffleSplit(n_splits=SPLITS, test_size=0.2, random_state=37863)\n",
        "    for train_index,valid_index in splitter.split(X):\n",
        "        X_train=X[train_index] # use iloc[] for dataframe\n",
        "        y_train=y[train_index]\n",
        "        X_valid=X[valid_index]\n",
        "        y_valid=y[valid_index]\n",
        "\n",
        "        print(\"BUILD MODEL\")\n",
        "        model=build_model(maxlen)\n",
        "\n",
        "        print(\"FIT\")\n",
        "        start_time=time.time()\n",
        "        # this is complaining about string to float\n",
        "        history=model.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
        "                epochs=EPOCHS, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
        "                validation_data=(X_valid,y_valid) )\n",
        "        end_time=time.time()\n",
        "        elapsed_time=(end_time-start_time)\n",
        "                        \n",
        "        fold += 1\n",
        "        print(\"Fold %d, %d epochs, %d sec\"%(fold,EPOCHS,elapsed_time))\n",
        "\n",
        "        pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "        plt.grid(True)\n",
        "        plt.gca().set_ylim(0,1)\n",
        "        plt.show()\n",
        "\n",
        "        scores = model.evaluate(X_valid, y_valid, verbose=0)\n",
        "        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "        # What are the other metrics_names?\n",
        "        # Try this from Geron page 505:\n",
        "        # np.mean(keras.losses.mean_squared_error(y_valid,y_pred))\n",
        "        cv_scores.append(scores[1] * 100)  \n",
        "    print()\n",
        "    print(\"Validation core mean %.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upc3S0eEgYZG"
      },
      "source": [
        "def just_train(model,X_train,y_train,maxlen):\n",
        "    print(\"FIT\")\n",
        "    start_time=time.time()\n",
        "    # this is complaining about string to float\n",
        "    history=model.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
        "            epochs=EPOCHS, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
        "            )  # no validation data\n",
        "    end_time=time.time()\n",
        "    elapsed_time=(end_time-start_time)\n",
        "    print(\"Train %d epochs, %d sec\"%(EPOCHS,elapsed_time))\n",
        "\n",
        "    pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "    plt.grid(True)\n",
        "    plt.gca().set_ylim(0,1)\n",
        "    plt.show()\n",
        "\n",
        "    #scores = model.evaluate(X_valid, y_valid, verbose=0)\n",
        "    #print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "    #cv_scores.append(scores[1] * 100)  \n",
        "    #print()\n",
        "    #print(\"Validation core mean %.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q-PEh7D9KdH"
      },
      "source": [
        "## Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8fNo6sn9KdH",
        "outputId": "fc0bfd28-0d07-42f0-8bca-a17c3be35c76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "print(\"Load data from files.\")\n",
        "nc_seq=load_fasta(NC_FILENAME,0)\n",
        "pc_seq=load_fasta(PC_FILENAME,1)\n",
        "all_seq=pd.concat((nc_seq,pc_seq),axis=0)\n",
        "\n",
        "print(\"Put aside the test portion.\")\n",
        "(train_set,test_set)=make_train_test(all_seq)\n",
        "# Do this later when using the test data:\n",
        "# (X_test,y_test)=separate_X_and_y(test_set)\n",
        "\n",
        "nc_seq=None\n",
        "pc_seq=None\n",
        "all_seq=None\n",
        "\n",
        "print(\"Ready: train_set\")\n",
        "train_set"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load data from files.\n",
            "Put aside the test portion.\n",
            "Ready: train_set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>seqnum</th>\n",
              "      <th>class</th>\n",
              "      <th>sequence</th>\n",
              "      <th>seqlen</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1280</th>\n",
              "      <td>1281</td>\n",
              "      <td>0</td>\n",
              "      <td>AGTCCCTCCCCAGCCCAGCAGTCCCTCCAGGCTACATCCAGGAGAC...</td>\n",
              "      <td>348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9088</th>\n",
              "      <td>9089</td>\n",
              "      <td>0</td>\n",
              "      <td>CAGCTCCTGGGATGGCCTCACCTGAGGAGACTCTTGGGCCTTGGCA...</td>\n",
              "      <td>534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6069</th>\n",
              "      <td>6070</td>\n",
              "      <td>1</td>\n",
              "      <td>AGATCTAGGGATGGGGATGGGGAGGAGAAGTGGGAATGGGAAATTG...</td>\n",
              "      <td>592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18549</th>\n",
              "      <td>18550</td>\n",
              "      <td>1</td>\n",
              "      <td>GACGTCTCCCGCGGGCGTCGGCAGGGTCGGCGGCGTCGGCAGCAGT...</td>\n",
              "      <td>945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15027</th>\n",
              "      <td>15028</td>\n",
              "      <td>1</td>\n",
              "      <td>GAGCGCGCGAGCCGGGCCCGGAGCGCACGCCGCCGCCGCCACCGCC...</td>\n",
              "      <td>4382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3386</th>\n",
              "      <td>3387</td>\n",
              "      <td>0</td>\n",
              "      <td>TTTATGTGGATTGTCTGTCTCATGCTTGTTTCACCAGGGTAGTTAC...</td>\n",
              "      <td>578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6495</th>\n",
              "      <td>6496</td>\n",
              "      <td>0</td>\n",
              "      <td>ATAATGGGAAACTAAGGGCAAGTTCTCATGTTCCTGGTCCTGGCTT...</td>\n",
              "      <td>562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6409</th>\n",
              "      <td>6410</td>\n",
              "      <td>1</td>\n",
              "      <td>GGGTTTATTACTACTGAAGGAAGAACGTGAGTAGGTTAGGATTTCG...</td>\n",
              "      <td>740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7640</th>\n",
              "      <td>7641</td>\n",
              "      <td>1</td>\n",
              "      <td>ACAGCTGTGTTTGGCTGCAGGGCCAAGAGCGCTGTCAAGAAGACCC...</td>\n",
              "      <td>3156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14108</th>\n",
              "      <td>14109</td>\n",
              "      <td>0</td>\n",
              "      <td>GAAGTGATTGCAAGTTCAGCAGCATGAAACTGCTCTTTATCCGTGC...</td>\n",
              "      <td>466</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30290 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       seqnum  class                                           sequence  seqlen\n",
              "1280     1281      0  AGTCCCTCCCCAGCCCAGCAGTCCCTCCAGGCTACATCCAGGAGAC...     348\n",
              "9088     9089      0  CAGCTCCTGGGATGGCCTCACCTGAGGAGACTCTTGGGCCTTGGCA...     534\n",
              "6069     6070      1  AGATCTAGGGATGGGGATGGGGAGGAGAAGTGGGAATGGGAAATTG...     592\n",
              "18549   18550      1  GACGTCTCCCGCGGGCGTCGGCAGGGTCGGCGGCGTCGGCAGCAGT...     945\n",
              "15027   15028      1  GAGCGCGCGAGCCGGGCCCGGAGCGCACGCCGCCGCCGCCACCGCC...    4382\n",
              "...       ...    ...                                                ...     ...\n",
              "3386     3387      0  TTTATGTGGATTGTCTGTCTCATGCTTGTTTCACCAGGGTAGTTAC...     578\n",
              "6495     6496      0  ATAATGGGAAACTAAGGGCAAGTTCTCATGTTCCTGGTCCTGGCTT...     562\n",
              "6409     6410      1  GGGTTTATTACTACTGAAGGAAGAACGTGAGTAGGTTAGGATTTCG...     740\n",
              "7640     7641      1  ACAGCTGTGTTTGGCTGCAGGGCCAAGAGCGCTGTCAAGAAGACCC...    3156\n",
              "14108   14109      0  GAAGTGATTGCAAGTTCAGCAGCATGAAACTGCTCTTTATCCGTGC...     466\n",
              "\n",
              "[30290 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lI2PjJsxgYZT"
      },
      "source": [
        "# Reuse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKLYbWycgYZU",
        "outputId": "5d9023cf-21b2-4c88-dd4e-7b45180cdb3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#reload=model1.load(FILENAME+'.short.model')\n",
        "#W = model1.get_weights()\n",
        "#scores = model1.evaluate(X_valid, y_valid, verbose=0)\n",
        "#print(\"%s: %.2f%%\" % (model1.metrics_names[1], scores[1]*100))\n",
        "\n",
        "min=1000\n",
        "max=2000\n",
        "print(\"Train on lengths %d to %d\"%(min,max))\n",
        "print(\"slice...\")\n",
        "subset=make_slice(train_set,min,max)\n",
        "print(\"kmers...\")\n",
        "(X_train,y_train)=make_kmers(max,subset)\n",
        "print(\"BUILD MODEL\")\n",
        "model=build_model(max)\n",
        "model=just_train(model,X_train,y_train,max)\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on lengths 1000 to 2000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (9273, 4)\n",
            "no long, no short (3368, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(3368, 1)\n",
            "sequence    GGCGGGGTCGACTGACGGTAACGGGGCAGAGAGGCTGTTCGCAGAG...\n",
            "Name: 12641, dtype: object\n",
            "1338\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[167 155 107 ...   0   0   0]\n",
            " [226 133  20 ...   0   0   0]\n",
            " [108 175 186 ...   0   0   0]\n",
            " ...\n",
            " [175 185 225 ...   0   0   0]\n",
            " [ 37 148  78 ...   0   0   0]\n",
            " [188 240 192 ...   0   0   0]]\n",
            "BUILD MODEL\n",
            "COMPILE\n",
            "FIT\n",
            "Epoch 1/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.7066 - accuracy: 0.5754\n",
            "Epoch 2/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.6803 - accuracy: 0.5875\n",
            "Epoch 3/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.6638 - accuracy: 0.5942\n",
            "Epoch 4/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.6460 - accuracy: 0.6125\n",
            "Epoch 5/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.6098 - accuracy: 0.6619\n",
            "Epoch 6/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.6025 - accuracy: 0.6816\n",
            "Epoch 7/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.5902 - accuracy: 0.7011\n",
            "Epoch 8/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.6244 - accuracy: 0.6522\n",
            "Epoch 9/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.6364 - accuracy: 0.6171\n",
            "Epoch 10/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.6025 - accuracy: 0.6807\n",
            "Epoch 11/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.5840 - accuracy: 0.7204\n",
            "Epoch 12/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5804 - accuracy: 0.7217\n",
            "Epoch 13/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.6267 - accuracy: 0.6819\n",
            "Epoch 14/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.6618 - accuracy: 0.6030\n",
            "Epoch 15/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.5978 - accuracy: 0.6540\n",
            "Epoch 16/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5885 - accuracy: 0.6982\n",
            "Epoch 17/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5840 - accuracy: 0.7247\n",
            "Epoch 18/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5982 - accuracy: 0.7204\n",
            "Epoch 19/200\n",
            "106/106 [==============================] - 23s 220ms/step - loss: 0.6047 - accuracy: 0.7145\n",
            "Epoch 20/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5965 - accuracy: 0.7224\n",
            "Epoch 21/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.6128 - accuracy: 0.7079\n",
            "Epoch 22/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.6325 - accuracy: 0.6772\n",
            "Epoch 23/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.6521 - accuracy: 0.6433\n",
            "Epoch 24/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5988 - accuracy: 0.7138\n",
            "Epoch 25/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.5679 - accuracy: 0.7423\n",
            "Epoch 26/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5642 - accuracy: 0.7393\n",
            "Epoch 27/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.5818 - accuracy: 0.7151\n",
            "Epoch 28/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5965 - accuracy: 0.6822\n",
            "Epoch 29/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.5792 - accuracy: 0.7083\n",
            "Epoch 30/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.5763 - accuracy: 0.7166\n",
            "Epoch 31/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5757 - accuracy: 0.7067\n",
            "Epoch 32/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.5545 - accuracy: 0.7395\n",
            "Epoch 33/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.5717 - accuracy: 0.6861\n",
            "Epoch 34/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.5608 - accuracy: 0.7343\n",
            "Epoch 35/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.6990 - accuracy: 0.5440\n",
            "Epoch 36/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.6634 - accuracy: 0.6185\n",
            "Epoch 37/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.6644 - accuracy: 0.6185\n",
            "Epoch 38/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.6581 - accuracy: 0.6185\n",
            "Epoch 39/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.6294 - accuracy: 0.6185\n",
            "Epoch 40/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5921 - accuracy: 0.6219\n",
            "Epoch 41/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.6543 - accuracy: 0.6293\n",
            "Epoch 42/200\n",
            "106/106 [==============================] - 23s 214ms/step - loss: 0.6374 - accuracy: 0.6172\n",
            "Epoch 43/200\n",
            "106/106 [==============================] - 23s 214ms/step - loss: 0.6304 - accuracy: 0.6159\n",
            "Epoch 44/200\n",
            "106/106 [==============================] - 23s 214ms/step - loss: 0.6287 - accuracy: 0.6184\n",
            "Epoch 45/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.6185 - accuracy: 0.6407\n",
            "Epoch 46/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.5932 - accuracy: 0.6836\n",
            "Epoch 47/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.5606 - accuracy: 0.6970\n",
            "Epoch 48/200\n",
            "106/106 [==============================] - 23s 214ms/step - loss: 0.5338 - accuracy: 0.7359\n",
            "Epoch 49/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.5383 - accuracy: 0.7433\n",
            "Epoch 50/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.5332 - accuracy: 0.7318\n",
            "Epoch 51/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.5195 - accuracy: 0.7394\n",
            "Epoch 52/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.5367 - accuracy: 0.7046\n",
            "Epoch 53/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.5292 - accuracy: 0.7425\n",
            "Epoch 54/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.5143 - accuracy: 0.7555\n",
            "Epoch 55/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.4975 - accuracy: 0.7618\n",
            "Epoch 56/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.4846 - accuracy: 0.7769\n",
            "Epoch 57/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.4752 - accuracy: 0.7947\n",
            "Epoch 58/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.4982 - accuracy: 0.7816\n",
            "Epoch 59/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.4751 - accuracy: 0.7906\n",
            "Epoch 60/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.4591 - accuracy: 0.8023\n",
            "Epoch 61/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.4523 - accuracy: 0.8107\n",
            "Epoch 62/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.4646 - accuracy: 0.7964\n",
            "Epoch 63/200\n",
            "106/106 [==============================] - 23s 214ms/step - loss: 0.4530 - accuracy: 0.8144\n",
            "Epoch 64/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.5969 - accuracy: 0.7425\n",
            "Epoch 65/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.6361 - accuracy: 0.6165\n",
            "Epoch 66/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.6039 - accuracy: 0.6639\n",
            "Epoch 67/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5895 - accuracy: 0.6808\n",
            "Epoch 68/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.5892 - accuracy: 0.6806\n",
            "Epoch 69/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.5855 - accuracy: 0.6815\n",
            "Epoch 70/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.5972 - accuracy: 0.6779\n",
            "Epoch 71/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5883 - accuracy: 0.6818\n",
            "Epoch 72/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5990 - accuracy: 0.6702\n",
            "Epoch 73/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.6418 - accuracy: 0.6529\n",
            "Epoch 74/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.6223 - accuracy: 0.6812\n",
            "Epoch 75/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.6098 - accuracy: 0.6835\n",
            "Epoch 76/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5928 - accuracy: 0.6853\n",
            "Epoch 77/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.5857 - accuracy: 0.6855\n",
            "Epoch 78/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.5845 - accuracy: 0.6888\n",
            "Epoch 79/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5728 - accuracy: 0.6959\n",
            "Epoch 80/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5871 - accuracy: 0.6775\n",
            "Epoch 81/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.5827 - accuracy: 0.6793\n",
            "Epoch 82/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5727 - accuracy: 0.6881\n",
            "Epoch 83/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5545 - accuracy: 0.7061\n",
            "Epoch 84/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5419 - accuracy: 0.7170\n",
            "Epoch 85/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.5809 - accuracy: 0.6794\n",
            "Epoch 86/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.5736 - accuracy: 0.6853\n",
            "Epoch 87/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5721 - accuracy: 0.6858\n",
            "Epoch 88/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5716 - accuracy: 0.6882\n",
            "Epoch 89/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.5710 - accuracy: 0.6891\n",
            "Epoch 90/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.5684 - accuracy: 0.6908\n",
            "Epoch 91/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.5675 - accuracy: 0.6924\n",
            "Epoch 92/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.5692 - accuracy: 0.6897\n",
            "Epoch 93/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5686 - accuracy: 0.6910\n",
            "Epoch 94/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.5661 - accuracy: 0.6924\n",
            "Epoch 95/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.5622 - accuracy: 0.6954\n",
            "Epoch 96/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.5633 - accuracy: 0.6954\n",
            "Epoch 97/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5638 - accuracy: 0.6945\n",
            "Epoch 98/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.5620 - accuracy: 0.6950\n",
            "Epoch 99/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.5605 - accuracy: 0.6948\n",
            "Epoch 100/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.5651 - accuracy: 0.6950\n",
            "Epoch 101/200\n",
            "106/106 [==============================] - 23s 214ms/step - loss: 0.5575 - accuracy: 0.7002\n",
            "Epoch 102/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.4883 - accuracy: 0.7481\n",
            "Epoch 103/200\n",
            "106/106 [==============================] - 23s 220ms/step - loss: 0.5617 - accuracy: 0.6963\n",
            "Epoch 104/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.5561 - accuracy: 0.6985\n",
            "Epoch 105/200\n",
            "106/106 [==============================] - 23s 214ms/step - loss: 0.5552 - accuracy: 0.6978\n",
            "Epoch 106/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.5551 - accuracy: 0.6981\n",
            "Epoch 107/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.5521 - accuracy: 0.7011\n",
            "Epoch 108/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.5490 - accuracy: 0.7036\n",
            "Epoch 109/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.5202 - accuracy: 0.7250\n",
            "Epoch 110/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.5001 - accuracy: 0.7566\n",
            "Epoch 111/200\n",
            "106/106 [==============================] - 23s 214ms/step - loss: 0.4741 - accuracy: 0.7803\n",
            "Epoch 112/200\n",
            "106/106 [==============================] - 23s 214ms/step - loss: 0.4880 - accuracy: 0.7696\n",
            "Epoch 113/200\n",
            "106/106 [==============================] - 23s 214ms/step - loss: 0.5086 - accuracy: 0.7273\n",
            "Epoch 114/200\n",
            "106/106 [==============================] - 23s 212ms/step - loss: 0.4627 - accuracy: 0.7827\n",
            "Epoch 115/200\n",
            "106/106 [==============================] - 23s 212ms/step - loss: 0.5597 - accuracy: 0.7251\n",
            "Epoch 116/200\n",
            "106/106 [==============================] - 22s 212ms/step - loss: 0.4644 - accuracy: 0.7927\n",
            "Epoch 117/200\n",
            "106/106 [==============================] - 22s 212ms/step - loss: 0.4472 - accuracy: 0.8138\n",
            "Epoch 118/200\n",
            "106/106 [==============================] - 23s 214ms/step - loss: 0.4273 - accuracy: 0.8219\n",
            "Epoch 119/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.4232 - accuracy: 0.8274\n",
            "Epoch 120/200\n",
            "106/106 [==============================] - 23s 214ms/step - loss: 0.4204 - accuracy: 0.8273\n",
            "Epoch 121/200\n",
            "106/106 [==============================] - 23s 213ms/step - loss: 0.5100 - accuracy: 0.7473\n",
            "Epoch 122/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.5117 - accuracy: 0.7455\n",
            "Epoch 123/200\n",
            "106/106 [==============================] - 23s 214ms/step - loss: 0.4443 - accuracy: 0.8163\n",
            "Epoch 124/200\n",
            "106/106 [==============================] - 22s 212ms/step - loss: 0.4191 - accuracy: 0.8336\n",
            "Epoch 125/200\n",
            "106/106 [==============================] - 23s 213ms/step - loss: 0.4359 - accuracy: 0.8216\n",
            "Epoch 126/200\n",
            "106/106 [==============================] - 23s 214ms/step - loss: 0.4050 - accuracy: 0.8397\n",
            "Epoch 127/200\n",
            "106/106 [==============================] - 23s 214ms/step - loss: 0.4821 - accuracy: 0.7761\n",
            "Epoch 128/200\n",
            "106/106 [==============================] - 23s 214ms/step - loss: 0.4755 - accuracy: 0.7589\n",
            "Epoch 129/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.4093 - accuracy: 0.8192\n",
            "Epoch 130/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.3978 - accuracy: 0.8423\n",
            "Epoch 131/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.3749 - accuracy: 0.8588\n",
            "Epoch 132/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.4118 - accuracy: 0.8378\n",
            "Epoch 133/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.4010 - accuracy: 0.8291\n",
            "Epoch 134/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.6399 - accuracy: 0.6902\n",
            "Epoch 135/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.4396 - accuracy: 0.8146\n",
            "Epoch 136/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.4321 - accuracy: 0.8278\n",
            "Epoch 137/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.4518 - accuracy: 0.8149\n",
            "Epoch 138/200\n",
            "106/106 [==============================] - 23s 214ms/step - loss: 0.6040 - accuracy: 0.6786\n",
            "Epoch 139/200\n",
            "106/106 [==============================] - 23s 214ms/step - loss: 0.5595 - accuracy: 0.6999\n",
            "Epoch 140/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.5555 - accuracy: 0.7041\n",
            "Epoch 141/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.5557 - accuracy: 0.7029\n",
            "Epoch 142/200\n",
            "106/106 [==============================] - 23s 215ms/step - loss: 0.5320 - accuracy: 0.7210\n",
            "Epoch 143/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.4352 - accuracy: 0.8094\n",
            "Epoch 144/200\n",
            "106/106 [==============================] - 23s 216ms/step - loss: 0.4073 - accuracy: 0.8320\n",
            "Epoch 145/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.3930 - accuracy: 0.8431\n",
            "Epoch 146/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.3851 - accuracy: 0.8501\n",
            "Epoch 147/200\n",
            "106/106 [==============================] - 23s 217ms/step - loss: 0.3721 - accuracy: 0.8546\n",
            "Epoch 148/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.3713 - accuracy: 0.8518\n",
            "Epoch 149/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.3629 - accuracy: 0.8601\n",
            "Epoch 150/200\n",
            "106/106 [==============================] - 23s 220ms/step - loss: 0.3502 - accuracy: 0.8652\n",
            "Epoch 151/200\n",
            "106/106 [==============================] - 23s 220ms/step - loss: 0.3470 - accuracy: 0.8662\n",
            "Epoch 152/200\n",
            "106/106 [==============================] - 23s 221ms/step - loss: 0.3527 - accuracy: 0.8645\n",
            "Epoch 153/200\n",
            "106/106 [==============================] - 24s 222ms/step - loss: 0.3431 - accuracy: 0.8681\n",
            "Epoch 154/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.3449 - accuracy: 0.8666\n",
            "Epoch 155/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.3298 - accuracy: 0.8750\n",
            "Epoch 156/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.4318 - accuracy: 0.8072\n",
            "Epoch 157/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.3975 - accuracy: 0.8297\n",
            "Epoch 158/200\n",
            "106/106 [==============================] - 23s 220ms/step - loss: 0.3574 - accuracy: 0.8565\n",
            "Epoch 159/200\n",
            "106/106 [==============================] - 23s 220ms/step - loss: 0.4959 - accuracy: 0.7516\n",
            "Epoch 160/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.4433 - accuracy: 0.7661\n",
            "Epoch 161/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.5050 - accuracy: 0.7138\n",
            "Epoch 162/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.5394 - accuracy: 0.6897\n",
            "Epoch 163/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.4359 - accuracy: 0.7800\n",
            "Epoch 164/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.3582 - accuracy: 0.8536\n",
            "Epoch 165/200\n",
            "106/106 [==============================] - 23s 220ms/step - loss: 0.3319 - accuracy: 0.8729\n",
            "Epoch 166/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.3442 - accuracy: 0.8676\n",
            "Epoch 167/200\n",
            "106/106 [==============================] - 23s 220ms/step - loss: 0.3273 - accuracy: 0.8776\n",
            "Epoch 168/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.3190 - accuracy: 0.8803\n",
            "Epoch 169/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.3214 - accuracy: 0.8792\n",
            "Epoch 170/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.3295 - accuracy: 0.8747\n",
            "Epoch 171/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.3181 - accuracy: 0.8815\n",
            "Epoch 172/200\n",
            "106/106 [==============================] - 23s 220ms/step - loss: 0.3236 - accuracy: 0.8808\n",
            "Epoch 173/200\n",
            "106/106 [==============================] - 23s 218ms/step - loss: 0.3233 - accuracy: 0.8796\n",
            "Epoch 174/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.4755 - accuracy: 0.7209\n",
            "Epoch 175/200\n",
            "106/106 [==============================] - 23s 220ms/step - loss: 0.4044 - accuracy: 0.8043\n",
            "Epoch 176/200\n",
            "106/106 [==============================] - 23s 220ms/step - loss: 0.3264 - accuracy: 0.8774\n",
            "Epoch 177/200\n",
            "106/106 [==============================] - 23s 219ms/step - loss: 0.3203 - accuracy: 0.8798\n",
            "Epoch 178/200\n",
            "106/106 [==============================] - 23s 220ms/step - loss: 0.3205 - accuracy: 0.8840\n",
            "Epoch 179/200\n",
            "106/106 [==============================] - 23s 220ms/step - loss: 0.3174 - accuracy: 0.8831\n",
            "Epoch 180/200\n",
            "106/106 [==============================] - 23s 220ms/step - loss: 0.3091 - accuracy: 0.8879\n",
            "Epoch 181/200\n",
            "106/106 [==============================] - 23s 221ms/step - loss: 0.3157 - accuracy: 0.8852\n",
            "Epoch 182/200\n",
            "106/106 [==============================] - 23s 220ms/step - loss: 0.3085 - accuracy: 0.8862\n",
            "Epoch 183/200\n",
            "106/106 [==============================] - 23s 220ms/step - loss: 0.3038 - accuracy: 0.8888\n",
            "Epoch 184/200\n",
            "106/106 [==============================] - 23s 220ms/step - loss: 0.3113 - accuracy: 0.8852\n",
            "Epoch 185/200\n",
            "106/106 [==============================] - 23s 222ms/step - loss: 0.3062 - accuracy: 0.8883\n",
            "Epoch 186/200\n",
            "106/106 [==============================] - 23s 221ms/step - loss: 0.3036 - accuracy: 0.8899\n",
            "Epoch 187/200\n",
            "106/106 [==============================] - 23s 221ms/step - loss: 0.3030 - accuracy: 0.8909\n",
            "Epoch 188/200\n",
            "106/106 [==============================] - 23s 221ms/step - loss: 0.3009 - accuracy: 0.8925\n",
            "Epoch 189/200\n",
            "106/106 [==============================] - 23s 221ms/step - loss: 0.2959 - accuracy: 0.8943\n",
            "Epoch 190/200\n",
            "106/106 [==============================] - 23s 220ms/step - loss: 0.3055 - accuracy: 0.8870\n",
            "Epoch 191/200\n",
            "106/106 [==============================] - 23s 221ms/step - loss: 0.3054 - accuracy: 0.8867\n",
            "Epoch 192/200\n",
            "106/106 [==============================] - 24s 222ms/step - loss: 0.2976 - accuracy: 0.8948\n",
            "Epoch 193/200\n",
            "106/106 [==============================] - 24s 223ms/step - loss: 0.2957 - accuracy: 0.8927\n",
            "Epoch 194/200\n",
            "106/106 [==============================] - 24s 222ms/step - loss: 0.3018 - accuracy: 0.8899\n",
            "Epoch 195/200\n",
            "106/106 [==============================] - 23s 221ms/step - loss: 0.2940 - accuracy: 0.8918\n",
            "Epoch 196/200\n",
            "106/106 [==============================] - 23s 221ms/step - loss: 0.3007 - accuracy: 0.8907\n",
            "Epoch 197/200\n",
            "106/106 [==============================] - 23s 221ms/step - loss: 0.3104 - accuracy: 0.8856\n",
            "Epoch 198/200\n",
            "106/106 [==============================] - 24s 223ms/step - loss: 0.2907 - accuracy: 0.8950\n",
            "Epoch 199/200\n",
            "106/106 [==============================] - 23s 221ms/step - loss: 0.2959 - accuracy: 0.8935\n",
            "Epoch 200/200\n",
            "106/106 [==============================] - 23s 221ms/step - loss: 0.2866 - accuracy: 0.8982\n",
            "Train 200 epochs, 4660 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEmSlrt_gYZZ"
      },
      "source": [
        "def evaluate(model,min,max):\n",
        "    print(\"Evaluate on lengths %d to %d\"%(min,max))\n",
        "    print(\"slice...\")\n",
        "    subset=make_slice(train_set,min,max)\n",
        "    print(\"kmers...\")\n",
        "    (X_valid,y_valid)=make_kmers(max,subset)\n",
        "    print(\"evaluate....\")\n",
        "    scores = model.evaluate(X_valid, y_valid, verbose=1)  # valid = train, expect 100%\n",
        "    print(\"Evaluated on lengths %d to %d\"%(min,max))\n",
        "    print(\"%s: %.2f%%\\n\" % (model.metrics_names[1], scores[1]*100))\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiNABsVNgYZc",
        "outputId": "a958a657-e54b-4485-8578-5fc5afa72ffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "evaluate(model,200,1000)\n",
        "evaluate(model,1000,2000)\n",
        "evaluate(model,2000,3000)\n",
        "evaluate(model,3000,5000)\n",
        "evaluate(model,5000,10000)\n",
        "evaluate(model,10000,30000)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate on lengths 200 to 1000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (30290, 4)\n",
            "no long, no short (8879, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(8879, 1)\n",
            "sequence    AGTCCCTCCCCAGCCCAGCAGTCCCTCCAGGCTACATCCAGGAGAC...\n",
            "Name: 1280, dtype: object\n",
            "348\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[ 46 182 214 ...   0   0   0]\n",
            " [ 36 142  56 ...   0   0   0]\n",
            " [135  28 110 ...   0   0   0]\n",
            " ...\n",
            " [147  73  36 ...   0   0   0]\n",
            " [228 143  57 ...   0   0   0]\n",
            " [131  12  47 ...   0   0   0]]\n",
            "evaluate....\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 2000) for input Tensor(\"embedding_input:0\", shape=(None, 2000), dtype=float32), but it was called on an input with incompatible shape (None, 1000).\n",
            "278/278 [==============================] - 15s 52ms/step - loss: 0.4465 - accuracy: 0.7944\n",
            "Evaluated on lengths 200 to 1000\n",
            "accuracy: 79.44%\n",
            "\n",
            "Evaluate on lengths 1000 to 2000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (9273, 4)\n",
            "no long, no short (3368, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(3368, 1)\n",
            "sequence    GGCGGGGTCGACTGACGGTAACGGGGCAGAGAGGCTGTTCGCAGAG...\n",
            "Name: 12641, dtype: object\n",
            "1338\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[167 155 107 ...   0   0   0]\n",
            " [226 133  20 ...   0   0   0]\n",
            " [108 175 186 ...   0   0   0]\n",
            " ...\n",
            " [175 185 225 ...   0   0   0]\n",
            " [ 37 148  78 ...   0   0   0]\n",
            " [188 240 192 ...   0   0   0]]\n",
            "evaluate....\n",
            "106/106 [==============================] - 10s 98ms/step - loss: 0.2422 - accuracy: 0.9092\n",
            "Evaluated on lengths 1000 to 2000\n",
            "accuracy: 90.92%\n",
            "\n",
            "Evaluate on lengths 2000 to 3000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (3221, 4)\n",
            "no long, no short (1351, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(1351, 1)\n",
            "sequence    GTCATTCTAGCTGCCTGCTGCCTCCGCAGCGTCCCCCCAGCTCTCC...\n",
            "Name: 19713, dtype: object\n",
            "2039\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[181 212  80 ...   0   0   0]\n",
            " [ 10  37 148 ...   0   0   0]\n",
            " [ 48 190 245 ...   0   0   0]\n",
            " ...\n",
            " [ 49 195  10 ...   0   0   0]\n",
            " [153  97 131 ...   0   0   0]\n",
            " [ 36 143  58 ...   0   0   0]]\n",
            "evaluate....\n",
            "43/43 [==============================] - 6s 146ms/step - loss: 0.2699 - accuracy: 0.8864\n",
            "Evaluated on lengths 2000 to 3000\n",
            "accuracy: 88.64%\n",
            "\n",
            "Evaluate on lengths 3000 to 5000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (1336, 4)\n",
            "no long, no short (895, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(895, 1)\n",
            "sequence    AGGCCGCGTCCGCCCGCGCCCGCTCTGGCCCCCGCGGAGCCGCGCA...\n",
            "Name: 19203, dtype: object\n",
            "3491\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[ 42 166 151 ...   0   0   0]\n",
            " [ 62 245 209 ...   0   0   0]\n",
            " [ 47 185 228 ...   0   0   0]\n",
            " ...\n",
            " [166 151  92 ...   0   0   0]\n",
            " [ 59 233 163 ...   0   0   0]\n",
            " [  3  11  41 ...   0   0   0]]\n",
            "evaluate....\n",
            "28/28 [==============================] - 7s 233ms/step - loss: 0.3321 - accuracy: 0.8633\n",
            "Evaluated on lengths 3000 to 5000\n",
            "accuracy: 86.33%\n",
            "\n",
            "Evaluate on lengths 5000 to 10000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (345, 4)\n",
            "no long, no short (314, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(314, 1)\n",
            "sequence    GGGAGCTCCGTTGTGCGTCGCTTAAGTGAGGGCGGCGGATGGGCGA...\n",
            "Name: 19766, dtype: object\n",
            "6207\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[169 163 138 ...   0   0   0]\n",
            " [167 155 106 ...   0   0   0]\n",
            " [ 19  76  47 ...   0   0   0]\n",
            " ...\n",
            " [ 91 107 169 ...   0   0   0]\n",
            " [ 62 247 218 ...   0   0   0]\n",
            " [229 145  67 ...   0   0   0]]\n",
            "evaluate....\n",
            "10/10 [==============================] - 4s 425ms/step - loss: 0.4476 - accuracy: 0.8172\n",
            "Evaluated on lengths 5000 to 10000\n",
            "accuracy: 81.72%\n",
            "\n",
            "Evaluate on lengths 10000 to 30000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (29, 4)\n",
            "no long, no short (29, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(29, 1)\n",
            "sequence    GCGCTAGCTCCCATGCTGGCCTCGGTGCCACTCGCGCGCCGGCCGC...\n",
            "Name: 7362, dtype: object\n",
            "11322\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[154 104 157 ...   0   0   0]\n",
            " [160 128 254 ...   0   0   0]\n",
            " [ 59 235 169 ...   0   0   0]\n",
            " ...\n",
            " [ 47 187 236 ...   0   0   0]\n",
            " [ 47 188 239 ...   0   0   0]\n",
            " [139  41 163 ...   0   0   0]]\n",
            "evaluate....\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.9418 - accuracy: 0.6127\n",
            "Evaluated on lengths 10000 to 30000\n",
            "accuracy: 61.27%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbieSKdOjeaL"
      },
      "source": [
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd3Wj_vI9KdP"
      },
      "source": [
        "## Len 200-1Kb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ8eW5Rg9KdQ"
      },
      "source": [
        "MINLEN=200\n",
        "MAXLEN=1000\n",
        "\n",
        "if False:\n",
        "  print (\"Compile the model\")\n",
        "  model=build_model(MAXLEN)\n",
        "  print (\"Summarize the model\")\n",
        "  print(model.summary())  # Print this only once\n",
        "  print(\"Working on full training set, slice by sequence length.\")\n",
        "  print(\"Slice size range [%d - %d)\"%(MINLEN,MAXLEN))\n",
        "  subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
        "  print (\"Sequence to Kmer\")\n",
        "  (X_train,y_train)=make_kmers(MAXLEN,subset)\n",
        "  X_train\n",
        "  X_train=make_frequencies(X_train)\n",
        "  X_train\n",
        "  print (\"Cross valiation\")\n",
        "  model1 = do_cross_validation(X_train,y_train,MAXLEN)\n",
        "  model1.save(FILENAME+'.short.model')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC68X4zr9KdU"
      },
      "source": [
        "## Len 1Kb-2Kb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nm3oU3h9KdV"
      },
      "source": [
        "MINLEN=1000\n",
        "MAXLEN=2000\n",
        "\n",
        "if False:\n",
        "    print (\"Compile the model\")\n",
        "    model=build_model(MAXLEN)\n",
        "    print (\"Summarize the model\")\n",
        "    print(model.summary())  # Print this only once\n",
        "    print(\"Working on full training set, slice by sequence length.\")\n",
        "    print(\"Slice size range [%d - %d)\"%(MINLEN,MAXLEN))\n",
        "    subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
        "    print (\"Sequence to Kmer\")\n",
        "    (X_train,y_train)=make_kmers(MAXLEN,subset)\n",
        "    X_train\n",
        "    X_train=make_frequencies(X_train)\n",
        "    X_train\n",
        "    print (\"Cross valiation\")\n",
        "    model2 = do_cross_validation(X_train,y_train,MAXLEN)\n",
        "    model2.save(FILENAME+'.medium.model')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyACRnZx9Kde"
      },
      "source": [
        "## Len 2Kb-3Kb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUxmLnQ-9Kde"
      },
      "source": [
        "MINLEN=2000\n",
        "MAXLEN=3000\n",
        "\n",
        "if False:\n",
        "    print (\"Compile the model\")\n",
        "    model=build_model(MAXLEN)\n",
        "    print (\"Summarize the model\")\n",
        "    print(model.summary())  # Print this only once\n",
        "    print(\"Working on full training set, slice by sequence length.\")\n",
        "    print(\"Slice size range [%d - %d)\"%(MINLEN,MAXLEN))\n",
        "    subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
        "    print (\"Sequence to Kmer\")\n",
        "    (X_train,y_train)=make_kmers(MAXLEN,subset)\n",
        "    X_train\n",
        "    X_train=make_frequencies(X_train)\n",
        "    X_train\n",
        "    print (\"Cross valiation\")\n",
        "    model3 = do_cross_validation(X_train,y_train,MAXLEN)\n",
        "    model3.save(FILENAME+'.long.model')"
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}