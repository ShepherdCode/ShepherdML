{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GRU_205e.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojm_6E9f9Kcf"
      },
      "source": [
        "# GRU 205\n",
        "* Operate on 16000 GenCode 34 seqs.\n",
        "* 5-way cross validation. Save best model per CV.\n",
        "* Report mean accuracy from final re-validation with best 5.\n",
        "* Use Adam with a learn rate decay schdule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh6XplUvC0j0",
        "outputId": "b8864753-62a2-4c18-82b2-91bd830527dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "NC_FILENAME='ncRNA.gc34.processed.fasta'\n",
        "PC_FILENAME='pcRNA.gc34.processed.fasta'\n",
        "DATAPATH=\"\"\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    PATH='/content/drive/'\n",
        "    drive.mount(PATH)\n",
        "    DATAPATH=PATH+'My Drive/data/'  # must end in \"/\"\n",
        "    NC_FILENAME = DATAPATH+NC_FILENAME\n",
        "    PC_FILENAME = DATAPATH+PC_FILENAME\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    DATAPATH=\"\" \n",
        "\n",
        "EPOCHS=200\n",
        "SPLITS=1\n",
        "K=1\n",
        "VOCABULARY_SIZE=4**K+1   # e.g. K=3 => 64 DNA K-mers + 'NNN'\n",
        "EMBED_DIMEN=16\n",
        "FILENAME='GRU205'\n",
        "NEURONS=64"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQY7aTj29Kch"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LayerNormalization\n",
        "import time\n",
        "dt='float32'\n",
        "tf.keras.backend.set_floatx(dt)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7jcg6Wl9Kc2"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLFNO1Xa9Kc3"
      },
      "source": [
        "def compile_model(model):\n",
        "    adam_default_learn_rate = 0.001\n",
        "    schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate = adam_default_learn_rate*10,\n",
        "        #decay_steps=100000, decay_rate=0.96, staircase=True)\n",
        "        decay_steps=10000, decay_rate=0.99, staircase=True)\n",
        "    # learn rate = initial_learning_rate * decay_rate ^ (step / decay_steps)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=schedule)\n",
        "    bc=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    print(\"COMPILE...\")\n",
        "    model.compile(loss=bc, optimizer=opt, metrics=[\"accuracy\"])\n",
        "    #model.compile(loss=bc, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    print(\"...COMPILED\")\n",
        "    return model\n",
        "\n",
        "def build_model():\n",
        "    act=\"tanh\"\n",
        "    embed_layer  = keras.layers.Embedding(\n",
        "        #VOCABULARY_SIZE, EMBED_DIMEN, input_length=1000, input_length=1000, mask_zero=True)\n",
        "        #input_dim=[None,VOCABULARY_SIZE], output_dim=EMBED_DIMEN, mask_zero=True)\n",
        "        input_dim=VOCABULARY_SIZE, output_dim=EMBED_DIMEN, mask_zero=True)\n",
        "    rnn1_layer = keras.layers.Bidirectional(\n",
        "      keras.layers.GRU(NEURONS, return_sequences=True, \n",
        "          input_shape=[1000,EMBED_DIMEN], activation=act, dropout=0.50))\n",
        "    rnn2_layer = keras.layers.Bidirectional(\n",
        "      keras.layers.GRU(NEURONS, return_sequences=False, \n",
        "        activation=act, dropout=0.50))\n",
        "    dense1_layer = keras.layers.Dense(NEURONS, activation=act,dtype=dt)\n",
        "    drop1_layer = keras.layers.Dropout(0.5)\n",
        "    dense2_layer = keras.layers.Dense(NEURONS, activation=act,dtype=dt)\n",
        "    drop2_layer = keras.layers.Dropout(0.5)\n",
        "    output_layer = keras.layers.Dense(1, activation=\"sigmoid\", dtype=dt)\n",
        "    mlp = keras.models.Sequential()\n",
        "    mlp.add(embed_layer)\n",
        "    mlp.add(rnn1_layer)\n",
        "    mlp.add(rnn2_layer)\n",
        "    mlp.add(dense1_layer)\n",
        "    mlp.add(drop1_layer)\n",
        "    mlp.add(dense2_layer)\n",
        "    mlp.add(drop2_layer)\n",
        "    mlp.add(output_layer)\n",
        "    mlpc = compile_model(mlp)\n",
        "    return mlpc"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV6k-xOm9Kcn"
      },
      "source": [
        "## Load and partition sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I-O_qzw9Kco"
      },
      "source": [
        "# Assume file was preprocessed to contain one line per seq.\n",
        "# Prefer Pandas dataframe but df does not support append.\n",
        "# For conversion to tensor, must avoid python lists.\n",
        "def load_fasta(filename,label):\n",
        "    DEFLINE='>'\n",
        "    labels=[]\n",
        "    seqs=[]\n",
        "    lens=[]\n",
        "    nums=[]\n",
        "    num=0\n",
        "    with open (filename,'r') as infile:\n",
        "        for line in infile:\n",
        "            if line[0]!=DEFLINE:\n",
        "                seq=line.rstrip()\n",
        "                num += 1   # first seqnum is 1\n",
        "                seqlen=len(seq)\n",
        "                nums.append(num)\n",
        "                labels.append(label)\n",
        "                seqs.append(seq)\n",
        "                lens.append(seqlen)\n",
        "    df1=pd.DataFrame(nums,columns=['seqnum'])\n",
        "    df2=pd.DataFrame(labels,columns=['class'])\n",
        "    df3=pd.DataFrame(seqs,columns=['sequence'])\n",
        "    df4=pd.DataFrame(lens,columns=['seqlen'])\n",
        "    df=pd.concat((df1,df2,df3,df4),axis=1)\n",
        "    return df\n",
        "\n",
        "def separate_X_and_y(data):\n",
        "    y=   data[['class']].copy()\n",
        "    X=   data.drop(columns=['class','seqnum','seqlen'])\n",
        "    return (X,y)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRAaO9jP9Kcr"
      },
      "source": [
        "## Make K-mers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8xcZ4Mr9Kcs"
      },
      "source": [
        "def make_kmer_table(K):\n",
        "    npad='N'*K\n",
        "    shorter_kmers=['']\n",
        "    for i in range(K):\n",
        "        longer_kmers=[]\n",
        "        for mer in shorter_kmers:\n",
        "            longer_kmers.append(mer+'A')\n",
        "            longer_kmers.append(mer+'C')\n",
        "            longer_kmers.append(mer+'G')\n",
        "            longer_kmers.append(mer+'T')\n",
        "        shorter_kmers = longer_kmers\n",
        "    all_kmers = shorter_kmers\n",
        "    kmer_dict = {}\n",
        "    kmer_dict[npad]=0\n",
        "    value=1\n",
        "    for mer in all_kmers:\n",
        "        kmer_dict[mer]=value\n",
        "        value += 1\n",
        "    return kmer_dict\n",
        "\n",
        "KMER_TABLE=make_kmer_table(K)\n",
        "\n",
        "def strings_to_vectors(data,uniform_len):\n",
        "    all_seqs=[]\n",
        "    for seq in data['sequence']:\n",
        "        i=0\n",
        "        seqlen=len(seq)\n",
        "        kmers=[]\n",
        "        while i < seqlen-K+1 -1:  # stop at minus one for spaced seed\n",
        "            #kmer=seq[i:i+2]+seq[i+3:i+5]    # SPACED SEED 2/1/2 for K=4\n",
        "            kmer=seq[i:i+K]  \n",
        "            i += 1\n",
        "            value=KMER_TABLE[kmer]\n",
        "            kmers.append(value)\n",
        "        pad_val=0\n",
        "        while i < uniform_len:\n",
        "            kmers.append(pad_val)\n",
        "            i += 1\n",
        "        all_seqs.append(kmers)\n",
        "    pd2d=pd.DataFrame(all_seqs)\n",
        "    return pd2d   # return 2D dataframe, uniform dimensions"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEtA0xiV9Kcv"
      },
      "source": [
        "def make_kmers(MAXLEN,train_set):\n",
        "    (X_train_all,y_train_all)=separate_X_and_y(train_set)\n",
        "    X_train_kmers=strings_to_vectors(X_train_all,MAXLEN)\n",
        "    # From pandas dataframe to numpy to list to numpy\n",
        "    num_seqs=len(X_train_kmers)\n",
        "    tmp_seqs=[]\n",
        "    for i in range(num_seqs):\n",
        "        kmer_sequence=X_train_kmers.iloc[i]\n",
        "        tmp_seqs.append(kmer_sequence)\n",
        "    X_train_kmers=np.array(tmp_seqs)\n",
        "    tmp_seqs=None\n",
        "    labels=y_train_all.to_numpy()\n",
        "    return (X_train_kmers,labels)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaXyySyO9Kcz"
      },
      "source": [
        "def make_frequencies(Xin):\n",
        "    Xout=[]\n",
        "    VOCABULARY_SIZE= 4**K + 1  # plus one for 'NNN'\n",
        "    for seq in Xin:\n",
        "        freqs =[0] * VOCABULARY_SIZE\n",
        "        total = 0\n",
        "        for kmerval in seq:\n",
        "            freqs[kmerval] += 1\n",
        "            total += 1\n",
        "        for c in range(VOCABULARY_SIZE):\n",
        "            freqs[c] = freqs[c]/total\n",
        "        Xout.append(freqs)\n",
        "    Xnum = np.asarray(Xout)\n",
        "    return (Xnum)\n",
        "def make_slice(data_set,min_len,max_len):\n",
        "    slice = data_set.query('seqlen <= '+str(max_len)+' & seqlen>= '+str(min_len))\n",
        "    return slice"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdIS2utq9Kc9"
      },
      "source": [
        "## Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVo4tbB_9Kc-"
      },
      "source": [
        "def do_cross_validation(X,y,given_model):\n",
        "    cv_scores = []\n",
        "    fold=0\n",
        "    splitter = ShuffleSplit(n_splits=SPLITS, test_size=0.1, random_state=37863)\n",
        "    for train_index,valid_index in splitter.split(X):\n",
        "        fold += 1\n",
        "        X_train=X[train_index] # use iloc[] for dataframe\n",
        "        y_train=y[train_index]\n",
        "        X_valid=X[valid_index]\n",
        "        y_valid=y[valid_index]        \n",
        "        # Avoid continually improving the same model.\n",
        "        model = compile_model(keras.models.clone_model(given_model))\n",
        "        bestname=DATAPATH+FILENAME+\".cv.\"+str(fold)+\".best\"\n",
        "        mycallbacks = [keras.callbacks.ModelCheckpoint(\n",
        "            filepath=bestname, save_best_only=True, \n",
        "            monitor='val_accuracy', mode='max')]   \n",
        "        print(\"FIT\")\n",
        "        start_time=time.time()\n",
        "        history=model.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
        "                epochs=EPOCHS, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
        "                callbacks=mycallbacks,\n",
        "                validation_data=(X_valid,y_valid) )\n",
        "        end_time=time.time()\n",
        "        elapsed_time=(end_time-start_time)                        \n",
        "        print(\"Fold %d, %d epochs, %d sec\"%(fold,EPOCHS,elapsed_time))\n",
        "        pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "        plt.grid(True)\n",
        "        plt.gca().set_ylim(0,1)\n",
        "        plt.show()\n",
        "        best_model=keras.models.load_model(bestname)\n",
        "        scores = best_model.evaluate(X_valid, y_valid, verbose=0)\n",
        "        print(\"%s: %.2f%%\" % (best_model.metrics_names[1], scores[1]*100))\n",
        "        cv_scores.append(scores[1] * 100)  \n",
        "    print()\n",
        "    print(\"%d-way Cross Validation mean %.2f%% (+/- %.2f%%)\" % (fold, np.mean(cv_scores), np.std(cv_scores)))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd3Wj_vI9KdP"
      },
      "source": [
        "## Train on RNA lengths 200-1Kb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8fNo6sn9KdH",
        "outputId": "ccd0bab2-674a-4eb6-f408-774394d617fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "MINLEN=200\n",
        "MAXLEN=1000\n",
        "print(\"Load data from files.\")\n",
        "nc_seq=load_fasta(NC_FILENAME,0)\n",
        "pc_seq=load_fasta(PC_FILENAME,1)\n",
        "train_set=pd.concat((nc_seq,pc_seq),axis=0)\n",
        "nc_seq=None\n",
        "pc_seq=None\n",
        "print(\"Ready: train_set\")\n",
        "#train_set\n",
        "subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
        "print (\"Data reshape\")\n",
        "(X_train,y_train)=make_kmers(MAXLEN,subset)\n",
        "#print (\"Data prep\")\n",
        "#X_train=make_frequencies(X_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load data from files.\n",
            "Ready: train_set\n",
            "Data reshape\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1HuSs8ZbeL4",
        "outputId": "5290a2ad-30d1-4980-8cd1-57de0e31369e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        }
      },
      "source": [
        "print (\"Compile the model\")\n",
        "model=build_model()\n",
        "print (\"Summarize the model\")\n",
        "print(model.summary())  # Print this only once\n",
        "model.save(DATAPATH+FILENAME+'.model')\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compile the model\n",
            "COMPILE...\n",
            "...COMPILED\n",
            "Summarize the model\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          80        \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 128)         31488     \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 128)               74496     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 118,545\n",
            "Trainable params: 118,545\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/data/GRU205.model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ8eW5Rg9KdQ",
        "outputId": "c703a0c5-086b-4835-e5a2-d92b588e67ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print (\"Cross valiation\")\n",
        "do_cross_validation(X_train,y_train,model)  \n",
        "print (\"Done\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross valiation\n",
            "COMPILE...\n",
            "...COMPILED\n",
            "FIT\n",
            "Epoch 1/200\n",
            "453/453 [==============================] - ETA: 0s - loss: 0.6725 - accuracy: 0.6125INFO:tensorflow:Assets written to: /content/drive/My Drive/data/GRU205.cv.1.best/assets\n",
            "453/453 [==============================] - 103s 226ms/step - loss: 0.6725 - accuracy: 0.6125 - val_loss: 0.6620 - val_accuracy: 0.6530\n",
            "Epoch 2/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6708 - accuracy: 0.6206 - val_loss: 0.6615 - val_accuracy: 0.6530\n",
            "Epoch 3/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6719 - accuracy: 0.6145 - val_loss: 0.6551 - val_accuracy: 0.6530\n",
            "Epoch 4/200\n",
            "453/453 [==============================] - 60s 134ms/step - loss: 0.6678 - accuracy: 0.6220 - val_loss: 0.6774 - val_accuracy: 0.6530\n",
            "Epoch 5/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6719 - accuracy: 0.6191 - val_loss: 0.6449 - val_accuracy: 0.6530\n",
            "Epoch 6/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6693 - accuracy: 0.6222 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 7/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6691 - accuracy: 0.6193 - val_loss: 0.6594 - val_accuracy: 0.6530\n",
            "Epoch 8/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6729 - accuracy: 0.6226 - val_loss: 0.6536 - val_accuracy: 0.6530\n",
            "Epoch 9/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6730 - accuracy: 0.6189 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 10/200\n",
            "453/453 [==============================] - 60s 134ms/step - loss: 0.6713 - accuracy: 0.6208 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 11/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6746 - accuracy: 0.6192 - val_loss: 0.6521 - val_accuracy: 0.6530\n",
            "Epoch 12/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6713 - accuracy: 0.6201 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 13/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6701 - accuracy: 0.6238 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 14/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6738 - accuracy: 0.6153 - val_loss: 0.7360 - val_accuracy: 0.6530\n",
            "Epoch 15/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6743 - accuracy: 0.6204 - val_loss: 0.6471 - val_accuracy: 0.6530\n",
            "Epoch 16/200\n",
            "453/453 [==============================] - 61s 136ms/step - loss: 0.6730 - accuracy: 0.6171 - val_loss: 0.6576 - val_accuracy: 0.6530\n",
            "Epoch 17/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6693 - accuracy: 0.6243 - val_loss: 0.6684 - val_accuracy: 0.6530\n",
            "Epoch 18/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6707 - accuracy: 0.6234 - val_loss: 0.6503 - val_accuracy: 0.6530\n",
            "Epoch 19/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6693 - accuracy: 0.6224 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 20/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6685 - accuracy: 0.6273 - val_loss: 0.6529 - val_accuracy: 0.6530\n",
            "Epoch 21/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6728 - accuracy: 0.6184 - val_loss: 0.6509 - val_accuracy: 0.6530\n",
            "Epoch 22/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6686 - accuracy: 0.6202 - val_loss: 0.6551 - val_accuracy: 0.6530\n",
            "Epoch 23/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6677 - accuracy: 0.6278 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 24/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6755 - accuracy: 0.6165 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 25/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6698 - accuracy: 0.6241 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 26/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6690 - accuracy: 0.6212 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 27/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6732 - accuracy: 0.6162 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 28/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6700 - accuracy: 0.6186 - val_loss: 0.6490 - val_accuracy: 0.6530\n",
            "Epoch 29/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6689 - accuracy: 0.6227 - val_loss: 0.6709 - val_accuracy: 0.6530\n",
            "Epoch 30/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6712 - accuracy: 0.6202 - val_loss: 0.6533 - val_accuracy: 0.6530\n",
            "Epoch 31/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6729 - accuracy: 0.6222 - val_loss: 0.6469 - val_accuracy: 0.6530\n",
            "Epoch 32/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6732 - accuracy: 0.6211 - val_loss: 0.6610 - val_accuracy: 0.6530\n",
            "Epoch 33/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6705 - accuracy: 0.6211 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 34/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6712 - accuracy: 0.6217 - val_loss: 0.6499 - val_accuracy: 0.6530\n",
            "Epoch 35/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6723 - accuracy: 0.6206 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 36/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6676 - accuracy: 0.6246 - val_loss: 0.6530 - val_accuracy: 0.6530\n",
            "Epoch 37/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6742 - accuracy: 0.6208 - val_loss: 0.6483 - val_accuracy: 0.6530\n",
            "Epoch 38/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6722 - accuracy: 0.6154 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 39/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6676 - accuracy: 0.6266 - val_loss: 0.6619 - val_accuracy: 0.6530\n",
            "Epoch 40/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6691 - accuracy: 0.6241 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 41/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6715 - accuracy: 0.6208 - val_loss: 0.6542 - val_accuracy: 0.6530\n",
            "Epoch 42/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6735 - accuracy: 0.6162 - val_loss: 0.6548 - val_accuracy: 0.6530\n",
            "Epoch 43/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6727 - accuracy: 0.6195 - val_loss: 0.6545 - val_accuracy: 0.6530\n",
            "Epoch 44/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6712 - accuracy: 0.6242 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 45/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6714 - accuracy: 0.6211 - val_loss: 0.6472 - val_accuracy: 0.6530\n",
            "Epoch 46/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6746 - accuracy: 0.6197 - val_loss: 0.6815 - val_accuracy: 0.6530\n",
            "Epoch 47/200\n",
            "453/453 [==============================] - 62s 136ms/step - loss: 0.6701 - accuracy: 0.6220 - val_loss: 0.6606 - val_accuracy: 0.6530\n",
            "Epoch 48/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6708 - accuracy: 0.6213 - val_loss: 0.6661 - val_accuracy: 0.6530\n",
            "Epoch 49/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6726 - accuracy: 0.6177 - val_loss: 0.6556 - val_accuracy: 0.6530\n",
            "Epoch 50/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6725 - accuracy: 0.6228 - val_loss: 0.6614 - val_accuracy: 0.6530\n",
            "Epoch 51/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6738 - accuracy: 0.6210 - val_loss: 0.6481 - val_accuracy: 0.6530\n",
            "Epoch 52/200\n",
            "453/453 [==============================] - 61s 136ms/step - loss: 0.6694 - accuracy: 0.6278 - val_loss: 0.6510 - val_accuracy: 0.6530\n",
            "Epoch 53/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6705 - accuracy: 0.6225 - val_loss: 0.6515 - val_accuracy: 0.6530\n",
            "Epoch 54/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6737 - accuracy: 0.6199 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 55/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6745 - accuracy: 0.6184 - val_loss: 0.6531 - val_accuracy: 0.6530\n",
            "Epoch 56/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6711 - accuracy: 0.6219 - val_loss: 0.6541 - val_accuracy: 0.6530\n",
            "Epoch 57/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6757 - accuracy: 0.6157 - val_loss: 0.6566 - val_accuracy: 0.6530\n",
            "Epoch 58/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6659 - accuracy: 0.6249 - val_loss: 0.6477 - val_accuracy: 0.6530\n",
            "Epoch 59/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6692 - accuracy: 0.6244 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 60/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6702 - accuracy: 0.6211 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 61/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6703 - accuracy: 0.6231 - val_loss: 0.6566 - val_accuracy: 0.6530\n",
            "Epoch 62/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6721 - accuracy: 0.6185 - val_loss: 0.6688 - val_accuracy: 0.6530\n",
            "Epoch 63/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6700 - accuracy: 0.6251 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 64/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6707 - accuracy: 0.6239 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
            "Epoch 65/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6724 - accuracy: 0.6208 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 66/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6752 - accuracy: 0.6157 - val_loss: 0.6559 - val_accuracy: 0.6530\n",
            "Epoch 67/200\n",
            "453/453 [==============================] - 62s 137ms/step - loss: 0.6695 - accuracy: 0.6246 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 68/200\n",
            "453/453 [==============================] - 62s 137ms/step - loss: 0.6748 - accuracy: 0.6202 - val_loss: 0.6660 - val_accuracy: 0.6530\n",
            "Epoch 69/200\n",
            "453/453 [==============================] - 62s 137ms/step - loss: 0.6701 - accuracy: 0.6255 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 70/200\n",
            "453/453 [==============================] - 62s 137ms/step - loss: 0.6702 - accuracy: 0.6205 - val_loss: 0.6497 - val_accuracy: 0.6530\n",
            "Epoch 71/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6713 - accuracy: 0.6192 - val_loss: 0.6509 - val_accuracy: 0.6530\n",
            "Epoch 72/200\n",
            "453/453 [==============================] - 62s 136ms/step - loss: 0.6704 - accuracy: 0.6229 - val_loss: 0.6688 - val_accuracy: 0.6530\n",
            "Epoch 73/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6733 - accuracy: 0.6182 - val_loss: 0.7010 - val_accuracy: 0.6530\n",
            "Epoch 74/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6718 - accuracy: 0.6241 - val_loss: 0.6527 - val_accuracy: 0.6530\n",
            "Epoch 75/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6701 - accuracy: 0.6255 - val_loss: 0.6664 - val_accuracy: 0.6530\n",
            "Epoch 76/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6694 - accuracy: 0.6211 - val_loss: 0.6531 - val_accuracy: 0.6530\n",
            "Epoch 77/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6704 - accuracy: 0.6263 - val_loss: 0.6477 - val_accuracy: 0.6530\n",
            "Epoch 78/200\n",
            "453/453 [==============================] - 62s 136ms/step - loss: 0.6715 - accuracy: 0.6202 - val_loss: 0.6667 - val_accuracy: 0.6530\n",
            "Epoch 79/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6655 - accuracy: 0.6220 - val_loss: 0.6485 - val_accuracy: 0.6530\n",
            "Epoch 80/200\n",
            "453/453 [==============================] - 61s 136ms/step - loss: 0.6734 - accuracy: 0.6214 - val_loss: 0.6500 - val_accuracy: 0.6530\n",
            "Epoch 81/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6706 - accuracy: 0.6233 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 82/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6715 - accuracy: 0.6201 - val_loss: 0.6476 - val_accuracy: 0.6530\n",
            "Epoch 83/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6691 - accuracy: 0.6229 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
            "Epoch 84/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6715 - accuracy: 0.6238 - val_loss: 0.6576 - val_accuracy: 0.6530\n",
            "Epoch 85/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6730 - accuracy: 0.6193 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 86/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6746 - accuracy: 0.6177 - val_loss: 0.6549 - val_accuracy: 0.6530\n",
            "Epoch 87/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6745 - accuracy: 0.6191 - val_loss: 0.6485 - val_accuracy: 0.6530\n",
            "Epoch 88/200\n",
            "453/453 [==============================] - 62s 136ms/step - loss: 0.6700 - accuracy: 0.6292 - val_loss: 0.6849 - val_accuracy: 0.6530\n",
            "Epoch 89/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6699 - accuracy: 0.6208 - val_loss: 0.6942 - val_accuracy: 0.3470\n",
            "Epoch 90/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6709 - accuracy: 0.6220 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 91/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6712 - accuracy: 0.6210 - val_loss: 0.6474 - val_accuracy: 0.6530\n",
            "Epoch 92/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6720 - accuracy: 0.6156 - val_loss: 0.6489 - val_accuracy: 0.6530\n",
            "Epoch 93/200\n",
            "453/453 [==============================] - 62s 137ms/step - loss: 0.6733 - accuracy: 0.6174 - val_loss: 0.6663 - val_accuracy: 0.6530\n",
            "Epoch 94/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6676 - accuracy: 0.6293 - val_loss: 0.6594 - val_accuracy: 0.6530\n",
            "Epoch 95/200\n",
            "453/453 [==============================] - 61s 136ms/step - loss: 0.6723 - accuracy: 0.6240 - val_loss: 0.6841 - val_accuracy: 0.6530\n",
            "Epoch 96/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6675 - accuracy: 0.6280 - val_loss: 0.6635 - val_accuracy: 0.6530\n",
            "Epoch 97/200\n",
            "453/453 [==============================] - 62s 136ms/step - loss: 0.6723 - accuracy: 0.6234 - val_loss: 0.6539 - val_accuracy: 0.6530\n",
            "Epoch 98/200\n",
            "453/453 [==============================] - 63s 140ms/step - loss: 0.6693 - accuracy: 0.6273 - val_loss: 0.6480 - val_accuracy: 0.6530\n",
            "Epoch 99/200\n",
            "453/453 [==============================] - 62s 136ms/step - loss: 0.6696 - accuracy: 0.6243 - val_loss: 0.6535 - val_accuracy: 0.6530\n",
            "Epoch 100/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6726 - accuracy: 0.6182 - val_loss: 0.6476 - val_accuracy: 0.6530\n",
            "Epoch 101/200\n",
            "453/453 [==============================] - 62s 136ms/step - loss: 0.6675 - accuracy: 0.6231 - val_loss: 0.6540 - val_accuracy: 0.6530\n",
            "Epoch 102/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6688 - accuracy: 0.6244 - val_loss: 0.6766 - val_accuracy: 0.6530\n",
            "Epoch 103/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6752 - accuracy: 0.6117 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 104/200\n",
            "453/453 [==============================] - 62s 136ms/step - loss: 0.6770 - accuracy: 0.6126 - val_loss: 0.6571 - val_accuracy: 0.6530\n",
            "Epoch 105/200\n",
            "453/453 [==============================] - 63s 138ms/step - loss: 0.6737 - accuracy: 0.6186 - val_loss: 0.6471 - val_accuracy: 0.6530\n",
            "Epoch 106/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6730 - accuracy: 0.6236 - val_loss: 0.6684 - val_accuracy: 0.6530\n",
            "Epoch 107/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6732 - accuracy: 0.6216 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 108/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6716 - accuracy: 0.6142 - val_loss: 0.6530 - val_accuracy: 0.6530\n",
            "Epoch 109/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6722 - accuracy: 0.6164 - val_loss: 0.6471 - val_accuracy: 0.6530\n",
            "Epoch 110/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6705 - accuracy: 0.6186 - val_loss: 0.6482 - val_accuracy: 0.6530\n",
            "Epoch 111/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6683 - accuracy: 0.6281 - val_loss: 0.6477 - val_accuracy: 0.6530\n",
            "Epoch 112/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6733 - accuracy: 0.6218 - val_loss: 0.6600 - val_accuracy: 0.6530\n",
            "Epoch 113/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6736 - accuracy: 0.6206 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 114/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6694 - accuracy: 0.6234 - val_loss: 0.6740 - val_accuracy: 0.6530\n",
            "Epoch 115/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6723 - accuracy: 0.6258 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 116/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6668 - accuracy: 0.6244 - val_loss: 0.6602 - val_accuracy: 0.6530\n",
            "Epoch 117/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6723 - accuracy: 0.6186 - val_loss: 0.6496 - val_accuracy: 0.6530\n",
            "Epoch 118/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6703 - accuracy: 0.6221 - val_loss: 0.6477 - val_accuracy: 0.6530\n",
            "Epoch 119/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6694 - accuracy: 0.6258 - val_loss: 0.6558 - val_accuracy: 0.6530\n",
            "Epoch 120/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6709 - accuracy: 0.6204 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 121/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6722 - accuracy: 0.6224 - val_loss: 0.6564 - val_accuracy: 0.6530\n",
            "Epoch 122/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6711 - accuracy: 0.6230 - val_loss: 0.6698 - val_accuracy: 0.6530\n",
            "Epoch 123/200\n",
            "453/453 [==============================] - 61s 136ms/step - loss: 0.6695 - accuracy: 0.6231 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 124/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6720 - accuracy: 0.6211 - val_loss: 0.6606 - val_accuracy: 0.6530\n",
            "Epoch 125/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6697 - accuracy: 0.6211 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 126/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6707 - accuracy: 0.6237 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 127/200\n",
            "453/453 [==============================] - 61s 136ms/step - loss: 0.6701 - accuracy: 0.6206 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 128/200\n",
            "453/453 [==============================] - 61s 136ms/step - loss: 0.6684 - accuracy: 0.6231 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 129/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6698 - accuracy: 0.6233 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 130/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6700 - accuracy: 0.6251 - val_loss: 0.6474 - val_accuracy: 0.6530\n",
            "Epoch 131/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6728 - accuracy: 0.6237 - val_loss: 0.6618 - val_accuracy: 0.6530\n",
            "Epoch 132/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6678 - accuracy: 0.6248 - val_loss: 0.6627 - val_accuracy: 0.6530\n",
            "Epoch 133/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6712 - accuracy: 0.6227 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 134/200\n",
            "453/453 [==============================] - 62s 137ms/step - loss: 0.6714 - accuracy: 0.6227 - val_loss: 0.6471 - val_accuracy: 0.6530\n",
            "Epoch 135/200\n",
            "453/453 [==============================] - 62s 136ms/step - loss: 0.6683 - accuracy: 0.6260 - val_loss: 0.6523 - val_accuracy: 0.6530\n",
            "Epoch 136/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6724 - accuracy: 0.6204 - val_loss: 0.6474 - val_accuracy: 0.6530\n",
            "Epoch 137/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6702 - accuracy: 0.6236 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
            "Epoch 138/200\n",
            "453/453 [==============================] - 61s 136ms/step - loss: 0.6716 - accuracy: 0.6244 - val_loss: 0.6513 - val_accuracy: 0.6530\n",
            "Epoch 139/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6729 - accuracy: 0.6189 - val_loss: 0.6619 - val_accuracy: 0.6530\n",
            "Epoch 140/200\n",
            "453/453 [==============================] - 60s 134ms/step - loss: 0.6705 - accuracy: 0.6231 - val_loss: 0.6582 - val_accuracy: 0.6530\n",
            "Epoch 141/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6707 - accuracy: 0.6197 - val_loss: 0.6552 - val_accuracy: 0.6530\n",
            "Epoch 142/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6666 - accuracy: 0.6251 - val_loss: 0.6537 - val_accuracy: 0.6530\n",
            "Epoch 143/200\n",
            "453/453 [==============================] - 61s 136ms/step - loss: 0.6690 - accuracy: 0.6246 - val_loss: 0.6489 - val_accuracy: 0.6530\n",
            "Epoch 144/200\n",
            "453/453 [==============================] - 62s 136ms/step - loss: 0.6740 - accuracy: 0.6168 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 145/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6706 - accuracy: 0.6256 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 146/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6726 - accuracy: 0.6211 - val_loss: 0.6627 - val_accuracy: 0.6530\n",
            "Epoch 147/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6697 - accuracy: 0.6244 - val_loss: 0.6982 - val_accuracy: 0.3470\n",
            "Epoch 148/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6705 - accuracy: 0.6249 - val_loss: 0.6471 - val_accuracy: 0.6530\n",
            "Epoch 149/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6697 - accuracy: 0.6269 - val_loss: 0.6512 - val_accuracy: 0.6530\n",
            "Epoch 150/200\n",
            "453/453 [==============================] - 60s 134ms/step - loss: 0.6709 - accuracy: 0.6212 - val_loss: 0.6568 - val_accuracy: 0.6530\n",
            "Epoch 151/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6706 - accuracy: 0.6202 - val_loss: 0.6592 - val_accuracy: 0.6530\n",
            "Epoch 152/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6734 - accuracy: 0.6200 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 153/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6680 - accuracy: 0.6253 - val_loss: 0.6696 - val_accuracy: 0.6530\n",
            "Epoch 154/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6704 - accuracy: 0.6186 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 155/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6687 - accuracy: 0.6232 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 156/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6697 - accuracy: 0.6223 - val_loss: 0.6485 - val_accuracy: 0.6530\n",
            "Epoch 157/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6725 - accuracy: 0.6211 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 158/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6689 - accuracy: 0.6238 - val_loss: 0.6511 - val_accuracy: 0.6530\n",
            "Epoch 159/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6737 - accuracy: 0.6194 - val_loss: 0.6552 - val_accuracy: 0.6530\n",
            "Epoch 160/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6751 - accuracy: 0.6189 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 161/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6677 - accuracy: 0.6253 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 162/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6701 - accuracy: 0.6257 - val_loss: 0.6512 - val_accuracy: 0.6530\n",
            "Epoch 163/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6697 - accuracy: 0.6268 - val_loss: 0.6529 - val_accuracy: 0.6530\n",
            "Epoch 164/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6698 - accuracy: 0.6243 - val_loss: 0.6519 - val_accuracy: 0.6530\n",
            "Epoch 165/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6705 - accuracy: 0.6215 - val_loss: 0.6568 - val_accuracy: 0.6530\n",
            "Epoch 166/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6688 - accuracy: 0.6236 - val_loss: 0.6476 - val_accuracy: 0.6530\n",
            "Epoch 167/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6715 - accuracy: 0.6206 - val_loss: 0.6527 - val_accuracy: 0.6530\n",
            "Epoch 168/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6686 - accuracy: 0.6249 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 169/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6696 - accuracy: 0.6238 - val_loss: 0.6482 - val_accuracy: 0.6530\n",
            "Epoch 170/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6743 - accuracy: 0.6165 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 171/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6749 - accuracy: 0.6204 - val_loss: 0.6500 - val_accuracy: 0.6530\n",
            "Epoch 172/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6687 - accuracy: 0.6236 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
            "Epoch 173/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6721 - accuracy: 0.6190 - val_loss: 0.6469 - val_accuracy: 0.6530\n",
            "Epoch 174/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6707 - accuracy: 0.6269 - val_loss: 0.6588 - val_accuracy: 0.6530\n",
            "Epoch 175/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6683 - accuracy: 0.6271 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 176/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6682 - accuracy: 0.6239 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 177/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6708 - accuracy: 0.6255 - val_loss: 0.6480 - val_accuracy: 0.6530\n",
            "Epoch 178/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6713 - accuracy: 0.6253 - val_loss: 0.6473 - val_accuracy: 0.6530\n",
            "Epoch 179/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6658 - accuracy: 0.6287 - val_loss: 0.6487 - val_accuracy: 0.6530\n",
            "Epoch 180/200\n",
            "453/453 [==============================] - 61s 136ms/step - loss: 0.6693 - accuracy: 0.6233 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 181/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6683 - accuracy: 0.6261 - val_loss: 0.6469 - val_accuracy: 0.6530\n",
            "Epoch 182/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6759 - accuracy: 0.6134 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 183/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6714 - accuracy: 0.6197 - val_loss: 0.6570 - val_accuracy: 0.6530\n",
            "Epoch 184/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6716 - accuracy: 0.6211 - val_loss: 0.6480 - val_accuracy: 0.6530\n",
            "Epoch 185/200\n",
            "453/453 [==============================] - 61s 135ms/step - loss: 0.6702 - accuracy: 0.6235 - val_loss: 0.6699 - val_accuracy: 0.6530\n",
            "Epoch 186/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6699 - accuracy: 0.6236 - val_loss: 0.6564 - val_accuracy: 0.6530\n",
            "Epoch 187/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6681 - accuracy: 0.6229 - val_loss: 0.6544 - val_accuracy: 0.6530\n",
            "Epoch 188/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6702 - accuracy: 0.6253 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 189/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6745 - accuracy: 0.6135 - val_loss: 0.6562 - val_accuracy: 0.6530\n",
            "Epoch 190/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6682 - accuracy: 0.6264 - val_loss: 0.6717 - val_accuracy: 0.6530\n",
            "Epoch 191/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6696 - accuracy: 0.6196 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 192/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6682 - accuracy: 0.6264 - val_loss: 0.6470 - val_accuracy: 0.6530\n",
            "Epoch 193/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6718 - accuracy: 0.6200 - val_loss: 0.6483 - val_accuracy: 0.6530\n",
            "Epoch 194/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6675 - accuracy: 0.6275 - val_loss: 0.6829 - val_accuracy: 0.6530\n",
            "Epoch 195/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6708 - accuracy: 0.6209 - val_loss: 0.6477 - val_accuracy: 0.6530\n",
            "Epoch 196/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6704 - accuracy: 0.6233 - val_loss: 0.6569 - val_accuracy: 0.6530\n",
            "Epoch 197/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6726 - accuracy: 0.6224 - val_loss: 0.6640 - val_accuracy: 0.6530\n",
            "Epoch 198/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6725 - accuracy: 0.6242 - val_loss: 0.6503 - val_accuracy: 0.6530\n",
            "Epoch 199/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6699 - accuracy: 0.6231 - val_loss: 0.6557 - val_accuracy: 0.6530\n",
            "Epoch 200/200\n",
            "453/453 [==============================] - 60s 133ms/step - loss: 0.6682 - accuracy: 0.6262 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Fold 1, 200 epochs, 12228 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 65.30%\n",
            "\n",
            "1-way Cross Validation mean 65.30% (+/- 0.00%)\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4fh2GI8beMQ"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}