{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MLP_214.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojm_6E9f9Kcf"
      },
      "source": [
        "# MLP 214\n",
        "* Operate on 16000 GenCode 34 seqs.\n",
        "* 5-way cross validation. Save best model per CV.\n",
        "* Report mean accuracy from final re-validation with best 5.\n",
        "* Use Adam with a learn rate decay schdule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh6XplUvC0j0",
        "outputId": "700e5d80-1442-4f54-a525-1d17e2057290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "NC_FILENAME='ncRNA.gc34.processed.fasta'\n",
        "PC_FILENAME='pcRNA.gc34.processed.fasta'\n",
        "DATAPATH=\"\"\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    PATH='/content/drive/'\n",
        "    drive.mount(PATH)\n",
        "    DATAPATH=PATH+'My Drive/data/'  # must end in \"/\"\n",
        "    NC_FILENAME = DATAPATH+NC_FILENAME\n",
        "    PC_FILENAME = DATAPATH+PC_FILENAME\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    DATAPATH=\"\" \n",
        "\n",
        "EPOCHS=200\n",
        "SPLITS=5\n",
        "K=3\n",
        "VOCABULARY_SIZE=4**K+1   # e.g. K=3 => 64 DNA K-mers + 'NNN'\n",
        "EMBED_DIMEN=16\n",
        "FILENAME='MLP214'\n",
        "NEURONS=16"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQY7aTj29Kch"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LayerNormalization\n",
        "import time\n",
        "dt='float32'\n",
        "tf.keras.backend.set_floatx(dt)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7jcg6Wl9Kc2"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLFNO1Xa9Kc3"
      },
      "source": [
        "def compile_model(model):\n",
        "    adam_default_learn_rate = 0.001\n",
        "    schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate = adam_default_learn_rate*10,\n",
        "        #decay_steps=100000, decay_rate=0.96, staircase=True)\n",
        "        decay_steps=10000, decay_rate=0.99, staircase=True)\n",
        "    # learn rate = initial_learning_rate * decay_rate ^ (step / decay_steps)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=schedule)\n",
        "    bc=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    print(\"COMPILE...\")\n",
        "    model.compile(loss=bc, optimizer=opt, metrics=[\"accuracy\"])\n",
        "    print(\"...COMPILED\")\n",
        "    return model\n",
        "\n",
        "def build_model(maxlen):\n",
        "    act=\"elu\"\n",
        "    embed_layer  = keras.layers.Embedding(\n",
        "        input_dim=VOCABULARY_SIZE, output_dim=EMBED_DIMEN)\n",
        "    #dense1_layer = keras.layers.Dense(NEURONS, activation=act,dtype=dt,\n",
        "    #                input_dim=VOCABULARY_SIZE)  # match embed output\n",
        "    dense1_layer = keras.layers.Dense(NEURONS, activation=act,dtype=dt,\n",
        "                    input_dim=[maxlen,EMBED_DIMEN])  # match embed output\n",
        "    drop1_layer = keras.layers.Dropout(0.5)\n",
        "    dense2_layer = keras.layers.Dense(NEURONS, activation=act,dtype=dt)\n",
        "    drop2_layer = keras.layers.Dropout(0.5)\n",
        "    #dense3_layer = keras.layers.Dense(NEURONS, activation=act,dtype=dt)\n",
        "    output_layer = keras.layers.Dense(1,  activation=\"sigmoid\",dtype=dt)\n",
        "    mlp = keras.models.Sequential()\n",
        "    mlp.add(embed_layer)\n",
        "    mlp.add(dense1_layer)\n",
        "    mlp.add(drop1_layer)\n",
        "    mlp.add(dense2_layer)\n",
        "    mlp.add(drop2_layer)\n",
        "    #mlp.add(dense3_layer)\n",
        "    mlp.add(output_layer)\n",
        "    mlpc = compile_model(mlp)\n",
        "    return mlpc"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV6k-xOm9Kcn"
      },
      "source": [
        "## Load and partition sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I-O_qzw9Kco"
      },
      "source": [
        "# Assume file was preprocessed to contain one line per seq.\n",
        "# Prefer Pandas dataframe but df does not support append.\n",
        "# For conversion to tensor, must avoid python lists.\n",
        "def load_fasta(filename,label):\n",
        "    DEFLINE='>'\n",
        "    labels=[]\n",
        "    seqs=[]\n",
        "    lens=[]\n",
        "    nums=[]\n",
        "    num=0\n",
        "    with open (filename,'r') as infile:\n",
        "        for line in infile:\n",
        "            if line[0]!=DEFLINE:\n",
        "                seq=line.rstrip()\n",
        "                num += 1   # first seqnum is 1\n",
        "                seqlen=len(seq)\n",
        "                nums.append(num)\n",
        "                labels.append(label)\n",
        "                seqs.append(seq)\n",
        "                lens.append(seqlen)\n",
        "    df1=pd.DataFrame(nums,columns=['seqnum'])\n",
        "    df2=pd.DataFrame(labels,columns=['class'])\n",
        "    df3=pd.DataFrame(seqs,columns=['sequence'])\n",
        "    df4=pd.DataFrame(lens,columns=['seqlen'])\n",
        "    df=pd.concat((df1,df2,df3,df4),axis=1)\n",
        "    return df\n",
        "\n",
        "def separate_X_and_y(data):\n",
        "    y=   data[['class']].copy()\n",
        "    X=   data.drop(columns=['class','seqnum','seqlen'])\n",
        "    return (X,y)\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRAaO9jP9Kcr"
      },
      "source": [
        "## Make K-mers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8xcZ4Mr9Kcs"
      },
      "source": [
        "def make_kmer_table(K):\n",
        "    npad='N'*K\n",
        "    shorter_kmers=['']\n",
        "    for i in range(K):\n",
        "        longer_kmers=[]\n",
        "        for mer in shorter_kmers:\n",
        "            longer_kmers.append(mer+'A')\n",
        "            longer_kmers.append(mer+'C')\n",
        "            longer_kmers.append(mer+'G')\n",
        "            longer_kmers.append(mer+'T')\n",
        "        shorter_kmers = longer_kmers\n",
        "    all_kmers = shorter_kmers\n",
        "    kmer_dict = {}\n",
        "    kmer_dict[npad]=0\n",
        "    value=1\n",
        "    for mer in all_kmers:\n",
        "        kmer_dict[mer]=value\n",
        "        value += 1\n",
        "    return kmer_dict\n",
        "\n",
        "KMER_TABLE=make_kmer_table(K)\n",
        "\n",
        "def strings_to_vectors(data,uniform_len):\n",
        "    all_seqs=[]\n",
        "    for seq in data['sequence']:\n",
        "        i=0\n",
        "        seqlen=len(seq)\n",
        "        kmers=[]\n",
        "        while i < seqlen-K+1 -1:  # stop at minus one for spaced seed\n",
        "            #kmer=seq[i:i+2]+seq[i+3:i+5]    # SPACED SEED 2/1/2 for K=4\n",
        "            kmer=seq[i:i+K]  \n",
        "            i += 1\n",
        "            value=KMER_TABLE[kmer]\n",
        "            kmers.append(value)\n",
        "        pad_val=0\n",
        "        while i < uniform_len:\n",
        "            kmers.append(pad_val)\n",
        "            i += 1\n",
        "        all_seqs.append(kmers)\n",
        "    pd2d=pd.DataFrame(all_seqs)\n",
        "    return pd2d   # return 2D dataframe, uniform dimensions"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEtA0xiV9Kcv"
      },
      "source": [
        "def make_kmers(MAXLEN,train_set):\n",
        "    (X_train_all,y_train_all)=separate_X_and_y(train_set)\n",
        "    X_train_kmers=strings_to_vectors(X_train_all,MAXLEN)\n",
        "    # From pandas dataframe to numpy to list to numpy\n",
        "    num_seqs=len(X_train_kmers)\n",
        "    tmp_seqs=[]\n",
        "    for i in range(num_seqs):\n",
        "        kmer_sequence=X_train_kmers.iloc[i]\n",
        "        tmp_seqs.append(kmer_sequence)\n",
        "    X_train_kmers=np.array(tmp_seqs)\n",
        "    tmp_seqs=None\n",
        "    labels=y_train_all.to_numpy()\n",
        "    return (X_train_kmers,labels)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaXyySyO9Kcz"
      },
      "source": [
        "def make_frequencies(Xin):\n",
        "    Xout=[]\n",
        "    VOCABULARY_SIZE= 4**K + 1  # plus one for 'NNN'\n",
        "    for seq in Xin:\n",
        "        freqs =[0] * VOCABULARY_SIZE\n",
        "        total = 0\n",
        "        for kmerval in seq:\n",
        "            freqs[kmerval] += 1\n",
        "            total += 1\n",
        "        for c in range(VOCABULARY_SIZE):\n",
        "            freqs[c] = freqs[c]/total\n",
        "        Xout.append(freqs)\n",
        "    Xnum = np.asarray(Xout)\n",
        "    return (Xnum)\n",
        "def make_slice(data_set,min_len,max_len):\n",
        "    slice = data_set.query('seqlen <= '+str(max_len)+' & seqlen>= '+str(min_len))\n",
        "    return slice"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdIS2utq9Kc9"
      },
      "source": [
        "## Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVo4tbB_9Kc-"
      },
      "source": [
        "def do_cross_validation(X,y,given_model):\n",
        "    cv_scores = []\n",
        "    fold=0\n",
        "    splitter = ShuffleSplit(n_splits=SPLITS, test_size=0.1, random_state=37863)\n",
        "    for train_index,valid_index in splitter.split(X):\n",
        "        fold += 1\n",
        "        X_train=X[train_index] # use iloc[] for dataframe\n",
        "        y_train=y[train_index]\n",
        "        X_valid=X[valid_index]\n",
        "        y_valid=y[valid_index]        \n",
        "        # Avoid continually improving the same model.\n",
        "        model = compile_model(keras.models.clone_model(given_model))\n",
        "        bestname=DATAPATH+FILENAME+\".cv.\"+str(fold)+\".best\"\n",
        "        mycallbacks = [keras.callbacks.ModelCheckpoint(\n",
        "            filepath=bestname, save_best_only=True, \n",
        "            monitor='val_accuracy', mode='max')]   \n",
        "        print(\"FIT\")\n",
        "        start_time=time.time()\n",
        "        history=model.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
        "                epochs=EPOCHS, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
        "                callbacks=mycallbacks,\n",
        "                validation_data=(X_valid,y_valid) )\n",
        "        end_time=time.time()\n",
        "        elapsed_time=(end_time-start_time)                        \n",
        "        print(\"Fold %d, %d epochs, %d sec\"%(fold,EPOCHS,elapsed_time))\n",
        "        pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "        plt.grid(True)\n",
        "        plt.gca().set_ylim(0,1)\n",
        "        plt.show()\n",
        "        best_model=keras.models.load_model(bestname)\n",
        "        scores = best_model.evaluate(X_valid, y_valid, verbose=0)\n",
        "        print(\"%s: %.2f%%\" % (best_model.metrics_names[1], scores[1]*100))\n",
        "        cv_scores.append(scores[1] * 100)  \n",
        "    print()\n",
        "    print(\"%d-way Cross Validation mean %.2f%% (+/- %.2f%%)\" % (fold, np.mean(cv_scores), np.std(cv_scores)))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd3Wj_vI9KdP"
      },
      "source": [
        "## Train on RNA lengths 200-1Kb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8fNo6sn9KdH",
        "outputId": "462f6157-218d-4978-abeb-03cdf91da37e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        }
      },
      "source": [
        "MINLEN=200\n",
        "MAXLEN=1000\n",
        "print(\"Load data from files.\")\n",
        "nc_seq=load_fasta(NC_FILENAME,0)\n",
        "pc_seq=load_fasta(PC_FILENAME,1)\n",
        "train_set=pd.concat((nc_seq,pc_seq),axis=0)\n",
        "nc_seq=None\n",
        "pc_seq=None\n",
        "print(\"Ready: train_set\")\n",
        "#train_set\n",
        "print (\"Compile the model\")\n",
        "model=build_model(MAXLEN)\n",
        "print (\"Summarize the model\")\n",
        "print(model.summary())  # Print this only once\n",
        "model.save(DATAPATH+FILENAME+'.model')\n",
        "print (\"Data prep\")\n",
        "subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
        "print (\"Data reshape\")\n",
        "(X_train,y_train)=make_kmers(MAXLEN,subset)\n",
        "X_train=make_frequencies(X_train)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load data from files.\n",
            "Ready: train_set\n",
            "Compile the model\n",
            "COMPILE...\n",
            "...COMPILED\n",
            "Summarize the model\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, None, 16)          1040      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, None, 16)          272       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, None, 16)          0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, None, 16)          272       \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, None, 16)          0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, None, 1)           17        \n",
            "=================================================================\n",
            "Total params: 1,601\n",
            "Trainable params: 1,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/data/MLP214.model/assets\n",
            "Data prep\n",
            "Data reshape\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ8eW5Rg9KdQ",
        "outputId": "4a7af785-8cd1-469a-e7db-05561c72e6d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print (\"Cross valiation\")\n",
        "do_cross_validation(X_train,y_train,model)  \n",
        "print (\"Done\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross valiation\n",
            "COMPILE...\n",
            "...COMPILED\n",
            "FIT\n",
            "Epoch 1/200\n",
            "453/453 [==============================] - ETA: 0s - loss: 0.6562 - accuracy: 0.6391INFO:tensorflow:Assets written to: /content/drive/My Drive/data/MLP214.cv.1.best/assets\n",
            "453/453 [==============================] - 3s 7ms/step - loss: 0.6562 - accuracy: 0.6391 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 2/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6544 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 3/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 4/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 5/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 6/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 7/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 8/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 9/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6549 - accuracy: 0.6392 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 10/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 11/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 12/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 13/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6468 - val_accuracy: 0.6530\n",
            "Epoch 14/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6476 - val_accuracy: 0.6530\n",
            "Epoch 15/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 16/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 17/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
            "Epoch 18/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 19/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 20/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6471 - val_accuracy: 0.6530\n",
            "Epoch 21/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 22/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 23/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 24/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6502 - val_accuracy: 0.6530\n",
            "Epoch 25/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 26/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 27/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 28/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 29/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6474 - val_accuracy: 0.6530\n",
            "Epoch 30/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 31/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6547 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 32/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 33/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6469 - val_accuracy: 0.6530\n",
            "Epoch 34/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 35/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6396 - val_loss: 0.6471 - val_accuracy: 0.6530\n",
            "Epoch 36/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 37/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 38/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
            "Epoch 39/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 40/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 41/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 42/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6544 - accuracy: 0.6395 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 43/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 44/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6543 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 45/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
            "Epoch 46/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 47/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 48/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 49/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 50/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 51/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 52/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6472 - val_accuracy: 0.6530\n",
            "Epoch 53/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 54/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 55/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6397 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
            "Epoch 56/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 57/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 58/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 59/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6549 - accuracy: 0.6394 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 60/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 61/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 62/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 63/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 64/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6469 - val_accuracy: 0.6530\n",
            "Epoch 65/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 66/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6474 - val_accuracy: 0.6530\n",
            "Epoch 67/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 68/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 69/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 70/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 71/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 72/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 73/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6546 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 74/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 75/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 76/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 77/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 78/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 79/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 80/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6396 - val_loss: 0.6473 - val_accuracy: 0.6530\n",
            "Epoch 81/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 82/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 83/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6546 - accuracy: 0.6394 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 84/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6469 - val_accuracy: 0.6530\n",
            "Epoch 85/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 86/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 87/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6488 - val_accuracy: 0.6530\n",
            "Epoch 88/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 89/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6472 - val_accuracy: 0.6530\n",
            "Epoch 90/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6543 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 91/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 92/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 93/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 94/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 95/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 96/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 97/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 98/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 99/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6468 - val_accuracy: 0.6530\n",
            "Epoch 100/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 101/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6547 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 102/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 103/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 104/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 105/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6483 - val_accuracy: 0.6530\n",
            "Epoch 106/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 107/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6468 - val_accuracy: 0.6530\n",
            "Epoch 108/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 109/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6471 - val_accuracy: 0.6530\n",
            "Epoch 110/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 111/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 112/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 113/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 114/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 115/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6396 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 116/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 117/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 118/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 119/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 120/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 121/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 122/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 123/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 124/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 125/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 126/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6471 - val_accuracy: 0.6530\n",
            "Epoch 127/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 128/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6471 - val_accuracy: 0.6530\n",
            "Epoch 129/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6543 - accuracy: 0.6397 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
            "Epoch 130/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 131/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 132/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 133/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 134/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6547 - accuracy: 0.6383 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 135/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6472 - val_accuracy: 0.6530\n",
            "Epoch 136/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 137/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 138/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 139/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6485 - val_accuracy: 0.6530\n",
            "Epoch 140/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 141/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 142/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 143/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 144/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 145/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 146/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6543 - accuracy: 0.6393 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 147/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 148/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 149/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 150/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6543 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 151/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 152/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 153/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 154/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 155/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 156/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 157/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 158/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 159/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 160/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 161/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
            "Epoch 162/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6471 - val_accuracy: 0.6530\n",
            "Epoch 163/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6547 - accuracy: 0.6394 - val_loss: 0.6468 - val_accuracy: 0.6530\n",
            "Epoch 164/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6396 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 165/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 166/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 167/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6468 - val_accuracy: 0.6530\n",
            "Epoch 168/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 169/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 170/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 171/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6470 - val_accuracy: 0.6530\n",
            "Epoch 172/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 173/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 174/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6544 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 175/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 176/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 177/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6397 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
            "Epoch 178/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 179/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6477 - val_accuracy: 0.6530\n",
            "Epoch 180/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 181/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 182/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 183/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 184/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 185/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6542 - accuracy: 0.6392 - val_loss: 0.6468 - val_accuracy: 0.6530\n",
            "Epoch 186/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 187/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 188/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 189/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 190/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 191/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 192/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6548 - accuracy: 0.6396 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 193/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 194/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 195/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6474 - val_accuracy: 0.6530\n",
            "Epoch 196/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 197/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 198/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 199/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 200/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6469 - val_accuracy: 0.6530\n",
            "Fold 1, 200 epochs, 384 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 65.30%\n",
            "COMPILE...\n",
            "...COMPILED\n",
            "FIT\n",
            "Epoch 1/200\n",
            "451/453 [============================>.] - ETA: 0s - loss: 0.6556 - accuracy: 0.6404INFO:tensorflow:Assets written to: /content/drive/My Drive/data/MLP214.cv.2.best/assets\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.6557 - accuracy: 0.6402 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 2/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 3/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 4/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 5/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 6/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 7/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 8/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6514 - val_accuracy: 0.6449\n",
            "Epoch 9/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 10/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 11/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 12/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 13/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
            "Epoch 14/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6403 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 15/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6522 - val_accuracy: 0.6449\n",
            "Epoch 16/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 17/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6514 - val_accuracy: 0.6449\n",
            "Epoch 18/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 19/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 20/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 21/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
            "Epoch 22/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 23/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
            "Epoch 24/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
            "Epoch 25/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 26/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 27/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 28/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 29/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6522 - val_accuracy: 0.6449\n",
            "Epoch 30/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6405 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 31/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 32/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 33/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 34/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 35/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6515 - val_accuracy: 0.6449\n",
            "Epoch 36/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 37/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 38/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 39/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 40/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 41/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 42/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 43/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6401 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 44/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 45/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 46/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
            "Epoch 47/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 48/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 49/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
            "Epoch 50/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6404 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 51/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 52/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6515 - val_accuracy: 0.6449\n",
            "Epoch 53/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 54/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 55/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 56/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6403 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 57/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 58/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 59/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
            "Epoch 60/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 61/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 62/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
            "Epoch 63/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 64/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 65/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 66/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6404 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 67/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
            "Epoch 68/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6405 - val_loss: 0.6514 - val_accuracy: 0.6449\n",
            "Epoch 69/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 70/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6555 - val_accuracy: 0.6449\n",
            "Epoch 71/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 72/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 73/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
            "Epoch 74/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 75/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 76/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 77/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 78/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 79/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 80/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6403 - val_loss: 0.6519 - val_accuracy: 0.6449\n",
            "Epoch 81/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 82/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
            "Epoch 83/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 84/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 85/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 86/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 87/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 88/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 89/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 90/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 91/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 92/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6519 - val_accuracy: 0.6449\n",
            "Epoch 93/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 94/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
            "Epoch 95/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
            "Epoch 96/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 97/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 98/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 99/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 100/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 101/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
            "Epoch 102/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 103/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 104/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 105/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 106/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 107/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 108/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6403 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 109/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6516 - val_accuracy: 0.6449\n",
            "Epoch 110/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 111/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 112/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 113/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 114/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 115/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 116/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6403 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 117/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6518 - val_accuracy: 0.6449\n",
            "Epoch 118/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 119/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 120/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 121/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 122/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 123/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 124/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 125/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 126/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6400 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 127/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
            "Epoch 128/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 129/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 130/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6518 - val_accuracy: 0.6449\n",
            "Epoch 131/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 132/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 133/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6404 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
            "Epoch 134/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 135/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 136/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 137/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 138/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 139/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 140/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 141/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 142/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 143/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 144/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
            "Epoch 145/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6526 - val_accuracy: 0.6449\n",
            "Epoch 146/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 147/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 148/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 149/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 150/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 151/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 152/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 153/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 154/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 155/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 156/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 157/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 158/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6405 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
            "Epoch 159/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 160/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 161/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 162/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 163/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 164/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 165/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6543 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 166/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 167/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 168/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 169/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 170/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 171/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6514 - val_accuracy: 0.6449\n",
            "Epoch 172/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 173/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 174/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 175/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 176/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 177/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6406 - val_loss: 0.6611 - val_accuracy: 0.6449\n",
            "Epoch 178/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 179/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 180/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 181/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 182/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 183/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6403 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Epoch 184/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 185/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6513 - val_accuracy: 0.6449\n",
            "Epoch 186/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 187/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6513 - val_accuracy: 0.6449\n",
            "Epoch 188/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6404 - val_loss: 0.6515 - val_accuracy: 0.6449\n",
            "Epoch 189/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 190/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
            "Epoch 191/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 192/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 193/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
            "Epoch 194/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 195/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
            "Epoch 196/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
            "Epoch 197/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6399 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
            "Epoch 198/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 199/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
            "Epoch 200/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
            "Fold 2, 200 epochs, 368 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 64.49%\n",
            "COMPILE...\n",
            "...COMPILED\n",
            "FIT\n",
            "Epoch 1/200\n",
            "450/453 [============================>.] - ETA: 0s - loss: 0.6547 - accuracy: 0.6424INFO:tensorflow:Assets written to: /content/drive/My Drive/data/MLP214.cv.3.best/assets\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.6547 - accuracy: 0.6423 - val_loss: 0.6638 - val_accuracy: 0.6263\n",
            "Epoch 2/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6526 - accuracy: 0.6427 - val_loss: 0.6631 - val_accuracy: 0.6263\n",
            "Epoch 3/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
            "Epoch 4/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 5/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
            "Epoch 6/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
            "Epoch 7/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
            "Epoch 8/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6526 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
            "Epoch 9/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
            "Epoch 10/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6628 - val_accuracy: 0.6263\n",
            "Epoch 11/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
            "Epoch 12/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6616 - val_accuracy: 0.6263\n",
            "Epoch 13/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
            "Epoch 14/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
            "Epoch 15/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6526 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 16/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 17/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6625 - val_accuracy: 0.6263\n",
            "Epoch 18/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6518 - accuracy: 0.6427 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
            "Epoch 19/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
            "Epoch 20/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6628 - val_accuracy: 0.6263\n",
            "Epoch 21/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 22/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
            "Epoch 23/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 24/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 25/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
            "Epoch 26/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6528 - accuracy: 0.6427 - val_loss: 0.6625 - val_accuracy: 0.6263\n",
            "Epoch 27/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6630 - val_accuracy: 0.6263\n",
            "Epoch 28/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6629 - val_accuracy: 0.6263\n",
            "Epoch 29/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
            "Epoch 30/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 31/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6427 - val_loss: 0.6624 - val_accuracy: 0.6263\n",
            "Epoch 32/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
            "Epoch 33/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 34/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 35/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6635 - val_accuracy: 0.6263\n",
            "Epoch 36/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 37/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
            "Epoch 38/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6618 - val_accuracy: 0.6263\n",
            "Epoch 39/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6650 - val_accuracy: 0.6263\n",
            "Epoch 40/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6527 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 41/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
            "Epoch 42/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
            "Epoch 43/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
            "Epoch 44/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 45/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6526 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
            "Epoch 46/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 47/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6622 - val_accuracy: 0.6263\n",
            "Epoch 48/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6628 - val_accuracy: 0.6263\n",
            "Epoch 49/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 50/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
            "Epoch 51/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6527 - accuracy: 0.6422 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 52/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
            "Epoch 53/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 54/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 55/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6422 - val_loss: 0.6632 - val_accuracy: 0.6263\n",
            "Epoch 56/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 57/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 58/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6426 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 59/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
            "Epoch 60/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 61/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 62/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 63/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
            "Epoch 64/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6621 - val_accuracy: 0.6263\n",
            "Epoch 65/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6625 - val_accuracy: 0.6263\n",
            "Epoch 66/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6426 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 67/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6621 - val_accuracy: 0.6263\n",
            "Epoch 68/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
            "Epoch 69/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
            "Epoch 70/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6426 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
            "Epoch 71/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
            "Epoch 72/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
            "Epoch 73/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
            "Epoch 74/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 75/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
            "Epoch 76/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
            "Epoch 77/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6625 - val_accuracy: 0.6263\n",
            "Epoch 78/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6625 - val_accuracy: 0.6263\n",
            "Epoch 79/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
            "Epoch 80/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 81/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 82/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6527 - accuracy: 0.6427 - val_loss: 0.6659 - val_accuracy: 0.6263\n",
            "Epoch 83/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 84/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
            "Epoch 85/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6519 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 86/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
            "Epoch 87/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 88/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
            "Epoch 89/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 90/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6628 - val_accuracy: 0.6263\n",
            "Epoch 91/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6624 - val_accuracy: 0.6263\n",
            "Epoch 92/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6420 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 93/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
            "Epoch 94/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 95/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
            "Epoch 96/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 97/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6627 - val_accuracy: 0.6263\n",
            "Epoch 98/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6655 - val_accuracy: 0.6263\n",
            "Epoch 99/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6416 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
            "Epoch 100/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
            "Epoch 101/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
            "Epoch 102/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 103/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 104/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6519 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 105/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 106/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6625 - val_accuracy: 0.6263\n",
            "Epoch 107/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6528 - accuracy: 0.6426 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 108/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
            "Epoch 109/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
            "Epoch 110/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6519 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 111/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
            "Epoch 112/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6627 - val_accuracy: 0.6263\n",
            "Epoch 113/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
            "Epoch 114/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 115/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
            "Epoch 116/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
            "Epoch 117/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 118/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6623 - val_accuracy: 0.6263\n",
            "Epoch 119/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 120/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
            "Epoch 121/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
            "Epoch 122/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6628 - val_accuracy: 0.6263\n",
            "Epoch 123/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6618 - val_accuracy: 0.6263\n",
            "Epoch 124/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6426 - val_loss: 0.6618 - val_accuracy: 0.6263\n",
            "Epoch 125/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6625 - val_accuracy: 0.6263\n",
            "Epoch 126/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
            "Epoch 127/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 128/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 129/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6623 - val_accuracy: 0.6263\n",
            "Epoch 130/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
            "Epoch 131/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6621 - val_accuracy: 0.6263\n",
            "Epoch 132/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
            "Epoch 133/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6627 - val_accuracy: 0.6263\n",
            "Epoch 134/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
            "Epoch 135/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
            "Epoch 136/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 137/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
            "Epoch 138/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 139/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6621 - val_accuracy: 0.6263\n",
            "Epoch 140/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6627 - val_accuracy: 0.6263\n",
            "Epoch 141/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
            "Epoch 142/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6423 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 143/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 144/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 145/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
            "Epoch 146/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 147/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 148/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
            "Epoch 149/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 150/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 151/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6626 - val_accuracy: 0.6263\n",
            "Epoch 152/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6623 - val_accuracy: 0.6263\n",
            "Epoch 153/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
            "Epoch 154/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6616 - val_accuracy: 0.6263\n",
            "Epoch 155/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6629 - val_accuracy: 0.6263\n",
            "Epoch 156/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
            "Epoch 157/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6625 - val_accuracy: 0.6263\n",
            "Epoch 158/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6426 - val_loss: 0.6691 - val_accuracy: 0.6263\n",
            "Epoch 159/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6424 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 160/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6628 - val_accuracy: 0.6263\n",
            "Epoch 161/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6519 - accuracy: 0.6427 - val_loss: 0.6622 - val_accuracy: 0.6263\n",
            "Epoch 162/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
            "Epoch 163/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 164/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6426 - val_loss: 0.6618 - val_accuracy: 0.6263\n",
            "Epoch 165/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6525 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 166/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
            "Epoch 167/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 168/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6632 - val_accuracy: 0.6263\n",
            "Epoch 169/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
            "Epoch 170/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6643 - val_accuracy: 0.6263\n",
            "Epoch 171/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
            "Epoch 172/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6622 - val_accuracy: 0.6263\n",
            "Epoch 173/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6616 - val_accuracy: 0.6263\n",
            "Epoch 174/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 175/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
            "Epoch 176/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 177/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6526 - accuracy: 0.6426 - val_loss: 0.6646 - val_accuracy: 0.6263\n",
            "Epoch 178/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6628 - val_accuracy: 0.6263\n",
            "Epoch 179/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
            "Epoch 180/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
            "Epoch 181/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 182/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6526 - accuracy: 0.6427 - val_loss: 0.6621 - val_accuracy: 0.6263\n",
            "Epoch 183/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6618 - val_accuracy: 0.6263\n",
            "Epoch 184/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6616 - val_accuracy: 0.6263\n",
            "Epoch 185/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
            "Epoch 186/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6526 - accuracy: 0.6426 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
            "Epoch 187/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6626 - val_accuracy: 0.6263\n",
            "Epoch 188/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6628 - val_accuracy: 0.6263\n",
            "Epoch 189/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
            "Epoch 190/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
            "Epoch 191/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
            "Epoch 192/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Epoch 193/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6629 - val_accuracy: 0.6263\n",
            "Epoch 194/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
            "Epoch 195/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6622 - val_accuracy: 0.6263\n",
            "Epoch 196/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
            "Epoch 197/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 198/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
            "Epoch 199/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6616 - val_accuracy: 0.6263\n",
            "Epoch 200/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
            "Fold 3, 200 epochs, 371 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 62.63%\n",
            "COMPILE...\n",
            "...COMPILED\n",
            "FIT\n",
            "Epoch 1/200\n",
            "453/453 [==============================] - ETA: 0s - loss: 0.6553 - accuracy: 0.6409INFO:tensorflow:Assets written to: /content/drive/My Drive/data/MLP214.cv.4.best/assets\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.6553 - accuracy: 0.6409 - val_loss: 0.6542 - val_accuracy: 0.6394\n",
            "Epoch 2/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 3/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6544 - val_accuracy: 0.6394\n",
            "Epoch 4/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 5/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 6/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 7/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 8/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 9/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 10/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6551 - val_accuracy: 0.6394\n",
            "Epoch 11/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 12/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 13/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6544 - val_accuracy: 0.6394\n",
            "Epoch 14/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 15/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 16/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6542 - val_accuracy: 0.6394\n",
            "Epoch 17/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 18/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 19/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6409 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 20/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 21/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 22/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 23/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6528 - accuracy: 0.6412 - val_loss: 0.6547 - val_accuracy: 0.6394\n",
            "Epoch 24/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 25/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 26/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6544 - val_accuracy: 0.6394\n",
            "Epoch 27/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6412 - val_loss: 0.6562 - val_accuracy: 0.6394\n",
            "Epoch 28/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 29/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 30/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 31/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6557 - val_accuracy: 0.6394\n",
            "Epoch 32/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 33/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 34/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 35/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 36/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 37/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 38/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 39/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 40/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 41/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6544 - val_accuracy: 0.6394\n",
            "Epoch 42/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
            "Epoch 43/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 44/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6543 - val_accuracy: 0.6394\n",
            "Epoch 45/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 46/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 47/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6545 - val_accuracy: 0.6394\n",
            "Epoch 48/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 49/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
            "Epoch 50/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 51/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 52/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 53/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 54/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 55/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 56/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 57/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 58/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 59/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
            "Epoch 60/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6545 - val_accuracy: 0.6394\n",
            "Epoch 61/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 62/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 63/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 64/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 65/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 66/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 67/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6558 - val_accuracy: 0.6394\n",
            "Epoch 68/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
            "Epoch 69/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 70/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6405 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 71/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 72/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 73/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
            "Epoch 74/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6528 - accuracy: 0.6412 - val_loss: 0.6546 - val_accuracy: 0.6394\n",
            "Epoch 75/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 76/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 77/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 78/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
            "Epoch 79/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 80/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 81/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 82/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 83/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 84/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 85/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6408 - val_loss: 0.6573 - val_accuracy: 0.6394\n",
            "Epoch 86/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 87/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 88/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 89/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 90/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 91/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 92/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 93/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 94/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6410 - val_loss: 0.6546 - val_accuracy: 0.6394\n",
            "Epoch 95/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 96/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 97/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 98/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 99/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 100/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 101/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6546 - val_accuracy: 0.6394\n",
            "Epoch 102/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6546 - val_accuracy: 0.6394\n",
            "Epoch 103/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 104/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 105/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 106/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6410 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 107/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6411 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
            "Epoch 108/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
            "Epoch 109/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 110/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 111/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 112/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6542 - val_accuracy: 0.6394\n",
            "Epoch 113/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 114/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6528 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 115/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 116/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6410 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 117/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6543 - val_accuracy: 0.6394\n",
            "Epoch 118/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 119/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 120/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6544 - val_accuracy: 0.6394\n",
            "Epoch 121/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
            "Epoch 122/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6410 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 123/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 124/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 125/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
            "Epoch 126/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 127/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 128/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 129/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 130/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6676 - val_accuracy: 0.6394\n",
            "Epoch 131/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 132/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 133/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 134/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 135/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 136/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 137/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 138/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
            "Epoch 139/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 140/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6545 - val_accuracy: 0.6394\n",
            "Epoch 141/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 142/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 143/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6410 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 144/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 145/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 146/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6409 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
            "Epoch 147/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 148/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 149/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 150/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 151/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 152/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6543 - val_accuracy: 0.6394\n",
            "Epoch 153/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 154/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6556 - val_accuracy: 0.6394\n",
            "Epoch 155/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6545 - val_accuracy: 0.6394\n",
            "Epoch 156/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 157/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 158/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 159/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 160/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6546 - val_accuracy: 0.6394\n",
            "Epoch 161/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 162/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6410 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 163/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 164/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 165/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
            "Epoch 166/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 167/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6527 - accuracy: 0.6412 - val_loss: 0.6546 - val_accuracy: 0.6394\n",
            "Epoch 168/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 169/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 170/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6528 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 171/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 172/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 173/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 174/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6409 - val_loss: 0.6543 - val_accuracy: 0.6394\n",
            "Epoch 175/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 176/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6542 - val_accuracy: 0.6394\n",
            "Epoch 177/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 178/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6545 - accuracy: 0.6403 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 179/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 180/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
            "Epoch 181/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 182/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 183/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6542 - val_accuracy: 0.6394\n",
            "Epoch 184/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6528 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 185/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6542 - val_accuracy: 0.6394\n",
            "Epoch 186/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 187/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 188/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 189/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6409 - val_loss: 0.6542 - val_accuracy: 0.6394\n",
            "Epoch 190/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 191/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 192/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 193/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6551 - val_accuracy: 0.6394\n",
            "Epoch 194/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
            "Epoch 195/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
            "Epoch 196/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 197/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6543 - val_accuracy: 0.6394\n",
            "Epoch 198/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Epoch 199/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
            "Epoch 200/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
            "Fold 4, 200 epochs, 369 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEzCAYAAAACSWsXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5wU9Znv8c9T3T0zMIPc1FGBRDzxznARlKiLgnhPFDUa4qqLJurLTdQknlyIusZNWDdqNLfDqiTrBVcXjQknnkg20RUW8I4u3gCRJV4GERAQGWAu3f2cP7pm6BlmoAcafj3j9+2rna7q6urnV7/q+nZVN1Xm7oiIiEg4UegCREREPu0UxiIiIoEpjEVERAJTGIuIiASmMBYREQlMYSwiIhLYDsPYzO41s9Vm9kYHj5uZ/dLMlpnZa2Z2VPHLFBER6b4K2TO+Hzh9O4+fARwc364E7tr1skRERD49dhjG7j4XWLedSSYA0z3neaCPme1frAJFRES6u2J8ZzwAeD9vuDYeJyIiIgVI7skXM7MryR3KpkePHiMHDRpUtHlns1miqHv8Hk1tKU1qS2lSW0qT2rKtpUuXfuTu+7T3WDHCeAWQn6oD43HbcPdpwDSAUaNG+YIFC4rw8jlz5sxh7NixRZtfSGpLaVJbSpPaUprUlm2Z2bsdPVaMjy2PA38X/6r688AGd19ZhPmKiIh8Kuxwz9jM/h0YC+xtZrXAD4EUgLvfDcwCzgSWAZuBy3ZXsSIiIt3RDsPY3S/cweMOfKNoFYmIiHzK7NEfcImISPE1NTVRW1tLfX196FJa9O7dm8WLF4cuoyg625aKigoGDhxIKpUq+DkKYxGRLq62tpZevXpx4IEHYmahywFg48aN9OrVK3QZRdGZtrg7a9eupba2lsGDBxf8Gt3jd+ciIp9i9fX19O/fv2SC+NPMzOjfv3+nj1IojEVEugEFcenYmb5QGIuIyC6rqqoKXUKXpjAWEREJTGEsIiJF4+5897vfZfTo0dTU1PDII48AsHLlSk444QSGDx/OkCFDmDdvHplMhksvvZQhQ4ZQU1PDz372s8DVh6NfU4uISNH8/ve/Z+HChTz77LM0NDRw9NFHc8IJJ/Dwww9z2mmnccMNN5DJZNi8eTMLFy5kxYoVvPHGGwB8/PHHgasPR2EsItKN/OP/e5NFH3xS1HkeccBe/PCsIwuadv78+Vx44YUkEgmqq6s58cQTeemllzj66KP56le/SlNTE+eccw7Dhw/noIMOYvny5VxzzTV84Qtf4NRTTy1q3V2JDlOLiMhud8IJJzB37lwGDBjApZdeyvTp0+nbty+vvvoqY8eO5e677+byyy8PXWYw2jMWEelGCt2D3V3GjBnDPffcw3nnnceaNWuYO3cut99+O++++y4DBw7kiiuuoKGhgVdeeYUzzzyTsrIyvvSlL3HooYdy8cUXB609JIWxiIgUzbnnnstzzz3HcccdRyKR4LbbbmO//fbjgQce4PbbbyeVSlFVVcX06dNZsWIFl112GdlsFoB//ud/Dlx9OApjERHZZXV1dUDuhBe33347N910U6tTSE6aNIlJkyZt87xXXnllj9VYyvSdsYiISGAKYxERkcAUxiIiIoEpjEVERAJTGIuIiASmMBYREQlMYSwiIhKYwlhERLqMdDoduoTdQmEsIiJFcc455zBy5EiOPPJI7rvvPgD+4z/+g6OOOophw4Yxfvx4IHeCkMsuu4yamhqGDh3K7373OwCqqqpa5vXYY49x6aWXAnDppZdy1VVXMXr0aL73ve/x4osvcuyxxzJixAiOO+443nrrLQAymQzf+c53GDJkCEOHDuVXv/oVTz/9NOecc07LfJ988knOPffcPbE4OkVn4BIRkaK499576devH1u2bGHkyJFMnDiRK664grlz5zJ48GDWrVsHwI9//GN69+7N66+/DsD69et3OO/a2lqeffZZEokEn3zyCfPmzSOZTPLUU09x/fXX87vf/Y5p06bxzjvvsHDhQpLJJOvWraNv3758/etfZ82aNeyzzz7cd999fPWrX92ty2FnKIxFRLqTP02GD18v7jz3q4EzfrLDyX75y18yc+ZMAFasWMG0adM44YQTGDx4MAD9+vUD4KmnnmLGjBktz+vbt+8O533BBReQSCQA2LBhA5MmTeLtt9/GzGhqamqZ71VXXUUymWz1epdccgn/9m//xmWXXcZzzz3H9OnTC235HqMwFhGRXTZnzhyeeuopnnvuOXr27MmYMWMYPnw4S5YsKXgeZtZyv76+vtVjlZWVLff/4R/+gXHjxjFz5kzeeecdxo4du935XnbZZZx11llUVFRwwQUXtIR1KSm9ikREZOcVsAe7O2zYsIG+ffvSs2dPlixZwksvvUR9fT1z587lr3/9a8th6n79+nHKKacwdepUfv7znwO5w9R9+/alurqaxYsXc+ihhzJz5sxWF5po+1oDBgwA4P77728Zf8opp3DPPfcwbty4lsPU/fr144ADDuCAAw5gypQpPPXUU7t9WewM/YBLRER22emnn046nebwww9n8uTJHH300eyzzz5MmzaN8847j2HDhjFx4kQAbrzxRtavX8+QIUMYNmwYs2fPBuAnP/kJX/ziFznuuOPYf//9O3yt733ve/zgBz9gxIgRrX5dffnll/OZz3yGoUOHMmzYMB5++OGWxy666CIGDRrE4YcfvpuWwK7RnrGIiOyy8vJy/vSnP7UMb9y4sWXP9owzzmg1bVVVFQ888MA28zj//PM5//zztxmfv/cLcOyxx7J06dKW4SlTpgCQTCa58847ufPOO7eZx/z587niiisKb9AepjAWEZFubeTIkVRWVnLHHXeELqVDCmMREenWXn755dAl7JC+MxYREQlMYSwiIhKYwlhERCQwhbGIiEhgCmMREZHAFMYiIrLH5V+hqa133nmHIUOG7MFqwlMYi4iIBKYwFhGRXTZ58mSmTp3aMnzLLbcwZcoUxo8fz1FHHUVNTQ1/+MMfOj3f+vr6lmsfjxgxouXUmW+++SbHHHMMw4cPZ+jQobz99tts2rSJL3zhCwwbNowhQ4bwyCOPFK19u5tO+iEi0o3c+uKtLFlX+JWSCnFYv8P4/jHf3+40EydO5Fvf+hbf+MY3AJg5cyZPPvkk1157LXvttRcfffQRn//85zn77LNbXZ1pR6ZOnYqZ8frrr7NkyRJOPfVUli5dyt133803v/lNLrroIhobG8lkMsyaNYsDDjiAJ554AshdUKKr0J6xiIjsshEjRrB69Wo++OADXn31Vfr06cN+++3H9ddfz9ChQzn55JNZsWIFq1at6tR858+fz8UXXwzAYYcdxmc/+1mWLl3Kscceyy233MKtt97Ku+++S48ePaipqeHJJ5/k+9//PvPmzaN37967o6m7hfaMRUS6kR3twe5OF1xwAY899hgffvgh5513Hg899BBr1qzh5ZdfJpVKceCBB25zneKd9bd/+7eMHj2aJ554gjPPPJN77rmHk046iVdeeYVZs2Zx4403Mn78eG666aaivN7upjAWEZGimDhxIldccQUfffQRTzzxBLNmzWLfffcllUoxe/Zs3n333U7Pc8yYMTz00EOcdNJJLF26lPfee49DDz2U5cuXc9BBB3Httdfy3nvv8dprr3HYYYfRr18/Lr74Yvr06cNvfvOb3dDK3UNhLCIiRXHkkUeyceNGBgwYwH777cdFF13EWWedRU1NDaNGjeKwww7r9Dy//vWv8/d///fU1NSQTCa5//77KS8v59FHH+XBBx8klUq1HA5/6aWX+O53v0sURaRSKe66667d0MrdQ2EsIiJF8/rrrwO56xnvvffePPfcc+1OV1dX1+E8DjzwQN544w0AKioquO+++7aZZvLkyUyePLnVuNNOO43TTjttZ0sPSj/gEhERCUx7xiIiEsTrr7/OJZdc0mpceXk5L7zwQqCKwikojM3sdOAXQAL4jbv/pM3jnwEeAPrE00x291lFrlVERLqRmpoaFi5cGLqMkrDDw9RmlgCmAmcARwAXmtkRbSa7EXjU3UcAXwH+pdiFioiIdFeFfGd8DLDM3Ze7eyMwA5jQZhoH9orv9wY+KF6JIiIi3Zu5+/YnMDsfON3dL4+HLwFGu/vVedPsD/wF6AtUAie7+8vtzOtK4EqA6urqkTNmzChWO6irq9vuVUC6ErWlNKktpUltgd69e/O5z31uN1S08zKZDIlEInQZRbEzbVm2bNk2p+McN27cy+4+qr3pi/UDrguB+939DjM7FnjQzIa4ezZ/InefBkwDGDVqlI8dO7ZILw9z5syhmPMLSW0pTWpLaVJbYPHixfTq1av4Be2CjRs3llxNO2tn2lJRUcGIESMKnr6Qw9QrgEF5wwPjcfm+BjwK4O7PARXA3gVXISIinyrd5WhGsRQSxi8BB5vZYDMrI/cDrcfbTPMeMB7AzA4nF8ZrilmoiIhIsaXT6dAlAAUcpnb3tJldDfyZ3D9butfd3zSzHwEL3P1x4H8Dvzazb5P7MdelvqMvo0VEpOg+vOUWGhYX9xKK5Ycfxn7XX7/daSZPnsygQYNaLqF4yy23UFlZyezZs1m/fj1NTU1MmTKFCRPa/v53W3V1dUyYMKHd502fPp2f/vSnmBlDhw7lwQcfZNWqVVx11VUsX74cgLvuuosDDjiAL37xiy1n8vrpT39KXV0dN998M2PHjmX48OHMnz+fCy+8kEMOOYQpU6bQ2NhI//79eeihh6iurqauro5rrrmGF198kUQiwQ9/+EM2bNjAa6+9xs9//nMAfv3rX7No0SJ+9rOf7fTyhQK/M47/zfCsNuNuyru/CDh+lyoREZEuq5jXM66oqGDmzJnbPG/RokVMmTKFZ599lr333pt169YBcO2113LiiScyc+ZMMpkMdXV1rF+/fruv0djYyIIFCwBYv349zz//PGbGb37zG2677TbuuOMOfvzjH9O7d2+ef/55evXqxfr160mlUvzTP/0Tt99+O6lUivvuu4977rlnl5efzsAlItKN7GgPdnfJv57xmjVrWq5n/O1vf5u5c+cSRVHL9Yz322+/7c7L3bn++uu3ed7TTz/NBRdcwN57536S1K9fPwCefvpppk+fDkAikaB37947DOOJEye23K+trWXixImsXLmSxsZGBg8eDMBTTz1F/r/66du3LwAnnXQSf/zjHzn88MNpamqipqamk0trWwpjEREpimJdz7gY10FOJpNks1v/QU/b51dWVrbcv+aaa7juuus4++yzmTNnDjfffPN253355Zdzyy23cNhhh3HZZZd1qq6O6EIRIiJSFBMnTmTGjBk89thjnHvuuWzYsGGnrmfc0fNOOukkfvvb37J27VqAlsPU48ePb7lcYiaTYcOGDVRXV7N69WrWrl1LQ0MDf/zjH7f7egMGDADggQceaBl/yimnMHXq1Jbh5r3t0aNH8/777/Pwww9z4YUXFrp4tkthLCIiRdHe9YwXLFhATU0N06dPL/h6xh0978gjj+SGG27gxBNPZNiwYVx33XUA/OIXv2D27NnU1NQwcuRIFi1aRCqV4qabbuKYY47hlFNO2e5r33zzzVxwwQWMHDmy5RA4wI033sj69esZPXo0w4YNY/bs2S2PffnLX+b4449vOXS9q3SYWkREiqYY1zPe3vMmTZrEpEmTWo2rrq7mD3/4wzbTXnvttVx77bXbjJ8zZ06r4QkTJrT7K++qqioeeOCBdk/6MX/+fL797W932IbO0p6xiIhIgT7++GMOOeQQevTowfjx44s2X+0Zi4hIEF3xesZ9+vRh6dKlRZ+vwlhERILQ9Yy30mFqEZFuQCc9LB070xcKYxGRLq6iooK1a9cqkEuAu7N27VoqKio69TwdphYR6eIGDhxIbW0ta9aUzvV56uvrOx1IpaqzbamoqGDgwIGdeg2FsYhIF5dKpVpO4Vgq5syZ06nr+ZayPdEWHaYWEREJTGEsIiISmMJYREQkMIWxiIhIYApjERGRwBTGIiIigSmMRUREAlMYi4iIBKYwFhERCUxhLCIiEpjCWEREJDCFsYiISGAKYxERkcAUxiIiIoEpjEVERAJTGIuIiASmMBYREQlMYSwi3VZTtokXV76Iu4cuRWS7FMYi0m3d8+o9fO0vX+Px/3k8dCki26UwFpFuac3mNUxfNB2AX/33r6hP1weuSKRj3SKMNzem2dTUvQ5DfbjpQ15e9TINmYZ2H896lg/qPiCdTe/hyrqW1ZtX89GWj0KX0WL5x8v5l4X/wrzaeTp0upvd9epdNGWa+Mfj/pFVm1fx0OKHQpe0U9LZNB9u+pCmTFPoUmQ3slAbhFGjRvmCBQuKMq+f/Nfvmb7oPnqVVdK3Zw8ybCHt9ZRHlZQnKjEgSwYni3sGiIgswmi+GQ5sSH/Ix021lNteVEUDqUhUUpYwmtjMlswnRJakR1RFIkrhnsVxwHHPkvEsjmEeQTxfiDBzIgPMwZ10NkuTN9CQ/ZgMTfSI+lHGXkRRBGRo8I2s2/IB9bYegATlDKgYQiqqyL0WWZqy9axufJuG7CaSVsG+ZZ+jLOqJYYBhRvz6AEZkEe655+bmEf/nDmRbhomncZzIkiStDPcsDdlNOFnKoh4kLEXWs2S8kfrsRjKepjyqJGnlQG5dalkuOFu2bKGiRwXu2biera9PSw3Nw7kPGdl4OUUGiWjrs8qjSiqi3iQs2SrImp/bPNQ8bl3T+3ySXglAr8S+7JWshrivM1lwh8is5YY5GW8g7fWkPfchqDLZl4qoF2BsrNtEZWUl7o7FyxmI/+aWflu5dS7XvvrsRj6oX9zyWP/UZ+lXNrB1G5qbYhYPGPlzNwwMtjTVsaFpDY6zV6o/lanKeNqtVeTWayftjbhnc/1Jlrr0ej5pWEsymQCgLOpBWdSTsqhHbj2L14MsWbLZLA2ZNE3pDMlERHkygVm8nrV6PYuXiREREVl83yKy7mSyGdLZLFnPkvYGmnwLAKmojKSVk4rKyXqa+uxG0t6IeRKIwJMYCZJRimSUIBH3VX6P19VtoqqqEs+twjiQMPifzc8zpNfpjN7rcv6y9hZWNrzJoIrhxIuwVd/l1sW8LvB4rfStXdK8zrU7Lh6fiIyyZEQyyt/Psbgr89YQd7JkyHp8i+9v2rKJ8ooUAAlLkfEmPmr8K2lvBIzKRF96JfehKtEfs4i277msZ9mS+YT67CeUWU8qEn3IZCIa0llSiYgeqQSJKF6bzPJ6z9mS3cjGpnUYCcqtPwnKicyJIscst63IksU9769n4vsZzBL0SPQiYWU0ZbewcfMnVPXshWeT1NU79U1QVW70LAePt8dGRGRJEpbAiGjyBtLZBhKWIhmVkc42kvbccPM2MLf9ycTrVJqMZ4gsXkcsSTJKApDxJjZl1rOhaSWRJeid2p+KqBcJSxBZkogEmXidc3cqElWkrKK5gwAoT/Tg4XPvZM6cOYwdO5ZdZWYvu/uo9h5L7vLcS8Dg/lX0r0iyxTfx3idr8UwP8DIs+hgSKyEOyJa/OFgW4nDK3Qdv6ku28Wjqk5vYVL6arDXm3pTZCjzTE0hjyQ/i58UbIret9+M3BJaJ5+l505D3nCTZdC/IprDUBqLkqvjNH+HpSjz9GZJNJ1CV3Ju66E3+2rC89bw8Sab+SLIN+xOVreb9HrVga7e+PuTCP76f2yTnbTjza/aoTW3xX8uC5TYAnukBGBZtBEvnHs+m8ExP3HtgiXosqmvVTs9/rc1btr6G59WB0XJwJh7v5D7KJBMJ0hkn67RMb9EWLPleS39tfV5bFvdnH9KbRwBGU4/3WJ/cCHFlkeW2jVvDMq47WwbZCjy7V27a5EYskbdn3f6BivaZx8u3edkmSG88laYNo0hUvs3qPi+wJvFW675ppfUH5fwI8mw5iWxfwPjYPoZoVdznbZ+b66tcn+b2rDy9F54ZFNfmEDViiYZcH0YN8TKN6/bch9VEFJHJbv0Q1fIa1ma4zfrXat1r7vtsGZ4tz00X1WH2MUSN4BGeqcrVaxmwJqJoC2YZnHT8YTJv2eZbt+0y8/RBPLf0KJ7NLMVSY6k44CPe3rK8neW8Y2btD1ur/+fW12w7Hw5bnpdfnyfIbZcSubY3b6M2xe2M31PZ+qPJNu6DJTfSlPqYDamPseTbHb6vPdMTz/TDonosWUtkWRKR5T4UbW/nK9ODbLovWJpE6kOImnC3retwy3bUttYaD0MCyGCJlRA1QbYczyZZvXkdWIayVJZk0llTD+nNW9uc285kgEyuT7NleDaFRenc+uqp3LDV5daRvDrcm3emEkATTl283mRofr95uops00ggw6aydfF7Obt1G+1RvG03LFGL2dajD46RoII9pVuE8cQhp1L9URljx47FPX5DuJP13Cfb/L/ZOEfz30zNe0YWjy9LRJgZ7s6mxgxN6TisoWXD7fEnZxyiyEhGRjIRkYyMVCLK7Q0DmazTlHEaM7l59CxLYEBdQ5rGTJbePVKUJxO456Zpyjjz5s3jjJPHtbSvvimz08tmVw58ODv35PzXnDdvHmPGjOnU83uWJVqWf11DmqznwrO5j6K489puIAuVjCISUesnu+f6qSmTjW9OJusk4r5NJIxn589n7IknEJm1Wqcy7ngWMu6tIjW/vpZ9EOvo8fzxts34VuteFirKcnupEH9N05BpExCt59PWM888w/HHH9/h480SZvSqSBJFuf7YsKUJd+L3SvNRGFr6Kxsvh0zW4z1iJxkZ5ckEZcmIsmSEse00WYdsNrcsIzN6liUoT0at2pDOZKlPZ2lKZ+MjGbk6npk/nzFjxhAZ8XvP2NKUoTGdpSK1dTll/ZKWfvO8bUQ2CxbRstfdvI41r3NR1LkVbXNjmoamrR8ctvZL6w5qr//nx20x2psm/pAZh36rbVF8tKZ5mUTxXm9kRkVq63Lc3JimMZ1tWeb528tMNrdsKssT9O1ZRhQZmaxT35Rhc2OGrHurWslrV3vr3jPPPMNxxx1HeSpBVXkuavLf0y3abGbabnfabsMcKEtGlCdz29vmtjVvQxvS2YK3ezu7DdkdukUYU7+BnptqYdWbWKaJRDZNogjfpRpQtYvzSMa3Hm3G92m+s37ra5XHt+pNS+C9rc/Yc5/Nim/fTUuoXNW29YUxoFdxy8lJ9YCqaoiSULcKGjZiQFl868i+m5ZQsXLn2tKirBLKe0FTPTRsBN/5D1rNesa3zhiwZQn91nauLUbeeruLIiDVyeckaf/9WL1pCVVt1rH2pkt08vV21s70R7P22lJMBdW2iZYjDQmgMr511oAtS+i/rgckyqD3IOjRB/tkBb02rmKbBC6C/G1o8WaagEFHF3OOHeoWYfzh5KupfvkZ3g1dSJH0B7WlBKktpUltKU3doS3le0fsN3PZHnmtbhHG7DWATZWDqKyMf8Ri8feQpXQMohPq6uqoqtrVffLSUJJtyWYg05g7/pUsy336LWBd2eW2uOf2hLOZ3DoaJYOtoyXZLztJbSlNLW3JZiDTAJk0JMtze8pdZdt80KA99lLdIoz3+9FPWFKkX7uVgjlz5nCk2lJy1JbSpLaUpu7Ulj2hW/w7YxERka5MYSwiIhKYwlhERCQwhbGIiEhgCmMREZHACgpjMzvdzN4ys2VmNrmDab5sZovM7E0ze7i4ZYqIiHRfO/ynTWaWAKYCpwC1wEtm9ri7L8qb5mDgB8Dx7r7ezPbdXQWLiIh0N4XsGR8DLHP35e7eCMwAJrSZ5gpgqruvB3D31cUtU0REpPsqJIwHAO/nDdfG4/IdAhxiZs+Y2fNmdnqxChQREenudng9YzM7Hzjd3S+Phy8BRrv71XnT/BFoAr4MDATmAjXu/nGbeV0JXAlQXV09csaMGUVrSLc8jVw3oLaUJrWlNKktpalYbRk3btwuXc94BZB/gs6B8bh8tcAL7t4E/NXMlgIHAy/lT+Tu04BpAKNGjfJinr6yWBd/LgVqS2lSW0qT2lKa1JbOKeQw9UvAwWY22MzKgK8Aj7eZ5v8CYwHMbG9yh6137ireIiIinzI7DGN3TwNXA38GFgOPuvubZvYjMzs7nuzPwFozWwTMBr7r7mt3V9EiIiLdSUFXbXL3WcCsNuNuyrvvwHXxTURERDpBZ+ASEREJTGEsIiISmMJYREQkMIWxiIhIYApjERGRwBTGIiIigSmMRUREAlMYi4iIBKYwFhERCUxhLCIiEpjCWEREJDCFsYiISGAKYxERkcAUxiIiIoEpjEVERAJTGIuIiASmMBYREQlMYSwiIhKYwlhERCQwhbGIiEhgCmMREZHAFMYiIiKBKYxFREQCUxiLiIgEpjAWEREJTGEsIiISmMJYREQkMIWxiIhIYApjERGRwBTGIiIigSmMRUREAlMYi4iIBKYwFhERCUxhLCIiEpjCWEREJDCFsYiISGAKYxERkcAUxiIiIoEpjEVERAJTGIuIiASmMBYREQlMYSwiIhKYwlhERCQwhbGIiEhgCmMREZHACgpjMzvdzN4ys2VmNnk7033JzNzMRhWvRBERke5th2FsZglgKnAGcARwoZkd0c50vYBvAi8Uu0gREZHurJA942OAZe6+3N0bgRnAhHam+zFwK1BfxPpERES6vULCeADwft5wbTyuhZkdBQxy9yeKWJuIiMingrn79icwOx843d0vj4cvAUa7+9XxcAQ8DVzq7u+Y2RzgO+6+oJ15XQlcCVBdXT1yxowZRWtIXV0dVVVVRZtfSGpLaVJbSpPaUprUlm2NGzfuZXdv/zdV7r7dG3As8Oe84R8AP8gb7g18BLwT3+qBD4BR25vvyJEjvZhmz55d1PmFpLaUJrWlNKktpUlt2RawwDvIxEIOU78EHGxmg82sDPgK8HhemG9w973d/UB3PxB4Hjjb29kzFhERkW3tMIzdPQ1cDfwZWAw86u5vmtmPzOzs3V2giIhId5csZCJ3nwXMajPupg6mHbvrZYmIiHx66AxcIiIigSmMRUREAlMYi4iIBKYwFhERCUxhLCIiEpjCWEREJDCFsYiISGAKYxERkcAUxiIiIoEpjEVERAJTGIuIiASmMBYREQlMYSwiIhKYwlhERCQwhbGIiAzTdrsAAAkoSURBVEhgCmMREZHAFMYiIiKBKYxFREQCUxiLiIgEpjAWEREJTGEsIiISmMJYREQkMIWxiIhIYApjERGRwBTGIiIigSmMRUREAlMYi4iIBKYwFhERCUxhLCIiEpjCWEREJDCFsYiISGAKYxERkcAUxiIiIoEpjEVERAJTGIuIiASmMBYREQlMYSwiIhKYwlhERCQwhbGIiEhgCmMREZHAFMYiIiKBKYxFREQCUxiLiIgEpjAWEREJTGEsIiISmMJYREQksILC2MxON7O3zGyZmU1u5/HrzGyRmb1mZv9pZp8tfqkiIiLd0w7D2MwSwFTgDOAI4EIzO6LNZP8NjHL3ocBjwG3FLlRERKS7KmTP+Bhgmbsvd/dGYAYwIX8Cd5/t7pvjweeBgcUtU0REpPsyd9/+BGbnA6e7++Xx8CXAaHe/uoPp/w/wobtPaeexK4ErAaqrq0fOmDFjF8vfqq6ujqqqqqLNLyS1pTSpLaVJbSlNasu2xo0b97K7j2rvseQuzz2PmV0MjAJObO9xd58GTAMYNWqUjx07tmivPWfOHIo5v5DUltKktpQmtaU0qS2dU0gYrwAG5Q0PjMe1YmYnAzcAJ7p7Q3HKExER6f4K+c74JeBgMxtsZmXAV4DH8ycwsxHAPcDZ7r66+GWKiIh0XzsMY3dPA1cDfwYWA4+6+5tm9iMzOzue7HagCvitmS00s8c7mJ2IiIi0UdB3xu4+C5jVZtxNefdPLnJdIiIinxo6A5eIiEhgCmMREZHAFMYiIiKBKYxFREQCUxiLiIgEpjAWEREJTGEsIiISmMJYREQkMIWxiIhIYApjERGRwBTGIiIigSmMRUREAlMYi4iIBKYwFhERCUxhLCIiEpjCWEREJDCFsYiISGAKYxERkcAUxiIiIoEpjEVERAJTGIuIiASmMBYREQlMYSwiIhKYwlhERCQwhbGIiEhgCmMREZHAFMYiIiKBKYxFREQCUxiLiIgEpjAWEREJTGEsIiISmMJYREQkMIWxiIhIYApjERGRwBTGIiIigSmMRUREAlMYi4iIBKYwFhERCUxhLCIiEpjCWEREJDCFsYiISGAKYxERkcAUxiIiIoEpjEVERAJTGIuIiARWUBib2elm9paZLTOzye08Xm5mj8SPv2BmBxa7UBERke5qh2FsZglgKnAGcARwoZkd0WayrwHr3f1zwM+AW4tdqIiISHdVyJ7xMcAyd1/u7o3ADGBCm2kmAA/E9x8DxpuZFa9MERGR7quQMB4AvJ83XBuPa3cad08DG4D+xShQRESku0vuyRczsyuBK+PBOjN7q4iz3xv4qIjzC0ltKU1qS2lSW0qT2rKtz3b0QCFhvAIYlDc8MB7X3jS1ZpYEegNr287I3acB0wp4zU4zswXuPmp3zHtPU1tKk9pSmtSW0qS2dE4hh6lfAg42s8FmVgZ8BXi8zTSPA5Pi++cDT7u7F69MERGR7muHe8bunjazq4E/AwngXnd/08x+BCxw98eBfwUeNLNlwDpygS0iIiIFKOg7Y3efBcxqM+6mvPv1wAXFLa3Tdsvh70DUltKktpQmtaU0qS2dYDqaLCIiEpZOhykiIhJYtwjjHZ2us5SZ2SAzm21mi8zsTTP7Zjz+ZjNbYWYL49uZoWsthJm9Y2avxzUviMf1M7Mnzezt+G/f0HXuiJkdmrfsF5rZJ2b2ra7SL2Z2r5mtNrM38sa12w+W88v4/fOamR0VrvJtddCW281sSVzvTDPrE48/0My25PXP3eEq31YHbelwnTKzH8T98paZnRam6vZ10JZH8trxjpktjMeXer90tB3ec+8Zd+/SN3I/Kvsf4CCgDHgVOCJ0XZ2of3/gqPh+L2ApudOO3gx8J3R9O9Ged4C924y7DZgc358M3Bq6zk62KQF8SO7fCHaJfgFOAI4C3thRPwBnAn8CDPg88ELo+gtoy6lAMr5/a15bDsyfrtRuHbSl3XUq3g68CpQDg+PtXCJ0G7bXljaP3wHc1EX6paPt8B57z3SHPeNCTtdZstx9pbu/Et/fCCxm2zOcdXX5p0t9ADgnYC07YzzwP+7+buhCCuXuc8n9y4Z8HfXDBGC65zwP9DGz/fdMpTvWXlvc/S+eO9sfwPPkzn9Q8jrol45MAGa4e4O7/xVYRm57VxK215b4dMhfBv59jxa1k7azHd5j75nuEMaFnK6zS7Dc1a5GAC/Eo66OD4Hc2xUO7cYc+IuZvWy5M64BVLv7yvj+h0B1mNJ22ldovVHpiv0CHfdDV38PfZXcXkqzwWb232b2X2Y2JlRRndTeOtWV+2UMsMrd384b1yX6pc12eI+9Z7pDGHcLZlYF/A74lrt/AtwF/C9gOLCS3CGfruBv3P0oclf5+oaZnZD/oOeO8XSZn/Bb7kQ3ZwO/jUd11X5ppav1Q0fM7AYgDTwUj1oJfMbdRwDXAQ+b2V6h6itQt1in2riQ1h9gu0S/tLMdbrG73zPdIYwLOV1nSTOzFLkV4CF3/z2Au69y94y7Z4FfU0KHp7bH3VfEf1cDM8nVvar5EE78d3W4CjvtDOAVd18FXbdfYh31Q5d8D5nZpcAXgYviDSXxId218f2XyX3PekiwIguwnXWqq/ZLEjgPeKR5XFfol/a2w+zB90x3CONCTtdZsuLvVv4VWOzud+aNz//+4VzgjbbPLTVmVmlmvZrvk/uRzRu0Pl3qJOAPYSrcKa0+4XfFfsnTUT88Dvxd/AvRzwMb8g7NlSQzOx34HnC2u2/OG7+P5a7BjpkdBBwMLA9TZWG2s049DnzFzMrNbDC5try4p+vbCScDS9y9tnlEqfdLR9th9uR7JvSv2IpxI/fLtqXkPm3dELqeTtb+N+QOfbwGLIxvZwIPAq/H4x8H9g9dawFtOYjcrz9fBd5s7gtyl9P8T+Bt4CmgX+haC2xPJbkLnvTOG9cl+oXcB4iVQBO577O+1lE/kPtF6NT4/fM6MCp0/QW0ZRm57+ya3zN3x9N+KV73FgKvAGeFrr+AtnS4TgE3xP3yFnBG6Pp31JZ4/P3AVW2mLfV+6Wg7vMfeMzoDl4iISGDd4TC1iIhIl6YwFhERCUxhLCIiEpjCWEREJDCFsYiISGAKYxERkcAUxiIiIoEpjEVERAL7/02Sp57NC/tmAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 63.94%\n",
            "COMPILE...\n",
            "...COMPILED\n",
            "FIT\n",
            "Epoch 1/200\n",
            "446/453 [============================>.] - ETA: 0s - loss: 0.6566 - accuracy: 0.6386INFO:tensorflow:Assets written to: /content/drive/My Drive/data/MLP214.cv.5.best/assets\n",
            "453/453 [==============================] - 3s 6ms/step - loss: 0.6562 - accuracy: 0.6392 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
            "Epoch 2/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6542 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 3/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6485 - val_accuracy: 0.6505\n",
            "Epoch 4/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6481 - val_accuracy: 0.6505\n",
            "Epoch 5/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
            "Epoch 6/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 7/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 8/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 9/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 10/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6542 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 11/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
            "Epoch 12/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 13/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 14/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 15/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6542 - accuracy: 0.6399 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 16/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6479 - val_accuracy: 0.6505\n",
            "Epoch 17/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
            "Epoch 18/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6479 - val_accuracy: 0.6505\n",
            "Epoch 19/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 20/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6477 - val_accuracy: 0.6505\n",
            "Epoch 21/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6483 - val_accuracy: 0.6505\n",
            "Epoch 22/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6478 - val_accuracy: 0.6505\n",
            "Epoch 23/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 24/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6544 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 25/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
            "Epoch 26/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6398 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
            "Epoch 27/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 28/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
            "Epoch 29/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 30/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6487 - val_accuracy: 0.6505\n",
            "Epoch 31/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 32/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6484 - val_accuracy: 0.6505\n",
            "Epoch 33/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
            "Epoch 34/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6542 - accuracy: 0.6400 - val_loss: 0.6483 - val_accuracy: 0.6505\n",
            "Epoch 35/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 36/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
            "Epoch 37/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 38/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6399 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 39/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
            "Epoch 40/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
            "Epoch 41/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 42/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 43/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
            "Epoch 44/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6545 - accuracy: 0.6392 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
            "Epoch 45/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 46/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 47/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
            "Epoch 48/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 49/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
            "Epoch 50/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6492 - val_accuracy: 0.6505\n",
            "Epoch 51/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
            "Epoch 52/200\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6482 - val_accuracy: 0.6505\n",
            "Epoch 53/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
            "Epoch 54/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 55/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
            "Epoch 56/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 57/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6543 - accuracy: 0.6399 - val_loss: 0.6485 - val_accuracy: 0.6505\n",
            "Epoch 58/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6478 - val_accuracy: 0.6505\n",
            "Epoch 59/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
            "Epoch 60/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
            "Epoch 61/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6400 - val_loss: 0.6486 - val_accuracy: 0.6505\n",
            "Epoch 62/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6546 - accuracy: 0.6398 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 63/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 64/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 65/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 66/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6478 - val_accuracy: 0.6505\n",
            "Epoch 67/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 68/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6477 - val_accuracy: 0.6505\n",
            "Epoch 69/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
            "Epoch 70/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 71/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6491 - val_accuracy: 0.6505\n",
            "Epoch 72/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 73/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
            "Epoch 74/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6496 - val_accuracy: 0.6505\n",
            "Epoch 75/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 76/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6478 - val_accuracy: 0.6505\n",
            "Epoch 77/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 78/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6482 - val_accuracy: 0.6505\n",
            "Epoch 79/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6549 - accuracy: 0.6395 - val_loss: 0.6479 - val_accuracy: 0.6505\n",
            "Epoch 80/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 81/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 82/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 83/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
            "Epoch 84/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 85/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
            "Epoch 86/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
            "Epoch 87/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 88/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 89/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6398 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 90/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
            "Epoch 91/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6506 - val_accuracy: 0.6505\n",
            "Epoch 92/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6546 - accuracy: 0.6394 - val_loss: 0.6477 - val_accuracy: 0.6505\n",
            "Epoch 93/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 94/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
            "Epoch 95/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 96/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6483 - val_accuracy: 0.6505\n",
            "Epoch 97/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6477 - val_accuracy: 0.6505\n",
            "Epoch 98/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6400 - val_loss: 0.6477 - val_accuracy: 0.6505\n",
            "Epoch 99/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6484 - val_accuracy: 0.6505\n",
            "Epoch 100/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6524 - val_accuracy: 0.6505\n",
            "Epoch 101/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6543 - accuracy: 0.6398 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
            "Epoch 102/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6488 - val_accuracy: 0.6505\n",
            "Epoch 103/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 104/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 105/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
            "Epoch 106/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 107/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6477 - val_accuracy: 0.6505\n",
            "Epoch 108/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
            "Epoch 109/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
            "Epoch 110/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
            "Epoch 111/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6493 - val_accuracy: 0.6505\n",
            "Epoch 112/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6478 - val_accuracy: 0.6505\n",
            "Epoch 113/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 114/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6482 - val_accuracy: 0.6505\n",
            "Epoch 115/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 116/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6398 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
            "Epoch 117/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6542 - accuracy: 0.6399 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
            "Epoch 118/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6479 - val_accuracy: 0.6505\n",
            "Epoch 119/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6479 - val_accuracy: 0.6505\n",
            "Epoch 120/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
            "Epoch 121/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6399 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 122/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 123/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6478 - val_accuracy: 0.6505\n",
            "Epoch 124/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6482 - val_accuracy: 0.6505\n",
            "Epoch 125/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
            "Epoch 126/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6539 - val_accuracy: 0.6505\n",
            "Epoch 127/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 128/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
            "Epoch 129/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 130/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 131/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6482 - val_accuracy: 0.6505\n",
            "Epoch 132/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 133/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 134/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 135/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
            "Epoch 136/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
            "Epoch 137/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6478 - val_accuracy: 0.6505\n",
            "Epoch 138/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 139/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6548 - accuracy: 0.6389 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 140/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6400 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
            "Epoch 141/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
            "Epoch 142/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 143/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 144/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 145/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 146/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6543 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 147/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 148/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 149/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 150/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 151/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 152/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 153/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 154/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
            "Epoch 155/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
            "Epoch 156/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6400 - val_loss: 0.6485 - val_accuracy: 0.6505\n",
            "Epoch 157/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 158/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6399 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 159/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 160/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 161/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 162/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6479 - val_accuracy: 0.6505\n",
            "Epoch 163/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
            "Epoch 164/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
            "Epoch 165/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6483 - val_accuracy: 0.6505\n",
            "Epoch 166/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6477 - val_accuracy: 0.6505\n",
            "Epoch 167/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6481 - val_accuracy: 0.6505\n",
            "Epoch 168/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6479 - val_accuracy: 0.6505\n",
            "Epoch 169/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6544 - accuracy: 0.6400 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
            "Epoch 170/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 171/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 172/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 173/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 174/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
            "Epoch 175/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
            "Epoch 176/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 177/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 178/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 179/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
            "Epoch 180/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 181/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 182/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6481 - val_accuracy: 0.6505\n",
            "Epoch 183/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 184/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6479 - val_accuracy: 0.6505\n",
            "Epoch 185/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
            "Epoch 186/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6543 - accuracy: 0.6400 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
            "Epoch 187/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 188/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6400 - val_loss: 0.6483 - val_accuracy: 0.6505\n",
            "Epoch 189/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
            "Epoch 190/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6396 - val_loss: 0.6478 - val_accuracy: 0.6505\n",
            "Epoch 191/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6478 - val_accuracy: 0.6505\n",
            "Epoch 192/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
            "Epoch 193/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
            "Epoch 194/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 195/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
            "Epoch 196/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6478 - val_accuracy: 0.6505\n",
            "Epoch 197/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 198/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
            "Epoch 199/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
            "Epoch 200/200\n",
            "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
            "Fold 5, 200 epochs, 366 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 65.05%\n",
            "\n",
            "5-way Cross Validation mean 64.28% (+/- 0.95%)\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN0K-1e2g4WN"
      },
      "source": [
        ""
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E37va5UZg4WQ"
      },
      "source": [
        ""
      ],
      "execution_count": 42,
      "outputs": []
    }
  ]
}