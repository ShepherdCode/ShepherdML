{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MLP_121.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojm_6E9f9Kcf"
      },
      "source": [
        "# MLP 121\n",
        "Train on long, evaluate on all"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh6XplUvC0j0",
        "outputId": "05385c10-a7b8-419a-b943-269e7e2b3f8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "PATH='/content/drive/'\n",
        "drive.mount(PATH)\n",
        "DATAPATH=PATH+'My Drive/data/'\n",
        "PC_FILENAME = DATAPATH+'pcRNA.fasta'\n",
        "NC_FILENAME = DATAPATH+'ncRNA.fasta'\n",
        "#PC_FILENAME = 'pcRNA.fasta'\n",
        "#NC_FILENAME = 'ncRNA.fasta'\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQY7aTj29Kch"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LayerNormalization\n",
        "import time\n",
        "\n",
        "dt='float32'\n",
        "tf.keras.backend.set_floatx(dt)\n",
        "\n",
        "EPOCHS=200\n",
        "SPLITS=1\n",
        "K=4\n",
        "VOCABULARY_SIZE=4**K+1   # e.g. K=3 => 64 DNA K-mers + 'NNN'\n",
        "EMBED_DIMEN=16\n",
        "FILENAME='MLP121'"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV6k-xOm9Kcn"
      },
      "source": [
        "## Load and partition sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I-O_qzw9Kco"
      },
      "source": [
        "# Assume file was preprocessed to contain one line per seq.\n",
        "# Prefer Pandas dataframe but df does not support append.\n",
        "# For conversion to tensor, must avoid python lists.\n",
        "def load_fasta(filename,label):\n",
        "    DEFLINE='>'\n",
        "    labels=[]\n",
        "    seqs=[]\n",
        "    lens=[]\n",
        "    nums=[]\n",
        "    num=0\n",
        "    with open (filename,'r') as infile:\n",
        "        for line in infile:\n",
        "            if line[0]!=DEFLINE:\n",
        "                seq=line.rstrip()\n",
        "                num += 1   # first seqnum is 1\n",
        "                seqlen=len(seq)\n",
        "                nums.append(num)\n",
        "                labels.append(label)\n",
        "                seqs.append(seq)\n",
        "                lens.append(seqlen)\n",
        "    df1=pd.DataFrame(nums,columns=['seqnum'])\n",
        "    df2=pd.DataFrame(labels,columns=['class'])\n",
        "    df3=pd.DataFrame(seqs,columns=['sequence'])\n",
        "    df4=pd.DataFrame(lens,columns=['seqlen'])\n",
        "    df=pd.concat((df1,df2,df3,df4),axis=1)\n",
        "    return df\n",
        "\n",
        "# Split into train/test stratified by sequence length.\n",
        "def sizebin(df):\n",
        "    return pd.cut(df[\"seqlen\"],\n",
        "                              bins=[0,1000,2000,4000,8000,16000,np.inf],\n",
        "                              labels=[0,1,2,3,4,5])\n",
        "def make_train_test(data):\n",
        "    bin_labels= sizebin(data)\n",
        "    from sklearn.model_selection import StratifiedShuffleSplit\n",
        "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=37863)\n",
        "    # split(x,y) expects that y is the labels. \n",
        "    # Trick: Instead of y, give it it the bin labels that we generated.\n",
        "    for train_index,test_index in splitter.split(data,bin_labels):\n",
        "        train_set = data.iloc[train_index]\n",
        "        test_set = data.iloc[test_index]\n",
        "    return (train_set,test_set)\n",
        "\n",
        "def separate_X_and_y(data):\n",
        "    y=   data[['class']].copy()\n",
        "    X=   data.drop(columns=['class','seqnum','seqlen'])\n",
        "    return (X,y)\n",
        "\n",
        "def make_slice(data_set,min_len,max_len):\n",
        "    print(\"original \"+str(data_set.shape))\n",
        "    too_short = data_set[ data_set['seqlen'] < min_len ].index\n",
        "    no_short=data_set.drop(too_short)\n",
        "    print(\"no short \"+str(no_short.shape))\n",
        "    too_long = no_short[ no_short['seqlen'] >= max_len ].index\n",
        "    no_long_no_short=no_short.drop(too_long)\n",
        "    print(\"no long, no short \"+str(no_long_no_short.shape))\n",
        "    return no_long_no_short\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRAaO9jP9Kcr"
      },
      "source": [
        "## Make K-mers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8xcZ4Mr9Kcs"
      },
      "source": [
        "def make_kmer_table(K):\n",
        "    npad='N'*K\n",
        "    shorter_kmers=['']\n",
        "    for i in range(K):\n",
        "        longer_kmers=[]\n",
        "        for mer in shorter_kmers:\n",
        "            longer_kmers.append(mer+'A')\n",
        "            longer_kmers.append(mer+'C')\n",
        "            longer_kmers.append(mer+'G')\n",
        "            longer_kmers.append(mer+'T')\n",
        "        shorter_kmers = longer_kmers\n",
        "    all_kmers = shorter_kmers\n",
        "    kmer_dict = {}\n",
        "    kmer_dict[npad]=0\n",
        "    value=1\n",
        "    for mer in all_kmers:\n",
        "        kmer_dict[mer]=value\n",
        "        value += 1\n",
        "    return kmer_dict\n",
        "\n",
        "KMER_TABLE=make_kmer_table(K)\n",
        "\n",
        "def strings_to_vectors(data,uniform_len):\n",
        "    all_seqs=[]\n",
        "    for seq in data['sequence']:\n",
        "        i=0\n",
        "        seqlen=len(seq)\n",
        "        kmers=[]\n",
        "        while i < seqlen-K+1 -1:  # stop at minus one for spaced seed\n",
        "            #kmer=seq[i:i+2]+seq[i+3:i+5]    # SPACED SEED 2/1/2 for K=4\n",
        "            kmer=seq[i:i+K]  \n",
        "            i += 1\n",
        "            value=KMER_TABLE[kmer]\n",
        "            kmers.append(value)\n",
        "        pad_val=0\n",
        "        while i < uniform_len:\n",
        "            kmers.append(pad_val)\n",
        "            i += 1\n",
        "        all_seqs.append(kmers)\n",
        "    pd2d=pd.DataFrame(all_seqs)\n",
        "    return pd2d   # return 2D dataframe, uniform dimensions"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEtA0xiV9Kcv"
      },
      "source": [
        "def make_kmers(MAXLEN,train_set):\n",
        "    (X_train_all,y_train_all)=separate_X_and_y(train_set)\n",
        "\n",
        "    # The returned values are Pandas dataframes.\n",
        "    # print(X_train_all.shape,y_train_all.shape)\n",
        "    # (X_train_all,y_train_all)\n",
        "    # y: Pandas dataframe to Python list.\n",
        "    # y_train_all=y_train_all.values.tolist()\n",
        "    # The sequences lengths are bounded but not uniform.\n",
        "    X_train_all\n",
        "    print(type(X_train_all))\n",
        "    print(X_train_all.shape)\n",
        "    print(X_train_all.iloc[0])\n",
        "    print(len(X_train_all.iloc[0]['sequence']))\n",
        "\n",
        "    # X: List of string to List of uniform-length ordered lists of K-mers.\n",
        "    X_train_kmers=strings_to_vectors(X_train_all,MAXLEN)\n",
        "    # X: true 2D array (no more lists)\n",
        "    X_train_kmers.shape\n",
        "\n",
        "    print(\"transform...\")\n",
        "    # From pandas dataframe to numpy to list to numpy\n",
        "    print(type(X_train_kmers))\n",
        "    num_seqs=len(X_train_kmers)\n",
        "    tmp_seqs=[]\n",
        "    for i in range(num_seqs):\n",
        "        kmer_sequence=X_train_kmers.iloc[i]\n",
        "        tmp_seqs.append(kmer_sequence)\n",
        "    X_train_kmers=np.array(tmp_seqs)\n",
        "    tmp_seqs=None\n",
        "    print(type(X_train_kmers))\n",
        "    print(X_train_kmers)\n",
        "\n",
        "    labels=y_train_all.to_numpy()\n",
        "    return (X_train_kmers,labels)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaXyySyO9Kcz"
      },
      "source": [
        "def make_frequencies(Xin):\n",
        "    # Input:  numpy X(numseq,seqlen)  list of vectors of kmerval where val0=NNN,val1=AAA,etc. \n",
        "    # Output: numpy X(numseq,65)    list of frequencies of 0,1,etc.\n",
        "    Xout=[]\n",
        "    VOCABULARY_SIZE= 4**K + 1  # plus one for 'NNN'\n",
        "    for seq in Xin:\n",
        "        freqs =[0] * VOCABULARY_SIZE\n",
        "        total = 0\n",
        "        for kmerval in seq:\n",
        "            freqs[kmerval] += 1\n",
        "            total += 1\n",
        "        for c in range(VOCABULARY_SIZE):\n",
        "            freqs[c] = freqs[c]/total\n",
        "        Xout.append(freqs)\n",
        "    Xnum = np.asarray(Xout)\n",
        "    return (Xnum)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7jcg6Wl9Kc2"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLFNO1Xa9Kc3"
      },
      "source": [
        "def build_model(maxlen):\n",
        "    act=\"sigmoid\"\n",
        "\n",
        "    embed_layer  = keras.layers.Embedding(\n",
        "        VOCABULARY_SIZE,EMBED_DIMEN,input_length=maxlen);\n",
        "    \n",
        "    neurons=16\n",
        "    dense1_layer = keras.layers.Dense(neurons, activation=act,dtype=dt,input_dim=VOCABULARY_SIZE)\n",
        "    dense2_layer = keras.layers.Dense(neurons, activation=act,dtype=dt)\n",
        "    dense3_layer = keras.layers.Dense(neurons, activation=act,dtype=dt)\n",
        "    output_layer = keras.layers.Dense(1,  activation=act,dtype=dt)\n",
        "\n",
        "    mlp = keras.models.Sequential()\n",
        "    #mlp.add(embed_layer)\n",
        "    mlp.add(dense1_layer)\n",
        "    mlp.add(dense2_layer)\n",
        "    #mlp.add(dense3_layer)\n",
        "    mlp.add(output_layer)\n",
        "    \n",
        "    bc=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    print(\"COMPILE...\")\n",
        "    mlp.compile(loss=bc, optimizer=\"Adam\",metrics=[\"accuracy\"])\n",
        "    print(\"...COMPILED\")\n",
        "    return mlp"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdIS2utq9Kc9"
      },
      "source": [
        "## Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVo4tbB_9Kc-"
      },
      "source": [
        "def do_cross_validation(X,y,maxlen):\n",
        "    model = None\n",
        "    cv_scores = []\n",
        "    fold=0\n",
        "    splitter = ShuffleSplit(n_splits=SPLITS, test_size=0.2, random_state=37863)\n",
        "    for train_index,valid_index in splitter.split(X):\n",
        "        X_train=X[train_index] # use iloc[] for dataframe\n",
        "        y_train=y[train_index]\n",
        "        X_valid=X[valid_index]\n",
        "        y_valid=y[valid_index]\n",
        "\n",
        "        print(\"BUILD MODEL\")\n",
        "        model=build_model(maxlen)\n",
        "\n",
        "        print(\"FIT\")\n",
        "        start_time=time.time()\n",
        "        # this is complaining about string to float\n",
        "        history=model.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
        "                epochs=EPOCHS, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
        "                validation_data=(X_valid,y_valid) )\n",
        "        end_time=time.time()\n",
        "        elapsed_time=(end_time-start_time)\n",
        "                        \n",
        "        fold += 1\n",
        "        print(\"Fold %d, %d epochs, %d sec\"%(fold,EPOCHS,elapsed_time))\n",
        "\n",
        "        pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "        plt.grid(True)\n",
        "        plt.gca().set_ylim(0,1)\n",
        "        plt.show()\n",
        "\n",
        "        scores = model.evaluate(X_valid, y_valid, verbose=0)\n",
        "        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "        # What are the other metrics_names?\n",
        "        # Try this from Geron page 505:\n",
        "        # np.mean(keras.losses.mean_squared_error(y_valid,y_pred))\n",
        "        cv_scores.append(scores[1] * 100)  \n",
        "    print()\n",
        "    print(\"Validation core mean %.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n",
        "    return model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upc3S0eEgYZG"
      },
      "source": [
        "def just_train(model,X_train,y_train,maxlen):\n",
        "    print(\"FIT\")\n",
        "    start_time=time.time()\n",
        "    # this is complaining about string to float\n",
        "    history=model.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
        "            epochs=EPOCHS, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
        "            )  # no validation data\n",
        "    end_time=time.time()\n",
        "    elapsed_time=(end_time-start_time)\n",
        "    print(\"Train %d epochs, %d sec\"%(EPOCHS,elapsed_time))\n",
        "\n",
        "    pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "    plt.grid(True)\n",
        "    plt.gca().set_ylim(0,1)\n",
        "    plt.show()\n",
        "\n",
        "    #scores = model.evaluate(X_valid, y_valid, verbose=0)\n",
        "    #print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "    #cv_scores.append(scores[1] * 100)  \n",
        "    #print()\n",
        "    #print(\"Validation core mean %.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n",
        "    return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q-PEh7D9KdH"
      },
      "source": [
        "## Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8fNo6sn9KdH",
        "outputId": "abd8ef40-8e18-439e-d06a-6d0e606fe6e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "print(\"Load data from files.\")\n",
        "nc_seq=load_fasta(NC_FILENAME,0)\n",
        "pc_seq=load_fasta(PC_FILENAME,1)\n",
        "all_seq=pd.concat((nc_seq,pc_seq),axis=0)\n",
        "\n",
        "print(\"Put aside the test portion.\")\n",
        "(train_set,test_set)=make_train_test(all_seq)\n",
        "# Do this later when using the test data:\n",
        "# (X_test,y_test)=separate_X_and_y(test_set)\n",
        "\n",
        "nc_seq=None\n",
        "pc_seq=None\n",
        "all_seq=None\n",
        "\n",
        "print(\"Ready: train_set\")\n",
        "train_set"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load data from files.\n",
            "Put aside the test portion.\n",
            "Ready: train_set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>seqnum</th>\n",
              "      <th>class</th>\n",
              "      <th>sequence</th>\n",
              "      <th>seqlen</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1280</th>\n",
              "      <td>1281</td>\n",
              "      <td>0</td>\n",
              "      <td>AGTCCCTCCCCAGCCCAGCAGTCCCTCCAGGCTACATCCAGGAGAC...</td>\n",
              "      <td>348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9088</th>\n",
              "      <td>9089</td>\n",
              "      <td>0</td>\n",
              "      <td>CAGCTCCTGGGATGGCCTCACCTGAGGAGACTCTTGGGCCTTGGCA...</td>\n",
              "      <td>534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6069</th>\n",
              "      <td>6070</td>\n",
              "      <td>1</td>\n",
              "      <td>AGATCTAGGGATGGGGATGGGGAGGAGAAGTGGGAATGGGAAATTG...</td>\n",
              "      <td>592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18549</th>\n",
              "      <td>18550</td>\n",
              "      <td>1</td>\n",
              "      <td>GACGTCTCCCGCGGGCGTCGGCAGGGTCGGCGGCGTCGGCAGCAGT...</td>\n",
              "      <td>945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15027</th>\n",
              "      <td>15028</td>\n",
              "      <td>1</td>\n",
              "      <td>GAGCGCGCGAGCCGGGCCCGGAGCGCACGCCGCCGCCGCCACCGCC...</td>\n",
              "      <td>4382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3386</th>\n",
              "      <td>3387</td>\n",
              "      <td>0</td>\n",
              "      <td>TTTATGTGGATTGTCTGTCTCATGCTTGTTTCACCAGGGTAGTTAC...</td>\n",
              "      <td>578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6495</th>\n",
              "      <td>6496</td>\n",
              "      <td>0</td>\n",
              "      <td>ATAATGGGAAACTAAGGGCAAGTTCTCATGTTCCTGGTCCTGGCTT...</td>\n",
              "      <td>562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6409</th>\n",
              "      <td>6410</td>\n",
              "      <td>1</td>\n",
              "      <td>GGGTTTATTACTACTGAAGGAAGAACGTGAGTAGGTTAGGATTTCG...</td>\n",
              "      <td>740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7640</th>\n",
              "      <td>7641</td>\n",
              "      <td>1</td>\n",
              "      <td>ACAGCTGTGTTTGGCTGCAGGGCCAAGAGCGCTGTCAAGAAGACCC...</td>\n",
              "      <td>3156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14108</th>\n",
              "      <td>14109</td>\n",
              "      <td>0</td>\n",
              "      <td>GAAGTGATTGCAAGTTCAGCAGCATGAAACTGCTCTTTATCCGTGC...</td>\n",
              "      <td>466</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30290 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       seqnum  class                                           sequence  seqlen\n",
              "1280     1281      0  AGTCCCTCCCCAGCCCAGCAGTCCCTCCAGGCTACATCCAGGAGAC...     348\n",
              "9088     9089      0  CAGCTCCTGGGATGGCCTCACCTGAGGAGACTCTTGGGCCTTGGCA...     534\n",
              "6069     6070      1  AGATCTAGGGATGGGGATGGGGAGGAGAAGTGGGAATGGGAAATTG...     592\n",
              "18549   18550      1  GACGTCTCCCGCGGGCGTCGGCAGGGTCGGCGGCGTCGGCAGCAGT...     945\n",
              "15027   15028      1  GAGCGCGCGAGCCGGGCCCGGAGCGCACGCCGCCGCCGCCACCGCC...    4382\n",
              "...       ...    ...                                                ...     ...\n",
              "3386     3387      0  TTTATGTGGATTGTCTGTCTCATGCTTGTTTCACCAGGGTAGTTAC...     578\n",
              "6495     6496      0  ATAATGGGAAACTAAGGGCAAGTTCTCATGTTCCTGGTCCTGGCTT...     562\n",
              "6409     6410      1  GGGTTTATTACTACTGAAGGAAGAACGTGAGTAGGTTAGGATTTCG...     740\n",
              "7640     7641      1  ACAGCTGTGTTTGGCTGCAGGGCCAAGAGCGCTGTCAAGAAGACCC...    3156\n",
              "14108   14109      0  GAAGTGATTGCAAGTTCAGCAGCATGAAACTGCTCTTTATCCGTGC...     466\n",
              "\n",
              "[30290 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lI2PjJsxgYZT"
      },
      "source": [
        "# Reuse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKLYbWycgYZU",
        "outputId": "46db946d-7243-4b31-9e75-a7666dc3a000",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#reload=model1.load(FILENAME+'.short.model')\n",
        "#W = model1.get_weights()\n",
        "#scores = model1.evaluate(X_valid, y_valid, verbose=0)\n",
        "#print(\"%s: %.2f%%\" % (model1.metrics_names[1], scores[1]*100))\n",
        "\n",
        "min=2000\n",
        "max=3000\n",
        "print(\"Train on lengths %d to %d\"%(min,max))\n",
        "print(\"slice...\")\n",
        "subset=make_slice(train_set,min,max)\n",
        "print(\"kmers...\")\n",
        "(X_train,y_train)=make_kmers(max,subset)\n",
        "print(\"frequencies...\")\n",
        "X_train=make_frequencies(X_train)\n",
        "print(\"BUILD MODEL\")\n",
        "model=build_model(max)\n",
        "model=just_train(model,X_train,y_train,max)\n",
        "\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on lengths 2000 to 3000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (3221, 4)\n",
            "no long, no short (1351, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(1351, 1)\n",
            "sequence    GTCATTCTAGCTGCCTGCTGCCTCCGCAGCGTCCCCCCAGCTCTCC...\n",
            "Name: 19713, dtype: object\n",
            "2039\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[181 212  80 ...   0   0   0]\n",
            " [ 10  37 148 ...   0   0   0]\n",
            " [ 48 190 245 ...   0   0   0]\n",
            " ...\n",
            " [ 49 195  10 ...   0   0   0]\n",
            " [153  97 131 ...   0   0   0]\n",
            " [ 36 143  58 ...   0   0   0]]\n",
            "frequencies...\n",
            "BUILD MODEL\n",
            "COMPILE...\n",
            "...COMPILED\n",
            "FIT\n",
            "Epoch 1/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6171 - accuracy: 0.7024\n",
            "Epoch 2/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6094 - accuracy: 0.7024\n",
            "Epoch 3/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6092 - accuracy: 0.7024\n",
            "Epoch 4/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6086 - accuracy: 0.7024\n",
            "Epoch 5/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6083 - accuracy: 0.7024\n",
            "Epoch 6/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6081 - accuracy: 0.7024\n",
            "Epoch 7/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6081 - accuracy: 0.7024\n",
            "Epoch 8/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6078 - accuracy: 0.7024\n",
            "Epoch 9/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6075 - accuracy: 0.7024\n",
            "Epoch 10/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6073 - accuracy: 0.7024\n",
            "Epoch 11/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6069 - accuracy: 0.7024\n",
            "Epoch 12/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6075 - accuracy: 0.7024\n",
            "Epoch 13/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6066 - accuracy: 0.7024\n",
            "Epoch 14/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6062 - accuracy: 0.7024\n",
            "Epoch 15/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6058 - accuracy: 0.7024\n",
            "Epoch 16/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6054 - accuracy: 0.7024\n",
            "Epoch 17/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6051 - accuracy: 0.7024\n",
            "Epoch 18/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6049 - accuracy: 0.7024\n",
            "Epoch 19/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6046 - accuracy: 0.7024\n",
            "Epoch 20/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6038 - accuracy: 0.7024\n",
            "Epoch 21/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6035 - accuracy: 0.7024\n",
            "Epoch 22/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6033 - accuracy: 0.7024\n",
            "Epoch 23/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6022 - accuracy: 0.7024\n",
            "Epoch 24/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6016 - accuracy: 0.7024\n",
            "Epoch 25/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6012 - accuracy: 0.7024\n",
            "Epoch 26/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.6005 - accuracy: 0.7024\n",
            "Epoch 27/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5998 - accuracy: 0.7024\n",
            "Epoch 28/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5988 - accuracy: 0.7024\n",
            "Epoch 29/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5979 - accuracy: 0.7024\n",
            "Epoch 30/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5970 - accuracy: 0.7024\n",
            "Epoch 31/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5961 - accuracy: 0.7024\n",
            "Epoch 32/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5951 - accuracy: 0.7024\n",
            "Epoch 33/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5938 - accuracy: 0.7024\n",
            "Epoch 34/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5925 - accuracy: 0.7024\n",
            "Epoch 35/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5914 - accuracy: 0.7024\n",
            "Epoch 36/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5901 - accuracy: 0.7024\n",
            "Epoch 37/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5890 - accuracy: 0.7024\n",
            "Epoch 38/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5877 - accuracy: 0.7024\n",
            "Epoch 39/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5856 - accuracy: 0.7024\n",
            "Epoch 40/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5839 - accuracy: 0.7024\n",
            "Epoch 41/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5824 - accuracy: 0.7024\n",
            "Epoch 42/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5807 - accuracy: 0.7024\n",
            "Epoch 43/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5786 - accuracy: 0.7024\n",
            "Epoch 44/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5776 - accuracy: 0.7024\n",
            "Epoch 45/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5751 - accuracy: 0.7024\n",
            "Epoch 46/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5735 - accuracy: 0.7024\n",
            "Epoch 47/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5712 - accuracy: 0.7024\n",
            "Epoch 48/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5687 - accuracy: 0.7024\n",
            "Epoch 49/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5674 - accuracy: 0.7024\n",
            "Epoch 50/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5649 - accuracy: 0.7024\n",
            "Epoch 51/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5632 - accuracy: 0.7032\n",
            "Epoch 52/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5609 - accuracy: 0.7024\n",
            "Epoch 53/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5586 - accuracy: 0.7032\n",
            "Epoch 54/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5568 - accuracy: 0.7032\n",
            "Epoch 55/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5544 - accuracy: 0.7039\n",
            "Epoch 56/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5524 - accuracy: 0.7039\n",
            "Epoch 57/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5505 - accuracy: 0.7054\n",
            "Epoch 58/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5493 - accuracy: 0.7061\n",
            "Epoch 59/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5467 - accuracy: 0.7047\n",
            "Epoch 60/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5448 - accuracy: 0.7098\n",
            "Epoch 61/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5430 - accuracy: 0.7091\n",
            "Epoch 62/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5411 - accuracy: 0.7106\n",
            "Epoch 63/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5395 - accuracy: 0.7135\n",
            "Epoch 64/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5375 - accuracy: 0.7135\n",
            "Epoch 65/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5360 - accuracy: 0.7150\n",
            "Epoch 66/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7261\n",
            "Epoch 67/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7261\n",
            "Epoch 68/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5317 - accuracy: 0.7328\n",
            "Epoch 69/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5298 - accuracy: 0.7187\n",
            "Epoch 70/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5281 - accuracy: 0.7343\n",
            "Epoch 71/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5280 - accuracy: 0.7239\n",
            "Epoch 72/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5249 - accuracy: 0.7365\n",
            "Epoch 73/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5254 - accuracy: 0.7298\n",
            "Epoch 74/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5222 - accuracy: 0.7350\n",
            "Epoch 75/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5211 - accuracy: 0.7313\n",
            "Epoch 76/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5203 - accuracy: 0.7417\n",
            "Epoch 77/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5181 - accuracy: 0.7387\n",
            "Epoch 78/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5168 - accuracy: 0.7417\n",
            "Epoch 79/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5154 - accuracy: 0.7483\n",
            "Epoch 80/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5140 - accuracy: 0.7461\n",
            "Epoch 81/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5134 - accuracy: 0.7432\n",
            "Epoch 82/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5117 - accuracy: 0.7498\n",
            "Epoch 83/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5099 - accuracy: 0.7498\n",
            "Epoch 84/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5087 - accuracy: 0.7498\n",
            "Epoch 85/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5083 - accuracy: 0.7461\n",
            "Epoch 86/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5052 - accuracy: 0.7506\n",
            "Epoch 87/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5044 - accuracy: 0.7580\n",
            "Epoch 88/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7513\n",
            "Epoch 89/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7483\n",
            "Epoch 90/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4997 - accuracy: 0.7513\n",
            "Epoch 91/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4982 - accuracy: 0.7565\n",
            "Epoch 92/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4969 - accuracy: 0.7572\n",
            "Epoch 93/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4954 - accuracy: 0.7535\n",
            "Epoch 94/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4936 - accuracy: 0.7631\n",
            "Epoch 95/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4925 - accuracy: 0.7617\n",
            "Epoch 96/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4904 - accuracy: 0.7691\n",
            "Epoch 97/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4887 - accuracy: 0.7639\n",
            "Epoch 98/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4874 - accuracy: 0.7624\n",
            "Epoch 99/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4859 - accuracy: 0.7683\n",
            "Epoch 100/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4844 - accuracy: 0.7654\n",
            "Epoch 101/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4821 - accuracy: 0.7735\n",
            "Epoch 102/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4809 - accuracy: 0.7661\n",
            "Epoch 103/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4792 - accuracy: 0.7757\n",
            "Epoch 104/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4774 - accuracy: 0.7757\n",
            "Epoch 105/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4760 - accuracy: 0.7765\n",
            "Epoch 106/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4746 - accuracy: 0.7742\n",
            "Epoch 107/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4724 - accuracy: 0.7846\n",
            "Epoch 108/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4706 - accuracy: 0.7728\n",
            "Epoch 109/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4700 - accuracy: 0.7802\n",
            "Epoch 110/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4671 - accuracy: 0.7802\n",
            "Epoch 111/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4650 - accuracy: 0.7816\n",
            "Epoch 112/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4651 - accuracy: 0.7868\n",
            "Epoch 113/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4616 - accuracy: 0.7794\n",
            "Epoch 114/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4607 - accuracy: 0.7839\n",
            "Epoch 115/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4582 - accuracy: 0.7890\n",
            "Epoch 116/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4562 - accuracy: 0.7920\n",
            "Epoch 117/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4541 - accuracy: 0.7890\n",
            "Epoch 118/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4523 - accuracy: 0.7927\n",
            "Epoch 119/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4504 - accuracy: 0.7957\n",
            "Epoch 120/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4486 - accuracy: 0.7942\n",
            "Epoch 121/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4475 - accuracy: 0.7927\n",
            "Epoch 122/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4449 - accuracy: 0.7994\n",
            "Epoch 123/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4433 - accuracy: 0.8038\n",
            "Epoch 124/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4418 - accuracy: 0.8046\n",
            "Epoch 125/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4393 - accuracy: 0.8001\n",
            "Epoch 126/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4375 - accuracy: 0.8098\n",
            "Epoch 127/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4361 - accuracy: 0.8120\n",
            "Epoch 128/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4333 - accuracy: 0.8046\n",
            "Epoch 129/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4321 - accuracy: 0.8083\n",
            "Epoch 130/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4302 - accuracy: 0.8083\n",
            "Epoch 131/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4273 - accuracy: 0.8090\n",
            "Epoch 132/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4259 - accuracy: 0.8157\n",
            "Epoch 133/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4234 - accuracy: 0.8083\n",
            "Epoch 134/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4218 - accuracy: 0.8135\n",
            "Epoch 135/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4199 - accuracy: 0.8172\n",
            "Epoch 136/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4174 - accuracy: 0.8216\n",
            "Epoch 137/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4169 - accuracy: 0.8216\n",
            "Epoch 138/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4161 - accuracy: 0.8201\n",
            "Epoch 139/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4136 - accuracy: 0.8209\n",
            "Epoch 140/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4111 - accuracy: 0.8209\n",
            "Epoch 141/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4082 - accuracy: 0.8238\n",
            "Epoch 142/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4062 - accuracy: 0.8231\n",
            "Epoch 143/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4078 - accuracy: 0.8231\n",
            "Epoch 144/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4029 - accuracy: 0.8298\n",
            "Epoch 145/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.4013 - accuracy: 0.8312\n",
            "Epoch 146/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3992 - accuracy: 0.8379\n",
            "Epoch 147/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3979 - accuracy: 0.8312\n",
            "Epoch 148/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3959 - accuracy: 0.8327\n",
            "Epoch 149/200\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.3943 - accuracy: 0.8320\n",
            "Epoch 150/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3927 - accuracy: 0.8275\n",
            "Epoch 151/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3905 - accuracy: 0.8438\n",
            "Epoch 152/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3915 - accuracy: 0.8327\n",
            "Epoch 153/200\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.3877 - accuracy: 0.8386\n",
            "Epoch 154/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3859 - accuracy: 0.8372\n",
            "Epoch 155/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3863 - accuracy: 0.8394\n",
            "Epoch 156/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3828 - accuracy: 0.8386\n",
            "Epoch 157/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3811 - accuracy: 0.8394\n",
            "Epoch 158/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3797 - accuracy: 0.8453\n",
            "Epoch 159/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3786 - accuracy: 0.8438\n",
            "Epoch 160/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3762 - accuracy: 0.8372\n",
            "Epoch 161/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3759 - accuracy: 0.8460\n",
            "Epoch 162/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3745 - accuracy: 0.8446\n",
            "Epoch 163/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3722 - accuracy: 0.8497\n",
            "Epoch 164/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3728 - accuracy: 0.8483\n",
            "Epoch 165/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3691 - accuracy: 0.8416\n",
            "Epoch 166/200\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.3677 - accuracy: 0.8483\n",
            "Epoch 167/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3666 - accuracy: 0.8460\n",
            "Epoch 168/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3648 - accuracy: 0.8505\n",
            "Epoch 169/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3635 - accuracy: 0.8460\n",
            "Epoch 170/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3623 - accuracy: 0.8490\n",
            "Epoch 171/200\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.3605 - accuracy: 0.8505\n",
            "Epoch 172/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3600 - accuracy: 0.8527\n",
            "Epoch 173/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3572 - accuracy: 0.8594\n",
            "Epoch 174/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3583 - accuracy: 0.8497\n",
            "Epoch 175/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3554 - accuracy: 0.8512\n",
            "Epoch 176/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3543 - accuracy: 0.8564\n",
            "Epoch 177/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3539 - accuracy: 0.8579\n",
            "Epoch 178/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3562 - accuracy: 0.8475\n",
            "Epoch 179/200\n",
            "43/43 [==============================] - 0s 4ms/step - loss: 0.3526 - accuracy: 0.8527\n",
            "Epoch 180/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3526 - accuracy: 0.8497\n",
            "Epoch 181/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3481 - accuracy: 0.8557\n",
            "Epoch 182/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3475 - accuracy: 0.8579\n",
            "Epoch 183/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3466 - accuracy: 0.8571\n",
            "Epoch 184/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3467 - accuracy: 0.8549\n",
            "Epoch 185/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3434 - accuracy: 0.8668\n",
            "Epoch 186/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3447 - accuracy: 0.8586\n",
            "Epoch 187/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3416 - accuracy: 0.8601\n",
            "Epoch 188/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3425 - accuracy: 0.8601\n",
            "Epoch 189/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3409 - accuracy: 0.8645\n",
            "Epoch 190/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3387 - accuracy: 0.8675\n",
            "Epoch 191/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3383 - accuracy: 0.8616\n",
            "Epoch 192/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3381 - accuracy: 0.8601\n",
            "Epoch 193/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3354 - accuracy: 0.8638\n",
            "Epoch 194/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3367 - accuracy: 0.8645\n",
            "Epoch 195/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3367 - accuracy: 0.8601\n",
            "Epoch 196/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3342 - accuracy: 0.8623\n",
            "Epoch 197/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3328 - accuracy: 0.8616\n",
            "Epoch 198/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3319 - accuracy: 0.8645\n",
            "Epoch 199/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3303 - accuracy: 0.8631\n",
            "Epoch 200/200\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3290 - accuracy: 0.8682\n",
            "Train 200 epochs, 28 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEmSlrt_gYZZ"
      },
      "source": [
        "def evaluate(model,min,max):\n",
        "    print(\"Evaluate on lengths %d to %d\"%(min,max))\n",
        "    print(\"slice...\")\n",
        "    subset=make_slice(train_set,min,max)\n",
        "    print(\"kmers...\")\n",
        "    (X_valid,y_valid)=make_kmers(max,subset)\n",
        "    print(\"frequencies...\")\n",
        "    X_valid=make_frequencies(X_valid)\n",
        "    print(\"evaluate....\")\n",
        "    scores = model.evaluate(X_valid, y_valid, verbose=1)  # valid = train, expect 100%\n",
        "    print(\"Evaluated on lengths %d to %d\"%(min,max))\n",
        "    print(\"%s: %.2f%%\\n\" % (model.metrics_names[1], scores[1]*100))\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiNABsVNgYZc",
        "outputId": "3866e152-beef-4ca2-a6f9-25274c4430d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "evaluate(model,200,1000)\n",
        "evaluate(model,1000,2000)\n",
        "evaluate(model,2000,3000)\n",
        "evaluate(model,3000,5000)\n",
        "evaluate(model,5000,10000)\n",
        "evaluate(model,10000,30000)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate on lengths 200 to 1000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (30290, 4)\n",
            "no long, no short (8879, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(8879, 1)\n",
            "sequence    AGTCCCTCCCCAGCCCAGCAGTCCCTCCAGGCTACATCCAGGAGAC...\n",
            "Name: 1280, dtype: object\n",
            "348\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[ 46 182 214 ...   0   0   0]\n",
            " [ 36 142  56 ...   0   0   0]\n",
            " [135  28 110 ...   0   0   0]\n",
            " ...\n",
            " [147  73  36 ...   0   0   0]\n",
            " [228 143  57 ...   0   0   0]\n",
            " [131  12  47 ...   0   0   0]]\n",
            "frequencies...\n",
            "evaluate....\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.5845 - accuracy: 0.6868\n",
            "Evaluated on lengths 200 to 1000\n",
            "accuracy: 68.68%\n",
            "\n",
            "Evaluate on lengths 1000 to 2000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (9273, 4)\n",
            "no long, no short (3368, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(3368, 1)\n",
            "sequence    GGCGGGGTCGACTGACGGTAACGGGGCAGAGAGGCTGTTCGCAGAG...\n",
            "Name: 12641, dtype: object\n",
            "1338\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[167 155 107 ...   0   0   0]\n",
            " [226 133  20 ...   0   0   0]\n",
            " [108 175 186 ...   0   0   0]\n",
            " ...\n",
            " [175 185 225 ...   0   0   0]\n",
            " [ 37 148  78 ...   0   0   0]\n",
            " [188 240 192 ...   0   0   0]]\n",
            "frequencies...\n",
            "evaluate....\n",
            "106/106 [==============================] - 0s 3ms/step - loss: 0.4629 - accuracy: 0.7672\n",
            "Evaluated on lengths 1000 to 2000\n",
            "accuracy: 76.72%\n",
            "\n",
            "Evaluate on lengths 2000 to 3000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (3221, 4)\n",
            "no long, no short (1351, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(1351, 1)\n",
            "sequence    GTCATTCTAGCTGCCTGCTGCCTCCGCAGCGTCCCCCCAGCTCTCC...\n",
            "Name: 19713, dtype: object\n",
            "2039\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[181 212  80 ...   0   0   0]\n",
            " [ 10  37 148 ...   0   0   0]\n",
            " [ 48 190 245 ...   0   0   0]\n",
            " ...\n",
            " [ 49 195  10 ...   0   0   0]\n",
            " [153  97 131 ...   0   0   0]\n",
            " [ 36 143  58 ...   0   0   0]]\n",
            "frequencies...\n",
            "evaluate....\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 0.3295 - accuracy: 0.8638\n",
            "Evaluated on lengths 2000 to 3000\n",
            "accuracy: 86.38%\n",
            "\n",
            "Evaluate on lengths 3000 to 5000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (1336, 4)\n",
            "no long, no short (895, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(895, 1)\n",
            "sequence    AGGCCGCGTCCGCCCGCGCCCGCTCTGGCCCCCGCGGAGCCGCGCA...\n",
            "Name: 19203, dtype: object\n",
            "3491\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[ 42 166 151 ...   0   0   0]\n",
            " [ 62 245 209 ...   0   0   0]\n",
            " [ 47 185 228 ...   0   0   0]\n",
            " ...\n",
            " [166 151  92 ...   0   0   0]\n",
            " [ 59 233 163 ...   0   0   0]\n",
            " [  3  11  41 ...   0   0   0]]\n",
            "frequencies...\n",
            "evaluate....\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3403 - accuracy: 0.8514\n",
            "Evaluated on lengths 3000 to 5000\n",
            "accuracy: 85.14%\n",
            "\n",
            "Evaluate on lengths 5000 to 10000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (345, 4)\n",
            "no long, no short (314, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(314, 1)\n",
            "sequence    GGGAGCTCCGTTGTGCGTCGCTTAAGTGAGGGCGGCGGATGGGCGA...\n",
            "Name: 19766, dtype: object\n",
            "6207\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[169 163 138 ...   0   0   0]\n",
            " [167 155 106 ...   0   0   0]\n",
            " [ 19  76  47 ...   0   0   0]\n",
            " ...\n",
            " [ 91 107 169 ...   0   0   0]\n",
            " [ 62 247 218 ...   0   0   0]\n",
            " [229 145  67 ...   0   0   0]]\n",
            "frequencies...\n",
            "evaluate....\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4118 - accuracy: 0.8344\n",
            "Evaluated on lengths 5000 to 10000\n",
            "accuracy: 83.44%\n",
            "\n",
            "Evaluate on lengths 10000 to 30000\n",
            "slice...\n",
            "original (30290, 4)\n",
            "no short (29, 4)\n",
            "no long, no short (29, 4)\n",
            "kmers...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(29, 1)\n",
            "sequence    GCGCTAGCTCCCATGCTGGCCTCGGTGCCACTCGCGCGCCGGCCGC...\n",
            "Name: 7362, dtype: object\n",
            "11322\n",
            "transform...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "[[154 104 157 ...   0   0   0]\n",
            " [160 128 254 ...   0   0   0]\n",
            " [ 59 235 169 ...   0   0   0]\n",
            " ...\n",
            " [ 47 187 236 ...   0   0   0]\n",
            " [ 47 188 239 ...   0   0   0]\n",
            " [139  41 163 ...   0   0   0]]\n",
            "frequencies...\n",
            "evaluate....\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6046 - accuracy: 0.5862\n",
            "Evaluated on lengths 10000 to 30000\n",
            "accuracy: 58.62%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbieSKdOjeaL"
      },
      "source": [
        "\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd3Wj_vI9KdP"
      },
      "source": [
        "## Len 200-1Kb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ8eW5Rg9KdQ"
      },
      "source": [
        "MINLEN=200\n",
        "MAXLEN=1000\n",
        "\n",
        "if False:\n",
        "  print (\"Compile the model\")\n",
        "  model=build_model(MAXLEN)\n",
        "  print (\"Summarize the model\")\n",
        "  print(model.summary())  # Print this only once\n",
        "  print(\"Working on full training set, slice by sequence length.\")\n",
        "  print(\"Slice size range [%d - %d)\"%(MINLEN,MAXLEN))\n",
        "  subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
        "  print (\"Sequence to Kmer\")\n",
        "  (X_train,y_train)=make_kmers(MAXLEN,subset)\n",
        "  X_train\n",
        "  X_train=make_frequencies(X_train)\n",
        "  X_train\n",
        "  print (\"Cross valiation\")\n",
        "  model1 = do_cross_validation(X_train,y_train,MAXLEN)\n",
        "  model1.save(FILENAME+'.short.model')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC68X4zr9KdU"
      },
      "source": [
        "## Len 1Kb-2Kb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nm3oU3h9KdV"
      },
      "source": [
        "MINLEN=1000\n",
        "MAXLEN=2000\n",
        "\n",
        "if False:\n",
        "    print (\"Compile the model\")\n",
        "    model=build_model(MAXLEN)\n",
        "    print (\"Summarize the model\")\n",
        "    print(model.summary())  # Print this only once\n",
        "    print(\"Working on full training set, slice by sequence length.\")\n",
        "    print(\"Slice size range [%d - %d)\"%(MINLEN,MAXLEN))\n",
        "    subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
        "    print (\"Sequence to Kmer\")\n",
        "    (X_train,y_train)=make_kmers(MAXLEN,subset)\n",
        "    X_train\n",
        "    X_train=make_frequencies(X_train)\n",
        "    X_train\n",
        "    print (\"Cross valiation\")\n",
        "    model2 = do_cross_validation(X_train,y_train,MAXLEN)\n",
        "    model2.save(FILENAME+'.medium.model')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyACRnZx9Kde"
      },
      "source": [
        "## Len 2Kb-3Kb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUxmLnQ-9Kde"
      },
      "source": [
        "MINLEN=2000\n",
        "MAXLEN=3000\n",
        "\n",
        "if False:\n",
        "    print (\"Compile the model\")\n",
        "    model=build_model(MAXLEN)\n",
        "    print (\"Summarize the model\")\n",
        "    print(model.summary())  # Print this only once\n",
        "    print(\"Working on full training set, slice by sequence length.\")\n",
        "    print(\"Slice size range [%d - %d)\"%(MINLEN,MAXLEN))\n",
        "    subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
        "    print (\"Sequence to Kmer\")\n",
        "    (X_train,y_train)=make_kmers(MAXLEN,subset)\n",
        "    X_train\n",
        "    X_train=make_frequencies(X_train)\n",
        "    X_train\n",
        "    print (\"Cross valiation\")\n",
        "    model3 = do_cross_validation(X_train,y_train,MAXLEN)\n",
        "    model3.save(FILENAME+'.long.model')"
      ],
      "execution_count": 32,
      "outputs": []
    }
  ]
}