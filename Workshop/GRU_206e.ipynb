{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GRU_206e.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojm_6E9f9Kcf"
      },
      "source": [
        "# GRU 206\n",
        "* Operate on 16000 GenCode 34 seqs.\n",
        "* 5-way cross validation. Save best model per CV.\n",
        "* Report mean accuracy from final re-validation with best 5.\n",
        "* Use Adam with a learn rate decay schdule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh6XplUvC0j0",
        "outputId": "a66775e1-4d18-4629-e869-327ac8cf4c9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "NC_FILENAME='ncRNA.gc34.processed.fasta'\n",
        "PC_FILENAME='pcRNA.gc34.processed.fasta'\n",
        "DATAPATH=\"\"\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    PATH='/content/drive/'\n",
        "    drive.mount(PATH)\n",
        "    DATAPATH=PATH+'My Drive/data/'  # must end in \"/\"\n",
        "    NC_FILENAME = DATAPATH+NC_FILENAME\n",
        "    PC_FILENAME = DATAPATH+PC_FILENAME\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    DATAPATH=\"\" \n",
        "\n",
        "EPOCHS=200\n",
        "SPLITS=1\n",
        "K=5\n",
        "VOCABULARY_SIZE=4**K+1   # e.g. K=3 => 64 DNA K-mers + 'NNN'\n",
        "EMBED_DIMEN=16\n",
        "FILENAME='GRU206'\n",
        "NEURONS=64"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQY7aTj29Kch"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LayerNormalization\n",
        "import time\n",
        "dt='float32'\n",
        "tf.keras.backend.set_floatx(dt)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7jcg6Wl9Kc2"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLFNO1Xa9Kc3"
      },
      "source": [
        "def compile_model(model):\n",
        "    adam_default_learn_rate = 0.001\n",
        "    schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate = adam_default_learn_rate*10,\n",
        "        #decay_steps=100000, decay_rate=0.96, staircase=True)\n",
        "        decay_steps=10000, decay_rate=0.99, staircase=True)\n",
        "    # learn rate = initial_learning_rate * decay_rate ^ (step / decay_steps)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=schedule)\n",
        "    bc=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    print(\"COMPILE...\")\n",
        "    model.compile(loss=bc, optimizer=opt, metrics=[\"accuracy\"])\n",
        "    #model.compile(loss=bc, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    print(\"...COMPILED\")\n",
        "    return model\n",
        "\n",
        "def build_model():\n",
        "    act=\"tanh\"\n",
        "    embed_layer  = keras.layers.Embedding(\n",
        "        #VOCABULARY_SIZE, EMBED_DIMEN, input_length=1000, input_length=1000, mask_zero=True)\n",
        "        #input_dim=[None,VOCABULARY_SIZE], output_dim=EMBED_DIMEN, mask_zero=True)\n",
        "        input_dim=VOCABULARY_SIZE, output_dim=EMBED_DIMEN, mask_zero=True)\n",
        "    rnn1_layer = keras.layers.Bidirectional(\n",
        "      keras.layers.GRU(NEURONS, return_sequences=True, \n",
        "          input_shape=[1000,EMBED_DIMEN], activation=act, dropout=0.50))\n",
        "    rnn2_layer = keras.layers.Bidirectional(\n",
        "      keras.layers.GRU(NEURONS, return_sequences=False, \n",
        "        activation=act, dropout=0.50))\n",
        "    dense1_layer = keras.layers.Dense(NEURONS, activation=act,dtype=dt)\n",
        "    drop1_layer = keras.layers.Dropout(0.5)\n",
        "    dense2_layer = keras.layers.Dense(NEURONS, activation=act,dtype=dt)\n",
        "    drop2_layer = keras.layers.Dropout(0.5)\n",
        "    output_layer = keras.layers.Dense(1, activation=\"sigmoid\", dtype=dt)\n",
        "    mlp = keras.models.Sequential()\n",
        "    mlp.add(embed_layer)\n",
        "    mlp.add(rnn1_layer)\n",
        "    mlp.add(rnn2_layer)\n",
        "    mlp.add(dense1_layer)\n",
        "    mlp.add(drop1_layer)\n",
        "    mlp.add(dense2_layer)\n",
        "    mlp.add(drop2_layer)\n",
        "    mlp.add(output_layer)\n",
        "    mlpc = compile_model(mlp)\n",
        "    return mlpc"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV6k-xOm9Kcn"
      },
      "source": [
        "## Load and partition sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I-O_qzw9Kco"
      },
      "source": [
        "# Assume file was preprocessed to contain one line per seq.\n",
        "# Prefer Pandas dataframe but df does not support append.\n",
        "# For conversion to tensor, must avoid python lists.\n",
        "def load_fasta(filename,label):\n",
        "    DEFLINE='>'\n",
        "    labels=[]\n",
        "    seqs=[]\n",
        "    lens=[]\n",
        "    nums=[]\n",
        "    num=0\n",
        "    with open (filename,'r') as infile:\n",
        "        for line in infile:\n",
        "            if line[0]!=DEFLINE:\n",
        "                seq=line.rstrip()\n",
        "                num += 1   # first seqnum is 1\n",
        "                seqlen=len(seq)\n",
        "                nums.append(num)\n",
        "                labels.append(label)\n",
        "                seqs.append(seq)\n",
        "                lens.append(seqlen)\n",
        "    df1=pd.DataFrame(nums,columns=['seqnum'])\n",
        "    df2=pd.DataFrame(labels,columns=['class'])\n",
        "    df3=pd.DataFrame(seqs,columns=['sequence'])\n",
        "    df4=pd.DataFrame(lens,columns=['seqlen'])\n",
        "    df=pd.concat((df1,df2,df3,df4),axis=1)\n",
        "    return df\n",
        "\n",
        "def separate_X_and_y(data):\n",
        "    y=   data[['class']].copy()\n",
        "    X=   data.drop(columns=['class','seqnum','seqlen'])\n",
        "    return (X,y)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRAaO9jP9Kcr"
      },
      "source": [
        "## Make K-mers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8xcZ4Mr9Kcs"
      },
      "source": [
        "def make_kmer_table(K):\n",
        "    npad='N'*K\n",
        "    shorter_kmers=['']\n",
        "    for i in range(K):\n",
        "        longer_kmers=[]\n",
        "        for mer in shorter_kmers:\n",
        "            longer_kmers.append(mer+'A')\n",
        "            longer_kmers.append(mer+'C')\n",
        "            longer_kmers.append(mer+'G')\n",
        "            longer_kmers.append(mer+'T')\n",
        "        shorter_kmers = longer_kmers\n",
        "    all_kmers = shorter_kmers\n",
        "    kmer_dict = {}\n",
        "    kmer_dict[npad]=0\n",
        "    value=1\n",
        "    for mer in all_kmers:\n",
        "        kmer_dict[mer]=value\n",
        "        value += 1\n",
        "    return kmer_dict\n",
        "\n",
        "KMER_TABLE=make_kmer_table(K)\n",
        "\n",
        "def strings_to_vectors(data,uniform_len):\n",
        "    all_seqs=[]\n",
        "    for seq in data['sequence']:\n",
        "        i=0\n",
        "        seqlen=len(seq)\n",
        "        kmers=[]\n",
        "        while i < seqlen-K+1 -1:  # stop at minus one for spaced seed\n",
        "            #kmer=seq[i:i+2]+seq[i+3:i+5]    # SPACED SEED 2/1/2 for K=4\n",
        "            kmer=seq[i:i+K]  \n",
        "            i += 1\n",
        "            value=KMER_TABLE[kmer]\n",
        "            kmers.append(value)\n",
        "        pad_val=0\n",
        "        while i < uniform_len:\n",
        "            kmers.append(pad_val)\n",
        "            i += 1\n",
        "        all_seqs.append(kmers)\n",
        "    pd2d=pd.DataFrame(all_seqs)\n",
        "    return pd2d   # return 2D dataframe, uniform dimensions"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEtA0xiV9Kcv"
      },
      "source": [
        "def make_kmers(MAXLEN,train_set):\n",
        "    (X_train_all,y_train_all)=separate_X_and_y(train_set)\n",
        "    X_train_kmers=strings_to_vectors(X_train_all,MAXLEN)\n",
        "    # From pandas dataframe to numpy to list to numpy\n",
        "    num_seqs=len(X_train_kmers)\n",
        "    tmp_seqs=[]\n",
        "    for i in range(num_seqs):\n",
        "        kmer_sequence=X_train_kmers.iloc[i]\n",
        "        tmp_seqs.append(kmer_sequence)\n",
        "    X_train_kmers=np.array(tmp_seqs)\n",
        "    tmp_seqs=None\n",
        "    labels=y_train_all.to_numpy()\n",
        "    return (X_train_kmers,labels)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaXyySyO9Kcz"
      },
      "source": [
        "def make_frequencies(Xin):\n",
        "    Xout=[]\n",
        "    VOCABULARY_SIZE= 4**K + 1  # plus one for 'NNN'\n",
        "    for seq in Xin:\n",
        "        freqs =[0] * VOCABULARY_SIZE\n",
        "        total = 0\n",
        "        for kmerval in seq:\n",
        "            freqs[kmerval] += 1\n",
        "            total += 1\n",
        "        for c in range(VOCABULARY_SIZE):\n",
        "            freqs[c] = freqs[c]/total\n",
        "        Xout.append(freqs)\n",
        "    Xnum = np.asarray(Xout)\n",
        "    return (Xnum)\n",
        "def make_slice(data_set,min_len,max_len):\n",
        "    slice = data_set.query('seqlen <= '+str(max_len)+' & seqlen>= '+str(min_len))\n",
        "    return slice"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdIS2utq9Kc9"
      },
      "source": [
        "## Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVo4tbB_9Kc-"
      },
      "source": [
        "def do_cross_validation(X,y,given_model):\n",
        "    cv_scores = []\n",
        "    fold=0\n",
        "    splitter = ShuffleSplit(n_splits=SPLITS, test_size=0.1, random_state=37863)\n",
        "    for train_index,valid_index in splitter.split(X):\n",
        "        fold += 1\n",
        "        X_train=X[train_index] # use iloc[] for dataframe\n",
        "        y_train=y[train_index]\n",
        "        X_valid=X[valid_index]\n",
        "        y_valid=y[valid_index]        \n",
        "        # Avoid continually improving the same model.\n",
        "        model = compile_model(keras.models.clone_model(given_model))\n",
        "        bestname=DATAPATH+FILENAME+\".cv.\"+str(fold)+\".best\"\n",
        "        mycallbacks = [keras.callbacks.ModelCheckpoint(\n",
        "            filepath=bestname, save_best_only=True, \n",
        "            monitor='val_accuracy', mode='max')]   \n",
        "        print(\"FIT\")\n",
        "        start_time=time.time()\n",
        "        history=model.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
        "                epochs=EPOCHS, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
        "                callbacks=mycallbacks,\n",
        "                validation_data=(X_valid,y_valid) )\n",
        "        end_time=time.time()\n",
        "        elapsed_time=(end_time-start_time)                        \n",
        "        print(\"Fold %d, %d epochs, %d sec\"%(fold,EPOCHS,elapsed_time))\n",
        "        pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "        plt.grid(True)\n",
        "        plt.gca().set_ylim(0,1)\n",
        "        plt.show()\n",
        "        best_model=keras.models.load_model(bestname)\n",
        "        scores = best_model.evaluate(X_valid, y_valid, verbose=0)\n",
        "        print(\"%s: %.2f%%\" % (best_model.metrics_names[1], scores[1]*100))\n",
        "        cv_scores.append(scores[1] * 100)  \n",
        "    print()\n",
        "    print(\"%d-way Cross Validation mean %.2f%% (+/- %.2f%%)\" % (fold, np.mean(cv_scores), np.std(cv_scores)))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd3Wj_vI9KdP"
      },
      "source": [
        "## Train on RNA lengths 200-1Kb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8fNo6sn9KdH",
        "outputId": "fbe0dcf9-5978-4157-c41c-7e9a54b3d2a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "MINLEN=200\n",
        "MAXLEN=1000\n",
        "print(\"Load data from files.\")\n",
        "nc_seq=load_fasta(NC_FILENAME,0)\n",
        "pc_seq=load_fasta(PC_FILENAME,1)\n",
        "train_set=pd.concat((nc_seq,pc_seq),axis=0)\n",
        "nc_seq=None\n",
        "pc_seq=None\n",
        "print(\"Ready: train_set\")\n",
        "#train_set\n",
        "subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
        "print (\"Data reshape\")\n",
        "(X_train,y_train)=make_kmers(MAXLEN,subset)\n",
        "#print (\"Data prep\")\n",
        "#X_train=make_frequencies(X_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load data from files.\n",
            "Ready: train_set\n",
            "Data reshape\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1HuSs8ZbeL4",
        "outputId": "40a1a208-9c30-4309-c9c3-089bb9a05a3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        }
      },
      "source": [
        "print (\"Compile the model\")\n",
        "model=build_model()\n",
        "print (\"Summarize the model\")\n",
        "print(model.summary())  # Print this only once\n",
        "model.save(DATAPATH+FILENAME+'.model')\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compile the model\n",
            "COMPILE...\n",
            "...COMPILED\n",
            "Summarize the model\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          16400     \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 128)         31488     \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 128)               74496     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 134,865\n",
            "Trainable params: 134,865\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/data/GRU206.model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ8eW5Rg9KdQ",
        "outputId": "0358587b-e25a-4a7d-e138-bdbb59afae7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print (\"Cross valiation\")\n",
        "do_cross_validation(X_train,y_train,model)  \n",
        "print (\"Done\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross valiation\n",
            "COMPILE...\n",
            "...COMPILED\n",
            "FIT\n",
            "Epoch 1/200\n",
            "453/453 [==============================] - ETA: 0s - loss: 0.6789 - accuracy: 0.6145INFO:tensorflow:Assets written to: /content/drive/My Drive/data/GRU206.cv.1.best/assets\n",
            "453/453 [==============================] - 106s 234ms/step - loss: 0.6789 - accuracy: 0.6145 - val_loss: 0.6553 - val_accuracy: 0.6530\n",
            "Epoch 2/200\n",
            "453/453 [==============================] - 64s 141ms/step - loss: 0.6753 - accuracy: 0.6180 - val_loss: 0.6523 - val_accuracy: 0.6530\n",
            "Epoch 3/200\n",
            "453/453 [==============================] - 64s 141ms/step - loss: 0.6724 - accuracy: 0.6173 - val_loss: 0.6481 - val_accuracy: 0.6530\n",
            "Epoch 4/200\n",
            "453/453 [==============================] - 65s 144ms/step - loss: 0.6778 - accuracy: 0.6084 - val_loss: 0.6578 - val_accuracy: 0.6530\n",
            "Epoch 5/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6688 - accuracy: 0.6221 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 6/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6712 - accuracy: 0.6267 - val_loss: 0.6546 - val_accuracy: 0.6530\n",
            "Epoch 7/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6728 - accuracy: 0.6246 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 8/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6709 - accuracy: 0.6188 - val_loss: 0.6515 - val_accuracy: 0.6530\n",
            "Epoch 9/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6715 - accuracy: 0.6216 - val_loss: 0.6472 - val_accuracy: 0.6530\n",
            "Epoch 10/200\n",
            "453/453 [==============================] - 66s 147ms/step - loss: 0.6690 - accuracy: 0.6237 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 11/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6738 - accuracy: 0.6191 - val_loss: 0.6538 - val_accuracy: 0.6530\n",
            "Epoch 12/200\n",
            "453/453 [==============================] - 64s 141ms/step - loss: 0.6716 - accuracy: 0.6220 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 13/200\n",
            "453/453 [==============================] - 62s 136ms/step - loss: 0.6700 - accuracy: 0.6197 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 14/200\n",
            "453/453 [==============================] - 62s 138ms/step - loss: 0.6735 - accuracy: 0.6195 - val_loss: 0.6553 - val_accuracy: 0.6530\n",
            "Epoch 15/200\n",
            "453/453 [==============================] - 62s 137ms/step - loss: 0.6717 - accuracy: 0.6253 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 16/200\n",
            "453/453 [==============================] - 62s 137ms/step - loss: 0.6737 - accuracy: 0.6153 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 17/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6693 - accuracy: 0.6206 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 18/200\n",
            "453/453 [==============================] - 60s 132ms/step - loss: 0.6702 - accuracy: 0.6215 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
            "Epoch 19/200\n",
            "453/453 [==============================] - 61s 134ms/step - loss: 0.6740 - accuracy: 0.6161 - val_loss: 0.6770 - val_accuracy: 0.6530\n",
            "Epoch 20/200\n",
            "453/453 [==============================] - 62s 136ms/step - loss: 0.6775 - accuracy: 0.6159 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 21/200\n",
            "453/453 [==============================] - 61s 136ms/step - loss: 0.6710 - accuracy: 0.6232 - val_loss: 0.6765 - val_accuracy: 0.6530\n",
            "Epoch 22/200\n",
            "453/453 [==============================] - 62s 136ms/step - loss: 0.6758 - accuracy: 0.6159 - val_loss: 0.6576 - val_accuracy: 0.6530\n",
            "Epoch 23/200\n",
            "453/453 [==============================] - 62s 137ms/step - loss: 0.6692 - accuracy: 0.6255 - val_loss: 0.6643 - val_accuracy: 0.6530\n",
            "Epoch 24/200\n",
            "453/453 [==============================] - 63s 139ms/step - loss: 0.6737 - accuracy: 0.6182 - val_loss: 0.6687 - val_accuracy: 0.6530\n",
            "Epoch 25/200\n",
            "453/453 [==============================] - 63s 138ms/step - loss: 0.6713 - accuracy: 0.6210 - val_loss: 0.6680 - val_accuracy: 0.6530\n",
            "Epoch 26/200\n",
            "453/453 [==============================] - 63s 139ms/step - loss: 0.6728 - accuracy: 0.6189 - val_loss: 0.6502 - val_accuracy: 0.6530\n",
            "Epoch 27/200\n",
            "453/453 [==============================] - 63s 139ms/step - loss: 0.6691 - accuracy: 0.6202 - val_loss: 0.6519 - val_accuracy: 0.6530\n",
            "Epoch 28/200\n",
            "453/453 [==============================] - 63s 139ms/step - loss: 0.6723 - accuracy: 0.6176 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 29/200\n",
            "453/453 [==============================] - 63s 138ms/step - loss: 0.6703 - accuracy: 0.6187 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 30/200\n",
            "453/453 [==============================] - 63s 139ms/step - loss: 0.6734 - accuracy: 0.6231 - val_loss: 0.6483 - val_accuracy: 0.6530\n",
            "Epoch 31/200\n",
            "453/453 [==============================] - 64s 141ms/step - loss: 0.6719 - accuracy: 0.6221 - val_loss: 0.6501 - val_accuracy: 0.6530\n",
            "Epoch 32/200\n",
            "453/453 [==============================] - 63s 140ms/step - loss: 0.6720 - accuracy: 0.6203 - val_loss: 0.6484 - val_accuracy: 0.6530\n",
            "Epoch 33/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6728 - accuracy: 0.6211 - val_loss: 0.6558 - val_accuracy: 0.6530\n",
            "Epoch 34/200\n",
            "453/453 [==============================] - 64s 141ms/step - loss: 0.6696 - accuracy: 0.6251 - val_loss: 0.6559 - val_accuracy: 0.6530\n",
            "Epoch 35/200\n",
            "453/453 [==============================] - 63s 140ms/step - loss: 0.6664 - accuracy: 0.6269 - val_loss: 0.6614 - val_accuracy: 0.6530\n",
            "Epoch 36/200\n",
            "453/453 [==============================] - 64s 140ms/step - loss: 0.6714 - accuracy: 0.6229 - val_loss: 0.6674 - val_accuracy: 0.6530\n",
            "Epoch 37/200\n",
            "453/453 [==============================] - 63s 139ms/step - loss: 0.6733 - accuracy: 0.6187 - val_loss: 0.6600 - val_accuracy: 0.6530\n",
            "Epoch 38/200\n",
            "453/453 [==============================] - 63s 139ms/step - loss: 0.6686 - accuracy: 0.6271 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 39/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6710 - accuracy: 0.6273 - val_loss: 0.6544 - val_accuracy: 0.6530\n",
            "Epoch 40/200\n",
            "453/453 [==============================] - 64s 142ms/step - loss: 0.6685 - accuracy: 0.6241 - val_loss: 0.6558 - val_accuracy: 0.6530\n",
            "Epoch 41/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6712 - accuracy: 0.6214 - val_loss: 0.6481 - val_accuracy: 0.6530\n",
            "Epoch 42/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6696 - accuracy: 0.6245 - val_loss: 0.6507 - val_accuracy: 0.6530\n",
            "Epoch 43/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6722 - accuracy: 0.6194 - val_loss: 0.6566 - val_accuracy: 0.6530\n",
            "Epoch 44/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6740 - accuracy: 0.6175 - val_loss: 0.6506 - val_accuracy: 0.6530\n",
            "Epoch 45/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6731 - accuracy: 0.6193 - val_loss: 0.6477 - val_accuracy: 0.6530\n",
            "Epoch 46/200\n",
            "453/453 [==============================] - 65s 144ms/step - loss: 0.6691 - accuracy: 0.6277 - val_loss: 0.7200 - val_accuracy: 0.6530\n",
            "Epoch 47/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6719 - accuracy: 0.6168 - val_loss: 0.6541 - val_accuracy: 0.6530\n",
            "Epoch 48/200\n",
            "453/453 [==============================] - 64s 142ms/step - loss: 0.6693 - accuracy: 0.6249 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 49/200\n",
            "453/453 [==============================] - 64s 142ms/step - loss: 0.6770 - accuracy: 0.6133 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
            "Epoch 50/200\n",
            "453/453 [==============================] - 65s 144ms/step - loss: 0.6735 - accuracy: 0.6220 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 51/200\n",
            "453/453 [==============================] - 64s 142ms/step - loss: 0.6700 - accuracy: 0.6222 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
            "Epoch 52/200\n",
            "453/453 [==============================] - 64s 142ms/step - loss: 0.6707 - accuracy: 0.6241 - val_loss: 0.6640 - val_accuracy: 0.6530\n",
            "Epoch 53/200\n",
            "453/453 [==============================] - 65s 142ms/step - loss: 0.6761 - accuracy: 0.6183 - val_loss: 0.6499 - val_accuracy: 0.6530\n",
            "Epoch 54/200\n",
            "453/453 [==============================] - 64s 141ms/step - loss: 0.6702 - accuracy: 0.6271 - val_loss: 0.6473 - val_accuracy: 0.6530\n",
            "Epoch 55/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6713 - accuracy: 0.6247 - val_loss: 0.6475 - val_accuracy: 0.6530\n",
            "Epoch 56/200\n",
            "453/453 [==============================] - 64s 142ms/step - loss: 0.6716 - accuracy: 0.6272 - val_loss: 0.6486 - val_accuracy: 0.6530\n",
            "Epoch 57/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6693 - accuracy: 0.6196 - val_loss: 0.6807 - val_accuracy: 0.6530\n",
            "Epoch 58/200\n",
            "453/453 [==============================] - 64s 142ms/step - loss: 0.6729 - accuracy: 0.6189 - val_loss: 0.6489 - val_accuracy: 0.6530\n",
            "Epoch 59/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6697 - accuracy: 0.6215 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 60/200\n",
            "453/453 [==============================] - 65s 142ms/step - loss: 0.6711 - accuracy: 0.6248 - val_loss: 0.6475 - val_accuracy: 0.6530\n",
            "Epoch 61/200\n",
            "453/453 [==============================] - 64s 141ms/step - loss: 0.6683 - accuracy: 0.6260 - val_loss: 0.6485 - val_accuracy: 0.6530\n",
            "Epoch 62/200\n",
            "453/453 [==============================] - 63s 140ms/step - loss: 0.6690 - accuracy: 0.6245 - val_loss: 0.6588 - val_accuracy: 0.6530\n",
            "Epoch 63/200\n",
            "453/453 [==============================] - 65s 142ms/step - loss: 0.6695 - accuracy: 0.6280 - val_loss: 0.6512 - val_accuracy: 0.6530\n",
            "Epoch 64/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6720 - accuracy: 0.6221 - val_loss: 0.6475 - val_accuracy: 0.6530\n",
            "Epoch 65/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6693 - accuracy: 0.6243 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Epoch 66/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6712 - accuracy: 0.6219 - val_loss: 0.6553 - val_accuracy: 0.6530\n",
            "Epoch 67/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6704 - accuracy: 0.6209 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 68/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6720 - accuracy: 0.6211 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 69/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6690 - accuracy: 0.6241 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 70/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6726 - accuracy: 0.6154 - val_loss: 0.6528 - val_accuracy: 0.6530\n",
            "Epoch 71/200\n",
            "453/453 [==============================] - 66s 147ms/step - loss: 0.6707 - accuracy: 0.6227 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 72/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6732 - accuracy: 0.6220 - val_loss: 0.6523 - val_accuracy: 0.6530\n",
            "Epoch 73/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6737 - accuracy: 0.6166 - val_loss: 0.6753 - val_accuracy: 0.6530\n",
            "Epoch 74/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6741 - accuracy: 0.6202 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
            "Epoch 75/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6716 - accuracy: 0.6224 - val_loss: 0.6473 - val_accuracy: 0.6530\n",
            "Epoch 76/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6717 - accuracy: 0.6220 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 77/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6710 - accuracy: 0.6233 - val_loss: 0.6488 - val_accuracy: 0.6530\n",
            "Epoch 78/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6693 - accuracy: 0.6233 - val_loss: 0.6478 - val_accuracy: 0.6530\n",
            "Epoch 79/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6693 - accuracy: 0.6251 - val_loss: 0.6479 - val_accuracy: 0.6530\n",
            "Epoch 80/200\n",
            "453/453 [==============================] - 64s 142ms/step - loss: 0.6710 - accuracy: 0.6195 - val_loss: 0.7082 - val_accuracy: 0.6530\n",
            "Epoch 81/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6733 - accuracy: 0.6215 - val_loss: 0.6489 - val_accuracy: 0.6530\n",
            "Epoch 82/200\n",
            "453/453 [==============================] - 65s 144ms/step - loss: 0.6689 - accuracy: 0.6240 - val_loss: 0.6532 - val_accuracy: 0.6530\n",
            "Epoch 83/200\n",
            "453/453 [==============================] - 64s 142ms/step - loss: 0.6733 - accuracy: 0.6184 - val_loss: 0.6488 - val_accuracy: 0.6530\n",
            "Epoch 84/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6713 - accuracy: 0.6211 - val_loss: 0.6751 - val_accuracy: 0.6530\n",
            "Epoch 85/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6723 - accuracy: 0.6270 - val_loss: 0.6484 - val_accuracy: 0.6530\n",
            "Epoch 86/200\n",
            "453/453 [==============================] - 63s 140ms/step - loss: 0.6687 - accuracy: 0.6270 - val_loss: 0.6492 - val_accuracy: 0.6530\n",
            "Epoch 87/200\n",
            "453/453 [==============================] - 64s 141ms/step - loss: 0.6704 - accuracy: 0.6233 - val_loss: 0.6476 - val_accuracy: 0.6530\n",
            "Epoch 88/200\n",
            "453/453 [==============================] - 64s 142ms/step - loss: 0.6699 - accuracy: 0.6237 - val_loss: 0.6544 - val_accuracy: 0.6530\n",
            "Epoch 89/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6719 - accuracy: 0.6200 - val_loss: 0.6498 - val_accuracy: 0.6530\n",
            "Epoch 90/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6718 - accuracy: 0.6212 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 91/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6694 - accuracy: 0.6240 - val_loss: 0.7284 - val_accuracy: 0.3470\n",
            "Epoch 92/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6679 - accuracy: 0.6264 - val_loss: 0.6530 - val_accuracy: 0.6530\n",
            "Epoch 93/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6708 - accuracy: 0.6197 - val_loss: 0.6884 - val_accuracy: 0.6530\n",
            "Epoch 94/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6692 - accuracy: 0.6251 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
            "Epoch 95/200\n",
            "453/453 [==============================] - 64s 142ms/step - loss: 0.6696 - accuracy: 0.6206 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 96/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6707 - accuracy: 0.6227 - val_loss: 0.6517 - val_accuracy: 0.6530\n",
            "Epoch 97/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6716 - accuracy: 0.6195 - val_loss: 0.7177 - val_accuracy: 0.3470\n",
            "Epoch 98/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6760 - accuracy: 0.6151 - val_loss: 0.7082 - val_accuracy: 0.6530\n",
            "Epoch 99/200\n",
            "453/453 [==============================] - 65s 144ms/step - loss: 0.6706 - accuracy: 0.6202 - val_loss: 0.6662 - val_accuracy: 0.6530\n",
            "Epoch 100/200\n",
            "453/453 [==============================] - 65s 144ms/step - loss: 0.6699 - accuracy: 0.6210 - val_loss: 0.6477 - val_accuracy: 0.6530\n",
            "Epoch 101/200\n",
            "453/453 [==============================] - 65s 144ms/step - loss: 0.6721 - accuracy: 0.6222 - val_loss: 0.6482 - val_accuracy: 0.6530\n",
            "Epoch 102/200\n",
            "453/453 [==============================] - 65s 144ms/step - loss: 0.6709 - accuracy: 0.6240 - val_loss: 0.6520 - val_accuracy: 0.6530\n",
            "Epoch 103/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6696 - accuracy: 0.6259 - val_loss: 0.6578 - val_accuracy: 0.6530\n",
            "Epoch 104/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6710 - accuracy: 0.6224 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 105/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6706 - accuracy: 0.6244 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 106/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6717 - accuracy: 0.6213 - val_loss: 0.6621 - val_accuracy: 0.6530\n",
            "Epoch 107/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6717 - accuracy: 0.6228 - val_loss: 0.6882 - val_accuracy: 0.6530\n",
            "Epoch 108/200\n",
            "453/453 [==============================] - 66s 147ms/step - loss: 0.6726 - accuracy: 0.6186 - val_loss: 0.6491 - val_accuracy: 0.6530\n",
            "Epoch 109/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6713 - accuracy: 0.6219 - val_loss: 0.6478 - val_accuracy: 0.6530\n",
            "Epoch 110/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6691 - accuracy: 0.6253 - val_loss: 0.6648 - val_accuracy: 0.6530\n",
            "Epoch 111/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6691 - accuracy: 0.6271 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 112/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6683 - accuracy: 0.6249 - val_loss: 0.6776 - val_accuracy: 0.6530\n",
            "Epoch 113/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6709 - accuracy: 0.6231 - val_loss: 0.6485 - val_accuracy: 0.6530\n",
            "Epoch 114/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6750 - accuracy: 0.6188 - val_loss: 0.6570 - val_accuracy: 0.6530\n",
            "Epoch 115/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6692 - accuracy: 0.6236 - val_loss: 0.6561 - val_accuracy: 0.6530\n",
            "Epoch 116/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6683 - accuracy: 0.6275 - val_loss: 0.6595 - val_accuracy: 0.6530\n",
            "Epoch 117/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6673 - accuracy: 0.6280 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
            "Epoch 118/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6689 - accuracy: 0.6220 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
            "Epoch 119/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6698 - accuracy: 0.6222 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
            "Epoch 120/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6700 - accuracy: 0.6247 - val_loss: 0.6741 - val_accuracy: 0.6530\n",
            "Epoch 121/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6699 - accuracy: 0.6249 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 122/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6721 - accuracy: 0.6179 - val_loss: 0.6524 - val_accuracy: 0.6530\n",
            "Epoch 123/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6740 - accuracy: 0.6142 - val_loss: 0.6564 - val_accuracy: 0.6530\n",
            "Epoch 124/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6724 - accuracy: 0.6230 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 125/200\n",
            "453/453 [==============================] - 68s 149ms/step - loss: 0.6723 - accuracy: 0.6213 - val_loss: 0.6477 - val_accuracy: 0.6530\n",
            "Epoch 126/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6687 - accuracy: 0.6230 - val_loss: 0.6966 - val_accuracy: 0.3470\n",
            "Epoch 127/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6677 - accuracy: 0.6235 - val_loss: 0.6590 - val_accuracy: 0.6530\n",
            "Epoch 128/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6683 - accuracy: 0.6252 - val_loss: 0.6475 - val_accuracy: 0.6530\n",
            "Epoch 129/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6651 - accuracy: 0.6249 - val_loss: 0.6506 - val_accuracy: 0.6530\n",
            "Epoch 130/200\n",
            "453/453 [==============================] - 66s 147ms/step - loss: 0.6713 - accuracy: 0.6218 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 131/200\n",
            "453/453 [==============================] - 68s 150ms/step - loss: 0.6711 - accuracy: 0.6223 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 132/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6742 - accuracy: 0.6157 - val_loss: 0.6526 - val_accuracy: 0.6530\n",
            "Epoch 133/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6684 - accuracy: 0.6257 - val_loss: 0.6535 - val_accuracy: 0.6530\n",
            "Epoch 134/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6680 - accuracy: 0.6233 - val_loss: 0.6469 - val_accuracy: 0.6530\n",
            "Epoch 135/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6721 - accuracy: 0.6206 - val_loss: 0.6470 - val_accuracy: 0.6530\n",
            "Epoch 136/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6692 - accuracy: 0.6262 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 137/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6711 - accuracy: 0.6221 - val_loss: 0.6627 - val_accuracy: 0.6530\n",
            "Epoch 138/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6725 - accuracy: 0.6211 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 139/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6691 - accuracy: 0.6230 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 140/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6688 - accuracy: 0.6286 - val_loss: 0.6497 - val_accuracy: 0.6530\n",
            "Epoch 141/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6694 - accuracy: 0.6258 - val_loss: 0.6717 - val_accuracy: 0.6530\n",
            "Epoch 142/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6704 - accuracy: 0.6212 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 143/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6739 - accuracy: 0.6194 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 144/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6703 - accuracy: 0.6237 - val_loss: 0.6468 - val_accuracy: 0.6530\n",
            "Epoch 145/200\n",
            "453/453 [==============================] - 66s 147ms/step - loss: 0.6699 - accuracy: 0.6217 - val_loss: 0.6476 - val_accuracy: 0.6530\n",
            "Epoch 146/200\n",
            "453/453 [==============================] - 66s 147ms/step - loss: 0.6712 - accuracy: 0.6235 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 147/200\n",
            "453/453 [==============================] - 66s 147ms/step - loss: 0.6694 - accuracy: 0.6227 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
            "Epoch 148/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6678 - accuracy: 0.6246 - val_loss: 0.6513 - val_accuracy: 0.6530\n",
            "Epoch 149/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6724 - accuracy: 0.6175 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 150/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6686 - accuracy: 0.6234 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 151/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6696 - accuracy: 0.6211 - val_loss: 0.6472 - val_accuracy: 0.6530\n",
            "Epoch 152/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6708 - accuracy: 0.6229 - val_loss: 0.6593 - val_accuracy: 0.6530\n",
            "Epoch 153/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6701 - accuracy: 0.6264 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 154/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6683 - accuracy: 0.6280 - val_loss: 0.6553 - val_accuracy: 0.6530\n",
            "Epoch 155/200\n",
            "453/453 [==============================] - 66s 147ms/step - loss: 0.6702 - accuracy: 0.6234 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 156/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6711 - accuracy: 0.6193 - val_loss: 0.6952 - val_accuracy: 0.6530\n",
            "Epoch 157/200\n",
            "453/453 [==============================] - 65s 145ms/step - loss: 0.6715 - accuracy: 0.6222 - val_loss: 0.6628 - val_accuracy: 0.6530\n",
            "Epoch 158/200\n",
            "453/453 [==============================] - 66s 147ms/step - loss: 0.6688 - accuracy: 0.6213 - val_loss: 0.6567 - val_accuracy: 0.6530\n",
            "Epoch 159/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6722 - accuracy: 0.6234 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 160/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6721 - accuracy: 0.6224 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 161/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6696 - accuracy: 0.6192 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 162/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6682 - accuracy: 0.6256 - val_loss: 0.6510 - val_accuracy: 0.6530\n",
            "Epoch 163/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6750 - accuracy: 0.6179 - val_loss: 0.6470 - val_accuracy: 0.6530\n",
            "Epoch 164/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6676 - accuracy: 0.6262 - val_loss: 0.6469 - val_accuracy: 0.6530\n",
            "Epoch 165/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6686 - accuracy: 0.6257 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 166/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6693 - accuracy: 0.6277 - val_loss: 0.6549 - val_accuracy: 0.6530\n",
            "Epoch 167/200\n",
            "453/453 [==============================] - 66s 147ms/step - loss: 0.6736 - accuracy: 0.6193 - val_loss: 0.6505 - val_accuracy: 0.6530\n",
            "Epoch 168/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6725 - accuracy: 0.6176 - val_loss: 0.6558 - val_accuracy: 0.6530\n",
            "Epoch 169/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6691 - accuracy: 0.6235 - val_loss: 0.6545 - val_accuracy: 0.6530\n",
            "Epoch 170/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6697 - accuracy: 0.6222 - val_loss: 0.6493 - val_accuracy: 0.6530\n",
            "Epoch 171/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6706 - accuracy: 0.6240 - val_loss: 0.6545 - val_accuracy: 0.6530\n",
            "Epoch 172/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6736 - accuracy: 0.6195 - val_loss: 0.6514 - val_accuracy: 0.6530\n",
            "Epoch 173/200\n",
            "453/453 [==============================] - 69s 152ms/step - loss: 0.6731 - accuracy: 0.6199 - val_loss: 0.6470 - val_accuracy: 0.6530\n",
            "Epoch 174/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6687 - accuracy: 0.6220 - val_loss: 0.6541 - val_accuracy: 0.6530\n",
            "Epoch 175/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6740 - accuracy: 0.6183 - val_loss: 0.6510 - val_accuracy: 0.6530\n",
            "Epoch 176/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6674 - accuracy: 0.6240 - val_loss: 0.6760 - val_accuracy: 0.6530\n",
            "Epoch 177/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6694 - accuracy: 0.6246 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 178/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6678 - accuracy: 0.6295 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
            "Epoch 179/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6685 - accuracy: 0.6184 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 180/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6680 - accuracy: 0.6252 - val_loss: 0.6499 - val_accuracy: 0.6530\n",
            "Epoch 181/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6716 - accuracy: 0.6229 - val_loss: 0.6504 - val_accuracy: 0.6530\n",
            "Epoch 182/200\n",
            "453/453 [==============================] - 65s 145ms/step - loss: 0.6702 - accuracy: 0.6205 - val_loss: 0.6498 - val_accuracy: 0.6530\n",
            "Epoch 183/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6676 - accuracy: 0.6247 - val_loss: 0.6540 - val_accuracy: 0.6530\n",
            "Epoch 184/200\n",
            "453/453 [==============================] - 66s 145ms/step - loss: 0.6718 - accuracy: 0.6198 - val_loss: 0.6477 - val_accuracy: 0.6530\n",
            "Epoch 185/200\n",
            "453/453 [==============================] - 65s 144ms/step - loss: 0.6687 - accuracy: 0.6271 - val_loss: 0.6591 - val_accuracy: 0.6530\n",
            "Epoch 186/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6714 - accuracy: 0.6206 - val_loss: 0.6591 - val_accuracy: 0.6530\n",
            "Epoch 187/200\n",
            "453/453 [==============================] - 67s 147ms/step - loss: 0.6724 - accuracy: 0.6215 - val_loss: 0.6857 - val_accuracy: 0.6530\n",
            "Epoch 188/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6693 - accuracy: 0.6244 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
            "Epoch 189/200\n",
            "453/453 [==============================] - 67s 149ms/step - loss: 0.6697 - accuracy: 0.6220 - val_loss: 0.6592 - val_accuracy: 0.6530\n",
            "Epoch 190/200\n",
            "453/453 [==============================] - 66s 146ms/step - loss: 0.6691 - accuracy: 0.6240 - val_loss: 0.6513 - val_accuracy: 0.6530\n",
            "Epoch 191/200\n",
            "453/453 [==============================] - 67s 148ms/step - loss: 0.6723 - accuracy: 0.6185 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
            "Epoch 192/200\n",
            "453/453 [==============================] - 65s 144ms/step - loss: 0.6684 - accuracy: 0.6293 - val_loss: 0.6533 - val_accuracy: 0.6530\n",
            "Epoch 193/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6695 - accuracy: 0.6214 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
            "Epoch 194/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6696 - accuracy: 0.6234 - val_loss: 0.6623 - val_accuracy: 0.6530\n",
            "Epoch 195/200\n",
            "453/453 [==============================] - 65s 143ms/step - loss: 0.6717 - accuracy: 0.6216 - val_loss: 0.6474 - val_accuracy: 0.6530\n",
            "Epoch 196/200\n",
            "453/453 [==============================] - 64s 141ms/step - loss: 0.6709 - accuracy: 0.6240 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
            "Epoch 197/200\n",
            "453/453 [==============================] - 63s 139ms/step - loss: 0.6714 - accuracy: 0.6221 - val_loss: 0.6470 - val_accuracy: 0.6530\n",
            "Epoch 198/200\n",
            "453/453 [==============================] - 62s 137ms/step - loss: 0.6745 - accuracy: 0.6196 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
            "Epoch 199/200\n",
            "453/453 [==============================] - 62s 137ms/step - loss: 0.6668 - accuracy: 0.6269 - val_loss: 0.6491 - val_accuracy: 0.6530\n",
            "Epoch 200/200\n",
            "453/453 [==============================] - 62s 137ms/step - loss: 0.6720 - accuracy: 0.6261 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
            "Fold 1, 200 epochs, 13161 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 65.30%\n",
            "\n",
            "1-way Cross Validation mean 65.30% (+/- 0.00%)\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4fh2GI8beMQ"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}