{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojm_6E9f9Kcf"
   },
   "source": [
    "# MLP 212\n",
    "* Operate on 16000 GenCode 34 seqs.\n",
    "* 5-way cross validation. Save best model per CV.\n",
    "* Report mean accuracy from final re-validation with best 5.\n",
    "* Use Adam with a learn rate decay schdule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "hh6XplUvC0j0",
    "outputId": "55ec2c95-de63-45af-80b2-0d65fe573ec9"
   },
   "outputs": [],
   "source": [
    "NC_FILENAME='ncRNA.gc34.processed.fasta'\n",
    "PC_FILENAME='pcRNA.gc34.processed.fasta'\n",
    "DATAPATH=\"\"\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    PATH='/content/drive/'\n",
    "    drive.mount(PATH)\n",
    "    DATAPATH=PATH+'My Drive/data/'  # must end in \"/\"\n",
    "    NC_FILENAME = DATAPATH+NC_FILENAME\n",
    "    PC_FILENAME = DATAPATH+PC_FILENAME\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    DATAPATH=\"\" \n",
    "\n",
    "EPOCHS=200\n",
    "SPLITS=5\n",
    "K=1\n",
    "VOCABULARY_SIZE=4**K+1   # e.g. K=3 => 64 DNA K-mers + 'NNN'\n",
    "EMBED_DIMEN=16\n",
    "FILENAME='MLP212'\n",
    "NEURONS=16  # with 50% dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VQY7aTj29Kch"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LayerNormalization\n",
    "import time\n",
    "dt='float32'\n",
    "tf.keras.backend.set_floatx(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7jcg6Wl9Kc2"
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qLFNO1Xa9Kc3"
   },
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    adam_default_learn_rate = 0.001\n",
    "    schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate = adam_default_learn_rate*10,\n",
    "        #decay_steps=100000, decay_rate=0.96, staircase=True)\n",
    "        decay_steps=10000, decay_rate=0.99, staircase=True)\n",
    "    # learn rate = initial_learning_rate * decay_rate ^ (step / decay_steps)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=schedule)\n",
    "    bc=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    print(\"COMPILE...\")\n",
    "    model.compile(loss=bc, optimizer=opt, metrics=[\"accuracy\"])\n",
    "    print(\"...COMPILED\")\n",
    "    return model\n",
    "\n",
    "def build_model(maxlen):\n",
    "    act=\"elu\"\n",
    "    embed_layer  = keras.layers.Embedding(\n",
    "        input_dim=VOCABULARY_SIZE, output_dim=EMBED_DIMEN)\n",
    "    dense1_layer = keras.layers.Dense(NEURONS, activation=act,dtype=dt,\n",
    "                    input_dim=[maxlen,EMBED_DIMEN])  # match embed output\n",
    "    drop1_layer = keras.layers.Dropout(0.5)\n",
    "    dense2_layer = keras.layers.Dense(NEURONS, activation=act,dtype=dt)\n",
    "    drop2_layer = keras.layers.Dropout(0.5)\n",
    "    #dense3_layer = keras.layers.Dense(NEURONS, activation=act,dtype=dt)\n",
    "    output_layer = keras.layers.Dense(1,  activation=\"sigmoid\",dtype=dt)\n",
    "    mlp = keras.models.Sequential()\n",
    "    mlp.add(embed_layer)\n",
    "    mlp.add(dense1_layer)\n",
    "    mlp.add(drop1_layer)\n",
    "    mlp.add(dense2_layer)\n",
    "    mlp.add(drop2_layer)\n",
    "    #mlp.add(dense3_layer)\n",
    "    mlp.add(output_layer)\n",
    "    mlpc = compile_model(mlp)\n",
    "    return mlpc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WV6k-xOm9Kcn"
   },
   "source": [
    "## Load and partition sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1I-O_qzw9Kco"
   },
   "outputs": [],
   "source": [
    "# Assume file was preprocessed to contain one line per seq.\n",
    "# Prefer Pandas dataframe but df does not support append.\n",
    "# For conversion to tensor, must avoid python lists.\n",
    "def load_fasta(filename,label):\n",
    "    DEFLINE='>'\n",
    "    labels=[]\n",
    "    seqs=[]\n",
    "    lens=[]\n",
    "    nums=[]\n",
    "    num=0\n",
    "    with open (filename,'r') as infile:\n",
    "        for line in infile:\n",
    "            if line[0]!=DEFLINE:\n",
    "                seq=line.rstrip()\n",
    "                num += 1   # first seqnum is 1\n",
    "                seqlen=len(seq)\n",
    "                nums.append(num)\n",
    "                labels.append(label)\n",
    "                seqs.append(seq)\n",
    "                lens.append(seqlen)\n",
    "    df1=pd.DataFrame(nums,columns=['seqnum'])\n",
    "    df2=pd.DataFrame(labels,columns=['class'])\n",
    "    df3=pd.DataFrame(seqs,columns=['sequence'])\n",
    "    df4=pd.DataFrame(lens,columns=['seqlen'])\n",
    "    df=pd.concat((df1,df2,df3,df4),axis=1)\n",
    "    return df\n",
    "\n",
    "def separate_X_and_y(data):\n",
    "    y=   data[['class']].copy()\n",
    "    X=   data.drop(columns=['class','seqnum','seqlen'])\n",
    "    return (X,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRAaO9jP9Kcr"
   },
   "source": [
    "## Make K-mers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "e8xcZ4Mr9Kcs"
   },
   "outputs": [],
   "source": [
    "def make_kmer_table(K):\n",
    "    npad='N'*K\n",
    "    shorter_kmers=['']\n",
    "    for i in range(K):\n",
    "        longer_kmers=[]\n",
    "        for mer in shorter_kmers:\n",
    "            longer_kmers.append(mer+'A')\n",
    "            longer_kmers.append(mer+'C')\n",
    "            longer_kmers.append(mer+'G')\n",
    "            longer_kmers.append(mer+'T')\n",
    "        shorter_kmers = longer_kmers\n",
    "    all_kmers = shorter_kmers\n",
    "    kmer_dict = {}\n",
    "    kmer_dict[npad]=0\n",
    "    value=1\n",
    "    for mer in all_kmers:\n",
    "        kmer_dict[mer]=value\n",
    "        value += 1\n",
    "    return kmer_dict\n",
    "\n",
    "KMER_TABLE=make_kmer_table(K)\n",
    "\n",
    "def strings_to_vectors(data,uniform_len):\n",
    "    all_seqs=[]\n",
    "    for seq in data['sequence']:\n",
    "        i=0\n",
    "        seqlen=len(seq)\n",
    "        kmers=[]\n",
    "        while i < seqlen-K+1 -1:  # stop at minus one for spaced seed\n",
    "            #kmer=seq[i:i+2]+seq[i+3:i+5]    # SPACED SEED 2/1/2 for K=4\n",
    "            kmer=seq[i:i+K]  \n",
    "            i += 1\n",
    "            value=KMER_TABLE[kmer]\n",
    "            kmers.append(value)\n",
    "        pad_val=0\n",
    "        while i < uniform_len:\n",
    "            kmers.append(pad_val)\n",
    "            i += 1\n",
    "        all_seqs.append(kmers)\n",
    "    pd2d=pd.DataFrame(all_seqs)\n",
    "    return pd2d   # return 2D dataframe, uniform dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sEtA0xiV9Kcv"
   },
   "outputs": [],
   "source": [
    "def make_kmers(MAXLEN,train_set):\n",
    "    (X_train_all,y_train_all)=separate_X_and_y(train_set)\n",
    "    X_train_kmers=strings_to_vectors(X_train_all,MAXLEN)\n",
    "    # From pandas dataframe to numpy to list to numpy\n",
    "    num_seqs=len(X_train_kmers)\n",
    "    tmp_seqs=[]\n",
    "    for i in range(num_seqs):\n",
    "        kmer_sequence=X_train_kmers.iloc[i]\n",
    "        tmp_seqs.append(kmer_sequence)\n",
    "    X_train_kmers=np.array(tmp_seqs)\n",
    "    tmp_seqs=None\n",
    "    labels=y_train_all.to_numpy()\n",
    "    return (X_train_kmers,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jaXyySyO9Kcz"
   },
   "outputs": [],
   "source": [
    "def make_frequencies(Xin):\n",
    "    Xout=[]\n",
    "    VOCABULARY_SIZE= 4**K + 1  # plus one for 'NNN'\n",
    "    for seq in Xin:\n",
    "        freqs =[0] * VOCABULARY_SIZE\n",
    "        total = 0\n",
    "        for kmerval in seq:\n",
    "            freqs[kmerval] += 1\n",
    "            total += 1\n",
    "        for c in range(VOCABULARY_SIZE):\n",
    "            freqs[c] = freqs[c]/total\n",
    "        Xout.append(freqs)\n",
    "    Xnum = np.asarray(Xout)\n",
    "    return (Xnum)\n",
    "def make_slice(data_set,min_len,max_len):\n",
    "    slice = data_set.query('seqlen <= '+str(max_len)+' & seqlen>= '+str(min_len))\n",
    "    return slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdIS2utq9Kc9"
   },
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BVo4tbB_9Kc-"
   },
   "outputs": [],
   "source": [
    "def do_cross_validation(X,y,given_model):\n",
    "    cv_scores = []\n",
    "    fold=0\n",
    "    splitter = ShuffleSplit(n_splits=SPLITS, test_size=0.1, random_state=37863)\n",
    "    for train_index,valid_index in splitter.split(X):\n",
    "        fold += 1\n",
    "        X_train=X[train_index] # use iloc[] for dataframe\n",
    "        y_train=y[train_index]\n",
    "        X_valid=X[valid_index]\n",
    "        y_valid=y[valid_index]        \n",
    "        # Avoid continually improving the same model.\n",
    "        model = compile_model(keras.models.clone_model(given_model))\n",
    "        bestname=DATAPATH+FILENAME+\".cv.\"+str(fold)+\".best\"\n",
    "        mycallbacks = [keras.callbacks.ModelCheckpoint(\n",
    "            filepath=bestname, save_best_only=True, \n",
    "            monitor='val_accuracy', mode='max')]   \n",
    "        print(\"FIT\")\n",
    "        start_time=time.time()\n",
    "        history=model.fit(X_train, y_train, # batch_size=10, default=32 works nicely\n",
    "                epochs=EPOCHS, verbose=1,  # verbose=1 for ascii art, verbose=0 for none\n",
    "                callbacks=mycallbacks,\n",
    "                validation_data=(X_valid,y_valid) )\n",
    "        end_time=time.time()\n",
    "        elapsed_time=(end_time-start_time)                        \n",
    "        print(\"Fold %d, %d epochs, %d sec\"%(fold,EPOCHS,elapsed_time))\n",
    "        pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "        plt.grid(True)\n",
    "        plt.gca().set_ylim(0,1)\n",
    "        plt.show()\n",
    "        best_model=keras.models.load_model(bestname)\n",
    "        scores = best_model.evaluate(X_valid, y_valid, verbose=0)\n",
    "        print(\"%s: %.2f%%\" % (best_model.metrics_names[1], scores[1]*100))\n",
    "        cv_scores.append(scores[1] * 100)  \n",
    "    print()\n",
    "    print(\"%d-way Cross Validation mean %.2f%% (+/- %.2f%%)\" % (fold, np.mean(cv_scores), np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd3Wj_vI9KdP"
   },
   "source": [
    "## Train on RNA lengths 200-1Kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "f8fNo6sn9KdH",
    "outputId": "3ae85214-8d37-402b-bf0d-095ae907f4e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from files.\n",
      "Ready: train_set\n",
      "Compile the model\n",
      "COMPILE...\n",
      "...COMPILED\n",
      "Summarize the model\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          80        \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 16)          272       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 16)          272       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, None, 1)           17        \n",
      "=================================================================\n",
      "Total params: 641\n",
      "Trainable params: 641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: MLP212.model/assets\n",
      "Data prep\n",
      "Data reshape\n"
     ]
    }
   ],
   "source": [
    "MINLEN=200\n",
    "MAXLEN=1000\n",
    "print(\"Load data from files.\")\n",
    "nc_seq=load_fasta(NC_FILENAME,0)\n",
    "pc_seq=load_fasta(PC_FILENAME,1)\n",
    "train_set=pd.concat((nc_seq,pc_seq),axis=0)\n",
    "nc_seq=None\n",
    "pc_seq=None\n",
    "print(\"Ready: train_set\")\n",
    "#train_set\n",
    "print (\"Compile the model\")\n",
    "model=build_model(MAXLEN)\n",
    "print (\"Summarize the model\")\n",
    "print(model.summary())  # Print this only once\n",
    "model.save(DATAPATH+FILENAME+'.model')\n",
    "print (\"Data prep\")\n",
    "subset=make_slice(train_set,MINLEN,MAXLEN)# One array to two: X and y\n",
    "print (\"Data reshape\")\n",
    "(X_train,y_train)=make_kmers(MAXLEN,subset)\n",
    "X_train=make_frequencies(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mQ8eW5Rg9KdQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross valiation\n",
      "COMPILE...\n",
      "...COMPILED\n",
      "FIT\n",
      "Epoch 1/200\n",
      "434/453 [===========================>..] - ETA: 0s - loss: 0.6565 - accuracy: 0.6394INFO:tensorflow:Assets written to: MLP212.cv.1.best/assets\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6566 - accuracy: 0.6396 - val_loss: 0.6470 - val_accuracy: 0.6530\n",
      "Epoch 2/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6543 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 3/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
      "Epoch 4/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
      "Epoch 5/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
      "Epoch 6/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
      "Epoch 7/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6470 - val_accuracy: 0.6530\n",
      "Epoch 8/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
      "Epoch 9/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
      "Epoch 10/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 11/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
      "Epoch 12/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
      "Epoch 13/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
      "Epoch 14/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6544 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
      "Epoch 15/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
      "Epoch 16/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6543 - accuracy: 0.6397 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
      "Epoch 17/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
      "Epoch 18/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
      "Epoch 19/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
      "Epoch 20/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
      "Epoch 21/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6393 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
      "Epoch 22/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
      "Epoch 23/200\n",
      "453/453 [==============================] - ETA: 0s - loss: 0.6538 - accuracy: 0.63 - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 24/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6480 - val_accuracy: 0.6530\n",
      "Epoch 25/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
      "Epoch 26/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
      "Epoch 27/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
      "Epoch 28/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6547 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 29/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
      "Epoch 30/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 31/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
      "Epoch 32/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
      "Epoch 33/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 34/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6544 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
      "Epoch 35/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
      "Epoch 36/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
      "Epoch 37/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
      "Epoch 38/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 39/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6397 - val_loss: 0.6469 - val_accuracy: 0.6530\n",
      "Epoch 40/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 41/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 42/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 43/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6396 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
      "Epoch 44/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
      "Epoch 45/200\n",
      "453/453 [==============================] - 2s 3ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 46/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 47/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 48/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
      "Epoch 49/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 50/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6395 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
      "Epoch 51/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 52/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6469 - val_accuracy: 0.6530\n",
      "Epoch 53/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
      "Epoch 54/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 55/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6475 - val_accuracy: 0.6530\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6544 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 57/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 58/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
      "Epoch 59/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 60/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6544 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 61/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
      "Epoch 62/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6547 - accuracy: 0.6397 - val_loss: 0.6483 - val_accuracy: 0.6530\n",
      "Epoch 63/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6490 - val_accuracy: 0.6530\n",
      "Epoch 64/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 65/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
      "Epoch 66/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 67/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 68/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6472 - val_accuracy: 0.6530\n",
      "Epoch 69/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
      "Epoch 70/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 71/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6544 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
      "Epoch 72/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
      "Epoch 73/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
      "Epoch 74/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 75/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 76/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 77/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
      "Epoch 78/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6546 - accuracy: 0.6395 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 79/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 80/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 81/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6471 - val_accuracy: 0.6530\n",
      "Epoch 82/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6543 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
      "Epoch 83/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 84/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
      "Epoch 85/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
      "Epoch 86/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6547 - accuracy: 0.6397 - val_loss: 0.6469 - val_accuracy: 0.6530\n",
      "Epoch 87/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
      "Epoch 88/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
      "Epoch 89/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 90/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6468 - val_accuracy: 0.6530\n",
      "Epoch 91/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
      "Epoch 92/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6395 - val_loss: 0.6603 - val_accuracy: 0.6530\n",
      "Epoch 93/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6543 - accuracy: 0.6395 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 94/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6470 - val_accuracy: 0.6530\n",
      "Epoch 95/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
      "Epoch 96/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
      "Epoch 97/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
      "Epoch 98/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
      "Epoch 99/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 100/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6548 - accuracy: 0.6391 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
      "Epoch 101/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
      "Epoch 102/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
      "Epoch 103/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
      "Epoch 104/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
      "Epoch 105/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 106/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
      "Epoch 107/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6546 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 108/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
      "Epoch 109/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
      "Epoch 110/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
      "Epoch 111/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 112/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6500 - val_accuracy: 0.6530\n",
      "Epoch 114/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6469 - val_accuracy: 0.6530\n",
      "Epoch 115/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 116/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6543 - accuracy: 0.6397 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
      "Epoch 117/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6543 - accuracy: 0.6396 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
      "Epoch 118/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
      "Epoch 119/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
      "Epoch 120/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6395 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
      "Epoch 121/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 122/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 123/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6469 - val_accuracy: 0.6530\n",
      "Epoch 124/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
      "Epoch 125/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 126/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6396 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 127/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 128/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 129/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
      "Epoch 130/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6479 - val_accuracy: 0.6530\n",
      "Epoch 131/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 132/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 133/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
      "Epoch 134/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
      "Epoch 135/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6546 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 136/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6471 - val_accuracy: 0.6530\n",
      "Epoch 137/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
      "Epoch 138/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
      "Epoch 139/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
      "Epoch 140/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
      "Epoch 141/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6543 - accuracy: 0.6397 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
      "Epoch 142/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
      "Epoch 143/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
      "Epoch 144/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
      "Epoch 145/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6478 - val_accuracy: 0.6530\n",
      "Epoch 146/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 147/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6469 - val_accuracy: 0.6530\n",
      "Epoch 148/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
      "Epoch 149/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
      "Epoch 150/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6544 - accuracy: 0.6396 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
      "Epoch 151/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6474 - val_accuracy: 0.6530\n",
      "Epoch 152/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6463 - val_accuracy: 0.6530\n",
      "Epoch 153/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
      "Epoch 154/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 155/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
      "Epoch 156/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
      "Epoch 157/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6551 - accuracy: 0.6392 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 158/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6543 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
      "Epoch 159/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 160/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 161/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6395 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
      "Epoch 162/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 163/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6475 - val_accuracy: 0.6530\n",
      "Epoch 164/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
      "Epoch 165/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6544 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
      "Epoch 166/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 167/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6482 - val_accuracy: 0.6530\n",
      "Epoch 168/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6479 - val_accuracy: 0.6530\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 170/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
      "Epoch 171/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6544 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 172/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6457 - val_accuracy: 0.6530\n",
      "Epoch 173/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 174/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
      "Epoch 175/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 176/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6471 - val_accuracy: 0.6530\n",
      "Epoch 177/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 178/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
      "Epoch 179/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 180/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6458 - val_accuracy: 0.6530\n",
      "Epoch 181/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6467 - val_accuracy: 0.6530\n",
      "Epoch 182/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
      "Epoch 183/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
      "Epoch 184/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 185/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
      "Epoch 186/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
      "Epoch 187/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6459 - val_accuracy: 0.6530\n",
      "Epoch 188/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6397 - val_loss: 0.6460 - val_accuracy: 0.6530\n",
      "Epoch 189/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6461 - val_accuracy: 0.6530\n",
      "Epoch 190/200\n",
      "453/453 [==============================] - 2s 3ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6466 - val_accuracy: 0.6530\n",
      "Epoch 191/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 192/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6543 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
      "Epoch 193/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6542 - accuracy: 0.6395 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 194/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6397 - val_loss: 0.6465 - val_accuracy: 0.6530\n",
      "Epoch 195/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
      "Epoch 196/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6397 - val_loss: 0.6462 - val_accuracy: 0.6530\n",
      "Epoch 197/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6474 - val_accuracy: 0.6530\n",
      "Epoch 198/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6397 - val_loss: 0.6456 - val_accuracy: 0.6530\n",
      "Epoch 199/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6542 - accuracy: 0.6397 - val_loss: 0.6464 - val_accuracy: 0.6530\n",
      "Epoch 200/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6396 - val_loss: 0.6474 - val_accuracy: 0.6530\n",
      "Fold 1, 200 epochs, 221 sec\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 65.30%\n",
      "COMPILE...\n",
      "...COMPILED\n",
      "FIT\n",
      "Epoch 1/200\n",
      "429/453 [===========================>..] - ETA: 0s - loss: 0.6556 - accuracy: 0.6414INFO:tensorflow:Assets written to: MLP212.cv.2.best/assets\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 0.6560 - accuracy: 0.6404 - val_loss: 0.6514 - val_accuracy: 0.6449\n",
      "Epoch 2/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 3/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 4/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 5/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6517 - val_accuracy: 0.6449\n",
      "Epoch 6/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6406 - val_loss: 0.6513 - val_accuracy: 0.6449\n",
      "Epoch 7/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 8/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 9/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6516 - val_accuracy: 0.6449\n",
      "Epoch 10/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
      "Epoch 11/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
      "Epoch 12/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 13/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 14/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
      "Epoch 15/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6522 - val_accuracy: 0.6449\n",
      "Epoch 16/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6406 - val_loss: 0.6513 - val_accuracy: 0.6449\n",
      "Epoch 17/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 18/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
      "Epoch 19/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
      "Epoch 20/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
      "Epoch 21/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6405 - val_loss: 0.6532 - val_accuracy: 0.6449\n",
      "Epoch 22/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 23/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6404 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 24/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6517 - val_accuracy: 0.6449\n",
      "Epoch 25/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 26/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
      "Epoch 27/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
      "Epoch 28/200\n",
      "453/453 [==============================] - 2s 3ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
      "Epoch 29/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6406 - val_loss: 0.6513 - val_accuracy: 0.6449\n",
      "Epoch 30/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 31/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 32/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6513 - val_accuracy: 0.6449\n",
      "Epoch 33/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6542 - accuracy: 0.6402 - val_loss: 0.6518 - val_accuracy: 0.6449\n",
      "Epoch 34/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
      "Epoch 35/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 36/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 37/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6524 - val_accuracy: 0.6449\n",
      "Epoch 38/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6519 - val_accuracy: 0.6449\n",
      "Epoch 39/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6405 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 40/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6402 - val_loss: 0.6527 - val_accuracy: 0.6449\n",
      "Epoch 41/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6513 - val_accuracy: 0.6449\n",
      "Epoch 42/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 43/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6406 - val_loss: 0.6523 - val_accuracy: 0.6449\n",
      "Epoch 44/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
      "Epoch 45/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 46/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6405 - val_loss: 0.6523 - val_accuracy: 0.6449\n",
      "Epoch 47/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 48/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 49/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 50/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 51/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6405 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 52/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 53/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 54/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6514 - val_accuracy: 0.6449\n",
      "Epoch 55/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6520 - val_accuracy: 0.6449\n",
      "Epoch 57/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6405 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
      "Epoch 58/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6406 - val_loss: 0.6513 - val_accuracy: 0.6449\n",
      "Epoch 59/200\n",
      "453/453 [==============================] - 2s 3ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 60/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 61/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 62/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6403 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
      "Epoch 63/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 64/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
      "Epoch 65/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 66/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 67/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
      "Epoch 68/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 69/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
      "Epoch 70/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 71/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 72/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 73/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 74/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 75/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
      "Epoch 76/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6547 - accuracy: 0.6404 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 77/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
      "Epoch 78/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 79/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 80/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 81/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6513 - val_accuracy: 0.6449\n",
      "Epoch 82/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
      "Epoch 83/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 84/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6527 - val_accuracy: 0.6449\n",
      "Epoch 85/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6403 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 86/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
      "Epoch 87/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
      "Epoch 88/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6516 - val_accuracy: 0.6449\n",
      "Epoch 89/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 90/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
      "Epoch 91/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 92/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 93/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 94/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 95/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
      "Epoch 96/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6538 - val_accuracy: 0.6449\n",
      "Epoch 97/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6404 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 98/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 99/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 100/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 101/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 102/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 103/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6529 - val_accuracy: 0.6449\n",
      "Epoch 104/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 105/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6403 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 106/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 107/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 108/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 109/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 110/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 111/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 112/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6404 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 114/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 115/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
      "Epoch 116/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
      "Epoch 117/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
      "Epoch 118/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 119/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 120/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6402 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 121/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 122/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6514 - val_accuracy: 0.6449\n",
      "Epoch 123/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 124/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
      "Epoch 125/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 126/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
      "Epoch 127/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 128/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6516 - val_accuracy: 0.6449\n",
      "Epoch 129/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
      "Epoch 130/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6405 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 131/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 132/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
      "Epoch 133/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
      "Epoch 134/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6406 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
      "Epoch 135/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 136/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 137/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 138/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 139/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 140/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
      "Epoch 141/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6517 - val_accuracy: 0.6449\n",
      "Epoch 142/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6514 - val_accuracy: 0.6449\n",
      "Epoch 143/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 144/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6405 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 145/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
      "Epoch 146/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
      "Epoch 147/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
      "Epoch 148/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 149/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 150/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6404 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 151/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 152/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 153/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 154/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6523 - val_accuracy: 0.6449\n",
      "Epoch 155/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 156/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 157/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 158/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 159/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 160/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6404 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
      "Epoch 161/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 162/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6507 - val_accuracy: 0.6449\n",
      "Epoch 163/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 164/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6399 - val_loss: 0.6513 - val_accuracy: 0.6449\n",
      "Epoch 165/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 166/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
      "Epoch 167/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 168/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 170/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 171/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6511 - val_accuracy: 0.6449\n",
      "Epoch 172/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
      "Epoch 173/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 174/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 175/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6510 - val_accuracy: 0.6449\n",
      "Epoch 176/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6405 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 177/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 178/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 179/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6513 - val_accuracy: 0.6449\n",
      "Epoch 180/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 181/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 182/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
      "Epoch 183/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 184/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
      "Epoch 185/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 186/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 187/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 188/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 189/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 190/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 191/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6509 - val_accuracy: 0.6449\n",
      "Epoch 192/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 193/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6512 - val_accuracy: 0.6449\n",
      "Epoch 194/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 195/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6542 - accuracy: 0.6403 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Epoch 196/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
      "Epoch 197/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6406 - val_loss: 0.6508 - val_accuracy: 0.6449\n",
      "Epoch 198/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6406 - val_loss: 0.6505 - val_accuracy: 0.6449\n",
      "Epoch 199/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6406 - val_loss: 0.6513 - val_accuracy: 0.6449\n",
      "Epoch 200/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6406 - val_loss: 0.6506 - val_accuracy: 0.6449\n",
      "Fold 2, 200 epochs, 228 sec\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 64.49%\n",
      "COMPILE...\n",
      "...COMPILED\n",
      "FIT\n",
      "Epoch 1/200\n",
      "439/453 [============================>.] - ETA: 0s - loss: 0.6540 - accuracy: 0.6433INFO:tensorflow:Assets written to: MLP212.cv.3.best/assets\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6545 - accuracy: 0.6423 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 2/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6529 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
      "Epoch 3/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
      "Epoch 4/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6622 - val_accuracy: 0.6263\n",
      "Epoch 5/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
      "Epoch 6/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6621 - val_accuracy: 0.6263\n",
      "Epoch 7/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
      "Epoch 8/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 9/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
      "Epoch 10/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6525 - accuracy: 0.6426 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
      "Epoch 11/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6527 - accuracy: 0.6414 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
      "Epoch 12/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 13/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6616 - val_accuracy: 0.6263\n",
      "Epoch 14/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 15/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6616 - val_accuracy: 0.6263\n",
      "Epoch 16/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6628 - val_accuracy: 0.6263\n",
      "Epoch 17/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 18/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 19/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6527 - accuracy: 0.6427 - val_loss: 0.6651 - val_accuracy: 0.6263\n",
      "Epoch 20/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6621 - val_accuracy: 0.6263\n",
      "Epoch 21/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6618 - val_accuracy: 0.6263\n",
      "Epoch 22/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
      "Epoch 23/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6525 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
      "Epoch 24/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6621 - val_accuracy: 0.6263\n",
      "Epoch 25/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6700 - val_accuracy: 0.6263\n",
      "Epoch 26/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6624 - val_accuracy: 0.6263\n",
      "Epoch 27/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6621 - val_accuracy: 0.6263\n",
      "Epoch 28/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6426 - val_loss: 0.6627 - val_accuracy: 0.6263\n",
      "Epoch 29/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
      "Epoch 30/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6525 - accuracy: 0.6427 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
      "Epoch 31/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6633 - val_accuracy: 0.6263\n",
      "Epoch 32/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
      "Epoch 33/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 34/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6526 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
      "Epoch 35/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6623 - val_accuracy: 0.6263\n",
      "Epoch 36/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6618 - val_accuracy: 0.6263\n",
      "Epoch 37/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6526 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 38/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6427 - val_loss: 0.6616 - val_accuracy: 0.6263\n",
      "Epoch 39/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6652 - val_accuracy: 0.6263\n",
      "Epoch 40/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
      "Epoch 41/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6623 - val_accuracy: 0.6263\n",
      "Epoch 42/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
      "Epoch 43/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 44/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 45/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 46/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
      "Epoch 47/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6524 - accuracy: 0.6425 - val_loss: 0.6638 - val_accuracy: 0.6263\n",
      "Epoch 48/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6527 - accuracy: 0.6426 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
      "Epoch 49/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
      "Epoch 50/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6625 - val_accuracy: 0.6263\n",
      "Epoch 51/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 52/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6527 - accuracy: 0.6427 - val_loss: 0.6641 - val_accuracy: 0.6263\n",
      "Epoch 53/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
      "Epoch 54/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
      "Epoch 55/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
      "Epoch 57/200\n",
      "453/453 [==============================] - 2s 3ms/step - loss: 0.6533 - accuracy: 0.6426 - val_loss: 0.6658 - val_accuracy: 0.6263\n",
      "Epoch 58/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
      "Epoch 59/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
      "Epoch 60/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
      "Epoch 61/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
      "Epoch 62/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
      "Epoch 63/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 64/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6621 - val_accuracy: 0.6263\n",
      "Epoch 65/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6616 - val_accuracy: 0.6263\n",
      "Epoch 66/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
      "Epoch 67/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6525 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 68/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6616 - val_accuracy: 0.6263\n",
      "Epoch 69/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
      "Epoch 70/200\n",
      "453/453 [==============================] - 2s 3ms/step - loss: 0.6532 - accuracy: 0.6420 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 71/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
      "Epoch 72/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
      "Epoch 73/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
      "Epoch 74/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 75/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 76/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6424 - val_loss: 0.6649 - val_accuracy: 0.6263\n",
      "Epoch 77/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
      "Epoch 78/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6525 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
      "Epoch 79/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6636 - val_accuracy: 0.6263\n",
      "Epoch 80/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 81/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 82/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
      "Epoch 83/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6528 - accuracy: 0.6427 - val_loss: 0.6634 - val_accuracy: 0.6263\n",
      "Epoch 84/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 85/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6525 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
      "Epoch 86/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 87/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6635 - val_accuracy: 0.6263\n",
      "Epoch 88/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6424 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
      "Epoch 89/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 90/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6525 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
      "Epoch 91/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6618 - val_accuracy: 0.6263\n",
      "Epoch 92/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
      "Epoch 93/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
      "Epoch 94/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
      "Epoch 95/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
      "Epoch 96/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6426 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
      "Epoch 97/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
      "Epoch 98/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
      "Epoch 99/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6618 - val_accuracy: 0.6263\n",
      "Epoch 100/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
      "Epoch 101/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6519 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 102/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
      "Epoch 103/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 104/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
      "Epoch 105/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6616 - val_accuracy: 0.6263\n",
      "Epoch 106/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 107/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 108/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6643 - val_accuracy: 0.6263\n",
      "Epoch 109/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6616 - val_accuracy: 0.6263\n",
      "Epoch 110/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
      "Epoch 111/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 112/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 114/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6527 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
      "Epoch 115/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
      "Epoch 116/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
      "Epoch 117/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 118/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
      "Epoch 119/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
      "Epoch 120/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6616 - val_accuracy: 0.6263\n",
      "Epoch 121/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
      "Epoch 122/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
      "Epoch 123/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6527 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 124/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6527 - accuracy: 0.6425 - val_loss: 0.6625 - val_accuracy: 0.6263\n",
      "Epoch 125/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6526 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 126/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 127/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
      "Epoch 128/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6642 - val_accuracy: 0.6263\n",
      "Epoch 129/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6641 - val_accuracy: 0.6263\n",
      "Epoch 130/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 131/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
      "Epoch 132/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6427 - val_loss: 0.6618 - val_accuracy: 0.6263\n",
      "Epoch 133/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6427 - val_loss: 0.6643 - val_accuracy: 0.6263\n",
      "Epoch 134/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6668 - val_accuracy: 0.6263\n",
      "Epoch 135/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6623 - val_accuracy: 0.6263\n",
      "Epoch 136/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6424 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
      "Epoch 137/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
      "Epoch 138/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6616 - val_accuracy: 0.6263\n",
      "Epoch 139/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6629 - val_accuracy: 0.6263\n",
      "Epoch 140/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 141/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6641 - val_accuracy: 0.6263\n",
      "Epoch 142/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
      "Epoch 143/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6616 - val_accuracy: 0.6263\n",
      "Epoch 144/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
      "Epoch 145/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6622 - val_accuracy: 0.6263\n",
      "Epoch 146/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6716 - val_accuracy: 0.6263\n",
      "Epoch 147/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 148/200\n",
      "453/453 [==============================] - 2s 3ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6626 - val_accuracy: 0.6263\n",
      "Epoch 149/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6625 - val_accuracy: 0.6263\n",
      "Epoch 150/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 151/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6424 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 152/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
      "Epoch 153/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
      "Epoch 154/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 155/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6616 - val_accuracy: 0.6263\n",
      "Epoch 156/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6624 - val_accuracy: 0.6263\n",
      "Epoch 157/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6621 - val_accuracy: 0.6263\n",
      "Epoch 158/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6427 - val_loss: 0.6639 - val_accuracy: 0.6263\n",
      "Epoch 159/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6618 - val_accuracy: 0.6263\n",
      "Epoch 160/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6733 - val_accuracy: 0.6263\n",
      "Epoch 161/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
      "Epoch 162/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
      "Epoch 163/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6525 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
      "Epoch 164/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
      "Epoch 165/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6427 - val_loss: 0.6628 - val_accuracy: 0.6263\n",
      "Epoch 166/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
      "Epoch 167/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 168/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6619 - val_accuracy: 0.6263\n",
      "Epoch 170/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6525 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
      "Epoch 171/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6627 - val_accuracy: 0.6263\n",
      "Epoch 172/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
      "Epoch 173/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
      "Epoch 174/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
      "Epoch 175/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 176/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
      "Epoch 177/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6528 - accuracy: 0.6426 - val_loss: 0.6635 - val_accuracy: 0.6263\n",
      "Epoch 178/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6527 - accuracy: 0.6425 - val_loss: 0.6617 - val_accuracy: 0.6263\n",
      "Epoch 179/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
      "Epoch 180/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 181/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
      "Epoch 182/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6525 - accuracy: 0.6427 - val_loss: 0.6618 - val_accuracy: 0.6263\n",
      "Epoch 183/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6426 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 184/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
      "Epoch 185/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
      "Epoch 186/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
      "Epoch 187/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6631 - val_accuracy: 0.6263\n",
      "Epoch 188/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6633 - val_accuracy: 0.6263\n",
      "Epoch 189/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6623 - val_accuracy: 0.6263\n",
      "Epoch 190/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6427 - val_loss: 0.6612 - val_accuracy: 0.6263\n",
      "Epoch 191/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 192/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6426 - val_loss: 0.6609 - val_accuracy: 0.6263\n",
      "Epoch 193/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6611 - val_accuracy: 0.6263\n",
      "Epoch 194/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6623 - val_accuracy: 0.6263\n",
      "Epoch 195/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6425 - val_loss: 0.6610 - val_accuracy: 0.6263\n",
      "Epoch 196/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6618 - val_accuracy: 0.6263\n",
      "Epoch 197/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6637 - val_accuracy: 0.6263\n",
      "Epoch 198/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6524 - accuracy: 0.6427 - val_loss: 0.6646 - val_accuracy: 0.6263\n",
      "Epoch 199/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6427 - val_loss: 0.6615 - val_accuracy: 0.6263\n",
      "Epoch 200/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6521 - accuracy: 0.6427 - val_loss: 0.6624 - val_accuracy: 0.6263\n",
      "Fold 3, 200 epochs, 227 sec\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 62.63%\n",
      "COMPILE...\n",
      "...COMPILED\n",
      "FIT\n",
      "Epoch 1/200\n",
      "446/453 [============================>.] - ETA: 0s - loss: 0.6559 - accuracy: 0.6407INFO:tensorflow:Assets written to: MLP212.cv.4.best/assets\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 0.6557 - accuracy: 0.6410 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 2/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6412 - val_loss: 0.6546 - val_accuracy: 0.6394\n",
      "Epoch 3/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 4/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 5/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6528 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 6/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
      "Epoch 7/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 8/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 9/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 10/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6562 - val_accuracy: 0.6394\n",
      "Epoch 11/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 12/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6554 - val_accuracy: 0.6394\n",
      "Epoch 13/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6547 - val_accuracy: 0.6394\n",
      "Epoch 14/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6409 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
      "Epoch 15/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
      "Epoch 16/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 17/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 18/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6542 - val_accuracy: 0.6394\n",
      "Epoch 19/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 20/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
      "Epoch 21/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 22/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6542 - val_accuracy: 0.6394\n",
      "Epoch 23/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6409 - val_loss: 0.6544 - val_accuracy: 0.6394\n",
      "Epoch 24/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6543 - val_accuracy: 0.6394\n",
      "Epoch 25/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6542 - val_accuracy: 0.6394\n",
      "Epoch 26/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
      "Epoch 27/200\n",
      "453/453 [==============================] - 2s 3ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 28/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6578 - val_accuracy: 0.6394\n",
      "Epoch 29/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 30/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 31/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 32/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 33/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
      "Epoch 34/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 35/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6412 - val_loss: 0.6542 - val_accuracy: 0.6394\n",
      "Epoch 36/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 37/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
      "Epoch 38/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 39/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 40/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6547 - val_accuracy: 0.6394\n",
      "Epoch 41/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6402 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 42/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6544 - val_accuracy: 0.6394\n",
      "Epoch 43/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 44/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
      "Epoch 45/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6408 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
      "Epoch 46/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6547 - val_accuracy: 0.6394\n",
      "Epoch 47/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6553 - val_accuracy: 0.6394\n",
      "Epoch 48/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 49/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 50/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 51/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6542 - val_accuracy: 0.6394\n",
      "Epoch 52/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 53/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6410 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 54/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 55/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
      "Epoch 57/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
      "Epoch 58/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 59/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 60/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 61/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 62/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 63/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 64/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 65/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
      "Epoch 66/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 67/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 68/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
      "Epoch 69/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 70/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6411 - val_loss: 0.6561 - val_accuracy: 0.6394\n",
      "Epoch 71/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 72/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 73/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6543 - val_accuracy: 0.6394\n",
      "Epoch 74/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 75/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
      "Epoch 76/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 77/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6542 - accuracy: 0.6412 - val_loss: 0.6547 - val_accuracy: 0.6394\n",
      "Epoch 78/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 79/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 80/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 81/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6528 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 82/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 83/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6546 - val_accuracy: 0.6394\n",
      "Epoch 84/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 85/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
      "Epoch 86/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
      "Epoch 87/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 88/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6542 - val_accuracy: 0.6394\n",
      "Epoch 89/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
      "Epoch 90/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 91/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 92/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 93/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 94/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 95/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6545 - val_accuracy: 0.6394\n",
      "Epoch 96/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6553 - val_accuracy: 0.6394\n",
      "Epoch 97/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6407 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 98/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 99/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 100/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
      "Epoch 101/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6542 - val_accuracy: 0.6394\n",
      "Epoch 102/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 103/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 104/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 105/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
      "Epoch 106/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 107/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 108/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6411 - val_loss: 0.6544 - val_accuracy: 0.6394\n",
      "Epoch 109/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 110/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6543 - val_accuracy: 0.6394\n",
      "Epoch 111/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6602 - val_accuracy: 0.6394\n",
      "Epoch 112/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6412 - val_loss: 0.6544 - val_accuracy: 0.6394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 114/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6544 - val_accuracy: 0.6394\n",
      "Epoch 115/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 116/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6555 - val_accuracy: 0.6394\n",
      "Epoch 117/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6559 - val_accuracy: 0.6394\n",
      "Epoch 118/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 119/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 120/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6543 - val_accuracy: 0.6394\n",
      "Epoch 121/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 122/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
      "Epoch 123/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6412 - val_loss: 0.6552 - val_accuracy: 0.6394\n",
      "Epoch 124/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 125/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 126/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 127/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 128/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6412 - val_loss: 0.6550 - val_accuracy: 0.6394\n",
      "Epoch 129/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 130/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 131/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6552 - val_accuracy: 0.6394\n",
      "Epoch 132/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 133/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6546 - val_accuracy: 0.6394\n",
      "Epoch 134/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6547 - val_accuracy: 0.6394\n",
      "Epoch 135/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6409 - val_loss: 0.6555 - val_accuracy: 0.6394\n",
      "Epoch 136/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 137/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 138/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
      "Epoch 139/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 140/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 141/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 142/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 143/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6552 - val_accuracy: 0.6394\n",
      "Epoch 144/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 145/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 146/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 147/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
      "Epoch 148/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
      "Epoch 149/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 150/200\n",
      "453/453 [==============================] - 2s 3ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 151/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
      "Epoch 152/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6546 - val_accuracy: 0.6394\n",
      "Epoch 153/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 154/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 155/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6412 - val_loss: 0.6543 - val_accuracy: 0.6394\n",
      "Epoch 156/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 157/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
      "Epoch 158/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
      "Epoch 159/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6548 - val_accuracy: 0.6394\n",
      "Epoch 160/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 161/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6411 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 162/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 163/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 164/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 165/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
      "Epoch 166/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6543 - val_accuracy: 0.6394\n",
      "Epoch 167/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 168/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6542 - val_accuracy: 0.6394\n",
      "Epoch 170/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6543 - val_accuracy: 0.6394\n",
      "Epoch 171/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6412 - val_loss: 0.6542 - val_accuracy: 0.6394\n",
      "Epoch 172/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 173/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6545 - val_accuracy: 0.6394\n",
      "Epoch 174/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6547 - val_accuracy: 0.6394\n",
      "Epoch 175/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 176/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 177/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 178/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6557 - val_accuracy: 0.6394\n",
      "Epoch 179/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6412 - val_loss: 0.6575 - val_accuracy: 0.6394\n",
      "Epoch 180/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 181/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6528 - accuracy: 0.6411 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 182/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 183/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 184/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 185/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 186/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6406 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 187/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
      "Epoch 188/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6542 - val_accuracy: 0.6394\n",
      "Epoch 189/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
      "Epoch 190/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6540 - val_accuracy: 0.6394\n",
      "Epoch 191/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 192/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 193/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 194/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6409 - val_loss: 0.6541 - val_accuracy: 0.6394\n",
      "Epoch 195/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 196/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 197/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 198/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6412 - val_loss: 0.6539 - val_accuracy: 0.6394\n",
      "Epoch 199/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6412 - val_loss: 0.6538 - val_accuracy: 0.6394\n",
      "Epoch 200/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6412 - val_loss: 0.6550 - val_accuracy: 0.6394\n",
      "Fold 4, 200 epochs, 208 sec\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 63.94%\n",
      "COMPILE...\n",
      "...COMPILED\n",
      "FIT\n",
      "Epoch 1/200\n",
      "439/453 [============================>.] - ETA: 0s - loss: 0.6560 - accuracy: 0.6391INFO:tensorflow:Assets written to: MLP212.cv.5.best/assets\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6564 - accuracy: 0.6384 - val_loss: 0.6486 - val_accuracy: 0.6505\n",
      "Epoch 2/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6543 - accuracy: 0.6400 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
      "Epoch 3/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6483 - val_accuracy: 0.6505\n",
      "Epoch 4/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 5/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 6/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 7/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6399 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 8/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6489 - val_accuracy: 0.6505\n",
      "Epoch 9/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
      "Epoch 10/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
      "Epoch 11/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 12/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6485 - val_accuracy: 0.6505\n",
      "Epoch 13/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 14/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 15/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6398 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 16/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
      "Epoch 17/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 18/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 19/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 20/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 21/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6545 - accuracy: 0.6399 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 22/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 23/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
      "Epoch 24/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 25/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 26/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 27/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6543 - accuracy: 0.6399 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 28/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6398 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 29/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 30/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 31/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 32/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 33/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 34/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6398 - val_loss: 0.6478 - val_accuracy: 0.6505\n",
      "Epoch 35/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 36/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6477 - val_accuracy: 0.6505\n",
      "Epoch 37/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
      "Epoch 38/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
      "Epoch 39/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6396 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
      "Epoch 40/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
      "Epoch 41/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
      "Epoch 42/200\n",
      "453/453 [==============================] - 2s 3ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 43/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6542 - accuracy: 0.6400 - val_loss: 0.6478 - val_accuracy: 0.6505\n",
      "Epoch 44/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 45/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 46/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6477 - val_accuracy: 0.6505\n",
      "Epoch 47/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
      "Epoch 48/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
      "Epoch 49/200\n",
      "453/453 [==============================] - 2s 3ms/step - loss: 0.6541 - accuracy: 0.6400 - val_loss: 0.6477 - val_accuracy: 0.6505\n",
      "Epoch 50/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 51/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6535 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 52/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 53/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6485 - val_accuracy: 0.6505\n",
      "Epoch 54/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 55/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6478 - val_accuracy: 0.6505\n",
      "Epoch 57/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
      "Epoch 58/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6496 - val_accuracy: 0.6505\n",
      "Epoch 59/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 60/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 61/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 62/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6487 - val_accuracy: 0.6505\n",
      "Epoch 63/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6542 - accuracy: 0.6400 - val_loss: 0.6490 - val_accuracy: 0.6505\n",
      "Epoch 64/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6400 - val_loss: 0.6477 - val_accuracy: 0.6505\n",
      "Epoch 65/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6478 - val_accuracy: 0.6505\n",
      "Epoch 66/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
      "Epoch 67/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 68/200\n",
      "453/453 [==============================] - 2s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 69/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6542 - accuracy: 0.6400 - val_loss: 0.6487 - val_accuracy: 0.6505\n",
      "Epoch 70/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 71/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 72/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
      "Epoch 73/200\n",
      "453/453 [==============================] - 2s 3ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 74/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6397 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 75/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6484 - val_accuracy: 0.6505\n",
      "Epoch 76/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6477 - val_accuracy: 0.6505\n",
      "Epoch 77/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
      "Epoch 78/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
      "Epoch 79/200\n",
      "453/453 [==============================] - 2s 3ms/step - loss: 0.6540 - accuracy: 0.6399 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 80/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
      "Epoch 81/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 82/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6543 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 83/200\n",
      "453/453 [==============================] - 2s 3ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
      "Epoch 84/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 85/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6399 - val_loss: 0.6485 - val_accuracy: 0.6505\n",
      "Epoch 86/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 87/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6477 - val_accuracy: 0.6505\n",
      "Epoch 88/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6400 - val_loss: 0.6490 - val_accuracy: 0.6505\n",
      "Epoch 89/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6484 - val_accuracy: 0.6505\n",
      "Epoch 90/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6487 - val_accuracy: 0.6505\n",
      "Epoch 91/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 92/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 93/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
      "Epoch 94/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6481 - val_accuracy: 0.6505\n",
      "Epoch 95/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
      "Epoch 96/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6397 - val_loss: 0.6477 - val_accuracy: 0.6505\n",
      "Epoch 97/200\n",
      "453/453 [==============================] - 2s 3ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 98/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 99/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 100/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 101/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
      "Epoch 102/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6478 - val_accuracy: 0.6505\n",
      "Epoch 103/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 104/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6477 - val_accuracy: 0.6505\n",
      "Epoch 105/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6542 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 106/200\n",
      "453/453 [==============================] - 2s 3ms/step - loss: 0.6544 - accuracy: 0.6400 - val_loss: 0.6481 - val_accuracy: 0.6505\n",
      "Epoch 107/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 108/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 109/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6478 - val_accuracy: 0.6505\n",
      "Epoch 110/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6398 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
      "Epoch 111/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
      "Epoch 112/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 114/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 115/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 116/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6545 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 117/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 118/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 119/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6542 - accuracy: 0.6400 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
      "Epoch 120/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 121/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6479 - val_accuracy: 0.6505\n",
      "Epoch 122/200\n",
      "453/453 [==============================] - 2s 3ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
      "Epoch 123/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6399 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
      "Epoch 124/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6398 - val_loss: 0.6489 - val_accuracy: 0.6505\n",
      "Epoch 125/200\n",
      "453/453 [==============================] - 2s 3ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
      "Epoch 126/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 127/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 128/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6479 - val_accuracy: 0.6505\n",
      "Epoch 129/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6478 - val_accuracy: 0.6505\n",
      "Epoch 130/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6544 - accuracy: 0.6399 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 131/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6481 - val_accuracy: 0.6505\n",
      "Epoch 132/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6496 - val_accuracy: 0.6505\n",
      "Epoch 133/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 134/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 135/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 136/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
      "Epoch 137/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6492 - val_accuracy: 0.6505\n",
      "Epoch 138/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 139/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6481 - val_accuracy: 0.6505\n",
      "Epoch 140/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
      "Epoch 141/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 142/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6481 - val_accuracy: 0.6505\n",
      "Epoch 143/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6398 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 144/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 145/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
      "Epoch 146/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 147/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 148/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6491 - val_accuracy: 0.6505\n",
      "Epoch 149/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6400 - val_loss: 0.6479 - val_accuracy: 0.6505\n",
      "Epoch 150/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6546 - accuracy: 0.6397 - val_loss: 0.6477 - val_accuracy: 0.6505\n",
      "Epoch 151/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 152/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 153/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
      "Epoch 154/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 155/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 156/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6479 - val_accuracy: 0.6505\n",
      "Epoch 157/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 158/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6481 - val_accuracy: 0.6505\n",
      "Epoch 159/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 160/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
      "Epoch 161/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6542 - accuracy: 0.6400 - val_loss: 0.6483 - val_accuracy: 0.6505\n",
      "Epoch 162/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6477 - val_accuracy: 0.6505\n",
      "Epoch 163/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 0.6541 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 164/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6478 - val_accuracy: 0.6505\n",
      "Epoch 165/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6478 - val_accuracy: 0.6505\n",
      "Epoch 166/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6477 - val_accuracy: 0.6505\n",
      "Epoch 167/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6490 - val_accuracy: 0.6505\n",
      "Epoch 168/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6397 - val_loss: 0.6481 - val_accuracy: 0.6505\n",
      "Epoch 170/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 171/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
      "Epoch 172/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 173/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6505 - val_accuracy: 0.6505\n",
      "Epoch 174/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 175/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 176/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6474 - val_accuracy: 0.6505\n",
      "Epoch 177/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 178/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6399 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
      "Epoch 179/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
      "Epoch 180/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 181/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 182/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
      "Epoch 183/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 184/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 185/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6484 - val_accuracy: 0.6505\n",
      "Epoch 186/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 187/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6482 - val_accuracy: 0.6505\n",
      "Epoch 188/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6475 - val_accuracy: 0.6505\n",
      "Epoch 189/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6400 - val_loss: 0.6494 - val_accuracy: 0.6505\n",
      "Epoch 190/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 191/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6400 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 192/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6398 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 193/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6477 - val_accuracy: 0.6505\n",
      "Epoch 194/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6488 - val_accuracy: 0.6505\n",
      "Epoch 195/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Epoch 196/200\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 197/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6400 - val_loss: 0.6471 - val_accuracy: 0.6505\n",
      "Epoch 198/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6500 - val_accuracy: 0.6505\n",
      "Epoch 199/200\n",
      "453/453 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6399 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 200/200\n",
      "453/453 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6400 - val_loss: 0.6473 - val_accuracy: 0.6505\n",
      "Fold 5, 200 epochs, 270 sec\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 65.05%\n",
      "\n",
      "5-way Cross Validation mean 64.28% (+/- 0.95%)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print (\"Cross valiation\")\n",
    "do_cross_validation(X_train,y_train,model)  \n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jN0K-1e2g4WN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E37va5UZg4WQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MLP_211.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
