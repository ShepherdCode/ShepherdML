{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long-term memory.\n",
    "The simple RNN has short term memory.\n",
    "It rarely remembers over 10 time steps back.\n",
    "This is due to the huge data transformation at every time step.\n",
    "Nobody uses the simple RNN anymore.\n",
    "Everyone uses some form of enhanced long-term memory.\n",
    "These are capable of remembering about 100 time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM cells\n",
    "Long short-term memory has been improved since its introduction in 1997.\n",
    "\n",
    "The Keras LSTM converges faster and remembers more.\n",
    "There are two ways to use it.\n",
    "* Pass an LSTMCell() instance to a Keras RNN() layer.\n",
    "* Use the LSTM() layer. This is optimized for GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from pathlib import Path\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "def generate_time_series (batch_size, n_steps):\n",
    "    freq1, freq2, offset1, offset2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offset1) * (freq1 * 10 + 10))\n",
    "    series += 0.2 * np.sin((time - offset2) * (freq2 * 20 + 20))\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)\n",
    "    return series[..., np.newaxis].astype(np.float32)\n",
    "\n",
    "n_steps = 50\n",
    "series = generate_time_series(10000,n_steps+10)\n",
    "X_train,y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
    "X_valid,y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
    "X_test,y_test = series[9000:, :n_steps], series[9000:, -10:, 0]\n",
    "\n",
    "Y = np.empty((10000,n_steps,10))\n",
    "for step_ahead in range(1,10+1):\n",
    "    Y[:,:,step_ahead-1] = series[:,step_ahead:step_ahead+n_steps,0]\n",
    "y_train = Y[:7000]\n",
    "y_valid = Y[7000:9000]\n",
    "y_test  = Y[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "219/219 [==============================] - 7s 31ms/step - loss: 0.0742 - val_loss: 0.0472\n",
      "Epoch 2/5\n",
      "219/219 [==============================] - 6s 26ms/step - loss: 0.0441 - val_loss: 0.0411\n",
      "Epoch 3/5\n",
      "219/219 [==============================] - 5s 24ms/step - loss: 0.0397 - val_loss: 0.0407\n",
      "Epoch 4/5\n",
      "219/219 [==============================] - 6s 27ms/step - loss: 0.0375 - val_loss: 0.0360\n",
      "Epoch 5/5\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0357 - val_loss: 0.0348\n"
     ]
    }
   ],
   "source": [
    "# First, train the simple RNN.\n",
    "rnn1 = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20,return_sequences=True,input_shape=[None,1]),\n",
    "    keras.layers.SimpleRNN(20,return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "rnn1.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = rnn1.fit(X_train, y_train, epochs=5,\n",
    "                    validation_data=(X_valid, y_valid))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "219/219 [==============================] - 13s 59ms/step - loss: 0.0770 - val_loss: 0.0533\n",
      "Epoch 2/5\n",
      "219/219 [==============================] - 12s 55ms/step - loss: 0.0455 - val_loss: 0.0401\n",
      "Epoch 3/5\n",
      "219/219 [==============================] - 12s 55ms/step - loss: 0.0372 - val_loss: 0.0355\n",
      "Epoch 4/5\n",
      "219/219 [==============================] - 12s 55ms/step - loss: 0.0338 - val_loss: 0.0324\n",
      "Epoch 5/5\n",
      "219/219 [==============================] - 12s 55ms/step - loss: 0.0315 - val_loss: 0.0306\n"
     ]
    }
   ],
   "source": [
    "# Second, substitute LSTM for simple RNN.\n",
    "rnn2 = keras.models.Sequential([\n",
    "    keras.layers.LSTM(20,return_sequences=True,input_shape=[None,1]),\n",
    "    keras.layers.LSTM(20,return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "rnn2.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = rnn2.fit(X_train, y_train, epochs=5,\n",
    "                    validation_data=(X_valid, y_valid))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much slower with normalization (ach epoch took about twice as long)\n",
    "but loss went down faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geron has a great figure of the LSTM cell.\n",
    "See Figure 15.9 on page 516.\n",
    "It is also online [png](https://learning.oreilly.com/library/view/neural-networks-and/9781492037354/assets/mlst_1413.png) \n",
    "in [ch4](https://learning.oreilly.com/library/view/neural-networks-and/9781492037354/ch04.html).\n",
    "The equations for each layer are on page 517."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peephole LSTM\n",
    "This is a recent enhancement.\n",
    "It adds more highways for remembering prior cell state.\n",
    "Book says unclear when/if it works.\n",
    "It is marked experimental in Keras: [PeepholdLSTM](https://www.tensorflow.org/api_docs/python/tf/keras/experimental/PeepholeLSTMCell)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU cells\n",
    "Simplied LSTM that performs just as well.\n",
    "\n",
    "LSTM has 2 vectors carried forward at each time step. GRU merges c and h into just h. \n",
    "* LSTM c = cell state for long term memory\n",
    "* LSTM h = output of this hidden layer at previous time step.\n",
    "\n",
    "\n",
    "LSTM has 2 extra gates. GRU merges these into one gate.\n",
    "* LSTM Input gate takes in x, changes c and h.\n",
    "* LSTM Forget gate takes in c, changes c and h.\n",
    "\n",
    "GRU overwrites memory when it stores a new memory (huh?).\n",
    "\n",
    "GRU has extra gate controller that decides which part of memory to use (huh?).\n",
    "\n",
    "See figure on page 519. See equations on page 520."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "219/219 [==============================] - 15s 67ms/step - loss: 0.0773 - val_loss: 0.0547\n",
      "Epoch 2/5\n",
      "219/219 [==============================] - 14s 65ms/step - loss: 0.0488 - val_loss: 0.0453\n",
      "Epoch 3/5\n",
      "219/219 [==============================] - 14s 65ms/step - loss: 0.0434 - val_loss: 0.0412\n",
      "Epoch 4/5\n",
      "219/219 [==============================] - 14s 66ms/step - loss: 0.0395 - val_loss: 0.0372\n",
      "Epoch 5/5\n",
      "219/219 [==============================] - 14s 65ms/step - loss: 0.0350 - val_loss: 0.0324\n"
     ]
    }
   ],
   "source": [
    "# Third, substitute GRU for LSTM.\n",
    "rnn3 = keras.models.Sequential([\n",
    "    keras.layers.GRU(20,return_sequences=True,input_shape=[None,1]),\n",
    "    keras.layers.GRU(20,return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "rnn3.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = rnn3.fit(X_train, y_train, epochs=5,\n",
    "                    validation_data=(X_valid, y_valid))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D convolution\n",
    "To tackle sequences longer than 100 time steps,\n",
    "break the sequences into shorter pieces.\n",
    "This is similar to 2D convolution for CNN.\n",
    "\n",
    "Slide kernels (filters) over the sequence (or image).\n",
    "Each kernel learns to match of subsequence of length = kernel size.\n",
    "\n",
    "Geron says this is so effective that you can\n",
    "even omit the RNN/LSTM/GRU layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "219/219 [==============================] - 8s 38ms/step - loss: 0.0688 - val_loss: 0.0467\n",
      "Epoch 2/5\n",
      "219/219 [==============================] - 8s 35ms/step - loss: 0.0391 - val_loss: 0.0338\n",
      "Epoch 3/5\n",
      "219/219 [==============================] - 8s 34ms/step - loss: 0.0315 - val_loss: 0.0297\n",
      "Epoch 4/5\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 0.0277 - val_loss: 0.0261\n",
      "Epoch 5/5\n",
      "219/219 [==============================] - 8s 35ms/step - loss: 0.0251 - val_loss: 0.0244\n"
     ]
    }
   ],
   "source": [
    "# Fourth, add Conv1D.\n",
    "# This is equivalent to K-mers, K=2, skip every other one.\n",
    "rnn4 = keras.models.Sequential([\n",
    "    keras.layers.Conv1D(filters=20,\n",
    "                       kernel_size=4,  \n",
    "                       strides=2,\n",
    "                       padding=\"valid\",\n",
    "                       input_shape=[None,1]),\n",
    "    keras.layers.GRU(20,return_sequences=True), \n",
    "    keras.layers.GRU(20,return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "# Specify input shape in first layer only.\n",
    "\n",
    "rnn4.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = rnn4.fit(X_train, y_train[:,3::2], epochs=5,\n",
    "                    validation_data=(X_valid, y_valid[:,3::2]))\n",
    "# Note new shape of y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaveNet\n",
    "Invented 2016 for generating human speech.\n",
    "Stacked 1D convolutional layers.\n",
    "Doubling dilation rate means \n",
    "process every time step, then every other, etc. \n",
    "The paper used three iterations of 1,2,4,8,...512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "219/219 [==============================] - 4s 17ms/step - loss: 0.0656 - val_loss: 0.0353\n",
      "Epoch 2/5\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.0312 - val_loss: 0.0282\n",
      "Epoch 3/5\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.0274 - val_loss: 0.0273\n",
      "Epoch 4/5\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0255 - val_loss: 0.0244\n",
      "Epoch 5/5\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.0242 - val_loss: 0.0235\n"
     ]
    }
   ],
   "source": [
    "# Fifth, do WaveNet i.e. stacked 1D conv.\n",
    "rnn5 = keras.models.Sequential()\n",
    "rnn5.add(keras.layers.InputLayer(input_shape=[None,1]))\n",
    "for rate in (1,2,4,8)*2:\n",
    "    rnn5.add(keras.layers.Conv1D(filters=20,kernel_size=2,  \n",
    "            padding=\"causal\",activation=\"relu\",dilation_rate=rate))\n",
    "rnn5.add(keras.layers.Conv1D(filters=10,kernel_size=1))  \n",
    "\n",
    "rnn5.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = rnn5.fit(X_train, y_train, epochs=5,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
