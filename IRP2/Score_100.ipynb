{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PG-tGRnlFLA3"
   },
   "source": [
    "# Scoring Schemes\n",
    "Arabidopsis, bowtie, transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RmwUsVLFLA6",
    "outputId": "36a55933-da20-4b31-c254-955c18b7f89d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-07 07:35:47.820455\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OlzN9OdsFWEU",
    "outputId": "528a8bcc-c715-471f-b1b5-4880442b4ed1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-07 07:35:47.847635: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found\n",
      "Running on Mac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-07 07:35:56.638344: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "dt='float32'\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "tf.random.set_seed(42) # supposedly leads to reproducible results\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print('GPU device not found')\n",
    "else:\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print('Running on CoLab')\n",
    "    PATH='/content/drive/'\n",
    "    drive.mount(PATH)\n",
    "    DATA_DIR=PATH+'My Drive/data/IRP2/'  # must end in \"/\"\n",
    "    MODEL_DIR=PATH+'My Drive/data/IRP2/Models/'  # must end in \"/\"\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print('Running on Mac')\n",
    "    DATA_DIR=\"/Users/jasonmiller/WVU/BAM_ML/\"\n",
    "    MODEL_DIR=\"/Users/jasonmiller/WVU/BAM_ML/Models/\"\n",
    "SAVE_MODEL_FILENAME = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CIAT2G5DYwvS",
    "outputId": "e61d9141-a139-43ef-caec-1e02a95fcf8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.0\n",
      "sklearn 1.1.2\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print('Python',python_version())\n",
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(42) # supposedly sets scikit-learn\n",
    "import pandas as pd  # for plotting\n",
    "import time # sleep function\n",
    "from os.path import isfile\n",
    "import gzip\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn   # pip install --upgrade scikit-learn\n",
    "print('sklearn',sklearn.__version__)\n",
    "\n",
    "from tensorflow import keras\n",
    "# consider sklearn.metrics.classification_report\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "\n",
    "EPOCHS=150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtqdpJOxFLBA"
   },
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LnkpVKdMFLA-",
    "outputId": "ed4dcf21-0103-428a-ce84-9cf044246269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /Users/jasonmiller/WVU/BAM_ML/\n",
      "Data file 0 Arabidopsis/Bowtie/lyrata/read_stats.csv.gz\n",
      "Data file 1 Arabidopsis/Bowtie/halleri/read_stats.csv.gz\n",
      "Input lines for training: 10000\n"
     ]
    }
   ],
   "source": [
    "MAX_LINES_TO_LOAD =    1000000 # training - 1M lines requires 2GB RAM\n",
    "MAX_LINES_TO_LOAD =    10000 # use this for debugging\n",
    "\n",
    "VALID_PORTION = 0.20\n",
    "\n",
    "DATA_FILE_0 = 'Arabidopsis/Bowtie/lyrata/read_stats.csv.gz'\n",
    "DATA_FILE_1 = 'Arabidopsis/Bowtie/halleri/read_stats.csv.gz'\n",
    "\n",
    "print('Data directory: %s'%DATA_DIR)\n",
    "print('Data file 0 %s'%DATA_FILE_0)\n",
    "print('Data file 1 %s'%DATA_FILE_1)\n",
    "print('Input lines for training: %d'%MAX_LINES_TO_LOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "p35ehKV3Kq0z"
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self,filepath1,filepath2,rule=None,verbose=True):\n",
    "        self.files = [filepath1,filepath2]\n",
    "        self.alignments=[]\n",
    "        self.labels=[]\n",
    "        self.verbose = verbose\n",
    "        self.max_lines = None\n",
    "        self.ties = 0\n",
    "        self.predictions = []\n",
    "        self.rule = rule\n",
    "\n",
    "    def set_max_lines(self,lines):\n",
    "        '''Limit the dataset size to fit in RAM.'''\n",
    "        self.max_lines = lines\n",
    "        if self.verbose:\n",
    "            print('Maximum lines to load per file: %d'%lines)\n",
    "\n",
    "    def _count_lines_(self):\n",
    "        '''Show number of lines per input file.'''\n",
    "        count0 = 0\n",
    "        with gzip.open (self.files[0],'rt') as handle0:\n",
    "            for row in handle0:\n",
    "                count0 += 1\n",
    "        count1 = 0\n",
    "        with gzip.open(self.files[1],'rt') as handle1:\n",
    "            for row in handle1:\n",
    "                count1 += 1\n",
    "        minimum = min(count0,count1)\n",
    "        if self.verbose:\n",
    "            print('File0 size: %d %s'%(count0,self.files[0]))\n",
    "            print('File1 size: %d %s'%(count1,self.files[1]))\n",
    "        return minimum\n",
    "\n",
    "    def _load_line_(self,row):\n",
    "        '''Load data structure from one line of CSV file.'''\n",
    "        line = row.strip()\n",
    "        fields = line.split(',')\n",
    "        ints = [0] * 61\n",
    "        # These fields come straight from the input file.\n",
    "        # These fields are grouped by which read they describe.\n",
    "        # P1 R1 = Parent 1, Read 1\n",
    "        ints[0] = int(fields[0]) # P1 R1 AS\n",
    "        ints[1] = int(fields[1]) # P1 R1 ED\n",
    "        ints[2] = int(fields[2]) # P1 R1 MAT\n",
    "        ints[3] = int(fields[3]) # P1 R1 MM\n",
    "        ints[4] = int(fields[4]) # P1 R1 HQMM\n",
    "        ints[5] = int(fields[5]) # P1 R1 GO\n",
    "        ints[6] = int(fields[6]) # P1 R1 GE\n",
    "        ints[7] = int(fields[7]) # P1 R1 INS\n",
    "        ints[8] = int(fields[8]) # P1 R1 DELS\n",
    "        ints[9] = int(fields[9]) # P1 R1 HQINS\n",
    "        ints[10] = int(fields[10]) # P1 R1 HQDEL\n",
    "        # P1 R2 = Parent 1, Read 2\n",
    "        ints[11] = int(fields[11]) # P1 R2 AS\n",
    "        ints[12] = int(fields[12]) # P1 R2 ED\n",
    "        ints[13] = int(fields[13]) # P1 R2 MAT\n",
    "        ints[14] = int(fields[14]) # P1 R2 MM\n",
    "        ints[15] = int(fields[15]) # P1 R2 HQMM\n",
    "        ints[16] = int(fields[16]) # P1 R2 GO\n",
    "        ints[17] = int(fields[17]) # P1 R2 GE\n",
    "        ints[18] = int(fields[18]) # P1 R2 INS\n",
    "        ints[19] = int(fields[19]) # P1 R2 DELS\n",
    "        ints[20] = int(fields[20]) # P1 R2 HQINS\n",
    "        ints[21] = int(fields[21]) # P1 R2 HQDEL\n",
    "        # P2 R1 = Parent 2, Read 1\n",
    "        ints[22] = int(fields[22]) # P2 R1 AS\n",
    "        ints[23] = int(fields[23]) # P2 R1 ED\n",
    "        ints[24] = int(fields[24]) # P2 R1 MAT\n",
    "        ints[25] = int(fields[25]) # P2 R1 MM\n",
    "        ints[26] = int(fields[26]) # P2 R1 HQMM\n",
    "        ints[27] = int(fields[27]) # P2 R1 GO\n",
    "        ints[28] = int(fields[28]) # P2 R1 GE\n",
    "        ints[29] = int(fields[29]) # P2 R1 INS\n",
    "        ints[30] = int(fields[30]) # P2 R1 DELS\n",
    "        ints[31] = int(fields[31]) # P2 R1 HQINS\n",
    "        ints[32] = int(fields[32]) # P2 R1 HQDEL\n",
    "        # P2 R2 = Parent 2, Read 2\n",
    "        ints[33] = int(fields[33]) # P2 R2 AS\n",
    "        ints[34] = int(fields[34]) # P2 R2 ED\n",
    "        ints[35] = int(fields[35]) # P2 R2 MAT\n",
    "        ints[36] = int(fields[36]) # P2 R2 MM\n",
    "        ints[37] = int(fields[37]) # P2 R2 HQMM\n",
    "        ints[38] = int(fields[38]) # P2 R2 GO\n",
    "        ints[39] = int(fields[39]) # P2 R2 GE\n",
    "        ints[40] = int(fields[40]) # P2 R2 INS\n",
    "        ints[41] = int(fields[41]) # P2 R2 DELS\n",
    "        ints[42] = int(fields[42]) # P2 R2 HQINS\n",
    "        ints[43] = int(fields[43]) # P2 R2 HQDEL\n",
    "        # Fields that come in twos\n",
    "        ints[44] = int(fields[44]) # R1 length (of read)\n",
    "        ints[45] = int(fields[45]) # R2 length (of read)\n",
    "        ints[46] = int(fields[46]) # P1 span (of mapped read pair)\n",
    "        ints[47] = int(fields[47]) # P2 span (of mapped read pair)\n",
    "        # Read-wise differences\n",
    "        ints[48] = ints[47]-ints[46] # P2-P1 span diff\n",
    "        ints[49] = (ints[33]+ints[22])-(ints[11]+ints[0]) # AS diff\n",
    "        ints[50] = (ints[34]+ints[23])-(ints[12]+ints[1]) # ED diff\n",
    "        ints[51] = (ints[35]+ints[24])-(ints[13]+ints[2]) # MAT diff\n",
    "        ints[52] = (ints[36]+ints[25])-(ints[14]+ints[3]) # MM diff\n",
    "        ints[53] = (ints[37]+ints[26])-(ints[15]+ints[4]) # HQMM diff\n",
    "        ints[54] = (ints[38]+ints[27])-(ints[16]+ints[5]) # GO diff\n",
    "        ints[55] = (ints[39]+ints[28])-(ints[17]+ints[6]) # GE diff\n",
    "        ints[56] = (ints[40]+ints[29])-(ints[18]+ints[7]) # INS diff\n",
    "        ints[57] = (ints[41]+ints[30])-(ints[19]+ints[8]) # DELS diff\n",
    "        ints[58] = (ints[42]+ints[31])-(ints[20]+ints[9]) # HQINS diff\n",
    "        ints[59] = (ints[43]+ints[32])-(ints[21]+ints[10]) # HQDEL diff\n",
    "        # The feature-extraction program populated fields[36]\n",
    "        # to indicate which parent had higher alignment score.\n",
    "        # Values were 0=same, 1=parent1, 2=parent2.\n",
    "        # We change the values to -1=parent1, 0=unknown, +1=parent2\n",
    "        parent_choice = int(fields[48])\n",
    "        if parent_choice == 1:\n",
    "            ints[60] = -1  # not parent 2\n",
    "        elif parent_choice == 2:\n",
    "            ints[60] = 1  # is parent 2\n",
    "        elif parent_choice == 0:\n",
    "            ints[60] = 0\n",
    "        else:\n",
    "            raise Exception('Unrecognized parent choice:'+str(parent_choice))\n",
    "        if self.rule == 'random':\n",
    "            self.ties += 1\n",
    "            guess = random.randint(0,1)\n",
    "            self.predictions.append(guess)\n",
    "        elif self.rule == 'matches':\n",
    "            if ints[51] > 0:\n",
    "                self.predictions.append(1) # is parent 2\n",
    "            elif ints[51] < 0:\n",
    "                self.predictions.append(0) # not parent 2\n",
    "            else:   # guess\n",
    "                self.ties += 1\n",
    "                guess = random.randint(0,1)\n",
    "                self.predictions.append(guess)\n",
    "        else:\n",
    "            # Default: align score\n",
    "            # We change 1 to 0, 2 to 1, and 0 to 1 or 2 randomly.\n",
    "            parent_choice = int(fields[48])\n",
    "            if parent_choice == 1:\n",
    "                self.predictions.append(0)  # not parent 2\n",
    "            elif parent_choice == 2:\n",
    "                self.predictions.append(1)  # is parent 2\n",
    "            else: # guess\n",
    "                self.ties += 1\n",
    "                guess = random.randint(0,1)\n",
    "                self.predictions.append(guess)\n",
    "        # The transcript that this read pair aligned to.\n",
    "        # This is for pipelines that only process reads that map\n",
    "        # to same transcript in both parents and (filter the others).\n",
    "        # Pipelines that assign reads to parent, regardless of which gene,\n",
    "        # should ignore this value. (It only reflects first parent map anyway.)\n",
    "        transcript_id = fields[49] # TO DO: where to put this?\n",
    "        self.alignments.append(ints)\n",
    "\n",
    "    def count_ties(self):\n",
    "        return self.ties\n",
    "\n",
    "    def load_full_train_set(self):\n",
    "        '''Load full train set (to be used for train and valiation).\n",
    "           Use set_max_lines() to leave some data for the test set.'''\n",
    "        minimum = 0\n",
    "        train_size = self.max_lines\n",
    "        if self.verbose:\n",
    "            print('Trying to load %d lines per file...'%train_size)\n",
    "        try:\n",
    "            handle0 = gzip.open(self.files[0],'rt')\n",
    "            handle1 = gzip.open(self.files[1],'rt')\n",
    "            # Associate label 0 with data from file 0. Same for 1.\n",
    "            for i in range(train_size):\n",
    "                row = next(handle0)\n",
    "                self._load_line_(row)\n",
    "                self.labels.append(0)\n",
    "                row = next(handle1)\n",
    "                self._load_line_(row)\n",
    "                self.labels.append(1)\n",
    "            handle0.close()\n",
    "            handle1.close()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('Most likely, one file has too few reads.')\n",
    "            raise Exception('CANNOT LOAD DATA FROM FILE!')\n",
    "\n",
    "    def get_X_y(self):\n",
    "        loaded = len(self.alignments)\n",
    "        divider = int(loaded - loaded * VALID_PORTION)\n",
    "        X_train = np.array(self.alignments[:divider])\n",
    "        y_train = np.array(self.labels[:divider])\n",
    "        X_valid = np.array(self.alignments[divider:])\n",
    "        y_valid = np.array(self.labels[divider:])\n",
    "        if self.verbose:\n",
    "            print('Full train set size = '+str(len(self.alignments)))\n",
    "            print('Training/Validation partition: %d/%d'%(len(y_train),len(y_valid)))\n",
    "        return X_train,y_train, X_valid,y_valid\n",
    "\n",
    "    def get_predictions(self):\n",
    "        loaded = len(self.predictions)\n",
    "        divider = int(loaded - loaded * VALID_PORTION)\n",
    "        y_train = self.predictions[:divider]\n",
    "        y_valid = self.predictions[divider:]\n",
    "        return y_train, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkR5p_t4b4Ex"
   },
   "source": [
    "## Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VCzbj21FMpfU"
   },
   "outputs": [],
   "source": [
    "def show_performance(y_valid, yhat_classes, yhat_pred):\n",
    "    accuracy = accuracy_score(y_valid, yhat_classes)*100.\n",
    "    precision = precision_score(y_valid, yhat_classes)*100.\n",
    "    recall = recall_score(y_valid, yhat_classes)*100.\n",
    "    sensitivity = recall_score(y_valid, yhat_classes, pos_label=1)*100.\n",
    "    specificity = recall_score(y_valid, yhat_classes, pos_label=0)*100.\n",
    "    f1 = f1_score(y_valid, yhat_classes)*100.\n",
    "    mcc = matthews_corrcoef(y_valid, yhat_classes)\n",
    "    if yhat_pred is None:\n",
    "        # these stats are possible for probabilistic models only\n",
    "        auprc = 0.\n",
    "        auroc = 0.\n",
    "    else:\n",
    "        prc_Y, prc_X, prc_bins = precision_recall_curve(y_valid, yhat_pred)\n",
    "        auprc = auc(prc_X,prc_Y)*100.\n",
    "        auroc = roc_auc_score(y_valid, yhat_pred)*100.\n",
    "    values,counts=np.unique(yhat_classes, return_counts=True)\n",
    "    print('Predictions: ', dict(zip(values, counts)))\n",
    "    print('Accuracy: %.2f%% F1: %.2f%% MCC: %.4f' % (accuracy,f1,mcc))\n",
    "    print('Precision: %.2f%% Recall: %.2f%% AUPRC: %.2f%%' % (precision,recall,auprc))\n",
    "    print('Sensitivity: %.2f%% Specificity: %.2f%% AUROC: %.2f%%' % (sensitivity,specificity,auroc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7pcZVyvS_126",
    "outputId": "84dfe9b5-e9fe-4aa2-bb6f-3ac56514e158"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random\n",
      "2023-07-07 07:35:59.078167\n",
      "Maximum lines to load per file: 10000\n",
      "LOADING\n",
      "Trying to load 10000 lines per file...\n",
      "Number of ties: 20000\n",
      "2023-07-07 07:35:59.456456\n",
      "Full train set size = 20000\n",
      "Training/Validation partition: 16000/4000\n",
      "Predictions:  {0: 1960, 1: 2040}\n",
      "Accuracy: 49.50% F1: 50.00% MCC: -0.0100\n",
      "Precision: 49.51% Recall: 50.50% AUPRC: 0.00%\n",
      "Sensitivity: 50.50% Specificity: 48.50% AUROC: 0.00%\n"
     ]
    }
   ],
   "source": [
    "rule='random'\n",
    "print(rule)\n",
    "print(datetime.now())\n",
    "filepath0 = DATA_DIR+DATA_FILE_0\n",
    "filepath1 = DATA_DIR+DATA_FILE_1\n",
    "loader=DataLoader(filepath0,filepath1,rule=rule)\n",
    "loader.set_max_lines(MAX_LINES_TO_LOAD)\n",
    "print('LOADING')\n",
    "loader.load_full_train_set()\n",
    "print('Number of ties: %d' % loader.count_ties())\n",
    "aligner_predictions_train, aligner_predictions_valid = loader.get_predictions()\n",
    "print(datetime.now())\n",
    "X_train,y_train, X_valid,y_valid = loader.get_X_y()\n",
    "show_performance(y_valid, aligner_predictions_valid, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4S2yWyqUcHt9",
    "outputId": "c2a8d51b-e467-4773-c3a3-3395c06e2af9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "align score\n",
      "2023-07-07 07:35:59.598768\n",
      "Maximum lines to load per file: 10000\n",
      "LOADING\n",
      "Trying to load 10000 lines per file...\n",
      "Number of ties: 2751\n",
      "2023-07-07 07:35:59.943399\n",
      "Full train set size = 20000\n",
      "Training/Validation partition: 16000/4000\n",
      "Predictions:  {0: 2370, 1: 1630}\n",
      "Accuracy: 81.25% F1: 79.34% MCC: 0.6360\n",
      "Precision: 88.34% Recall: 72.00% AUPRC: 0.00%\n",
      "Sensitivity: 72.00% Specificity: 90.50% AUROC: 0.00%\n"
     ]
    }
   ],
   "source": [
    "rule='align score'\n",
    "print(rule)\n",
    "print(datetime.now())\n",
    "filepath0 = DATA_DIR+DATA_FILE_0\n",
    "filepath1 = DATA_DIR+DATA_FILE_1\n",
    "loader=DataLoader(filepath0,filepath1,rule=rule)\n",
    "loader.set_max_lines(MAX_LINES_TO_LOAD)\n",
    "print('LOADING')\n",
    "loader.load_full_train_set()\n",
    "print('Number of ties: %d' % loader.count_ties())\n",
    "aligner_predictions_train, aligner_predictions_valid = loader.get_predictions()\n",
    "print(datetime.now())\n",
    "X_train,y_train, X_valid,y_valid = loader.get_X_y()\n",
    "show_performance(y_valid, aligner_predictions_valid, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "a86dksKXYwvy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches\n",
      "2023-07-07 07:36:00.093624\n",
      "Maximum lines to load per file: 10000\n",
      "LOADING\n",
      "Trying to load 10000 lines per file...\n",
      "Number of ties: 2829\n",
      "2023-07-07 07:36:00.554144\n",
      "Full train set size = 20000\n",
      "Training/Validation partition: 16000/4000\n",
      "Predictions:  {0: 2402, 1: 1598}\n",
      "Accuracy: 80.90% F1: 78.77% MCC: 0.6309\n",
      "Precision: 88.67% Recall: 70.85% AUPRC: 0.00%\n",
      "Sensitivity: 70.85% Specificity: 90.95% AUROC: 0.00%\n"
     ]
    }
   ],
   "source": [
    "rule='matches'\n",
    "print(rule)\n",
    "print(datetime.now())\n",
    "filepath0 = DATA_DIR+DATA_FILE_0\n",
    "filepath1 = DATA_DIR+DATA_FILE_1\n",
    "loader=DataLoader(filepath0,filepath1,rule=rule)\n",
    "loader.set_max_lines(MAX_LINES_TO_LOAD)\n",
    "print('LOADING')\n",
    "loader.load_full_train_set()\n",
    "print('Number of ties: %d' % loader.count_ties())\n",
    "aligner_predictions_train, aligner_predictions_valid = loader.get_predictions()\n",
    "print(datetime.now())\n",
    "X_train,y_train, X_valid,y_valid = loader.get_X_y()\n",
    "show_performance(y_valid, aligner_predictions_valid, None)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
