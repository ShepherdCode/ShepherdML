{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG-tGRnlFLA3"
      },
      "source": [
        "# Scoring Schemes\n",
        "Arabidopsis, star, transcripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RmwUsVLFLA6",
        "outputId": "c52381f2-72ee-4702-be36-1d0bb79094dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-07 11:56:44.006223\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "print(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlzN9OdsFWEU",
        "outputId": "0ea098d4-7be5-427d-e18e-9d61dcdd8122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU device not found\n",
            "Running on CoLab\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "dt='float32'\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "tf.random.set_seed(42) # supposedly leads to reproducible results\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    print('GPU device not found')\n",
        "else:\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    print('Running on CoLab')\n",
        "    PATH='/content/drive/'\n",
        "    drive.mount(PATH)\n",
        "    DATA_DIR=PATH+'My Drive/data/IRP2/'  # must end in \"/\"\n",
        "    MODEL_DIR=PATH+'My Drive/data/IRP2/Models/'  # must end in \"/\"\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print('Running on Mac')\n",
        "    DATA_DIR=\"/Users/jasonmiller/WVU/BAM_ML/\"\n",
        "    MODEL_DIR=\"/Users/jasonmiller/WVU/BAM_ML/Models/\"\n",
        "SAVE_MODEL_FILENAME = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIAT2G5DYwvS",
        "outputId": "2d208d70-a754-40f2-e477-d82ba02827aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "sklearn 1.2.2\n"
          ]
        }
      ],
      "source": [
        "from platform import python_version\n",
        "print('Python',python_version())\n",
        "import random\n",
        "import numpy as np\n",
        "np.random.seed(42) # supposedly sets scikit-learn\n",
        "import pandas as pd  # for plotting\n",
        "import time # sleep function\n",
        "from os.path import isfile\n",
        "import gzip\n",
        "from matplotlib import pyplot as plt\n",
        "import sklearn   # pip install --upgrade scikit-learn\n",
        "print('sklearn',sklearn.__version__)\n",
        "\n",
        "from tensorflow import keras\n",
        "# consider sklearn.metrics.classification_report\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier as RFC\n",
        "\n",
        "EPOCHS=150"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtqdpJOxFLBA"
      },
      "source": [
        "## Data Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnkpVKdMFLA-",
        "outputId": "e1afb7b0-9321-422f-e58f-51543a81936d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data directory: /content/drive/My Drive/data/IRP2/\n",
            "Data file 0 Arabidopsis/STAR/lyrata/read_stats.csv.gz\n",
            "Data file 1 Arabidopsis/STAR/halleri/read_stats.csv.gz\n",
            "Input lines for training: 1000000\n"
          ]
        }
      ],
      "source": [
        "MAX_LINES_TO_LOAD =    1000000 # training - 1M lines requires 2GB RAM\n",
        "#MAX_LINES_TO_LOAD =    10000 # use this for debugging\n",
        "\n",
        "VALID_PORTION = 0.20\n",
        "\n",
        "DATA_FILE_0 = 'Arabidopsis/STAR/lyrata/read_stats.csv.gz'\n",
        "DATA_FILE_1 = 'Arabidopsis/STAR/halleri/read_stats.csv.gz'\n",
        "\n",
        "print('Data directory: %s'%DATA_DIR)\n",
        "print('Data file 0 %s'%DATA_FILE_0)\n",
        "print('Data file 1 %s'%DATA_FILE_1)\n",
        "print('Input lines for training: %d'%MAX_LINES_TO_LOAD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "p35ehKV3Kq0z"
      },
      "outputs": [],
      "source": [
        "class DataLoader():\n",
        "    def __init__(self,filepath1,filepath2,rule=None,verbose=True):\n",
        "        self.files = [filepath1,filepath2]\n",
        "        self.alignments=[]\n",
        "        self.labels=[]\n",
        "        self.verbose = verbose\n",
        "        self.max_lines = None\n",
        "        self.ties = 0\n",
        "        self.predictions = []\n",
        "        self.rule = rule\n",
        "\n",
        "    def set_max_lines(self,lines):\n",
        "        '''Limit the dataset size to fit in RAM.'''\n",
        "        self.max_lines = lines\n",
        "        if self.verbose:\n",
        "            print('Maximum lines to load per file: %d'%lines)\n",
        "\n",
        "    def _count_lines_(self):\n",
        "        '''Show number of lines per input file.'''\n",
        "        count0 = 0\n",
        "        with gzip.open (self.files[0],'rt') as handle0:\n",
        "            for row in handle0:\n",
        "                count0 += 1\n",
        "        count1 = 0\n",
        "        with gzip.open(self.files[1],'rt') as handle1:\n",
        "            for row in handle1:\n",
        "                count1 += 1\n",
        "        minimum = min(count0,count1)\n",
        "        if self.verbose:\n",
        "            print('File0 size: %d %s'%(count0,self.files[0]))\n",
        "            print('File1 size: %d %s'%(count1,self.files[1]))\n",
        "        return minimum\n",
        "\n",
        "    def _load_line_(self,row):\n",
        "        '''Load data structure from one line of CSV file.'''\n",
        "        line = row.strip()\n",
        "        fields = line.split(',')\n",
        "        ints = [0] * 61\n",
        "        # These fields come straight from the input file.\n",
        "        # These fields are grouped by which read they describe.\n",
        "        # P1 R1 = Parent 1, Read 1\n",
        "        ints[0] = int(fields[0]) # P1 R1 AS\n",
        "        ints[1] = int(fields[1]) # P1 R1 ED\n",
        "        ints[2] = int(fields[2]) # P1 R1 MAT\n",
        "        ints[3] = int(fields[3]) # P1 R1 MM\n",
        "        ints[4] = int(fields[4]) # P1 R1 HQMM\n",
        "        ints[5] = int(fields[5]) # P1 R1 GO\n",
        "        ints[6] = int(fields[6]) # P1 R1 GE\n",
        "        ints[7] = int(fields[7]) # P1 R1 INS\n",
        "        ints[8] = int(fields[8]) # P1 R1 DELS\n",
        "        ints[9] = int(fields[9]) # P1 R1 HQINS\n",
        "        ints[10] = int(fields[10]) # P1 R1 HQDEL\n",
        "        # P1 R2 = Parent 1, Read 2\n",
        "        ints[11] = int(fields[11]) # P1 R2 AS\n",
        "        ints[12] = int(fields[12]) # P1 R2 ED\n",
        "        ints[13] = int(fields[13]) # P1 R2 MAT\n",
        "        ints[14] = int(fields[14]) # P1 R2 MM\n",
        "        ints[15] = int(fields[15]) # P1 R2 HQMM\n",
        "        ints[16] = int(fields[16]) # P1 R2 GO\n",
        "        ints[17] = int(fields[17]) # P1 R2 GE\n",
        "        ints[18] = int(fields[18]) # P1 R2 INS\n",
        "        ints[19] = int(fields[19]) # P1 R2 DELS\n",
        "        ints[20] = int(fields[20]) # P1 R2 HQINS\n",
        "        ints[21] = int(fields[21]) # P1 R2 HQDEL\n",
        "        # P2 R1 = Parent 2, Read 1\n",
        "        ints[22] = int(fields[22]) # P2 R1 AS\n",
        "        ints[23] = int(fields[23]) # P2 R1 ED\n",
        "        ints[24] = int(fields[24]) # P2 R1 MAT\n",
        "        ints[25] = int(fields[25]) # P2 R1 MM\n",
        "        ints[26] = int(fields[26]) # P2 R1 HQMM\n",
        "        ints[27] = int(fields[27]) # P2 R1 GO\n",
        "        ints[28] = int(fields[28]) # P2 R1 GE\n",
        "        ints[29] = int(fields[29]) # P2 R1 INS\n",
        "        ints[30] = int(fields[30]) # P2 R1 DELS\n",
        "        ints[31] = int(fields[31]) # P2 R1 HQINS\n",
        "        ints[32] = int(fields[32]) # P2 R1 HQDEL\n",
        "        # P2 R2 = Parent 2, Read 2\n",
        "        ints[33] = int(fields[33]) # P2 R2 AS\n",
        "        ints[34] = int(fields[34]) # P2 R2 ED\n",
        "        ints[35] = int(fields[35]) # P2 R2 MAT\n",
        "        ints[36] = int(fields[36]) # P2 R2 MM\n",
        "        ints[37] = int(fields[37]) # P2 R2 HQMM\n",
        "        ints[38] = int(fields[38]) # P2 R2 GO\n",
        "        ints[39] = int(fields[39]) # P2 R2 GE\n",
        "        ints[40] = int(fields[40]) # P2 R2 INS\n",
        "        ints[41] = int(fields[41]) # P2 R2 DELS\n",
        "        ints[42] = int(fields[42]) # P2 R2 HQINS\n",
        "        ints[43] = int(fields[43]) # P2 R2 HQDEL\n",
        "        # Fields that come in twos\n",
        "        ints[44] = int(fields[44]) # R1 length (of read)\n",
        "        ints[45] = int(fields[45]) # R2 length (of read)\n",
        "        ints[46] = int(fields[46]) # P1 span (of mapped read pair)\n",
        "        ints[47] = int(fields[47]) # P2 span (of mapped read pair)\n",
        "        # Read-wise differences\n",
        "        ints[48] = ints[47]-ints[46] # P2-P1 span diff\n",
        "        ints[49] = (ints[33]+ints[22])-(ints[11]+ints[0]) # AS diff\n",
        "        ints[50] = (ints[34]+ints[23])-(ints[12]+ints[1]) # ED diff\n",
        "        ints[51] = (ints[35]+ints[24])-(ints[13]+ints[2]) # MAT diff\n",
        "        ints[52] = (ints[36]+ints[25])-(ints[14]+ints[3]) # MM diff\n",
        "        ints[53] = (ints[37]+ints[26])-(ints[15]+ints[4]) # HQMM diff\n",
        "        ints[54] = (ints[38]+ints[27])-(ints[16]+ints[5]) # GO diff\n",
        "        ints[55] = (ints[39]+ints[28])-(ints[17]+ints[6]) # GE diff\n",
        "        ints[56] = (ints[40]+ints[29])-(ints[18]+ints[7]) # INS diff\n",
        "        ints[57] = (ints[41]+ints[30])-(ints[19]+ints[8]) # DELS diff\n",
        "        ints[58] = (ints[42]+ints[31])-(ints[20]+ints[9]) # HQINS diff\n",
        "        ints[59] = (ints[43]+ints[32])-(ints[21]+ints[10]) # HQDEL diff\n",
        "        # The feature-extraction program populated fields[36]\n",
        "        # to indicate which parent had higher alignment score.\n",
        "        # Values were 0=same, 1=parent1, 2=parent2.\n",
        "        # We change the values to -1=parent1, 0=unknown, +1=parent2\n",
        "        parent_choice = int(fields[48])\n",
        "        if parent_choice == 1:\n",
        "            ints[60] = -1  # not parent 2\n",
        "        elif parent_choice == 2:\n",
        "            ints[60] = 1  # is parent 2\n",
        "        elif parent_choice == 0:\n",
        "            ints[60] = 0\n",
        "        else:\n",
        "            raise Exception('Unrecognized parent choice:'+str(parent_choice))\n",
        "        if self.rule == 'random':\n",
        "            self.ties += 1\n",
        "            guess = random.randint(0,1)\n",
        "            self.predictions.append(guess)\n",
        "        elif self.rule == 'matches':\n",
        "            if ints[51] > 0:\n",
        "                self.predictions.append(1) # is parent 2\n",
        "            elif ints[51] < 0:\n",
        "                self.predictions.append(0) # not parent 2\n",
        "            else:   # guess\n",
        "                self.ties += 1\n",
        "                guess = random.randint(0,1)\n",
        "                self.predictions.append(guess)\n",
        "        elif self.rule == 'edit':\n",
        "            if ints[50] < 0:\n",
        "                self.predictions.append(1) # is parent 2\n",
        "            elif ints[50] > 0:\n",
        "                self.predictions.append(0) # not parent 2\n",
        "            else:   # guess\n",
        "                self.ties += 1\n",
        "                guess = random.randint(0,1)\n",
        "                self.predictions.append(guess)\n",
        "        else:\n",
        "            # Default: align score\n",
        "            # We change 1 to 0, 2 to 1, and 0 to 1 or 2 randomly.\n",
        "            parent_choice = int(fields[48])\n",
        "            if parent_choice == 1:\n",
        "                self.predictions.append(0)  # not parent 2\n",
        "            elif parent_choice == 2:\n",
        "                self.predictions.append(1)  # is parent 2\n",
        "            else: # guess\n",
        "                self.ties += 1\n",
        "                guess = random.randint(0,1)\n",
        "                self.predictions.append(guess)\n",
        "        # The transcript that this read pair aligned to.\n",
        "        # This is for pipelines that only process reads that map\n",
        "        # to same transcript in both parents and (filter the others).\n",
        "        # Pipelines that assign reads to parent, regardless of which gene,\n",
        "        # should ignore this value. (It only reflects first parent map anyway.)\n",
        "        transcript_id = fields[49] # TO DO: where to put this?\n",
        "        self.alignments.append(ints)\n",
        "\n",
        "    def count_ties(self):\n",
        "        return self.ties\n",
        "\n",
        "    def load_full_train_set(self):\n",
        "        '''Load full train set (to be used for train and valiation).\n",
        "           Use set_max_lines() to leave some data for the test set.'''\n",
        "        minimum = 0\n",
        "        train_size = self.max_lines\n",
        "        if self.verbose:\n",
        "            print('Trying to load %d lines per file...'%train_size)\n",
        "        try:\n",
        "            handle0 = gzip.open(self.files[0],'rt')\n",
        "            handle1 = gzip.open(self.files[1],'rt')\n",
        "            # Associate label 0 with data from file 0. Same for 1.\n",
        "            for i in range(train_size):\n",
        "                row = next(handle0)\n",
        "                self._load_line_(row)\n",
        "                self.labels.append(0)\n",
        "                row = next(handle1)\n",
        "                self._load_line_(row)\n",
        "                self.labels.append(1)\n",
        "            handle0.close()\n",
        "            handle1.close()\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            print('Most likely, one file has too few reads.')\n",
        "            raise Exception('CANNOT LOAD DATA FROM FILE!')\n",
        "\n",
        "    def get_X_y(self):\n",
        "        loaded = len(self.alignments)\n",
        "        divider = int(loaded - loaded * VALID_PORTION)\n",
        "        X_train = np.array(self.alignments[:divider])\n",
        "        y_train = np.array(self.labels[:divider])\n",
        "        X_valid = np.array(self.alignments[divider:])\n",
        "        y_valid = np.array(self.labels[divider:])\n",
        "        if self.verbose:\n",
        "            print('Full train set size = '+str(len(self.alignments)))\n",
        "            print('Training/Validation partition: %d/%d'%(len(y_train),len(y_valid)))\n",
        "        return X_train,y_train, X_valid,y_valid\n",
        "\n",
        "    def get_predictions(self):\n",
        "        loaded = len(self.predictions)\n",
        "        divider = int(loaded - loaded * VALID_PORTION)\n",
        "        y_train = self.predictions[:divider]\n",
        "        y_valid = self.predictions[divider:]\n",
        "        return y_train, y_valid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkR5p_t4b4Ex"
      },
      "source": [
        "## Comparisons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "VCzbj21FMpfU"
      },
      "outputs": [],
      "source": [
        "def show_performance(y_valid, yhat_classes, yhat_pred):\n",
        "    accuracy = accuracy_score(y_valid, yhat_classes)*100.\n",
        "    precision = precision_score(y_valid, yhat_classes)*100.\n",
        "    recall = recall_score(y_valid, yhat_classes)*100.\n",
        "    sensitivity = recall_score(y_valid, yhat_classes, pos_label=1)*100.\n",
        "    specificity = recall_score(y_valid, yhat_classes, pos_label=0)*100.\n",
        "    f1 = f1_score(y_valid, yhat_classes)*100.\n",
        "    mcc = matthews_corrcoef(y_valid, yhat_classes)\n",
        "    if yhat_pred is None:\n",
        "        # these stats are possible for probabilistic models only\n",
        "        auprc = 0.\n",
        "        auroc = 0.\n",
        "    else:\n",
        "        prc_Y, prc_X, prc_bins = precision_recall_curve(y_valid, yhat_pred)\n",
        "        auprc = auc(prc_X,prc_Y)*100.\n",
        "        auroc = roc_auc_score(y_valid, yhat_pred)*100.\n",
        "    values,counts=np.unique(yhat_classes, return_counts=True)\n",
        "    print('Predictions: ', dict(zip(values, counts)))\n",
        "    print('Accuracy: %.2f%% F1: %.2f%% MCC: %.4f' % (accuracy,f1,mcc))\n",
        "    print('Precision: %.2f%% Recall: %.2f%% AUPRC: %.2f%%' % (precision,recall,auprc))\n",
        "    print('Sensitivity: %.2f%% Specificity: %.2f%% AUROC: %.2f%%' % (sensitivity,specificity,auroc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pcZVyvS_126",
        "outputId": "5e7c2dee-1c43-403c-dc92-dcf23c2edd07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random\n",
            "2023-07-07 11:56:45.439977\n",
            "Maximum lines to load per file: 1000000\n",
            "LOADING\n",
            "Trying to load 1000000 lines per file...\n",
            "Number of ties: 2000000\n",
            "2023-07-07 11:57:23.589503\n",
            "Full train set size = 2000000\n",
            "Training/Validation partition: 1600000/400000\n",
            "Predictions:  {0: 200552, 1: 199448}\n",
            "Accuracy: 50.14% F1: 50.07% MCC: 0.0027\n",
            "Precision: 50.14% Recall: 50.00% AUPRC: 0.00%\n",
            "Sensitivity: 50.00% Specificity: 50.28% AUROC: 0.00%\n"
          ]
        }
      ],
      "source": [
        "rule='random'\n",
        "print(rule)\n",
        "print(datetime.now())\n",
        "filepath0 = DATA_DIR+DATA_FILE_0\n",
        "filepath1 = DATA_DIR+DATA_FILE_1\n",
        "loader=DataLoader(filepath0,filepath1,rule=rule)\n",
        "loader.set_max_lines(MAX_LINES_TO_LOAD)\n",
        "print('LOADING')\n",
        "loader.load_full_train_set()\n",
        "print('Number of ties: %d' % loader.count_ties())\n",
        "aligner_predictions_train, aligner_predictions_valid = loader.get_predictions()\n",
        "print(datetime.now())\n",
        "X_train,y_train, X_valid,y_valid = loader.get_X_y()\n",
        "show_performance(y_valid, aligner_predictions_valid, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S2yWyqUcHt9",
        "outputId": "96485b29-4f4b-4aaf-b6e3-ad971c4feae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "align score\n",
            "2023-07-07 11:57:41.564023\n",
            "Maximum lines to load per file: 1000000\n",
            "LOADING\n",
            "Trying to load 1000000 lines per file...\n",
            "Number of ties: 294546\n",
            "2023-07-07 11:58:17.756666\n",
            "Full train set size = 2000000\n",
            "Training/Validation partition: 1600000/400000\n",
            "Predictions:  {0: 244138, 1: 155862}\n",
            "Accuracy: 80.94% F1: 78.57% MCC: 0.6344\n",
            "Precision: 89.70% Recall: 69.90% AUPRC: 0.00%\n",
            "Sensitivity: 69.90% Specificity: 91.97% AUROC: 0.00%\n"
          ]
        }
      ],
      "source": [
        "rule='align score'\n",
        "print(rule)\n",
        "print(datetime.now())\n",
        "filepath0 = DATA_DIR+DATA_FILE_0\n",
        "filepath1 = DATA_DIR+DATA_FILE_1\n",
        "loader=DataLoader(filepath0,filepath1,rule=rule)\n",
        "loader.set_max_lines(MAX_LINES_TO_LOAD)\n",
        "print('LOADING')\n",
        "loader.load_full_train_set()\n",
        "print('Number of ties: %d' % loader.count_ties())\n",
        "aligner_predictions_train, aligner_predictions_valid = loader.get_predictions()\n",
        "print(datetime.now())\n",
        "X_train,y_train, X_valid,y_valid = loader.get_X_y()\n",
        "show_performance(y_valid, aligner_predictions_valid, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "a86dksKXYwvy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a63c853-1f92-4b81-af71-fc8d6cd60721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matches\n",
            "2023-07-07 11:58:33.442525\n",
            "Maximum lines to load per file: 1000000\n",
            "LOADING\n",
            "Trying to load 1000000 lines per file...\n",
            "Number of ties: 297769\n",
            "2023-07-07 11:59:08.042443\n",
            "Full train set size = 2000000\n",
            "Training/Validation partition: 1600000/400000\n",
            "Predictions:  {0: 244992, 1: 155008}\n",
            "Accuracy: 80.91% F1: 78.49% MCC: 0.6345\n",
            "Precision: 89.89% Recall: 69.67% AUPRC: 0.00%\n",
            "Sensitivity: 69.67% Specificity: 92.16% AUROC: 0.00%\n"
          ]
        }
      ],
      "source": [
        "rule='matches'\n",
        "print(rule)\n",
        "print(datetime.now())\n",
        "filepath0 = DATA_DIR+DATA_FILE_0\n",
        "filepath1 = DATA_DIR+DATA_FILE_1\n",
        "loader=DataLoader(filepath0,filepath1,rule=rule)\n",
        "loader.set_max_lines(MAX_LINES_TO_LOAD)\n",
        "print('LOADING')\n",
        "loader.load_full_train_set()\n",
        "print('Number of ties: %d' % loader.count_ties())\n",
        "aligner_predictions_train, aligner_predictions_valid = loader.get_predictions()\n",
        "print(datetime.now())\n",
        "X_train,y_train, X_valid,y_valid = loader.get_X_y()\n",
        "show_performance(y_valid, aligner_predictions_valid, None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rule='edit'\n",
        "print(rule)\n",
        "print(datetime.now())\n",
        "filepath0 = DATA_DIR+DATA_FILE_0\n",
        "filepath1 = DATA_DIR+DATA_FILE_1\n",
        "loader=DataLoader(filepath0,filepath1,rule=rule)\n",
        "loader.set_max_lines(MAX_LINES_TO_LOAD)\n",
        "print('LOADING')\n",
        "loader.load_full_train_set()\n",
        "print('Number of ties: %d' % loader.count_ties())\n",
        "aligner_predictions_train, aligner_predictions_valid = loader.get_predictions()\n",
        "print(datetime.now())\n",
        "X_train,y_train, X_valid,y_valid = loader.get_X_y()\n",
        "show_performance(y_valid, aligner_predictions_valid, None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0VGJ1iKLWwA",
        "outputId": "ff5c593c-b28e-4978-8948-0d73873a65cc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "edit\n",
            "2023-07-07 11:59:24.668452\n",
            "Maximum lines to load per file: 1000000\n",
            "LOADING\n",
            "Trying to load 1000000 lines per file...\n",
            "Number of ties: 295841\n",
            "2023-07-07 11:59:59.699658\n",
            "Full train set size = 2000000\n",
            "Training/Validation partition: 1600000/400000\n",
            "Predictions:  {0: 244531, 1: 155469}\n",
            "Accuracy: 80.84% F1: 78.43% MCC: 0.6326\n",
            "Precision: 89.67% Recall: 69.70% AUPRC: 0.00%\n",
            "Sensitivity: 69.70% Specificity: 91.97% AUROC: 0.00%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}