{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PG-tGRnlFLA3"
   },
   "source": [
    "# Random Forest\n",
    "Restart with new input files that contain trainscript IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RmwUsVLFLA6",
    "outputId": "f111ed8f-4535-4ba7-f373-5d1f2763a7c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-07 09:04:23.442643\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OlzN9OdsFWEU",
    "outputId": "54a4fadb-2a80-4e5a-917e-7b7b7196049e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 09:04:23.475354: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found\n",
      "Running on Mac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 09:04:32.277812: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "dt='float32'\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "tf.random.set_seed(42) # supposedly leads to reproducible results\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print('GPU device not found')\n",
    "else:\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print('Running on CoLab')\n",
    "    PATH='/content/drive/'\n",
    "    drive.mount(PATH)\n",
    "    DATA_DIR=PATH+'My Drive/data/IRP2/'  # must end in \"/\"\n",
    "    MODEL_DIR=PATH+'My Drive/data/IRP2/Models/'  # must end in \"/\"\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print('Running on Mac')\n",
    "    DATA_DIR=\"/Users/jasonmiller/WVU/BAM_ML/\"\n",
    "    MODEL_DIR=\"/Users/jasonmiller/WVU/BAM_ML/Models/\"\n",
    "SAVE_MODEL_FILENAME = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PRX-UEr8FLA8",
    "outputId": "627dd8e8-1de9-4ac2-d21e-b822cef810d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.0\n",
      "sklearn 1.1.2\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print('Python',python_version())\n",
    "import numpy as np\n",
    "np.random.seed(42) # supposedly sets scikit-learn\n",
    "import pandas as pd  # for plotting\n",
    "import time # sleep function\n",
    "from os.path import isfile\n",
    "from matplotlib import pyplot as plt \n",
    "import sklearn   # pip install --upgrade scikit-learn\n",
    "print('sklearn',sklearn.__version__)\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "\n",
    "EPOCHS=150 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtqdpJOxFLBA"
   },
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LnkpVKdMFLA-",
    "outputId": "8af3f31a-3fc8-4659-ba5c-2b01d4396a30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /Users/jasonmiller/WVU/BAM_ML/\n",
      "Data file 0 MxM_BR4/ml_stats.csv\n",
      "Data file 1 SxS_BR4/ml_stats.csv\n",
      "Input lines for training: 1000\n"
     ]
    }
   ],
   "source": [
    "MAX_LINES_TO_LOAD =    1000000 # training - 1M lines requires 2GB RAM\n",
    "MAX_LINES_TO_LOAD =    1000 # use this for debugging\n",
    "VALID_PORTION = 0.20\n",
    "\n",
    "DATA_FILE_0 = 'MxM_BR4/ml_stats.csv'\n",
    "DATA_FILE_1 = 'SxS_BR4/ml_stats.csv'\n",
    "\n",
    "print('Data directory: %s'%DATA_DIR)\n",
    "print('Data file 0 %s'%DATA_FILE_0)\n",
    "print('Data file 1 %s'%DATA_FILE_1)\n",
    "print('Input lines for training: %d'%MAX_LINES_TO_LOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "p35ehKV3Kq0z"
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self,filepath1,filepath2,verbose=True):\n",
    "        self.files = [filepath1,filepath2]\n",
    "        self.alignments=[]\n",
    "        self.labels=[]\n",
    "        self.is_primary={'P':1, 'S':0}\n",
    "        self.verbose = verbose\n",
    "        self.max_lines = None\n",
    "        \n",
    "    def set_max_lines(self,lines):\n",
    "        '''Limit the dataset size to fit in RAM.'''\n",
    "        self.max_lines = lines\n",
    "        if self.verbose:\n",
    "            print('Maximum lines to load per file: %d'%lines)\n",
    "        \n",
    "    def _count_lines_(self):\n",
    "        '''Show number of lines per input file.'''\n",
    "        count0 = 0\n",
    "        with open (self.files[0],'r') as handle0:\n",
    "            for row in handle0:\n",
    "                count0 += 1\n",
    "        count1 = 0\n",
    "        with open(self.files[1],'r') as handle1:\n",
    "            for row in handle1:\n",
    "                count1 += 1\n",
    "        minimum = min(count0,count1)\n",
    "        if self.verbose:\n",
    "            print('File0 size: %d %s'%(count0,self.files[0]))\n",
    "            print('File1 size: %d %s'%(count1,self.files[1]))\n",
    "        return minimum\n",
    "        \n",
    "    def _load_line_(self,row):\n",
    "        '''Load data structure from one line of CSV file.'''\n",
    "        line = row.strip()\n",
    "        fields = line.split(',')\n",
    "        ints = [0] * 28\n",
    "        # These fields come straight from the input file.\n",
    "        # These fields are grouped by which read they describe.\n",
    "        # P1 R1 AS = Parent 1, Read 1, Alignment Score\n",
    "        ints[0] = int(fields[1]) # P1 R1 AS\n",
    "        ints[1] = int(fields[2]) # P1 R1 ED\n",
    "        ints[2] = int(fields[3]) # P1 R1 MM\n",
    "        ints[3] = int(fields[4]) # P1 R1 GO\n",
    "        ints[4] = int(fields[5]) # P1 R1 GE\n",
    "        ints[5] = int(fields[7]) # P1 R2 AS\n",
    "        ints[6] = int(fields[8]) # P1 R2 ED\n",
    "        ints[7] = int(fields[9]) # P1 R2 MM\n",
    "        ints[8] = int(fields[10]) # P1 R2 GO\n",
    "        ints[9] = int(fields[11]) # P1 R2 GE\n",
    "        ints[10] = int(fields[13]) # P2 R1 AS\n",
    "        ints[11] = int(fields[14]) # P2 R1 ED\n",
    "        ints[12] = int(fields[15]) # P2 R1 MM\n",
    "        ints[13] = int(fields[16]) # P2 R1 GO\n",
    "        ints[14] = int(fields[17]) # P2 R1 GE\n",
    "        ints[15] = int(fields[19]) # P2 R2 AS\n",
    "        ints[16] = int(fields[20]) # P2 R2 ED\n",
    "        ints[17] = int(fields[21]) # P2 R2 MM\n",
    "        ints[18] = int(fields[22]) # P2 R2 GO\n",
    "        ints[19] = int(fields[23]) # P2 R2 GE    \n",
    "        # We compute these 'diff' fields from the input data.\n",
    "        ints[20] = int(((ints[10]+ints[15])-(ints[0]+ints[5]))/2) # AS diff\n",
    "        ints[21] = int(((ints[11]+ints[16])-(ints[1]+ints[6]))/2) # ED diff\n",
    "        ints[22] = int(((ints[12]+ints[17])-(ints[2]+ints[7]))/2) # MM diff\n",
    "        ints[23] = int(((ints[13]+ints[18])-(ints[3]+ints[8]))/2) # GO diff\n",
    "        ints[24] = int(((ints[14]+ints[19])-(ints[4]+ints[9]))/2) # GE diff\n",
    "        # The data file contains one letter describing parent 2 (e.g. SxS).\n",
    "        # The letter is P for primary or S for secondary (derived from a BAM flag).\n",
    "        # We convert so is_primary==1 means parent 2 is the primary parent.\n",
    "        primary_letter = fields[12]  # parent 2 is P=primary or S=Secondary\n",
    "        primary_digit = self.is_primary[primary_letter] # convert P to 1, S to 0\n",
    "        ints[25] = primary_digit  # value 1 means Parent 2 is primary\n",
    "        # The span of this read pair along either parent.\n",
    "        ints[26] = int(fields[24]) # span on P1\n",
    "        ints[27] = int(fields[25]) # span on P2\n",
    "        # The transcript (in both parents) that this read pair aligned to.\n",
    "        # An upstream filter removes cases of different transcript per parent.\n",
    "        transcript_id = fields[26] # TO DO: where to put this?\n",
    "        self.alignments.append(ints)\n",
    "    \n",
    "    def load_full_train_set(self):\n",
    "        '''Load full train set (to be used for train and valiation).\n",
    "           Use set_max_lines() to leave some data for the test set.'''\n",
    "        minimum = 0\n",
    "        train_size = self.max_lines\n",
    "        if self.verbose:\n",
    "            print('Trying to load %d lines per file...'%train_size)\n",
    "        try:\n",
    "            handle0 = open(self.files[0],'r')\n",
    "            handle1 = open(self.files[1],'r')\n",
    "            # Associate label 0 with data from file 0. Same for 1.\n",
    "            for i in range(train_size):\n",
    "                row = next(handle0)\n",
    "                self._load_line_(row)\n",
    "                self.labels.append(0) \n",
    "                row = next(handle1)\n",
    "                self._load_line_(row)\n",
    "                self.labels.append(1)\n",
    "            handle0.close()\n",
    "            handle1.close()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise Exception('CANNOT LOAD DATA FROM FILE!')\n",
    "\n",
    "    def show_examples(self,head=6):\n",
    "        head = min(head,len(self.alignments))\n",
    "        for i in range(head):\n",
    "            print('From '+self.files[self.labels[i]])\n",
    "            print('Score,Edit,Mismatch,GapOpen,GapExtend')\n",
    "            print(self.alignments[i][0:5])\n",
    "            print(self.alignments[i][5:10])\n",
    "            print(self.alignments[i][10:15])\n",
    "            print(self.alignments[i][15:20])\n",
    "            print(self.alignments[i][20:26])\n",
    "            \n",
    "    def get_X_y(self):\n",
    "        loaded = len(self.alignments)\n",
    "        divider = int(loaded - loaded * VALID_PORTION)\n",
    "        X_train = np.array(self.alignments[:divider])\n",
    "        y_train = np.array(self.labels[:divider])\n",
    "        X_valid = np.array(self.alignments[divider:])\n",
    "        y_valid = np.array(self.labels[divider:])\n",
    "        if self.verbose:\n",
    "            print('Full train set size = '+str(len(self.alignments)))\n",
    "            print('Training/Validation partition: %d/%d'%(len(y_train),len(y_valid)))\n",
    "        return X_train,y_train, X_valid,y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7pcZVyvS_126",
    "outputId": "664f2168-adb1-4b65-9786-7f06f69d68b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-07 09:04:35.186846\n",
      "Maximum lines to load per file: 1000\n",
      "LOADING\n",
      "Trying to load 1000 lines per file...\n",
      "2023-06-07 09:04:35.216103\n",
      "From /Users/jasonmiller/WVU/BAM_ML/MxM_BR4/ml_stats.csv\n",
      "Score,Edit,Mismatch,GapOpen,GapExtend\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/SxS_BR4/ml_stats.csv\n",
      "Score,Edit,Mismatch,GapOpen,GapExtend\n",
      "[-25, 5, 5, 0, 0]\n",
      "[-10, 2, 2, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[17, -3, -3, 0, 0, 1]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/MxM_BR4/ml_stats.csv\n",
      "Score,Edit,Mismatch,GapOpen,GapExtend\n",
      "[-5, 1, 1, 0, 0]\n",
      "[-6, 2, 2, 0, 0]\n",
      "[-5, 1, 1, 0, 0]\n",
      "[-6, 2, 2, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/SxS_BR4/ml_stats.csv\n",
      "Score,Edit,Mismatch,GapOpen,GapExtend\n",
      "[-19, 4, 4, 0, 0]\n",
      "[-24, 6, 6, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[-4, 2, 2, 0, 0]\n",
      "[19, -4, -4, 0, 0, 1]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/MxM_BR4/ml_stats.csv\n",
      "Score,Edit,Mismatch,GapOpen,GapExtend\n",
      "[0, 0, 0, 0, 0]\n",
      "[-1, 1, 1, 0, 0]\n",
      "[-5, 1, 1, 0, 0]\n",
      "[-4, 2, 2, 0, 0]\n",
      "[-4, 1, 1, 0, 0, 0]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/SxS_BR4/ml_stats.csv\n",
      "Score,Edit,Mismatch,GapOpen,GapExtend\n",
      "[-3, 1, 1, 0, 0]\n",
      "[-11, 6, 6, 0, 0]\n",
      "[-3, 1, 1, 0, 0]\n",
      "[-11, 6, 6, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "filepath0 = DATA_DIR+DATA_FILE_0\n",
    "filepath1 = DATA_DIR+DATA_FILE_1\n",
    "loader=DataLoader(filepath0,filepath1)\n",
    "loader.set_max_lines(MAX_LINES_TO_LOAD)\n",
    "print('LOADING')\n",
    "loader.load_full_train_set()\n",
    "print(datetime.now())\n",
    "loader.show_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7uHn9Ib_129",
    "outputId": "9b29ce0c-5467-450c-9436-d63a56a1b561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train set size = 2000\n",
      "Training/Validation partition: 1600/400\n",
      "X train shape: \n",
      "(1600, 28)\n",
      "y train shape: \n",
      "(1600,)\n",
      "X valid shape: \n",
      "(400, 28)\n",
      "y valid shape: \n",
      "(400,)\n",
      "X[5]=\n",
      "[ -3   1   1   0   0 -11   6   6   0   0  -3   1   1   0   0 -11   6   6\n",
      "   0   0   0   0   0   0   0   1 260 260]\n",
      "y[5]=\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train, X_valid,y_valid = loader.get_X_y()\n",
    "print('X train shape: ')\n",
    "print(np.shape(X_train))\n",
    "print('y train shape: ')\n",
    "print(np.shape(y_train))\n",
    "print('X valid shape: ')\n",
    "print(np.shape(X_valid))\n",
    "print('y valid shape: ')\n",
    "print(np.shape(y_valid))\n",
    "print('X[5]=')\n",
    "print(X_train[5])\n",
    "print('y[5]=')\n",
    "print(y_train[5])\n",
    "#loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDZ6siB_Kq04"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AwMbRjm0FLBF"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    rfc = RFC()\n",
    "    return rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "clj-wufgFLBF",
    "outputId": "4374a6a9-c34b-4c1d-8123-26f6eb651e95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-07 09:04:35.315906\n",
      "RandomForestClassifier()\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "rfc_model=build_model()\n",
    "print(rfc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgrC1alOKq07"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPC9vPhB_13E",
    "outputId": "9740a853-24d0-47c6-befd-3d674a8da7d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-07 09:04:35.342628\n",
      "FIT\n",
      "2023-06-07 09:04:35.733362\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "print(\"FIT\")\n",
    "rfc_model.fit(X_train, y_train) # sample weight\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfgplJ3Ep8Vr"
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HCWG_w9_13F",
    "outputId": "5d57989e-a4a4-4700-ef8b-0fb64f0b8305"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-07 09:04:35.742567\n",
      "PREDICT\n",
      "debug pred [0.2, 0.8, 0.01]\n",
      "debug class [0 1 0]\n",
      "2023-06-07 09:04:35.807995\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())        \n",
    "print(\"PREDICT\")\n",
    "yhat_pairs=rfc_model.predict_proba(X_valid)  # [ prob of 0, prob of 1 ]\n",
    "yhat_pred=[pair[1] for pair in yhat_pairs]\n",
    "yhat_classes=rfc_model.predict(X_valid)  # 0 or 1\n",
    "\n",
    "print('debug pred',yhat_pred[:3])\n",
    "print('debug class',yhat_classes[:3])\n",
    "print(datetime.now())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Si8QbOpY_13G",
    "outputId": "f0405839-7fe8-42f5-fdbb-bd7f462d60d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distrib of scores: 0.47992505952380954 mean 0.4053745104299602 std\n",
      "Range of scores: 0.0 to 1.0\n",
      "Confusion matrix\n",
      " [[175  25]\n",
      " [ 37 163]]\n",
      "Normalized matrix\n",
      " [[0.4375 0.0625]\n",
      " [0.0925 0.4075]]\n",
      "Accuracy: 84.50% Precision: 86.70% Recall: 81.50%\n",
      "F1: 84.02% MCC: 0.6912\n",
      "AUPRC: 92.03% AUROC: 92.23%\n"
     ]
    }
   ],
   "source": [
    "print('Distrib of scores:',np.mean(yhat_pred),'mean',np.std(yhat_pred),'std')\n",
    "print('Range of scores:',np.min(yhat_pred),'to',np.max(yhat_pred))\n",
    "cm1 = confusion_matrix(y_valid,yhat_classes)\n",
    "print('Confusion matrix\\n',cm1)\n",
    "cm2 = confusion_matrix(y_valid,yhat_classes,normalize='all')\n",
    "print('Normalized matrix\\n',cm2)\n",
    "\n",
    "accuracy = accuracy_score(y_valid, yhat_classes)*100.\n",
    "precision = precision_score(y_valid, yhat_classes)*100.\n",
    "recall = recall_score(y_valid, yhat_classes)*100.\n",
    "f1 = f1_score(y_valid, yhat_classes)*100.\n",
    "prc_Y, prc_X, prc_bins = precision_recall_curve(y_valid, yhat_pred)\n",
    "auprc = auc(prc_X,prc_Y)*100.\n",
    "auroc = roc_auc_score(y_valid, yhat_pred)*100.\n",
    "mcc = matthews_corrcoef(y_valid, yhat_classes)\n",
    "\n",
    "print('Accuracy: %.2f%% Precision: %.2f%% Recall: %.2f%%' % (accuracy,precision,recall)) \n",
    "print('F1: %.2f%% MCC: %.4f' % (f1,mcc)) \n",
    "print('AUPRC: %.2f%% AUROC: %.2f%%' % (auprc,auroc)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSp5YCu1urzM"
   },
   "source": [
    "# TO DO\n",
    "Compute probability threshold.\n",
    "Apply threshold per transcriipt."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
