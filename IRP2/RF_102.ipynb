{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PG-tGRnlFLA3"
   },
   "source": [
    "# Random Forest\n",
    "Use numpy arrays for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RmwUsVLFLA6",
    "outputId": "42b44cd8-4b31-41f1-c242-3c45a2e52835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 12:36:28.337179\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OlzN9OdsFWEU",
    "outputId": "72c3c756-9055-42e2-f08d-4db873078d9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 12:36:28.402806: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found\n",
      "/Users/jasonmiller/WVU/BAM_ML/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 12:36:36.673140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "dt='float32'\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "tf.random.set_seed(42) # supposedly leads to reproducible results\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print('GPU device not found')\n",
    "else:\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print('Running on CoLab')\n",
    "    PATH='/content/drive/'\n",
    "    drive.mount(PATH)\n",
    "    DATA_DIR=PATH+'My Drive/data/IRP2/'  # must end in \"/\"\n",
    "    MODEL_DIR=PATH+'My Drive/data/IRP2/Models/'  # must end in \"/\"\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    DATA_DIR=\"/Users/jasonmiller/WVU/BAM_ML/\"\n",
    "    MODEL_DIR=\"/Users/jasonmiller/WVU/BAM_ML/Models/\"\n",
    "print(DATA_DIR)\n",
    "SAVE_MODEL_FILENAME = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PRX-UEr8FLA8",
    "outputId": "882cbc65-478a-4e89-e24f-c270727dd768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.0\n",
      "sklearn 1.1.2\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print('Python',python_version())\n",
    "import numpy as np\n",
    "np.random.seed(42) # supposedly sets scikit-learn\n",
    "import time # sleep function\n",
    "from os.path import isfile\n",
    "from matplotlib import pyplot as plt \n",
    "import sklearn   # pip install --upgrade scikit-learn\n",
    "print('sklearn',sklearn.__version__)\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "\n",
    "EPOCHS=150 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtqdpJOxFLBA"
   },
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LnkpVKdMFLA-",
    "outputId": "3473a184-a42b-410a-bfc4-a70ed4f824b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file 0 tiny_MxM/ml_stats.csv\n",
      "Data file 1 tiny_SxS/ml_stats.csv\n",
      "Maximum lines to load per file for training: 1000000\n"
     ]
    }
   ],
   "source": [
    "# Full dataset (may exceed RAM)\n",
    "DATA_FILE_0 = 'MxM_BR4/ml_stats.csv'\n",
    "DATA_FILE_1 = 'SxS_BR4/ml_stats.csv'\n",
    "# First million for testing\n",
    "DATA_FILE_0 = 'MxM_BR4/first_million_stats.csv'\n",
    "DATA_FILE_1 = 'SxS_BR4/first_million_stats.csv'\n",
    "# Tiny dataset for debugging\n",
    "DATA_FILE_0 = 'tiny_MxM/ml_stats.csv'\n",
    "DATA_FILE_1 = 'tiny_SxS/ml_stats.csv'\n",
    "\n",
    "print('Data file 0 %s'%DATA_FILE_0)\n",
    "print('Data file 1 %s'%DATA_FILE_1)\n",
    "TEST_PORTION=0.00   # set to 20% when using complete files (ml_stats)\n",
    "VALID_PORTION=0.20\n",
    "MAX_LINES_TO_LOAD=1000000  # million\n",
    "print('Maximum lines to load per file for training: %d'%MAX_LINES_TO_LOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "p35ehKV3Kq0z"
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self,filepath1,filepath2,verbose=True):\n",
    "        self.files = [filepath1,filepath2]\n",
    "        self.alignments=[]\n",
    "        self.labels=[]\n",
    "        self.is_primary={'P':1, 'S':0}\n",
    "        self.verbose = verbose\n",
    "        self.max_lines = None\n",
    "        \n",
    "    def set_max_lines(self,lines):\n",
    "        self.max_lines = lines\n",
    "        if self.verbose:\n",
    "            print('Maximum lines to load per file: %d'%lines)\n",
    "        \n",
    "    def _count_lines_(self):\n",
    "        count0 = 0\n",
    "        with open (self.files[0],'r') as handle0:\n",
    "            for row in handle0:\n",
    "                count0 += 1\n",
    "        count1 = 0\n",
    "        with open(self.files[1],'r') as handle1:\n",
    "            for row in handle1:\n",
    "                count1 += 1\n",
    "        minimum = min(count0,count1)\n",
    "        if self.verbose:\n",
    "            print('File0 size: %d %s'%(count0,self.files[0]))\n",
    "            print('File1 size: %d %s'%(count1,self.files[1]))\n",
    "        return minimum\n",
    "        \n",
    "    def _load_line_(self,row):\n",
    "        line = row.strip()\n",
    "        fields = line.split(',')\n",
    "        fields[0] =  self.is_primary[fields[0]]\n",
    "        fields[6] =  self.is_primary[fields[6]]\n",
    "        fields[12] = self.is_primary[fields[12]]\n",
    "        fields[18] = self.is_primary[fields[18]]\n",
    "        integers = [int(x) for x in fields]\n",
    "        self.alignments.append(integers)\n",
    "    \n",
    "    def load_full_train_set(self):\n",
    "        '''Load first 80% of the data (assumed to be in random order)'''\n",
    "        minimum = 0\n",
    "        try:\n",
    "            minimum = self._count_lines_()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise Exception('CANNOT COUNT LINES IN FILE!')\n",
    "        train_size = int(minimum - minimum * TEST_PORTION)\n",
    "        if self.max_lines is not None:\n",
    "            train_size = min(train_size,self.max_lines)\n",
    "        if self.verbose:\n",
    "            print('Trying to load %d lines per file...'%train_size)\n",
    "        try:\n",
    "            handle0 = open(self.files[0],'r')\n",
    "            handle1 = open(self.files[1],'r')\n",
    "            # Associate label 0 with data from file 0. Same for 1.\n",
    "            for i in range(train_size):\n",
    "                row = next(handle0)\n",
    "                self._load_line_(row)\n",
    "                self.labels.append(0) \n",
    "                row = next(handle1)\n",
    "                self._load_line_(row)\n",
    "                self.labels.append(1)\n",
    "            handle0.close()\n",
    "            handle1.close()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise Exception('CANNOT LOAD DATA FROM FILE!')\n",
    "\n",
    "    def show_examples(self,head=6):\n",
    "        head = min(head,len(self.alignments))\n",
    "        for i in range(head):\n",
    "            print('From '+self.files[self.labels[i]])\n",
    "            print('Primary,Score,Edit,Mismatch,GapOpen,GapExtend')\n",
    "            print(self.alignments[i][0:6])\n",
    "            print(self.alignments[i][6:12])\n",
    "            print(self.alignments[i][12:18])\n",
    "            print(self.alignments[i][18:24])\n",
    "            \n",
    "    def get_X_y(self):\n",
    "        loaded = len(self.alignments)\n",
    "        divider = int(loaded - loaded * VALID_PORTION)\n",
    "        X_train = np.array(self.alignments[:divider])\n",
    "        y_train = np.array(self.labels[:divider])\n",
    "        X_valid = np.array(self.alignments[divider:])\n",
    "        y_valid = np.array(self.labels[divider:])\n",
    "        if self.verbose:\n",
    "            print('Full train set size = '+str(len(self.alignments)))\n",
    "            print('Training/Validation partition: %d/%d'%(len(y_train),len(y_valid)))\n",
    "        return X_train,y_train, X_valid,y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7pcZVyvS_126",
    "outputId": "b34faf33-8e31-4036-c2aa-51370522c2c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 12:36:38.396488\n",
      "Maximum lines to load per file: 1000000\n",
      "LOADING\n",
      "File0 size: 4000 /Users/jasonmiller/WVU/BAM_ML/tiny_MxM/ml_stats.csv\n",
      "File1 size: 4000 /Users/jasonmiller/WVU/BAM_ML/tiny_SxS/ml_stats.csv\n",
      "Trying to load 4000 lines per file...\n",
      "2023-06-03 12:36:38.457304\n",
      "From /Users/jasonmiller/WVU/BAM_ML/tiny_MxM/ml_stats.csv\n",
      "Primary,Score,Edit,Mismatch,GapOpen,GapExtend\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/tiny_SxS/ml_stats.csv\n",
      "Primary,Score,Edit,Mismatch,GapOpen,GapExtend\n",
      "[0, -25, 5, 5, 0, 0]\n",
      "[0, -10, 2, 2, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/tiny_MxM/ml_stats.csv\n",
      "Primary,Score,Edit,Mismatch,GapOpen,GapExtend\n",
      "[0, -5, 1, 1, 0, 0]\n",
      "[0, -6, 2, 2, 0, 0]\n",
      "[1, -5, 1, 1, 0, 0]\n",
      "[1, -6, 2, 2, 0, 0]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/tiny_SxS/ml_stats.csv\n",
      "Primary,Score,Edit,Mismatch,GapOpen,GapExtend\n",
      "[0, -19, 4, 4, 0, 0]\n",
      "[0, -24, 6, 6, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "[1, -4, 2, 2, 0, 0]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/tiny_MxM/ml_stats.csv\n",
      "Primary,Score,Edit,Mismatch,GapOpen,GapExtend\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "[1, -1, 1, 1, 0, 0]\n",
      "[0, -5, 1, 1, 0, 0]\n",
      "[0, -4, 2, 2, 0, 0]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/tiny_SxS/ml_stats.csv\n",
      "Primary,Score,Edit,Mismatch,GapOpen,GapExtend\n",
      "[0, -3, 1, 1, 0, 0]\n",
      "[0, -11, 6, 6, 0, 0]\n",
      "[1, -3, 1, 1, 0, 0]\n",
      "[1, -11, 6, 6, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "filepath0 = DATA_DIR+DATA_FILE_0\n",
    "filepath1 = DATA_DIR+DATA_FILE_1\n",
    "loader=DataLoader(filepath0,filepath1)\n",
    "loader.set_max_lines(MAX_LINES_TO_LOAD)\n",
    "print('LOADING')\n",
    "loader.load_full_train_set()\n",
    "print(datetime.now())\n",
    "loader.show_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7uHn9Ib_129",
    "outputId": "78d4782d-d49a-437d-9d67-1554961ec2c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train set size = 8000\n",
      "Training/Validation partition: 6400/1600\n",
      "X train shape: \n",
      "(6400, 24)\n",
      "y train shape: \n",
      "(6400,)\n",
      "X valid shape: \n",
      "(1600, 24)\n",
      "y valid shape: \n",
      "(1600,)\n",
      "X[5]=\n",
      "[  0  -3   1   1   0   0   0 -11   6   6   0   0   1  -3   1   1   0   0\n",
      "   1 -11   6   6   0   0]\n",
      "y[5]=\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train, X_valid,y_valid = loader.get_X_y()\n",
    "print('X train shape: ')\n",
    "print(np.shape(X_train))\n",
    "print('y train shape: ')\n",
    "print(np.shape(y_train))\n",
    "print('X valid shape: ')\n",
    "print(np.shape(X_valid))\n",
    "print('y valid shape: ')\n",
    "print(np.shape(y_valid))\n",
    "print('X[5]=')\n",
    "print(X_train[5])\n",
    "print('y[5]=')\n",
    "print(y_train[5])\n",
    "#loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDZ6siB_Kq04"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AwMbRjm0FLBF"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    rfc = RFC()\n",
    "    return rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "clj-wufgFLBF",
    "outputId": "4f070042-0ac4-4928-f950-53fdd19b0aef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 12:36:38.529428\n",
      "RandomForestClassifier()\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "rfc_model=build_model()\n",
    "print(rfc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgrC1alOKq07"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPC9vPhB_13E",
    "outputId": "ddac2afd-0c06-4246-e72a-fc29a08c3954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 12:36:38.548558\n",
      "FIT\n",
      "2023-06-03 12:36:39.120654\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "print(\"FIT\")\n",
    "rfc_model.fit(X_train, y_train) # sample weight\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4HCWG_w9_13F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 12:36:39.132388\n",
      "PREDICT\n",
      "debug pred [0.5482040256387246, 1.0, 0.0]\n",
      "debug class [1 1 0]\n",
      "2023-06-03 12:36:39.245276\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())        \n",
    "print(\"PREDICT\")\n",
    "yhat_pairs=rfc_model.predict_proba(X_valid)  # [ prob of 0, prob of 1 ]\n",
    "yhat_pred=[pair[1] for pair in yhat_pairs]\n",
    "yhat_classes=rfc_model.predict(X_valid)  # 0 or 1\n",
    "\n",
    "print('debug pred',yhat_pred[:3])\n",
    "print('debug class',yhat_classes[:3])\n",
    "print(datetime.now())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Si8QbOpY_13G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distrib of scores: 0.5077772668579573 mean 0.38195105222524495 std\n",
      "Range of scores: 0.0 to 1.0\n",
      "Confusion matrix\n",
      " [[612 188]\n",
      " [111 689]]\n",
      "Normalized matrix\n",
      " [[0.3825   0.1175  ]\n",
      " [0.069375 0.430625]]\n",
      "Accuracy: 81.31% Precision: 78.56% Recall: 86.12%\n",
      "F1: 82.17% MCC: 0.6292\n",
      "AUPRC: 89.23% AUROC: 90.00%\n"
     ]
    }
   ],
   "source": [
    "print('Distrib of scores:',np.mean(yhat_pred),'mean',np.std(yhat_pred),'std')\n",
    "print('Range of scores:',np.min(yhat_pred),'to',np.max(yhat_pred))\n",
    "cm1 = confusion_matrix(y_valid,yhat_classes)\n",
    "print('Confusion matrix\\n',cm1)\n",
    "cm2 = confusion_matrix(y_valid,yhat_classes,normalize='all')\n",
    "print('Normalized matrix\\n',cm2)\n",
    "\n",
    "accuracy = accuracy_score(y_valid, yhat_classes)*100.\n",
    "precision = precision_score(y_valid, yhat_classes)*100.\n",
    "recall = recall_score(y_valid, yhat_classes)*100.\n",
    "f1 = f1_score(y_valid, yhat_classes)*100.\n",
    "prc_Y, prc_X, prc_bins = precision_recall_curve(y_valid, yhat_pred)\n",
    "auprc = auc(prc_X,prc_Y)*100.\n",
    "auroc = roc_auc_score(y_valid, yhat_pred)*100.\n",
    "mcc = matthews_corrcoef(y_valid, yhat_classes)\n",
    "\n",
    "print('Accuracy: %.2f%% Precision: %.2f%% Recall: %.2f%%' % (accuracy,precision,recall)) \n",
    "print('F1: %.2f%% MCC: %.4f' % (f1,mcc)) \n",
    "print('AUPRC: %.2f%% AUROC: %.2f%%' % (auprc,auroc)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjSVa72v4IsA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
