{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PG-tGRnlFLA3"
   },
   "source": [
    "# Random Forest\n",
    "HiSat on A.lyrata vs A.halleri.\n",
    "SRA normalized i.e. fastq with quality scores.\n",
    "Trimmed.\n",
    "No requirement to map to same scaffold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RmwUsVLFLA6",
    "outputId": "c377866a-38cb-4feb-989a-9a5942ca8314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-22 11:49:52.715316\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OlzN9OdsFWEU",
    "outputId": "7833b08e-6e59-42e6-d8d4-52cc628a3dce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 11:49:52.755051: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found\n",
      "Running on Mac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 11:50:01.689332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "dt='float32'\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "tf.random.set_seed(42) # supposedly leads to reproducible results\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print('GPU device not found')\n",
    "else:\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print('Running on CoLab')\n",
    "    PATH='/content/drive/'\n",
    "    drive.mount(PATH)\n",
    "    DATA_DIR=PATH+'My Drive/data/IRP2/'  # must end in \"/\"\n",
    "    MODEL_DIR=PATH+'My Drive/data/IRP2/Models/'  # must end in \"/\"\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print('Running on Mac')\n",
    "    DATA_DIR=\"/Users/jasonmiller/WVU/BAM_ML/\"\n",
    "    MODEL_DIR=\"/Users/jasonmiller/WVU/BAM_ML/Models/\"\n",
    "SAVE_MODEL_FILENAME = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PRX-UEr8FLA8",
    "outputId": "478d0c68-8aa1-4fd0-9c8d-bf7cb0a37fd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.0\n",
      "sklearn 1.1.2\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print('Python',python_version())\n",
    "import numpy as np\n",
    "np.random.seed(42) # supposedly sets scikit-learn\n",
    "import pandas as pd  # for plotting\n",
    "import time # sleep function\n",
    "from os.path import isfile\n",
    "import gzip\n",
    "from matplotlib import pyplot as plt \n",
    "import sklearn   # pip install --upgrade scikit-learn\n",
    "print('sklearn',sklearn.__version__)\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "\n",
    "EPOCHS=150 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtqdpJOxFLBA"
   },
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LnkpVKdMFLA-",
    "outputId": "b83a783a-31db-4377-d35e-0fffa3768c67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /Users/jasonmiller/WVU/BAM_ML/\n",
      "Data file 0 HiSat/lyrata/lyrata_read_stats.csv.gz\n",
      "Data file 1 HiSat/halleri/halleri_read_stats.csv.gz\n",
      "Input lines for training: 1000000\n"
     ]
    }
   ],
   "source": [
    "MAX_LINES_TO_LOAD =    1000000 # training - 1M lines requires 2GB RAM\n",
    "#MAX_LINES_TO_LOAD =    10000 # use this for debugging\n",
    "\n",
    "VALID_PORTION = 0.20\n",
    "\n",
    "DATA_FILE_0 = 'HiSat/lyrata/lyrata_read_stats.csv.gz'\n",
    "DATA_FILE_1 = 'HiSat/halleri/halleri_read_stats.csv.gz' \n",
    "\n",
    "print('Data directory: %s'%DATA_DIR)\n",
    "print('Data file 0 %s'%DATA_FILE_0)\n",
    "print('Data file 1 %s'%DATA_FILE_1)\n",
    "print('Input lines for training: %d'%MAX_LINES_TO_LOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUBLdpVEVQ3I",
    "outputId": "efc015e2-d4cb-4872-aa17-cea45e6509d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feature names:  46\n",
      "0 P1 R1 AS\n",
      "1 P1 R1 ED\n",
      "2 P1 R1 MM\n",
      "3 P1 R1 HQMM\n",
      "4 P1 R1 GO\n",
      "5 P1 R1 GE\n",
      "6 P1 R1 HQINS\n",
      "7 P1 R1 HQDEL\n",
      "8 P1 R2 AS\n",
      "9 P1 R2 ED\n",
      "10 P1 R2 MM\n",
      "11 P1 R2 HQMM\n",
      "12 P1 R2 GO\n",
      "13 P1 R2 GE\n",
      "14 P1 R2 HQINS\n",
      "15 P1 R2 HQDEL\n",
      "16 P2 R1 AS\n",
      "17 P2 R1 ED\n",
      "18 P2 R1 MM\n",
      "19 P2 R1 HQMM\n",
      "20 P2 R1 GO\n",
      "21 P2 R1 GE\n",
      "22 P2 R1 HQINS\n",
      "23 P2 R1 HQDEL\n",
      "24 P2 R2 AS\n",
      "25 P2 R2 ED\n",
      "26 P2 R2 MM\n",
      "27 P2 R2 HQMM\n",
      "28 P2 R2 GO\n",
      "29 P2 R2 GE\n",
      "30 P2 R2 HQINS\n",
      "31 P2 R2 HQDEL\n",
      "32 R1 length\n",
      "33 R2 length\n",
      "34 P1 span\n",
      "35 P2 span\n",
      "36 Span diff\n",
      "37 AS diff\n",
      "38 ED diff\n",
      "39 MM diff\n",
      "40 HQMM diff\n",
      "41 GO diff\n",
      "42 GE diff\n",
      "43 HQINS diff\n",
      "44 HQDEL diff\n",
      "45 P2 Primary\n"
     ]
    }
   ],
   "source": [
    "# P1 parent 1\n",
    "# R1 read 1\n",
    "# PS primary or secondary\n",
    "# AS bowtie alignment score (0 is best)\n",
    "# ED edit distance\n",
    "# MM mismatch count\n",
    "# GO gap open count\n",
    "# GE gap extend count\n",
    "feature_names = [\n",
    "    'P1 R1 AS',\n",
    "    'P1 R1 ED',\n",
    "    'P1 R1 MM',\n",
    "    'P1 R1 HQMM',\n",
    "    'P1 R1 GO',\n",
    "    'P1 R1 GE',\n",
    "    'P1 R1 HQINS',\n",
    "    'P1 R1 HQDEL',\n",
    "    'P1 R2 AS',\n",
    "    'P1 R2 ED',\n",
    "    'P1 R2 MM',\n",
    "    'P1 R2 HQMM',\n",
    "    'P1 R2 GO',\n",
    "    'P1 R2 GE',\n",
    "    'P1 R2 HQINS',\n",
    "    'P1 R2 HQDEL',\n",
    "    'P2 R1 AS',\n",
    "    'P2 R1 ED',\n",
    "    'P2 R1 MM',\n",
    "    'P2 R1 HQMM',\n",
    "    'P2 R1 GO',\n",
    "    'P2 R1 GE',\n",
    "    'P2 R1 HQINS',\n",
    "    'P2 R1 HQDEL',\n",
    "    'P2 R2 AS',\n",
    "    'P2 R2 ED',\n",
    "    'P2 R2 MM',\n",
    "    'P2 R2 HQMM',\n",
    "    'P2 R2 GO',\n",
    "    'P2 R2 GE',\n",
    "    'P2 R2 HQINS',\n",
    "    'P2 R2 HQDEL',\n",
    "    'R1 length',\n",
    "    'R2 length',\n",
    "    'P1 span',\n",
    "    'P2 span',\n",
    "    'Span diff',\n",
    "    'AS diff',\n",
    "    'ED diff',\n",
    "    'MM diff',\n",
    "    'HQMM diff',\n",
    "    'GO diff',\n",
    "    'GE diff',\n",
    "    'HQINS diff',\n",
    "    'HQDEL diff',\n",
    "    'P2 Primary']\n",
    "print('Total feature names: ',len(feature_names))\n",
    "for i in range(len(feature_names)):\n",
    "    print(i,feature_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "p35ehKV3Kq0z"
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self,filepath1,filepath2,verbose=True):\n",
    "        self.files = [filepath1,filepath2]\n",
    "        self.alignments=[]\n",
    "        self.labels=[]\n",
    "        self.verbose = verbose\n",
    "        self.max_lines = None\n",
    "        \n",
    "    def set_max_lines(self,lines):\n",
    "        '''Limit the dataset size to fit in RAM.'''\n",
    "        self.max_lines = lines\n",
    "        if self.verbose:\n",
    "            print('Maximum lines to load per file: %d'%lines)\n",
    "        \n",
    "    def _count_lines_(self):\n",
    "        '''Show number of lines per input file.'''\n",
    "        count0 = 0\n",
    "        with gzip.open (self.files[0],'rt') as handle0:\n",
    "            for row in handle0:\n",
    "                count0 += 1\n",
    "        count1 = 0\n",
    "        with gzip.open(self.files[1],'rt') as handle1:\n",
    "            for row in handle1:\n",
    "                count1 += 1\n",
    "        minimum = min(count0,count1)\n",
    "        if self.verbose:\n",
    "            print('File0 size: %d %s'%(count0,self.files[0]))\n",
    "            print('File1 size: %d %s'%(count1,self.files[1]))\n",
    "        return minimum\n",
    "        \n",
    "    def _load_line_(self,row):\n",
    "        '''Load data structure from one line of CSV file.'''\n",
    "        line = row.strip()\n",
    "        fields = line.split(',')\n",
    "        ints = [0] * 46\n",
    "        # These fields come straight from the input file.\n",
    "        # These fields are grouped by which read they describe.\n",
    "        # P1 R1 AS = Parent 1, Read 1, Alignment Score\n",
    "        ints[0] = int(fields[0]) # P1 R1 AS\n",
    "        ints[1] = int(fields[1]) # P1 R1 ED\n",
    "        ints[2] = int(fields[2]) # P1 R1 MM\n",
    "        ints[3] = int(fields[3]) # P1 R1 HQMM\n",
    "        ints[4] = int(fields[4]) # P1 R1 GO\n",
    "        ints[5] = int(fields[5]) # P1 R1 GE\n",
    "        ints[6] = int(fields[6]) # P1 R1 HQINS\n",
    "        ints[7] = int(fields[7]) # P1 R1 HQDEL\n",
    "        ints[8] = int(fields[8]) # P1 R2 AS\n",
    "        ints[9] = int(fields[9]) # P1 R2 ED\n",
    "        ints[10] = int(fields[10]) # P1 R2 MM\n",
    "        ints[11] = int(fields[11]) # P1 R2 HQMM\n",
    "        ints[12] = int(fields[12]) # P1 R2 GO\n",
    "        ints[13] = int(fields[13]) # P1 R2 GE\n",
    "        ints[14] = int(fields[14]) # P1 R2 HQINS\n",
    "        ints[15] = int(fields[15]) # P1 R2 HQDEL\n",
    "        ints[16] = int(fields[16]) # P2 R1 AS\n",
    "        ints[17] = int(fields[17]) # P2 R1 ED\n",
    "        ints[18] = int(fields[18]) # P2 R1 MM\n",
    "        ints[19] = int(fields[19]) # P2 R1 HQMM\n",
    "        ints[20] = int(fields[20]) # P2 R1 GO\n",
    "        ints[21] = int(fields[21]) # P2 R1 GE\n",
    "        ints[22] = int(fields[22]) # P2 R1 HQINS\n",
    "        ints[23] = int(fields[23]) # P2 R1 HQDEL\n",
    "        ints[24] = int(fields[24]) # P2 R2 AS\n",
    "        ints[25] = int(fields[25]) # P2 R2 ED\n",
    "        ints[26] = int(fields[26]) # P2 R2 MM\n",
    "        ints[27] = int(fields[27]) # P2 R2 HQMM\n",
    "        ints[28] = int(fields[28]) # P2 R2 GO\n",
    "        ints[29] = int(fields[29]) # P2 R2 GE    \n",
    "        ints[30] = int(fields[30]) # P2 R2 HQINS\n",
    "        ints[31] = int(fields[31]) # P2 R2 HQDEL \n",
    "        ints[32] = int(fields[32]) # R1 length \n",
    "        ints[33] = int(fields[33]) # R2 length \n",
    "        ints[34] = int(fields[34]) # P1 span \n",
    "        ints[35] = int(fields[35]) # P2 span \n",
    "        # We compute these 'diff' fields from the input data.\n",
    "        ints[36] = int(ints[35]-ints[34]) # P2-P1 span diff \n",
    "        ints[37] = int(((ints[16]+ints[24])-(ints[0]+ints[8]))/2) # AS diff\n",
    "        ints[38] = int(((ints[17]+ints[25])-(ints[1]+ints[9]))/2) # ED diff\n",
    "        ints[39] = int(((ints[18]+ints[26])-(ints[2]+ints[10]))/2) # MM diff\n",
    "        ints[40] = int(((ints[19]+ints[27])-(ints[3]+ints[11]))/2) # HQMM diff\n",
    "        ints[41] = int(((ints[20]+ints[28])-(ints[4]+ints[12]))/2) # GO diff\n",
    "        ints[42] = int(((ints[21]+ints[29])-(ints[5]+ints[13]))/2) # GE diff\n",
    "        ints[43] = int(((ints[22]+ints[30])-(ints[6]+ints[14]))/2) # HQINS diff\n",
    "        ints[44] = int(((ints[23]+ints[31])-(ints[7]+ints[15]))/2) # HQDEL diff\n",
    "        # Aligner choice for primary alignment\n",
    "        ints[45] = int(fields[36])  # 1=P1 usually M, 2=P2 usually S\n",
    "        # The span of this read pair along either parent.\n",
    "        # The transcript (in both parents) that this read pair aligned to.\n",
    "        # An upstream filter removes cases of different transcript per parent.\n",
    "        transcript_id = fields[36] # TO DO: where to put this?\n",
    "        self.alignments.append(ints)\n",
    "    \n",
    "    def load_full_train_set(self):\n",
    "        '''Load full train set (to be used for train and valiation).\n",
    "           Use set_max_lines() to leave some data for the test set.'''\n",
    "        minimum = 0\n",
    "        train_size = self.max_lines\n",
    "        if self.verbose:\n",
    "            print('Trying to load %d lines per file...'%train_size)\n",
    "        try:\n",
    "            handle0 = gzip.open(self.files[0],'rt')\n",
    "            handle1 = gzip.open(self.files[1],'rt')\n",
    "            # Associate label 0 with data from file 0. Same for 1.\n",
    "            for i in range(train_size):\n",
    "                row = next(handle0)\n",
    "                self._load_line_(row)\n",
    "                self.labels.append(0) \n",
    "                row = next(handle1)\n",
    "                self._load_line_(row)\n",
    "                self.labels.append(1)\n",
    "            handle0.close()\n",
    "            handle1.close()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('Most likely, one file has too few reads.')\n",
    "            raise Exception('CANNOT LOAD DATA FROM FILE!')\n",
    "\n",
    "    def show_examples(self,head=6):\n",
    "        head = min(head,len(self.alignments))\n",
    "        for i in range(head):\n",
    "            print('From '+self.files[self.labels[i]])\n",
    "            print('Score,Edit,MM,HQMM,GapOpen,GapExtend,HQINS,HQDEL')\n",
    "            print(self.alignments[i][0:8])\n",
    "            print(self.alignments[i][8:16])\n",
    "            print(self.alignments[i][16:24])\n",
    "            print(self.alignments[i][24:32])\n",
    "            \n",
    "    def get_X_y(self):\n",
    "        loaded = len(self.alignments)\n",
    "        divider = int(loaded - loaded * VALID_PORTION)\n",
    "        X_train = np.array(self.alignments[:divider])\n",
    "        y_train = np.array(self.labels[:divider])\n",
    "        X_valid = np.array(self.alignments[divider:])\n",
    "        y_valid = np.array(self.labels[divider:])\n",
    "        if self.verbose:\n",
    "            print('Full train set size = '+str(len(self.alignments)))\n",
    "            print('Training/Validation partition: %d/%d'%(len(y_train),len(y_valid)))\n",
    "        return X_train,y_train, X_valid,y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7pcZVyvS_126",
    "outputId": "32b0c13b-7f50-432d-bd6c-b5354b8bf313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-22 11:50:04.173524\n",
      "Maximum lines to load per file: 1000000\n",
      "LOADING\n",
      "Trying to load 1000000 lines per file...\n",
      "2023-06-22 11:50:34.175663\n",
      "From /Users/jasonmiller/WVU/BAM_ML/HiSat/lyrata/lyrata_read_stats.csv.gz\n",
      "Score,Edit,MM,HQMM,GapOpen,GapExtend,HQINS,HQDEL\n",
      "[-6, 2, 2, 0, 0, 0, 0, 0]\n",
      "[-1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-12, 3, 3, 0, 0, 0, 0, 0]\n",
      "[-7, 1, 1, 0, 0, 0, 0, 0]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/HiSat/halleri/halleri_read_stats.csv.gz\n",
      "Score,Edit,MM,HQMM,GapOpen,GapExtend,HQINS,HQDEL\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-20, 0, 0, 0, 0, 0, 0, 0]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/HiSat/lyrata/lyrata_read_stats.csv.gz\n",
      "Score,Edit,MM,HQMM,GapOpen,GapExtend,HQINS,HQDEL\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-1, 1, 1, 0, 0, 0, 0, 0]\n",
      "[-29, 1, 1, 0, 0, 0, 0, 0]\n",
      "[-12, 3, 3, 1, 0, 0, 0, 0]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/HiSat/halleri/halleri_read_stats.csv.gz\n",
      "Score,Edit,MM,HQMM,GapOpen,GapExtend,HQINS,HQDEL\n",
      "[-36, 6, 6, 0, 0, 0, 0, 0]\n",
      "[-25, 5, 5, 0, 0, 0, 0, 0]\n",
      "[-30, 5, 5, 0, 0, 0, 0, 0]\n",
      "[-31, 6, 6, 0, 0, 0, 0, 0]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/HiSat/lyrata/lyrata_read_stats.csv.gz\n",
      "Score,Edit,MM,HQMM,GapOpen,GapExtend,HQINS,HQDEL\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-1, 1, 1, 0, 0, 0, 0, 0]\n",
      "[-18, 3, 3, 0, 0, 0, 0, 0]\n",
      "[-17, 4, 4, 2, 0, 0, 0, 0]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/HiSat/halleri/halleri_read_stats.csv.gz\n",
      "Score,Edit,MM,HQMM,GapOpen,GapExtend,HQINS,HQDEL\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-6, 1, 1, 0, 0, 0, 0, 0]\n",
      "[-6, 1, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "filepath0 = DATA_DIR+DATA_FILE_0\n",
    "filepath1 = DATA_DIR+DATA_FILE_1\n",
    "loader=DataLoader(filepath0,filepath1)\n",
    "loader.set_max_lines(MAX_LINES_TO_LOAD)\n",
    "print('LOADING')\n",
    "loader.load_full_train_set()\n",
    "print(datetime.now())\n",
    "loader.show_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7uHn9Ib_129",
    "outputId": "fbdc00cd-074a-4be2-8e60-eb7b730dcfea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train set size = 2000000\n",
      "Training/Validation partition: 1600000/400000\n",
      "X train shape: \n",
      "(1600000, 46)\n",
      "y train shape: \n",
      "(1600000,)\n",
      "X valid shape: \n",
      "(400000, 46)\n",
      "y valid shape: \n",
      "(400000,)\n",
      "X[5]=\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  -6   1\n",
      "   1   0   0   0   0   0  -6   1   1   0   0   0   0   0  99 100 102 102\n",
      "   0  -6   1   1   0   0   0   0   0   1]\n",
      "y[5]=\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train, X_valid,y_valid = loader.get_X_y()\n",
    "print('X train shape: ')\n",
    "print(np.shape(X_train))\n",
    "print('y train shape: ')\n",
    "print(np.shape(y_train))\n",
    "print('X valid shape: ')\n",
    "print(np.shape(X_valid))\n",
    "print('y valid shape: ')\n",
    "print(np.shape(y_valid))\n",
    "print('X[5]=')\n",
    "print(X_train[5])\n",
    "print('y[5]=')\n",
    "print(y_train[5])\n",
    "#loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDZ6siB_Kq04"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "AwMbRjm0FLBF"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    rfc = RFC()\n",
    "    return rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "clj-wufgFLBF",
    "outputId": "93526acb-bdea-44af-ee71-6a5065715f25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-22 11:50:43.578247\n",
      "RandomForestClassifier()\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "rfc_model=build_model()\n",
    "print(rfc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgrC1alOKq07"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPC9vPhB_13E",
    "outputId": "8c3cd1d6-8a0c-4cf9-b58e-425a5f1a2889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-22 11:50:43.597893\n",
      "FIT\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "print(\"FIT\")\n",
    "rfc_model.fit(X_train, y_train) # sample weight\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfgplJ3Ep8Vr"
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HCWG_w9_13F",
    "outputId": "acfb4abf-401e-4c5c-f13b-46e17f9f644f"
   },
   "outputs": [],
   "source": [
    "print(datetime.now())        \n",
    "print(\"PREDICT\")\n",
    "yhat_pairs=rfc_model.predict_proba(X_valid)  # [ prob of 0, prob of 1 ]\n",
    "yhat_pred=[pair[1] for pair in yhat_pairs]\n",
    "yhat_classes=rfc_model.predict(X_valid)  # 0 or 1\n",
    "\n",
    "print('debug pred',yhat_pred[:3])\n",
    "print('debug class',yhat_classes[:3])\n",
    "print(datetime.now())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Si8QbOpY_13G",
    "outputId": "34ac5d3f-bb44-4fb7-e83c-88c36b9876f6"
   },
   "outputs": [],
   "source": [
    "print('Distrib of scores:',np.mean(yhat_pred),'mean',np.std(yhat_pred),'std')\n",
    "print('Range of scores:',np.min(yhat_pred),'to',np.max(yhat_pred))\n",
    "cm1 = confusion_matrix(y_valid,yhat_classes)\n",
    "print('Confusion matrix\\n',cm1)\n",
    "cm2 = confusion_matrix(y_valid,yhat_classes,normalize='all')\n",
    "print('Normalized matrix\\n',cm2)\n",
    "\n",
    "accuracy = accuracy_score(y_valid, yhat_classes)*100.\n",
    "precision = precision_score(y_valid, yhat_classes)*100.\n",
    "recall = recall_score(y_valid, yhat_classes)*100.\n",
    "f1 = f1_score(y_valid, yhat_classes)*100.\n",
    "prc_Y, prc_X, prc_bins = precision_recall_curve(y_valid, yhat_pred)\n",
    "auprc = auc(prc_X,prc_Y)*100.\n",
    "auroc = roc_auc_score(y_valid, yhat_pred)*100.\n",
    "mcc = matthews_corrcoef(y_valid, yhat_classes)\n",
    "\n",
    "print('Accuracy: %.2f%% Precision: %.2f%% Recall: %.2f%%' % (accuracy,precision,recall)) \n",
    "print('F1: %.2f%% MCC: %.4f' % (f1,mcc)) \n",
    "print('AUPRC: %.2f%% AUROC: %.2f%%' % (auprc,auroc)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "RjrrFkw9VQ3Z",
    "outputId": "09268438-9e1e-4e14-81a3-17de1f053b61"
   },
   "outputs": [],
   "source": [
    "importances = rfc_model.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rfc_model.estimators_], axis=0)\n",
    "forest_importances = pd.Series(importances, index=feature_names)\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Approximate Feature Importance\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forest_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkR5p_t4b4Ex"
   },
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fchQxD85b3hj"
   },
   "outputs": [],
   "source": [
    "class aligner_model():\n",
    "    # The aligner makes its choice this way.\n",
    "    # It designates one parent as primary and the other as secondary.\n",
    "    # Here, if primary is parent 2, then return a 1, otherwise 0.\n",
    "    def predict(self,X):\n",
    "        IS_PRIMARY = 45 # input feature #45\n",
    "        y = [x[IS_PRIMARY] for x in X] \n",
    "        guess = 0\n",
    "        for i in range(len(y)):\n",
    "            if y[i]==0:   # STAR scores were tied, so alternate guesses\n",
    "                y[i]=guess\n",
    "                guess = (guess+1)%2\n",
    "            else:\n",
    "                y[i] = y[i]-1  # Convert 1=M to 0=No, or 2=S to 1=Yes\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v_H6krNudqJu",
    "outputId": "0c4495e5-4c4d-4d39-dc6a-7e4e9f97b069"
   },
   "outputs": [],
   "source": [
    "print(y_valid[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4S2yWyqUcHt9",
    "outputId": "9c24bd6b-98a8-4690-d4ef-dd9f8fc282a2"
   },
   "outputs": [],
   "source": [
    "aln_model=aligner_model()\n",
    "aln_classes=aln_model.predict(X_valid)\n",
    "\n",
    "accuracy = accuracy_score(y_valid, aln_classes)*100.\n",
    "precision = precision_score(y_valid, aln_classes)*100.\n",
    "recall = recall_score(y_valid, aln_classes)*100.\n",
    "f1 = f1_score(y_valid, aln_classes)*100.\n",
    "mcc = matthews_corrcoef(y_valid, aln_classes)\n",
    "\n",
    "print('Accuracy: %.2f%% Precision: %.2f%% Recall: %.2f%%' % (accuracy,precision,recall)) \n",
    "print('F1: %.2f%% MCC: %.4f' % (f1,mcc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
