{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PG-tGRnlFLA3"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RmwUsVLFLA6",
    "outputId": "e3829598-d7de-4895-dbba-410e196ae45d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 10:58:46.383907\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OlzN9OdsFWEU",
    "outputId": "fb8898b3-b66d-421a-92ee-382fe73af8a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 10:58:46.434978: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found\n",
      "/Users/jasonmiller/WVU/BAM_ML/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 10:58:58.808019: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "dt='float32'\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "tf.random.set_seed(42) # supposedly leads to reproducible results\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print('GPU device not found')\n",
    "else:\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print('Running on CoLab')\n",
    "    PATH='/content/drive/'\n",
    "    drive.mount(PATH)\n",
    "    DATA_DIR=PATH+'My Drive/data/IRP2/'  # must end in \"/\"\n",
    "    MODEL_DIR=PATH+'My Drive/data/IRP2/Models/'  # must end in \"/\"\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    DATA_DIR=\"/Users/jasonmiller/WVU/BAM_ML/\"\n",
    "    MODEL_DIR=\"/Users/jasonmiller/WVU/BAM_ML/Models/\"\n",
    "print(DATA_DIR)\n",
    "SAVE_MODEL_FILENAME = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PRX-UEr8FLA8",
    "outputId": "37881a5d-48c0-49fe-e9ca-6dae1269b4d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.0\n",
      "sklearn 1.1.2\n",
      "Maximum lines to load per file for training: 1000000\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print('Python',python_version())\n",
    "import numpy as np\n",
    "np.random.seed(42) # supposedly sets scikit-learn\n",
    "import time # sleep function\n",
    "from os.path import isfile\n",
    "from matplotlib import pyplot as plt \n",
    "import sklearn   # pip install --upgrade scikit-learn\n",
    "print('sklearn',sklearn.__version__)\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "\n",
    "EPOCHS=150 \n",
    "TEST_PORTION=0.20\n",
    "MAX_LINES_TO_LOAD=1000000  # million\n",
    "print('Maximum lines to load per file for training: %d'%MAX_LINES_TO_LOAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtqdpJOxFLBA"
   },
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LnkpVKdMFLA-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file 0 MxM_BR4/ml_stats.csv\n",
      "Data file 1 SxS_BR4/ml_stats.csv\n"
     ]
    }
   ],
   "source": [
    "DATA_FILE_0 = 'tiny_MxM/ml_stats.csv'\n",
    "DATA_FILE_1 = 'tiny_SxS/ml_stats.csv'\n",
    "DATA_FILE_0 = 'MxM_BR4/ml_stats.csv'\n",
    "DATA_FILE_1 = 'SxS_BR4/ml_stats.csv'\n",
    "print('Data file 0 %s'%DATA_FILE_0)\n",
    "print('Data file 1 %s'%DATA_FILE_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "p35ehKV3Kq0z"
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self,filepath1,filepath2,verbose=True):\n",
    "        self.files = [filepath1,filepath2]\n",
    "        self.alignments=[]\n",
    "        self.labels=[]\n",
    "        self.is_primary={'P':1, 'S':0}\n",
    "        self.train_size = 0\n",
    "        self.verbose = verbose\n",
    "        self.max_lines = None\n",
    "        \n",
    "    def set_max_lines(self,lines):\n",
    "        self.max_lines = lines\n",
    "        if self.verbose:\n",
    "            print('Maximum lines to load per file: %d'%lines)\n",
    "        \n",
    "    def _count_lines_(self):\n",
    "        count0 = 0\n",
    "        with open (self.files[0],'r') as handle0:\n",
    "            for row in handle0:\n",
    "                count0 += 1\n",
    "        count1 = 0\n",
    "        with open(self.files[1],'r') as handle1:\n",
    "            for row in handle1:\n",
    "                count1 += 1\n",
    "        minimum = min(count0,count1)\n",
    "        train_size = int(minimum - minimum * TEST_PORTION)\n",
    "        if self.verbose:\n",
    "            print('File0 size: %d %s'%(count0,self.files[0]))\n",
    "            print('File1 size: %d %s'%(count1,self.files[1]))\n",
    "        return train_size\n",
    "        \n",
    "    def _load_line_(self,row):\n",
    "        line = row.strip()\n",
    "        fields = line.split(',')\n",
    "        fields[0] =  self.is_primary[fields[0]]\n",
    "        fields[6] =  self.is_primary[fields[6]]\n",
    "        fields[12] = self.is_primary[fields[12]]\n",
    "        fields[18] = self.is_primary[fields[18]]\n",
    "        integers = [int(x) for x in fields]\n",
    "        self.alignments.append(integers)\n",
    "    \n",
    "    def load_train_set(self):\n",
    "        '''Load first 80% of the data (assumed to be in random order)'''\n",
    "        try:\n",
    "            self.train_size = self._count_lines_()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise Exception('CANNOT COUNT LINES IN FILE!')\n",
    "        goal = 1000000000  # close to max int\n",
    "        if self.max_lines is not None:\n",
    "            goal = min(goal,self.max_lines)\n",
    "        if self.train_size is not None:\n",
    "            goal = min(goal,self.train_size)\n",
    "        if self.verbose:\n",
    "            print('Trying to load %d lines per file...'%goal)\n",
    "        try:\n",
    "            handle0 = open(self.files[0],'r')\n",
    "            handle1 = open(self.files[1],'r')\n",
    "            # Associate label 0 with data from file 0. Same for 1.\n",
    "            for i in range(goal):\n",
    "                row = next(handle0)\n",
    "                self._load_line_(row)\n",
    "                self.labels.append(0) \n",
    "                row = next(handle1)\n",
    "                self._load_line_(row)\n",
    "                self.labels.append(1)\n",
    "            handle0.close()\n",
    "            handle1.close()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise Exception('CANNOT LOAD DATA FROM FILE!')\n",
    "\n",
    "    def show_examples(self,head=6):\n",
    "        head = min(head,len(self.alignments))\n",
    "        for i in range(head):\n",
    "            print('From '+self.files[self.labels[i]])\n",
    "            print('Primary,Score,Edit,Mismatch,GapOpen,GapExtend')\n",
    "            print(self.alignments[i][0:6])\n",
    "            print(self.alignments[i][6:12])\n",
    "            print(self.alignments[i][12:18])\n",
    "            print(self.alignments[i][18:24])\n",
    "            \n",
    "    def get_X_y(self):\n",
    "        total_size = len(self.alignments)\n",
    "        divider = int(total_size - total_size * TEST_PORTION)\n",
    "        X_train = self.alignments[:divider]\n",
    "        y_train = self.labels[:divider]\n",
    "        X_valid = self.alignments[divider:]\n",
    "        y_valid = self.labels[divider:]\n",
    "        if self.verbose:\n",
    "            print('Full train set size = '+str(len(self.alignments)))\n",
    "            print('Training/Validation partition: %d/%d'%(len(y_train),len(y_valid)))\n",
    "        return X_train,y_train, X_valid,y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 10:59:01.922593\n",
      "Maximum lines to load per file: 1000000\n",
      "LOADING\n",
      "File0 size: 35240287 /Users/jasonmiller/WVU/BAM_ML/MxM_BR4/ml_stats.csv\n",
      "File1 size: 53122947 /Users/jasonmiller/WVU/BAM_ML/SxS_BR4/ml_stats.csv\n",
      "Trying to load 1000000 lines per file...\n",
      "2023-06-03 10:59:36.051294\n",
      "From /Users/jasonmiller/WVU/BAM_ML/MxM_BR4/ml_stats.csv\n",
      "Primary,Score,Edit,Mismatch,GapOpen,GapExtend\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/SxS_BR4/ml_stats.csv\n",
      "Primary,Score,Edit,Mismatch,GapOpen,GapExtend\n",
      "[0, -25, 5, 5, 0, 0]\n",
      "[0, -10, 2, 2, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/MxM_BR4/ml_stats.csv\n",
      "Primary,Score,Edit,Mismatch,GapOpen,GapExtend\n",
      "[0, -5, 1, 1, 0, 0]\n",
      "[0, -6, 2, 2, 0, 0]\n",
      "[1, -5, 1, 1, 0, 0]\n",
      "[1, -6, 2, 2, 0, 0]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/SxS_BR4/ml_stats.csv\n",
      "Primary,Score,Edit,Mismatch,GapOpen,GapExtend\n",
      "[0, -19, 4, 4, 0, 0]\n",
      "[0, -24, 6, 6, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "[1, -4, 2, 2, 0, 0]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/MxM_BR4/ml_stats.csv\n",
      "Primary,Score,Edit,Mismatch,GapOpen,GapExtend\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "[1, -1, 1, 1, 0, 0]\n",
      "[0, -5, 1, 1, 0, 0]\n",
      "[0, -4, 2, 2, 0, 0]\n",
      "From /Users/jasonmiller/WVU/BAM_ML/SxS_BR4/ml_stats.csv\n",
      "Primary,Score,Edit,Mismatch,GapOpen,GapExtend\n",
      "[0, -3, 1, 1, 0, 0]\n",
      "[0, -11, 6, 6, 0, 0]\n",
      "[1, -3, 1, 1, 0, 0]\n",
      "[1, -11, 6, 6, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "filepath0 = DATA_DIR+DATA_FILE_0\n",
    "filepath1 = DATA_DIR+DATA_FILE_1\n",
    "loader=DataLoader(filepath0,filepath1)\n",
    "loader.set_max_lines(MAX_LINES_TO_LOAD)\n",
    "print('LOADING')\n",
    "loader.load_train_set()\n",
    "print(datetime.now())\n",
    "loader.show_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train set size = 2000000\n",
      "Training/Validation partition: 1600000/400000\n",
      "X[5]=\n",
      "[0, -3, 1, 1, 0, 0, 0, -11, 6, 6, 0, 0, 1, -3, 1, 1, 0, 0, 1, -11, 6, 6, 0, 0]\n",
      "y[5]=\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train, X_valid,y_valid = loader.get_X_y()\n",
    "print('X[5]=')\n",
    "print(X_train[5])\n",
    "print('y[5]=')\n",
    "print(y_train[5])\n",
    "loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDZ6siB_Kq04"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AwMbRjm0FLBF"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    rfc = RFC()\n",
    "    return rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "clj-wufgFLBF",
    "outputId": "a3cfe7b4-e9da-432c-cd0f-effff1bb0059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 10:59:36.246286\n",
      "RandomForestClassifier()\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "model=build_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgrC1alOKq07"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 10:59:36.296257\n",
      "FIT\n",
      "2023-06-03 11:08:59.377887\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "print(\"FIT\")\n",
    "model.fit(X_train, y_train) # sample weight\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 11:08:59.386905\n",
      "PREDICT\n",
      "debug pred [0.0, 0.2702261904761905, 0.5483498278046592]\n",
      "debug class [0 0 1]\n",
      "2023-06-03 11:09:25.003300\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())        \n",
    "print(\"PREDICT\")\n",
    "yhat_pairs=model.predict_proba(X_valid)  # [ prob of 0, prob of 1 ]\n",
    "yhat_pred=[pair[1] for pair in yhat_pairs]\n",
    "yhat_classes=model.predict(X_valid)  # 0 or 1\n",
    "\n",
    "print('debug pred',yhat_pred[:3])\n",
    "print('debug class',yhat_classes[:3])\n",
    "print(datetime.now())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distrib of scores: 0.502600326716645 mean 0.3912884539604919 std\n",
      "Range of scores: 0.0 to 1.0\n",
      "Confusion matrix\n",
      " [[159918  40082]\n",
      " [ 23223 176777]]\n",
      "Normalized matrix\n",
      " [[0.399795  0.100205 ]\n",
      " [0.0580575 0.4419425]]\n",
      "Accuracy: 84.17% Precision: 81.52% Recall: 88.39%\n",
      "F1: 84.81% MCC: 0.6859\n",
      "AUPRC: 92.36% AUROC: 92.80%\n"
     ]
    }
   ],
   "source": [
    "print('Distrib of scores:',np.mean(yhat_pred),'mean',np.std(yhat_pred),'std')\n",
    "print('Range of scores:',np.min(yhat_pred),'to',np.max(yhat_pred))\n",
    "cm1 = confusion_matrix(y_valid,yhat_classes)\n",
    "print('Confusion matrix\\n',cm1)\n",
    "cm2 = confusion_matrix(y_valid,yhat_classes,normalize='all')\n",
    "print('Normalized matrix\\n',cm2)\n",
    "\n",
    "accuracy = accuracy_score(y_valid, yhat_classes)*100.\n",
    "precision = precision_score(y_valid, yhat_classes)*100.\n",
    "recall = recall_score(y_valid, yhat_classes)*100.\n",
    "f1 = f1_score(y_valid, yhat_classes)*100.\n",
    "prc_Y, prc_X, prc_bins = precision_recall_curve(y_valid, yhat_pred)\n",
    "auprc = auc(prc_X,prc_Y)*100.\n",
    "auroc = roc_auc_score(y_valid, yhat_pred)*100.\n",
    "mcc = matthews_corrcoef(y_valid, yhat_classes)\n",
    "\n",
    "print('Accuracy: %.2f%% Precision: %.2f%% Recall: %.2f%%' % (accuracy,precision,recall)) \n",
    "print('F1: %.2f%% MCC: %.4f' % (f1,mcc)) \n",
    "print('AUPRC: %.2f%% AUROC: %.2f%%' % (auprc,auroc)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjSVa72v4IsA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
