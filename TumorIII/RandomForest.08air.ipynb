{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest 08\n",
    "To reduce overfitting, use 1/10 of patches, restrict tree depth, and add trees. Patch-level accuracy increased to 73%.\n",
    "\n",
    "This ran on Air."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-23 11:09:26.217554\n",
      "Python 3.8.3\n",
      "sklearn 1.1.1\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "from platform import python_version\n",
    "print('Python',python_version())\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import random\n",
    "import sklearn   # pip install --upgrade scikit-learn\n",
    "print('sklearn',sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle X,y in tandem -- can cause shape problems\n",
    "from sklearn.utils import shuffle\n",
    "# The model\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our input is the output from the Data notebooks.\n",
    "# These csv files were filtered to remove give-away columns and bad rows.\n",
    "BASE_PATH='/home/jrm/Adjeroh/Naved/July_Run/CellProfilerFiltered/'  # alien\n",
    "BASE_PATH='/Users/jasonmiller/Downloads/CellProfilerFiltered/'   # air\n",
    "# This is the patch-level csv file: one row per patch, with nucleus totals.\n",
    "# Later, incorporate the nucleus-specific csv files.\n",
    "FILENAME='Process100_Image.csv'\n",
    "# This directory contains lists of patch filenames,\n",
    "# divided into 5 folds of train/valid splits.\n",
    "# Notice patch filename [0:19] is case ID, [0:23] is WSI ID,\n",
    "# and last column is the 6-way cancer class (zero to five) of the case.\n",
    "# For example, fold0_train.txt starts:\n",
    "# TCGA-06-0129-01Z-00-DX1_5400_5100.png, 0\n",
    "# TCGA-06-0129-01Z-00-DX1_5700_6000.png, 0\n",
    "LEN_PATIENT_ID = 19\n",
    "SPLITS_BASE='/home/jrm/Adjeroh/Naved/July_Run/TrainTestSplit/'  # alien\n",
    "SPLITS_BASE='/Users/jasonmiller/Downloads/TrainTestSplit/'    #air\n",
    "SPLITS_FILES = [\n",
    "    ('fold0_train.txt','fold0_test.txt'),\n",
    "    ('fold1_train.txt','fold1_test.txt'),\n",
    "    ('fold2_train.txt','fold2_test.txt'),\n",
    "    ('fold3_train.txt','fold3_test.txt'),\n",
    "    ('fold4_train.txt','fold4_test.txt'),\n",
    "]\n",
    "NUM_FOLDS = 5\n",
    "PATIENT_TO_CANCER = {}  # hash case ID to class number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn can do this but we were stumbling on shape & type problems\n",
    "def list_shuffle(X):   \n",
    "    L = len(X)\n",
    "    for i in range(L):\n",
    "        r = random.randrange(0,L)\n",
    "        if i!=r:\n",
    "            temp = X[i]\n",
    "            X[i] = X[r]\n",
    "            X[r] = temp\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a case ID = patient name = directory, load the csv file.\n",
    "# Expect a value like p='TCGA-S9-A6UB-01Z-00'\n",
    "def load_patient_data(p):\n",
    "    filepath=BASE_PATH+p+'/'+FILENAME\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given fold, load the case/patient IDs separated into training and validation.\n",
    "# Fold should be an in between 0 and 4 for 5-fold cross validation.\n",
    "# The returned list has unique strings like  TCGA-06-0129-01Z-00\n",
    "def _load_patient_names(filename):\n",
    "    patients=[]\n",
    "    with open(filename) as infile:\n",
    "        rows = csv.reader(infile)\n",
    "        for row in rows:\n",
    "            patient = row[0][:LEN_PATIENT_ID]\n",
    "            cancer_class = int(row[1])\n",
    "            if patient in PATIENT_TO_CANCER:\n",
    "                if PATIENT_TO_CANCER[patient] != cancer_class:\n",
    "                    raise Exception('One patient in two classes:',patient)\n",
    "            else:\n",
    "                PATIENT_TO_CANCER[patient] = cancer_class\n",
    "            patients.append(patient)\n",
    "    patients = list(np.unique(patients))\n",
    "    return patients\n",
    "def load_patient_names(fold):\n",
    "    filenames = SPLITS_FILES[fold]  # tuple of train,valid\n",
    "    train_patients = _load_patient_names(SPLITS_BASE+filenames[0])\n",
    "    valid_patients = _load_patient_names(SPLITS_BASE+filenames[1])\n",
    "    return train_patients,valid_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv into pandas dataframes and concatenate dataframes.\n",
    "# This is too slow! Like 10 min per invocation.\n",
    "def _load_train_valid_set1(patients):\n",
    "    X = None  # instances with features\n",
    "    y = None  # labels = cancer class\n",
    "    z = []  # patient_id for aggregation\n",
    "    for patient in patients:\n",
    "        df = load_patient_data(patient)\n",
    "        label = PATIENT_TO_CANCER[patient]\n",
    "        if X is None:\n",
    "            X = df \n",
    "            y = np.full(shape=len(df), fill_value=label, dtype=np.int8)\n",
    "        else:\n",
    "            X = pd.concat( (X, df) )\n",
    "            more = np.full(shape=len(df), fill_value=label, dtype=np.int8)\n",
    "            y = np.concatenate( (y, more) )\n",
    "        z = z + [patient]*len(df)  # same ID for all patches from one patient\n",
    "    return X,y,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv into list and hold off building the dataframe till the end.\n",
    "# This runs out of memory!\n",
    "def _load_train_valid_set2(patients):\n",
    "    X = []  # instances with features\n",
    "    y = []  # labels = cancer class\n",
    "    z = []  # patient_id for aggregation\n",
    "    header = None\n",
    "    patient_count = 0\n",
    "    for patient in patients:\n",
    "        row_count = 0\n",
    "        patient_count += 1\n",
    "        filepath=BASE_PATH+patient+'/'+FILENAME\n",
    "        label = PATIENT_TO_CANCER[patient]\n",
    "        with open(filepath) as infile:\n",
    "            rows = csv.reader(infile)\n",
    "            for row in rows:\n",
    "                if row_count == 0:\n",
    "                    if header is None:\n",
    "                        header = row\n",
    "                else:\n",
    "                    X.append(row)\n",
    "                    y.append(label)\n",
    "                    z.append(patient)\n",
    "                row_count += 1\n",
    "        print('Patients:',patient_count,'Rows:',row_count)\n",
    "    X = pd.DataFrame(X)\n",
    "    X.columns = header\n",
    "    return X,y,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slurp csv into numpy array.\n",
    "# This works!\n",
    "def _load_train_valid_set3(patients,save_mem=False):\n",
    "    X = None  # instances with features\n",
    "    y = None  # labels = cancer class\n",
    "    z = None  # patient_id for aggregation\n",
    "    for patient in patients:\n",
    "        filepath=BASE_PATH+patient+'/'+FILENAME\n",
    "        label = PATIENT_TO_CANCER[patient]\n",
    "        Xall = np.loadtxt(filepath,skiprows=1,delimiter=',')\n",
    "        if (save_mem):\n",
    "            Xi = Xall[0::10].copy()  # every tenth row\n",
    "        else:\n",
    "            Xi = Xall\n",
    "        yi = np.full(shape=len(Xi), fill_value=label, dtype=np.int8)\n",
    "        zi = [patient]*len(Xi)  # same ID for all patches from one patient\n",
    "        if X is None:\n",
    "            X = Xi\n",
    "            y = yi\n",
    "            z = zi\n",
    "        else:\n",
    "            X = np.concatenate( (X, Xi) )\n",
    "            y = np.concatenate( (y, yi) )\n",
    "            z = np.concatenate( (z, zi) )\n",
    "    X = pd.DataFrame(X)\n",
    "    # DataFrame column headers are not required for the machine learning.\n",
    "    # We'll add them just for debugging. Arbitrarily use first row of first file.\n",
    "    patient = patients[0]\n",
    "    filepath=BASE_PATH+patient+'/'+FILENAME\n",
    "    with open(filepath) as infile:\n",
    "        rows = csv.reader(infile)\n",
    "        for row in rows:\n",
    "            header = row\n",
    "            break\n",
    "    X.columns = header\n",
    "    return X,y,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv rows for one set of train+valid patient names.\n",
    "# This is slow, probably due to concatenating dataframes.\n",
    "# Consider a rewrite that concatenates csv before constructing a dataframe.\n",
    "def load_train_valid_set(train_patient_names,valid_patient_names):\n",
    "    X_train,y_train,z_train = _load_train_valid_set3(train_patient_names,True)\n",
    "    print('Loaded lengths X,y,z:',X_train.shape,len(y_train),len(z_train))\n",
    "    X_valid,y_valid,z_valid = _load_train_valid_set3(valid_patient_names,True)\n",
    "    print('Loaded lengths X,y,z:',X_valid.shape,len(y_valid),len(z_valid))\n",
    "    \n",
    "    # TO DO: raise exception if any valid patient is also a train patient\n",
    "    return X_train,y_train,z_train,X_valid,y_valid,z_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later, weight each patch label by the confidence i.e. score\n",
    "def aggregate_accuracy(y_pred,z_valid):\n",
    "    L = len(y_pred)\n",
    "    if L != len(z_valid):\n",
    "        raise Exception('Lengths of y and z do not match')\n",
    "    correct = {}\n",
    "    incorrect = {}\n",
    "    patients = np.unique(z_valid)\n",
    "    for patient in patients:\n",
    "        correct[patient]=0\n",
    "        incorrect[patient]=0\n",
    "    for i in range(L):\n",
    "        patient = z_valid[i]\n",
    "        label = PATIENT_TO_CANCER[patient]\n",
    "        pred = y_pred[i]\n",
    "        if pred == label:\n",
    "            correct[patient] += 1\n",
    "        else:\n",
    "            incorrect[patient] += 1\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for patient in patients:\n",
    "        denominator += 1\n",
    "        if correct[patient]>incorrect[patient]:\n",
    "            numerator += 1\n",
    "    accuracy = float(0)\n",
    "    if denominator>0:\n",
    "        accuracy = 100.0*numerator/denominator \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop: Load, Classify, Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_features(model):\n",
    "        # Prereqs: fit().\n",
    "        names = model.feature_names_in_\n",
    "        importances = model.feature_importances_\n",
    "        pairs = np.column_stack( (names,importances) )\n",
    "        top_array = sorted(pairs, key = lambda e:e[1], reverse=True)\n",
    "        # There must be a way to do this witout a loop!\n",
    "        top_list = []\n",
    "        for i in top_array:\n",
    "             top_list.append((i[1],i[0]))  # 0=feature_name, 1=importance\n",
    "        top_df = pd.DataFrame(top_list)\n",
    "        return top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-07-23 11:09:30.370279 Fold 0 Loading...\n",
      "Loaded lengths X,y,z: (6014, 5302) 6014 6014\n",
      "Loaded lengths X,y,z: (6014, 5302) 6014 6014\n",
      "2022-07-23 11:12:55.265168 Shuffle...\n",
      "2022-07-23 11:12:55.479983 Train...\n",
      "2022-07-23 11:14:08.259285 Ranked feature imporances...\n",
      "           0                                                  1\n",
      "0   0.018874                Median_Nucleus_AreaShape_MeanRadius\n",
      "1   0.013506  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_5...\n",
      "2   0.012955  Mean_Nucleus_Texture_InfoMeas1_Hematoxylin_5_0...\n",
      "3   0.012928                  Mean_Nucleus_AreaShape_MeanRadius\n",
      "4   0.012318  Mean_Nucleus_Texture_InfoMeas1_Hematoxylin_5_0...\n",
      "5   0.012306  Mean_Nucleus_Texture_InfoMeas1_Hematoxylin_4_0...\n",
      "6   0.011839  Mean_Nucleus_Texture_InfoMeas1_Hematoxylin_3_0...\n",
      "7   0.011836               Mean_Nucleus_AreaShape_MaximumRadius\n",
      "8   0.011302  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_5...\n",
      "9   0.011148  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_7...\n",
      "10  0.010544  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_4...\n",
      "2022-07-23 11:14:08.409597 Evaluate...\n",
      "Fold 0 Patch-level Training Accuracy: 73.47855004988361\n",
      "2022-07-23 11:14:09.178327 Validate...\n",
      "Fold 0 Patch-level Validation Accuracy: 73.47855004988361\n",
      "Fold 0 Patient-level Validation Accuracy: 62.65060240963855\n",
      "\n",
      "2022-07-23 11:14:09.909567 Fold 1 Loading...\n",
      "Loaded lengths X,y,z: (6077, 5302) 6077 6077\n",
      "Loaded lengths X,y,z: (6077, 5302) 6077 6077\n",
      "2022-07-23 11:17:37.852419 Shuffle...\n",
      "2022-07-23 11:17:38.092249 Train...\n",
      "2022-07-23 11:18:54.571303 Ranked feature imporances...\n",
      "           0                                                  1\n",
      "0   0.016931                  Mean_Nucleus_AreaShape_MeanRadius\n",
      "1   0.014276  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_7...\n",
      "2   0.013551  Mean_Nucleus_Texture_InfoMeas1_Hematoxylin_5_0...\n",
      "3   0.013220                Mean_Nucleus_AreaShape_MedianRadius\n",
      "4   0.012845               Mean_Nucleus_AreaShape_MaximumRadius\n",
      "5   0.012469  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_7...\n",
      "6   0.012289  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_5...\n",
      "7   0.011179  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_5...\n",
      "8   0.010346  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_4...\n",
      "9   0.010345  Mean_Nucleus_Texture_InfoMeas1_Hematoxylin_5_0...\n",
      "10  0.010264  Mean_Nucleus_Texture_InfoMeas1_Hematoxylin_7_0...\n",
      "2022-07-23 11:18:54.737498 Evaluate...\n",
      "Fold 1 Patch-level Training Accuracy: 73.86868520651637\n",
      "2022-07-23 11:18:55.507146 Validate...\n",
      "Fold 1 Patch-level Validation Accuracy: 73.86868520651637\n",
      "Fold 1 Patient-level Validation Accuracy: 59.036144578313255\n",
      "\n",
      "2022-07-23 11:18:56.244878 Fold 2 Loading...\n",
      "Loaded lengths X,y,z: (6198, 5302) 6198 6198\n",
      "Loaded lengths X,y,z: (6198, 5302) 6198 6198\n",
      "2022-07-23 11:22:23.848826 Shuffle...\n",
      "2022-07-23 11:22:24.054588 Train...\n",
      "2022-07-23 11:23:39.199892 Ranked feature imporances...\n",
      "           0                                                  1\n",
      "0   0.017056  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_7...\n",
      "1   0.015515  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_7...\n",
      "2   0.014332  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_4...\n",
      "3   0.012629             Median_Nucleus_AreaShape_MaximumRadius\n",
      "4   0.012337  Mean_Nucleus_Texture_InfoMeas1_Hematoxylin_3_0...\n",
      "5   0.011566      Median_Nucleus_Texture_Entropy_Eosin_4_02_256\n",
      "6   0.011544  Mean_Nucleus_Texture_InfoMeas1_Hematoxylin_5_0...\n",
      "7   0.011013              Median_Nucleus_AreaShape_MedianRadius\n",
      "8   0.010877                  Mean_Nucleus_AreaShape_MeanRadius\n",
      "9   0.010313  Mean_Nucleus_Texture_InfoMeas1_Hematoxylin_7_0...\n",
      "10  0.009897               Mean_Nucleus_AreaShape_MaximumRadius\n",
      "2022-07-23 11:23:39.322379 Evaluate...\n",
      "Fold 2 Patch-level Training Accuracy: 73.65279122297515\n",
      "2022-07-23 11:23:40.079502 Validate...\n",
      "Fold 2 Patch-level Validation Accuracy: 73.65279122297515\n",
      "Fold 2 Patient-level Validation Accuracy: 63.855421686746986\n",
      "\n",
      "2022-07-23 11:23:40.810402 Fold 3 Loading...\n",
      "Loaded lengths X,y,z: (5764, 5302) 5764 5764\n",
      "Loaded lengths X,y,z: (5764, 5302) 5764 5764\n",
      "2022-07-23 11:26:52.599247 Shuffle...\n",
      "2022-07-23 11:26:52.783654 Train...\n",
      "2022-07-23 11:28:02.970393 Ranked feature imporances...\n",
      "           0                                                  1\n",
      "0   0.015347                Median_Nucleus_AreaShape_MeanRadius\n",
      "1   0.012930  Mean_Nucleus_Texture_InfoMeas1_Hematoxylin_7_0...\n",
      "2   0.012704  Mean_Nucleus_Texture_InfoMeas1_Hematoxylin_4_0...\n",
      "3   0.012383  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_5...\n",
      "4   0.011205  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_4...\n",
      "5   0.010187  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_4...\n",
      "6   0.010158  Mean_Nucleus_Texture_InfoMeas1_Hematoxylin_5_0...\n",
      "7   0.010027  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_3...\n",
      "8   0.010016              Median_Nucleus_AreaShape_MedianRadius\n",
      "9   0.009496                Mean_Nucleus_AreaShape_MedianRadius\n",
      "10  0.009354  Mean_Nucleus_Texture_InfoMeas1_Hematoxylin_5_0...\n",
      "2022-07-23 11:28:03.090424 Evaluate...\n",
      "Fold 3 Patch-level Training Accuracy: 73.4385843164469\n",
      "2022-07-23 11:28:03.786876 Validate...\n",
      "Fold 3 Patch-level Validation Accuracy: 73.4385843164469\n",
      "Fold 3 Patient-level Validation Accuracy: 61.44578313253012\n",
      "\n",
      "2022-07-23 11:28:04.487326 Fold 4 Loading...\n",
      "Loaded lengths X,y,z: (5553, 5302) 5553 5553\n",
      "Loaded lengths X,y,z: (5553, 5302) 5553 5553\n",
      "2022-07-23 11:58:56.532699 Shuffle...\n",
      "2022-07-23 11:58:57.071275 Train...\n",
      "2022-07-23 12:00:08.716006 Ranked feature imporances...\n",
      "           0                                                  1\n",
      "0   0.015600  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_7...\n",
      "1   0.015545  Mean_Nucleus_Texture_InfoMeas1_Hematoxylin_7_0...\n",
      "2   0.013290                Mean_Nucleus_AreaShape_MedianRadius\n",
      "3   0.011977  Median_Nucleus_Intensity_IntegratedIntensity_H...\n",
      "4   0.011636  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_4...\n",
      "5   0.011570              Median_Nucleus_AreaShape_MedianRadius\n",
      "6   0.011543  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_7...\n",
      "7   0.011023  Mean_Nucleus_Texture_InfoMeas1_Hematoxylin_7_0...\n",
      "8   0.010532                Median_Nucleus_AreaShape_MeanRadius\n",
      "9   0.010289  Median_Nucleus_Texture_InfoMeas1_Hematoxylin_4...\n",
      "10  0.010079             Median_Nucleus_AreaShape_MaximumRadius\n",
      "2022-07-23 12:00:08.836044 Evaluate...\n",
      "Fold 4 Patch-level Training Accuracy: 72.86151629749685\n",
      "2022-07-23 12:00:09.526726 Validate...\n",
      "Fold 4 Patch-level Validation Accuracy: 72.86151629749685\n",
      "Fold 4 Patient-level Validation Accuracy: 65.06024096385542\n"
     ]
    }
   ],
   "source": [
    "patch_accuracies = []   # summary statistics\n",
    "patient_accuracies = [] # summary statistics\n",
    "\n",
    "for fold in range(NUM_FOLDS):\n",
    "    print()\n",
    "    print(datetime.datetime.now(),'Fold',fold,'Loading...')\n",
    "    train_patients,valid_patients = load_patient_names(fold)  \n",
    "    X_train,y_train,z_train,X_valid,y_valid,z_valid = \\\n",
    "        load_train_valid_set(train_patients,valid_patients)    \n",
    "    # print('Fold',fold,'patients train',train_patients,'patients valid',valid_patients)\n",
    "    # print('Fold:',fold,'patches train',X_train.shape,'patches valid',X_valid.shape)\n",
    "\n",
    "    # This shuffle is pro forma, not strictly necessary.\n",
    "    # CNN models are sensitive to train set order but RF models are not. \n",
    "    print(datetime.datetime.now(),'Shuffle...')\n",
    "    X_train,y_train = shuffle(X_train,y_train)\n",
    "\n",
    "    print(datetime.datetime.now(),'Train...')\n",
    "    # min_samples_leaf=1 (default) led to overfitting\n",
    "    rfc = RandomForestClassifier(max_depth=5,n_estimators=500)\n",
    "    rfc.fit(X_train,y_train)  # slow\n",
    "    \n",
    "    print(datetime.datetime.now(),'Ranked feature imporances...')\n",
    "    top = important_features(rfc)\n",
    "    #pd.set_option('display.max_rows', None)\n",
    "    print(top.loc[:10])\n",
    "    \n",
    "    print(datetime.datetime.now(),'Evaluate...')\n",
    "    y_pred = rfc.predict(X_train)\n",
    "    matches = np.count_nonzero(y_train==y_pred)\n",
    "    accuracy = 100.0 * matches / len(y_pred)\n",
    "    print('Fold',fold,'Patch-level Training Accuracy:',accuracy)\n",
    "    \n",
    "    print(datetime.datetime.now(),'Validate...')\n",
    "    y_pred = rfc.predict(X_valid)\n",
    "    matches = np.count_nonzero(y_valid==y_pred)\n",
    "    accuracy = 100.0 * matches / len(y_pred)\n",
    "    print('Fold',fold,'Patch-level Validation Accuracy:',accuracy)\n",
    "    patch_accuracies.append(accuracy)\n",
    "    \n",
    "    accuracy = aggregate_accuracy(y_pred,z_valid)\n",
    "    patient_accuracies.append(accuracy)\n",
    "    print('Fold',fold,'Patient-level Validation Accuracy:',accuracy)\n",
    "    \n",
    "    # This shouldn't be necessary but it seems to reduce memory footprint.\n",
    "    X_train=None\n",
    "    X_valid=None\n",
    "    y_train=None\n",
    "    y_valid=None\n",
    "    z_train=None\n",
    "    z_valid=None\n",
    "    rfc = None\n",
    "    y_pred = None\n",
    "    matches = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-23 12:00:10.236443\n",
      "Cross validation patch-level accuracy: [73.47855004988361, 73.86868520651637, 73.65279122297515, 73.4385843164469, 72.86151629749685]\n",
      "mean: 73.46002541866378 std: 0.3356121357953721\n",
      "Cross validation patient-level accuracy: [62.65060240963855, 59.036144578313255, 63.855421686746986, 61.44578313253012, 65.06024096385542]\n",
      "mean: 62.40963855421687 std: 2.0728494619379814\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "print('Cross validation patch-level accuracy:',patch_accuracies)\n",
    "print('mean:',np.mean(patch_accuracies),'std:',np.std(patch_accuracies))\n",
    "print('Cross validation patient-level accuracy:',patient_accuracies)\n",
    "print('mean:',np.mean(patient_accuracies),'std:',np.std(patient_accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
