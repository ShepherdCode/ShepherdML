{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "Start with text from Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Shakespeare\n",
    "surl=\"https://homl.info/shakespeare\"\n",
    "filepath=keras.utils.get_file(\"shakespeare.txt\",surl)\n",
    "with open(filepath) as fp:\n",
    "    stext = fp.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The keras tokenizer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "keras.preprocessing.text:\n",
    "    \n",
    "keras differentiates:\n",
    "    Sequences = list of words\n",
    "    Texts = list of Sequences\n",
    "\n",
    "Alternative to sklearn.preprocessing.OneHotEncoder is\n",
    "tf.keras.preprocessing.text.one_hot\n",
    "\n",
    "Alternative to tokenizer is\n",
    "tf.keras.preprocessing.text.text_to_word_sequence"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tokenizer parameters:\n",
    "\n",
    "num_words: the maximum number of words to keep, based\n",
    "    on word frequency. Only the most common `num_words-1` words will\n",
    "    be kept.\n",
    "filters: a string where each element is a character that will be\n",
    "    filtered from the texts. The default is all punctuation, plus\n",
    "    tabs and line breaks, minus the `'` character.\n",
    "lower: boolean. Whether to convert the texts to lowercase.\n",
    "split: str. Separator for word splitting.\n",
    "char_level: if True, every character will be treated as a token.\n",
    "oov_token: if given, it will be added to word_index and used to\n",
    "    replace out-of-vocabulary words during text_to_sequence calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text len = 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear\n"
     ]
    }
   ],
   "source": [
    "print(\"Text len = %d\"%len(stext))\n",
    "print(stext[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given array of string, tokenizer chops to words\n",
      "Num encoded words = 12632\n",
      "Encoding = dict_items([('the', 1), ('and', 2), ('to', 3), ('i\n"
     ]
    }
   ],
   "source": [
    "print(\"Given array of string, tokenizer chops to words\")\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=False,lower=True)\n",
    "text_as_array=[stext]\n",
    "tokenizer.fit_on_texts(text_as_array)   # this is fast and word-leel\n",
    "encoding = tokenizer.word_index.items()\n",
    "print(\"Num encoded words = \"+str(len(encoding)))\n",
    "print(\"Encoding = \"+str(encoding)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a string, tokenizer chops to chars regardless of parameter...\n",
      "Num encoded words = 28\n",
      "Encoding = dict_items([('e', 1), ('t', 2), ('o', 3), ('a', 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Given a string, tokenizer chops to chars regardless of parameter...\")\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=False,lower=True)\n",
    "tokenizer.fit_on_texts(stext)  \n",
    "encoding = tokenizer.word_index.items()\n",
    "print(\"Num encoded words = \"+str(len(encoding)))\n",
    "print(\"Encoding = \"+str(encoding)[:50])\n",
    "# This is slow.\n",
    "# At word level, tokenizer filters non-word characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given array of string and char_level param tokenizer chops to chars\n",
      "Num encoded words = 39\n",
      "Encoding = dict_items([(' ', 1), ('e', 2), ('t', 3), ('o', 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Given array of string and char_level param tokenizer chops to chars\")\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True,lower=True)\n",
    "text_as_array=[stext] \n",
    "tokenizer.fit_on_texts(text_as_array) \n",
    "encoding = tokenizer.word_index.items()\n",
    "print(\"Num encoded words = \"+str(len(encoding)))\n",
    "print(\"Encoding = \"+str(encoding)[:50])\n",
    "# This is fast.\n",
    "# At char level, tokenzer leaves non-word characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 total sequences\n",
      "1115394 array[0] len\n",
      "1115394 seq len\n",
      "First Citi\n",
      "[20, 6, 9, 8, 3, 1, 19, 6, 3, 6]\n"
     ]
    }
   ],
   "source": [
    "# Here are different ways of retrieving the encoded sequence.\n",
    "array_of_seq = tokenizer.texts_to_sequences(text_as_array)\n",
    "print(\"%d total sequences\"%len(array_of_seq))\n",
    "print(\"%d array[0] len\"%len(array_of_seq[0]))\n",
    "[sequence] = tokenizer.texts_to_sequences(text_as_array)\n",
    "print(\"%d seq len\"%len(sequence))\n",
    "print(stext[:10])\n",
    "print(sequence[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19  5  8  7  2  0 18  5  2  5]\n"
     ]
    }
   ],
   "source": [
    "# If we want encoding to start at 0,\n",
    "# this doesn't work on python list: encoded=sequence-1\n",
    "# but it does work in numpy.\n",
    "encoded=np.array(sequence)-1\n",
    "print(encoded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save this for later\n",
    "max_code = len(tokenizer.word_index)\n",
    "max_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validate, test sets\n",
    "Convert one long list of (encoded) characters\n",
    "to many training instances using tensor window function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (), types: tf.int64>\n"
     ]
    }
   ],
   "source": [
    "# Use TensorFlow Dataset.\n",
    "# In addition to from_tensor_slices(), it has methods\n",
    "# enumerate, filter, shuffle, skip, zip, interleave, apply, ...\n",
    "\n",
    "data_size = len(encoded)\n",
    "train_size = int(0.9 * data_size)\n",
    "train_np = encoded[:train_size]\n",
    "\n",
    "# Stackoverflow clarifies what this method does.\n",
    "# from_tensors() combines inputs, dataset contains a single element.\n",
    "# from_tensor_slices() dataset contains separate element for each input row.\n",
    "# The name really should be to_tensor_slices()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "print(dataset)\n",
    "# Known issue: this special kind of dataset is missing many methods\n",
    "# that other tensors have: cardinality(), to_numpy(), ...\n",
    "# Note: shape () means list of scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WindowDataset shapes: DatasetSpec(TensorSpec(shape=(), dtype=tf.int64, name=None), TensorShape([])), types: DatasetSpec(TensorSpec(shape=(), dtype=tf.int64, name=None), TensorShape([]))>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataset of datasets, each representing a window.\n",
    "\n",
    "n_steps=100  # max pattern the RNN can learn\n",
    "window_length = n_steps+1   # include one predicted letter\n",
    "# Use shift=1 because default is non-overlapping windows.\n",
    "# Use drop_remainder to avoid small last window.\n",
    "windows = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "windows  # nested dataset of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset shapes: (None,), types: tf.int64>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataset of tensors, each representing a window.\n",
    "\n",
    "# Call dataset.batch() on each window to create a tensor.\n",
    "tensors = windows.flat_map(lambda window: window.batch(window_length))\n",
    "tensors    # dataset of tensors\n",
    "# Note: shape (None,) means unknown dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (None, None), types: tf.int64>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consecutive overlapping windows are highly correlated!\n",
    "# This will bias gradient descent.\n",
    "# Solution is shuffle.\n",
    "\n",
    "batch_size=32\n",
    "buffer_size=10000\n",
    "shuffled = tensors.shuffle(buffer_size).batch(batch_size)\n",
    "shuffled  # Special Dataset called Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((None, None), (None, None)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use colons so e.g. X = 0-9 while y = 1-10.\n",
    "mapped = shuffled.map(lambda win: (win[:,:-1], win[:,1:]))\n",
    "mapped   # Special Dataset called Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((None, None, 39), (None, None)), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next use encoding or embedding.\n",
    "encoded = mapped.map(lambda X,y: (tf.one_hot(X,depth=max_code),y))\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train a model\n",
    "This takes hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeDistributed: apply a layer to each temporal slice.\n",
    "# GRU: the simpler LSTM using two gates.\n",
    "rnn = keras.models.Sequential([\n",
    "    keras.layers.GRU(128,return_sequences=True,\n",
    "                     input_shape=[None,max_code],\n",
    "                    dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128,return_sequences=True,\n",
    "                     input_shape=[None,max_code],\n",
    "                    dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(\n",
    "        keras.layers.Dense(max_code,activation=\"softmax\"))\n",
    "])\n",
    "rnn.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "    7/31370 [..............................] - ETA: 2:47:45 - loss: 2.2382"
     ]
    }
   ],
   "source": [
    "# Book says this takes many hours.\n",
    "# On my laptop, this needs 3 hours per epoch.\n",
    "spe = train_size//batch_size\n",
    "history=rnn.fit(encoded,epochs=5,steps_per_epoch=spe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
