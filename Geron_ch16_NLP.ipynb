{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Shakespeare\n",
    "surl=\"https://homl.info/shakespeare\"\n",
    "filepath=keras.utils.get_file(\"shakespeare.txt\",surl)\n",
    "with open(filepath) as fp:\n",
    "    stext = fp.read()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "keras.preprocessing.text:\n",
    "    \n",
    "keras differentiates:\n",
    "    Sequences = list of words\n",
    "    Texts = list of Sequences\n",
    "\n",
    "Alternative to sklearn.preprocessing.OneHotEncoder is\n",
    "tf.keras.preprocessing.text.one_hot\n",
    "\n",
    "Alternative to tokenizer is\n",
    "tf.keras.preprocessing.text.text_to_word_sequence"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tokenizer parameters:\n",
    "\n",
    "num_words: the maximum number of words to keep, based\n",
    "    on word frequency. Only the most common `num_words-1` words will\n",
    "    be kept.\n",
    "filters: a string where each element is a character that will be\n",
    "    filtered from the texts. The default is all punctuation, plus\n",
    "    tabs and line breaks, minus the `'` character.\n",
    "lower: boolean. Whether to convert the texts to lowercase.\n",
    "split: str. Separator for word splitting.\n",
    "char_level: if True, every character will be treated as a token.\n",
    "oov_token: if given, it will be added to word_index and used to\n",
    "    replace out-of-vocabulary words during text_to_sequence calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text len = 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear\n"
     ]
    }
   ],
   "source": [
    "print(\"Text len = %d\"%len(stext))\n",
    "print(stext[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given array of string, tokenizer chops to words\n",
      "Num encoded words = 12632\n",
      "Encoding = dict_items([('the', 1), ('and', 2), ('to', 3), ('i\n"
     ]
    }
   ],
   "source": [
    "print(\"Given array of string, tokenizer chops to words\")\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=False,lower=True)\n",
    "text_as_array=[stext]\n",
    "tokenizer.fit_on_texts(text_as_array)   # this is fast and word-leel\n",
    "encoding = tokenizer.word_index.items()\n",
    "print(\"Num encoded words = \"+str(len(encoding)))\n",
    "print(\"Encoding = \"+str(encoding)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a string, tokenizer chops to chars regardless of parameter...\n",
      "Num encoded words = 28\n",
      "Encoding = dict_items([('e', 1), ('t', 2), ('o', 3), ('a', 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Given a string, tokenizer chops to chars regardless of parameter...\")\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=False,lower=True)\n",
    "tokenizer.fit_on_texts(stext)  \n",
    "encoding = tokenizer.word_index.items()\n",
    "print(\"Num encoded words = \"+str(len(encoding)))\n",
    "print(\"Encoding = \"+str(encoding)[:50])\n",
    "# This is slow.\n",
    "# At word level, tokenizer filters non-word characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given array of string and char_level param tokenizer chops to chars\n",
      "Num encoded words = 39\n",
      "Encoding = dict_items([(' ', 1), ('e', 2), ('t', 3), ('o', 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Given array of string and char_level param tokenizer chops to chars\")\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True,lower=True)\n",
    "text_as_array=[stext] \n",
    "tokenizer.fit_on_texts(text_as_array) \n",
    "encoding = tokenizer.word_index.items()\n",
    "print(\"Num encoded words = \"+str(len(encoding)))\n",
    "print(\"Encoding = \"+str(encoding)[:50])\n",
    "# This is fast.\n",
    "# At char level, tokenzer leaves non-word characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 total sequences\n",
      "1115394 array[0] len\n",
      "1115394 seq len\n",
      "First Citi\n",
      "[20, 6, 9, 8, 3, 1, 19, 6, 3, 6]\n"
     ]
    }
   ],
   "source": [
    "# Here are different ways of retrieving the encoded sequence.\n",
    "array_of_seq = tokenizer.texts_to_sequences(text_as_array)\n",
    "print(\"%d total sequences\"%len(array_of_seq))\n",
    "print(\"%d array[0] len\"%len(array_of_seq[0]))\n",
    "[sequence] = tokenizer.texts_to_sequences(text_as_array)\n",
    "print(\"%d seq len\"%len(sequence))\n",
    "print(stext[:10])\n",
    "print(sequence[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19  5  8  7  2  0 18  5  2  5]\n"
     ]
    }
   ],
   "source": [
    "# If we want encoding to start at 0,\n",
    "# this doesn't work on python list: encoded=sequence-1\n",
    "# but it does work in numpy.\n",
    "import numpy as np\n",
    "encoded=np.array(sequence)-1\n",
    "print(encoded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
