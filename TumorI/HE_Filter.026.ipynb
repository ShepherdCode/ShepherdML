{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained H&E classifier\n",
    "Training set: central patches from Y-pos vs Y-neg images.  \n",
    "No attempt to distinguish background, stroma, tumor.  \n",
    "Retrain VGG.  \n",
    "Expect no classification.\n",
    "\n",
    "## Classify patches\n",
    "Use VGG. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy==1.22.1\n",
      "json==2.0.9\n",
      "keras.api._v2.keras==2.7.0\n",
      "pandas==1.4.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "#  os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"   # turns off GPU?\n",
    "import glob\n",
    "#import cv2 # OpenCV-Python\n",
    "#from PIL import Image\n",
    "#Image.MAX_IMAGE_PIXELS = None\n",
    "import numpy as np\n",
    "import json\n",
    "from tensorflow import keras\n",
    "import keras.layers as kl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "print('\\n'.join(f'{m.__name__}=={m.__version__}' for m in globals().values() if getattr(m, '__version__', None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will overwrite model at /home/jrm/Martinez/models/HE_Filter.026\n"
     ]
    }
   ],
   "source": [
    "DIR_IMAGES_IN = \"/home/jrm/Martinez/images/HEcenterpatch/\"  \n",
    "PATCH_SIZE=224  # matches VGG\n",
    "DIR_MODELS = \"/home/jrm/Martinez/models/\"\n",
    "FILE_MODEL = \"HE_Filter.026\"\n",
    "filepath=DIR_MODELS+FILE_MODEL\n",
    "print(\"Will overwrite model at \"+filepath)\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (PATCH_SIZE,PATCH_SIZE)\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30892 files belonging to 2 classes.\n",
      "Found 7722 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(DIR_IMAGES_IN, 'train')\n",
    "valid_dir = os.path.join(DIR_IMAGES_IN, 'valid')\n",
    "train_dataset = keras.utils.image_dataset_from_directory(\n",
    "    train_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
    "valid_dataset = keras.utils.image_dataset_from_directory(\n",
    "    valid_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize color channel ints to floats (0,1)\n",
    "# See https://www.tensorflow.org/tutorials/load_data/images\n",
    "\n",
    "normalization_layer = kl.Rescaling(1.0/255.0)\n",
    "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "valid_dataset = valid_dataset.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3)\n",
      "tf.Tensor([0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0], shape=(32,), dtype=int32)\n",
      "(32, 224, 224, 3)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1], shape=(32,), dtype=int32)\n",
      "(32, 224, 224, 3)\n",
      "tf.Tensor([0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1], shape=(32,), dtype=int32)\n",
      "(32, 224, 224, 3)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0], shape=(32,), dtype=int32)\n",
      "(32, 224, 224, 3)\n",
      "tf.Tensor([1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "(32, 224, 224, 3)\n",
      "tf.Tensor([0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "(32, 224, 224, 3)\n",
      "tf.Tensor([1 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1], shape=(32,), dtype=int32)\n",
      "(32, 224, 224, 3)\n",
      "tf.Tensor([1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1], shape=(32,), dtype=int32)\n",
      "(32, 224, 224, 3)\n",
      "tf.Tensor([0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(32,), dtype=int32)\n",
      "(32, 224, 224, 3)\n",
      "tf.Tensor([1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1], shape=(32,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "iterator = valid_dataset.take(10)\n",
    "for x in iterator:\n",
    "    print(x[0].shape)\n",
    "    print(x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG\n",
    "https://keras.io/guides/transfer_learning/  \n",
    "https://towardsdatascience.com/step-by-step-guide-to-using-pretrained-models-in-keras-c9097b647b29  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[3,3,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RandomUniform]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-822ea724496a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#model = keras.Model(inputs, outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mVGG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCHANNELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mVGG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/applications/vgg16.py\u001b[0m in \u001b[0;36mVGG16\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    152\u001b[0m   x = layers.Conv2D(\n\u001b[1;32m    153\u001b[0m       128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n\u001b[0;32m--> 154\u001b[0;31m   x = layers.Conv2D(\n\u001b[0m\u001b[1;32m    155\u001b[0m       128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n\u001b[1;32m    156\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'block2_pool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m   1829\u001b[0m       return self._generator.uniform(\n\u001b[1;32m   1830\u001b[0m           shape=shape, minval=minval, maxval=maxval, dtype=dtype)\n\u001b[0;32m-> 1831\u001b[0;31m     return tf.random.uniform(\n\u001b[0m\u001b[1;32m   1832\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1833\u001b[0m         seed=self.make_legacy_seed())\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3,3,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RandomUniform]"
     ]
    }
   ],
   "source": [
    "CLASSES = 2\n",
    "#base_model = VGG16(weights='imagenet',include_top=False)  # top= classification layer\n",
    "# optional: input_shape=(224, 224, 3)\n",
    "#base_model.trainable = False\n",
    "#inputs = keras.Input(shape=(PATCH_SIZE, PATCH_SIZE, CHANNELS))\n",
    "#x = base_model(inputs, training=False)\n",
    "\n",
    "#x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "#outputs = keras.layers.Dense(CLASSES)(x)\n",
    "#model = keras.Model(inputs, outputs)\n",
    "\n",
    "#x = keras.layers.Flatten()(x)\n",
    "#outputs = keras.layers.Dense(CLASSES)(x)\n",
    "#model = keras.Model(inputs, outputs)\n",
    "\n",
    "VGG = VGG16(weights='imagenet',include_top=False,input_shape=(PATCH_SIZE, PATCH_SIZE, CHANNELS))\n",
    "VGG.trainable = False\n",
    "model = keras.models.Sequential()\n",
    "model.add(VGG)\n",
    "model.add(kl.Flatten())\n",
    "model.add(kl.Dense(16))\n",
    "model.add(kl.Dropout(0.25))\n",
    "model.add(kl.Dense(CLASSES,activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = SGD or ADAM ?\n",
    "ADAM converges faster but SGD better in the long run.  \n",
    "https://medium.com/geekculture/a-2021-guide-to-improving-cnns-optimizers-adam-vs-sgd-495848ac6008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "           optimizer=\"sgd\",\n",
    "           metrics=[\"accuracy\"])\n",
    "EPOCHS=10\n",
    "start = time.time()\n",
    "hist = model.fit(train_dataset,validation_data=valid_dataset,epochs=EPOCHS)\n",
    "end = time.time()\n",
    "print(\"Elapsed time:\",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(hist.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0.0,1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the portion of patches that are Ypos is  \n",
    "35985 / (35985 + 10233) = 0.77859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
