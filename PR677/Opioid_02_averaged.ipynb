{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opioid Data\n",
    "HW #2 Part 1 - Dimensionality Reduction.  \n",
    "Use one summary vector per patient, specifically the monthly average per patient.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Patient files are in one of two directories: R or NR.  \n",
    "Each patient is represented by one CSV file.  \n",
    "Each row of each CSV contains readings from one day.    \n",
    "Here, we load each patient average across all days.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_validate\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "mycmap = colors.ListedColormap(['red','blue'])  # list color for label 0 then 1\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14  files of type R\n",
      "26  files of type N\n"
     ]
    }
   ],
   "source": [
    "pathR='data/ChunkedData_R/'\n",
    "pathN='data/ChunkedData_NR/'\n",
    "filesR = listdir(pathR)\n",
    "filesN = listdir(pathN)\n",
    "print(len(filesR),\" files of type R\")\n",
    "print(len(filesN),\" files of type N\")\n",
    "CLASS_SEPARATOR=13  # data[:13] vs data[13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read one CSV file. \n",
    "# Create a Pandas data frame.\n",
    "# Drop the date column.\n",
    "def file_mean (filepath):\n",
    "    mydata = pd.read_csv(filepath)\n",
    "    # Drop the date column.\n",
    "    mydata = mydata.drop('Date',axis=1) \n",
    "    # Transpose column of mean values into a row.\n",
    "    mymean = mydata.mean(axis=0).to_frame().T\n",
    "    myvar  = mydata.var (axis=0).to_frame().T\n",
    "    return mymean,myvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read directory of CSV files (R or NR). \n",
    "# Create one dataframe representing all files.\n",
    "# Retain only one row per file = column averages.\n",
    "def mean_per_file (directory,variance):\n",
    "    files = listdir(directory)\n",
    "    means = pd.DataFrame()\n",
    "    for fp in files:\n",
    "        dfp = directory+fp\n",
    "        m = file_mean(dfp)\n",
    "        # Let Pandas number the rows sequentially.\n",
    "        means = means.append(m,ignore_index=True)\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mean_per_file() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1f3f5343c6a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmeansR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_per_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmeansR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: mean_per_file() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "meansR = mean_per_file(pathR,True)\n",
    "meansR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meansN = mean_per_file(pathN,True)\n",
    "meansN.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data (R and NR) into one data frame.\n",
    "# Label R = positive = 1 = blue.\n",
    "# Label NR = negative = 0 = red.\n",
    "def make_labels(positives,negatives):\n",
    "    rows = positives.shape[0]\n",
    "    labelsP = pd.DataFrame(np.ones(rows,dtype=np.int8))  # one = blue\n",
    "    rows = negatives.shape[0]\n",
    "    labelsN = pd.DataFrame(np.zeros(rows,dtype=np.int8))  # zero = red\n",
    "    labelsAll = pd.concat((labelsP,labelsN),ignore_index=True)\n",
    "    return labelsAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meansAll = pd.concat((meansR,meansN),ignore_index=True)\n",
    "meansAll.shape\n",
    "labels_df = make_labels(meansR,meansN)\n",
    "print(labels_df.T)\n",
    "meansR = None\n",
    "meansN = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and covariance\n",
    "Normalize by subtracting the column mean from every column value.  \n",
    "Since columns have widely different numerical ranges,   \n",
    "also normalize by making each column have unit variance.  \n",
    "Note: without normalization, the covariance plot would be all black except for the few features with large absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features by shifting the mean to zero and scaling to unit variance.\n",
    "def scale_features(X):\n",
    "    s = StandardScaler()\n",
    "    z = s.fit_transform(X)\n",
    "    return z\n",
    "scaledMeans = scale_features(meansAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_covariance(X):\n",
    "    cv=X.T.dot(X)/len(X)\n",
    "    plt.imshow(cv, cmap='hot', interpolation='nearest')\n",
    "    plt.show()\n",
    "plot_covariance(scaledMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "PCA is an example of unsupervised learning.\n",
    "PCA does dimensionality reduction by a linear transformation\n",
    "to orthogonal axes where each successive axis \n",
    "captures most of the remaining variance.  \n",
    "\n",
    "It is important to normalize first.\n",
    "Otherwise, most variance will be explained by those columns with large absoulte values.\n",
    "That was our mistake in HW #1, when 67% of variance was explained by PC1.  \n",
    "We will examine prinicpal components 1, 2, 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_PC_variance_explained(model):\n",
    "    # Assumes at least 3 principal components, or else crashes.\n",
    "    e1,e2,e3=model.explained_variance_ratio_[:3]*100.0\n",
    "    print(\"Variance explained by PC1=%.2f%% PC2=%.2f%% PC3=%.2f%%\"%(e1,e2,e3))\n",
    "def show_PC_eigenvalues(model):\n",
    "    e1,e2,e3=model.lambdas_[:3]\n",
    "    print(\"Eigenvalues EV1=%.2f EV2=%.2f EV3=%.2f\"%(e1,e2,e3))\n",
    "def construct_PCA():\n",
    "    # Use the PCA class from sklearn.\n",
    "    # Linear dimensionality reduction using Singular Value Decomposition.\n",
    "    # This is unsupervised learning (but we'll use labels for visualization).\n",
    "    # Every transform returns a COPY of the data; see copy parameter.\n",
    "    # Does centering (setting mean = 0); no way to disable this.\n",
    "    # Does not do scaling (setting variance = 1); no way to enable this.\n",
    "    # Does NOT do whitening (setting variance = covariance = 1); see whitening parameter.\n",
    "    # When n_components = None, PCA uses min(features,instances).\n",
    "    # Can also be set to 'mle' or min percent of variance to explain.\n",
    "    model = PCA()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PCA of total feature space:\")\n",
    "pca=construct_PCA()\n",
    "pca.fit(scaledMeans)\n",
    "show_PC_variance_explained(pca)\n",
    "P_transformed=pca.fit_transform(scaledMeans)\n",
    "print(\"Shape of transformed data\",P_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA)\n",
    "LDA is a classification algorithm. \n",
    "LDA is an example of supervised learning.\n",
    "LDA finds a linear decision boundary.\n",
    "\n",
    "LDA can be used for dimensionality reduction.\n",
    "It projects the data onto some number of axes \n",
    "that most discriminate between the classes.\n",
    "The maximum number of dimensions is n_classes-1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_LDA():\n",
    "    # A classifier with a linear decision boundary, \n",
    "    # generated by fitting class conditional densities \n",
    "    # to the data and using Bayesâ€™ rule.\n",
    "    # Assumes a Gaussian density for each class.\n",
    "    # Assumes that all classes share the same covariance matrix.\n",
    "    # Solver = 'svd' (default), 'lsqr', or 'eigen'.\n",
    "    # By default, priors are inferred from inputs.\n",
    "    # By default, n_components = None, and LDA uses min(features,classes-1).  \n",
    "    # It is possible to ask LDA for the means, classes, priors,\n",
    "    # variance explained, decision boundary line, and within-class covariance.\n",
    "    return LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must use ravel() to convert shape (40,1) to (40,).\n",
    "labels_ravel=np.ravel(labels_df)\n",
    "lda=construct_LDA()\n",
    "training = cross_validate(lda, scaledMeans, labels_ravel, cv=5)\n",
    "training['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LDA of total feature space:\")\n",
    "lda.fit(scaledMeans,labels_ravel)\n",
    "# The linear decision boundary is a (n_features)-dimensional vector.\n",
    "print(\"Shape of decision boundary\",lda.coef_.shape)\n",
    "L_transformed = lda.transform(scaledMeans)\n",
    "# The transformed data is (n_classes-1)-dimensional i.e. 1D.\n",
    "print(\"Shape of transformed data\",L_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.1.a: Comparison of PCA vs LDA\n",
    "The LDA has more information, specifically, the labels.\n",
    "Also, since we have only two classes, the LDA must perform maximum separation on one axis.\n",
    "Thus, it is not surprising that the LDA achieves greater separation \n",
    "compared to the first principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(10,2)) \n",
    "ax1 = fig1.add_subplot(121)\n",
    "ax1.set_title('PCA first axis')\n",
    "# Extract list of PC1 coordinate for each of 40 intances.\n",
    "pc1_coords = [c[0] for c in P_transformed]\n",
    "ax1.hist(pc1_coords[:CLASS_SEPARATOR],histtype='step',color='blue') \n",
    "ax1.hist(pc1_coords[CLASS_SEPARATOR:],histtype='step',color='red')  \n",
    "ax2 = fig1.add_subplot(122)\n",
    "ax2.set_title('LDA first axis')\n",
    "ax2.hist(L_transformed[:CLASS_SEPARATOR],histtype='step',color='blue') \n",
    "ax2.hist(L_transformed[CLASS_SEPARATOR:],histtype='step',color='red')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.1.b: Discussion of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PCA (transformed,labels):  \n",
    "    lims=[-5,5]  #  Use this if defaults don't work:   ax1d.set_xlim(lims)\n",
    "    fig = plt.figure(figsize=(10,3))\n",
    "    #pc1,pc2,pc3 = np.split(transformed,3,axis=1)  # crashes if n_components != 3\n",
    "    pc1 = [c[0] for c in transformed]\n",
    "    pc2 = [c[1] for c in transformed]\n",
    "    pc3 = [c[2] for c in transformed]\n",
    "    # Plot first PC as histogram\n",
    "    ax1d = fig.add_subplot(131)\n",
    "    ax1d.set_title('First PC')\n",
    "    ax1d.hist(pc1[:CLASS_SEPARATOR],histtype='step',color='blue')  \n",
    "    ax1d.hist(pc1[CLASS_SEPARATOR:],histtype='step',color='red')  \n",
    "    # Plot first 2 PCs in 2D. \n",
    "    ax2d = fig.add_subplot(132)\n",
    "    ax2d.set_title('First 2 PCs')\n",
    "    ax2d.scatter(pc1,pc2,c=labels,cmap=mycmap)\n",
    "    # Plot first 3 PCs in 3D.\n",
    "    ax3d = fig.add_subplot(133,projection='3d')\n",
    "    ax3d.set_title('First 3 PCs')\n",
    "    ax3d.scatter(pc1,pc2,pc3,c=labels,cmap=mycmap)\n",
    "    # Output to screen.\n",
    "    plt.show()\n",
    "plot_PCA(P_transformed,labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scree(model):\n",
    "    pc_values = np.arange(model.n_components_) + 1\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "    plt.plot(pc_values, model.explained_variance_ratio_, 'ro-', linewidth=2)\n",
    "    plt.title('Scree Plot')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Proportion of Variance Explained')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "plot_scree(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Principal components (count,dimensions):\",pca.components_.shape)\n",
    "\n",
    "print(\"Additional variance explained, per principal component (first 10):\")\n",
    "for pc in range(0,10):\n",
    "    print(\"%d=%.3f \"%(pc+1,pca.explained_variance_ratio_[pc]),end=\" \")\n",
    "print()\n",
    "\n",
    "print(\"Cumulative variance explained, per principal component (first 10):\")\n",
    "sum = 0.0\n",
    "for pc in range(0,10):\n",
    "    sum += pca.explained_variance_ratio_[pc]\n",
    "    print(\"%d=%.3f \"%(pc+1,sum),end=\" \")\n",
    "print()\n",
    "\n",
    "def print_top_ten_features(pc,cols,cmps):\n",
    "    NUM=10\n",
    "    print(\"PC%d top ten features\"%pc)\n",
    "    n_features=len(cols)\n",
    "    mylist=[]\n",
    "    for i in range(0,n_features):\n",
    "        name = cols[i]\n",
    "        weight = cmps[pc-1][i]\n",
    "        triple = (i,name,np.abs(weight))   \n",
    "        mylist.append(triple)\n",
    "    myary = np.array(mylist,dtype=[('index',int),('name','S30'),('weight',float)])\n",
    "    sorted = np.sort(myary,order='weight')\n",
    "    for i in range(1,NUM+1):\n",
    "        val = sorted[-i]  # start at end and work to front\n",
    "        print(\"  # %3i (%30s) %7.4f\"%(val[0],val[1],val[2]))\n",
    "print_top_ten_features(1,meansAll.columns,pca.components_)\n",
    "print_top_ten_features(2,meansAll.columns,pca.components_)\n",
    "print_top_ten_features(3,meansAll.columns,pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meansAll.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
