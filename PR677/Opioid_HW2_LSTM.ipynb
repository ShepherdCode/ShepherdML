{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opioid Data - LSTM\n",
    "HW #2 Part 2 - Timeseries.  \n",
    "Use all rows per patient from about 30 consecutive days.\n",
    "Standardize all of it.\n",
    "Train LSTM classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data structures and scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathR='data/ChunkedData_R/'\n",
    "pathN='data/chunkedData_NR/'\n",
    "CLASS_SEPARATOR=13  # data[:13] vs data[13:]\n",
    "WITH_VARIANCE_COLUMNS=True   # Use mean and variance per patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_LIST = ['cohort','patient_name','date']\n",
    "labels_list = []\n",
    "features_df = pd.DataFrame()\n",
    "# Read one CSV file. \n",
    "# Load global lists\n",
    "def load_patient (filepath,cohort,patient_name):\n",
    "    global labels_df\n",
    "    global features_df\n",
    "    one_patient = pd.read_csv(filepath)\n",
    "    rows,cols = one_patient.shape\n",
    "    features_df = features_df.append(one_patient)\n",
    "    for rec in range(0,rows):\n",
    "        one_label=(cohort,patient_name,one_patient.loc[rec]['Date'])\n",
    "        labels_list.append(one_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read directory of CSV files (R or NR). \n",
    "# Given directory, load all the patients in that directory.\n",
    "# We use filenames as patient names.\n",
    "def load_cohort (cohort,directory):\n",
    "    file_names = listdir(directory)\n",
    "    for fp in file_names:\n",
    "        dfp = directory+fp\n",
    "        one_name = fp.split('.')[0]  # strip away .csv suffix\n",
    "        one_name = one_name[6:]    # strip away Daily_ prefix\n",
    "        one_patient = load_patient(dfp,cohort,one_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_cohort('R',pathR)\n",
    "load_cohort('N',pathN)\n",
    "features_df = features_df.drop('Date',axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features by shifting the mean to zero and scaling to unit variance.\n",
    "# Subtract the mean and divide by the std.dev: z = (x - u) / s\n",
    "def scale_features(df):\n",
    "    scaled = StandardScaler().fit_transform(df.values)\n",
    "    scaled_df = pd.DataFrame(scaled, index=df.index, columns=df.columns)\n",
    "    return scaled_df\n",
    "scaled_features = scale_features(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns dataframe.\n",
    "def patient_by_index(ndx):\n",
    "    prev_name='XXX'\n",
    "    name_index=-1\n",
    "    for i in range(0,len(labels_list)):\n",
    "        (cohort,name,date)=labels_list[i]\n",
    "        if not name == prev_name:\n",
    "            prev_name = name\n",
    "            name_index = name_index+1\n",
    "        if name_index == ndx:\n",
    "            return (cohort,name,date)\n",
    "    return None\n",
    "    \n",
    "def features_by_patient_index(ndx):\n",
    "    prev_name='XXX'\n",
    "    name_index=-1\n",
    "    min=1000000\n",
    "    max=-1\n",
    "    for i in range(0,len(labels_list)):\n",
    "        (cohort,name,date)=labels_list[i]\n",
    "        if not name == prev_name:\n",
    "            prev_name = name\n",
    "            name_index = name_index+1\n",
    "        if name_index == ndx:\n",
    "            if i<min:\n",
    "                min=i\n",
    "            if i>max:\n",
    "                max=i\n",
    "    one_p = features_df.iloc[min:max+1]\n",
    "    return (one_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient number: 1\n",
      "Patient cohort, name, start date: ('R', '2027_S2', '2020-03-12')\n",
      "Num records: 29\n"
     ]
    }
   ],
   "source": [
    "# Demo\n",
    "ndx=1\n",
    "my_feat = features_by_patient_index(ndx)\n",
    "print(\"Patient number:\",ndx)\n",
    "print(\"Patient cohort, name, start date:\",patient_by_index(ndx))\n",
    "print(\"Num records:\",len(my_feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM=259\n",
    "INPUT_DIM=3\n",
    "EMBED_DIM=32\n",
    "NEURONS=32\n",
    "def build_model():\n",
    "    embed_layer = keras.layers.Embedding(\n",
    "        input_dim=INPUT_DIM, output_dim=EMBED_DIM, mask_zero=True)\n",
    "    rnn1_layer = keras.layers.LSTM(NEURONS, return_sequences=True, \n",
    "          input_shape=[4,3,])\n",
    "    rnn2_layer = keras.layers.LSTM(NEURONS, return_sequences=False)\n",
    "    dense1_layer = keras.layers.Dense(NEURONS)\n",
    "    dense2_layer = keras.layers.Dense(NEURONS)\n",
    "    output_layer = keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ann = keras.models.Sequential()\n",
    "    #ann.add(embed_layer)\n",
    "    ann.add(rnn1_layer)\n",
    "    ann.add(rnn2_layer)\n",
    "    ann.add(dense1_layer)\n",
    "    ann.add(dense2_layer)\n",
    "    ann.add(output_layer)\n",
    "    compiled = compile_model(ann)\n",
    "    return compiled\n",
    "\n",
    "def compile_model(model):\n",
    "    bc=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    model.compile(loss=bc, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 4, 32)             4608      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 15,073\n",
      "Trainable params: 15,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm = build_model()\n",
    "lstm = compile_model(lstm)\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[4, 2, 6], [7, 1, 3], [5, 5, 0], [9, 3, 6]]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1=[4,2,6]\n",
    "t2=[7,1,3]\n",
    "t3=[5,5,0]\n",
    "t4=[9,3,6]\n",
    "p1=[t1,t2,t3,t4]\n",
    "data=[p1]\n",
    "data\n",
    "# samples, timesteps, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47582704]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
