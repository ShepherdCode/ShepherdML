{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opioid Data - LSTM\n",
    "HW #2 Part 2 - Timeseries.  \n",
    "Use all rows per patient from about 30 consecutive days.\n",
    "Standardize all of it.\n",
    "Train LSTM classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data structures and scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Masking\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From data file to Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathR='data/ChunkedData_R/'\n",
    "pathN='data/chunkedData_NR/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_globals():\n",
    "    global patient_list, labels_list, features_df,UNIFORM_NUM_DAYS\n",
    "    UNIFORM_NUM_DAYS = 29  # most daily records of any patient\n",
    "    patient_list = [] # list of 40 (cohort,patient_ID,first_row,last_row+1)\n",
    "    labels_list = [] # list of 1004 (cohort,patient_ID,date)\n",
    "    features_df = pd.DataFrame()  # 1004 rows of 259 numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read one CSV file. \n",
    "# Load global lists\n",
    "def load_patient (filepath,cohort,patient_ID):\n",
    "    global patient_list\n",
    "    global labels_df\n",
    "    global features_df\n",
    "    one_patient = pd.read_csv(filepath)\n",
    "    rows,cols = one_patient.shape\n",
    "    features_df = features_df.append(one_patient)\n",
    "    first_insert = len(labels_list)\n",
    "    last_insert = first_insert + rows\n",
    "    for rec in range(0,rows):\n",
    "        one_label=(cohort,patient_ID,one_patient.loc[rec]['Date'])\n",
    "        labels_list.append(one_label)\n",
    "    patient_list.append([cohort,patient_ID,first_insert,last_insert])\n",
    "\n",
    "# Read directory of CSV files (R or NR). \n",
    "# Given directory, load all the patients in that directory.\n",
    "# We use filenames as patient names.\n",
    "def load_cohort (cohort,directory):\n",
    "    file_names = listdir(directory)\n",
    "    for fp in file_names:\n",
    "        dfp = directory+fp\n",
    "        one_name = fp.split('.')[0]  # strip away .csv suffix\n",
    "        one_name = one_name[6:]    # strip away Daily_ prefix\n",
    "        one_patient = load_patient(dfp,cohort,one_name)\n",
    "\n",
    "# Standardize features by shifting the mean to zero and scaling to unit variance.\n",
    "# Subtract the mean and divide by the std.dev: z = (x - u) / s\n",
    "def scale_features(df):\n",
    "    scaled = StandardScaler().fit_transform(df.values)\n",
    "    scaled_df = pd.DataFrame(scaled, index=df.index, columns=df.columns)\n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients: 40\n",
      "Label rows: 1004\n",
      "Feature rows: 1004\n"
     ]
    }
   ],
   "source": [
    "initialize_globals()\n",
    "load_cohort('R',pathR)\n",
    "load_cohort('N',pathN)\n",
    "features_df = features_df.drop('Date',axis=1) \n",
    "scaled_features = scale_features(features_df)\n",
    "features_df = None\n",
    "print(\"Patients:\",len(patient_list))\n",
    "print(\"Label rows:\",len(labels_list))\n",
    "print(\"Feature rows:\",len(scaled_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare numpy arrays as required by tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y values, X values: 40 40\n",
      "Days, Features: 29 259\n"
     ]
    }
   ],
   "source": [
    "# Create numpy array of 40 1 or 0\n",
    "def make_labels():\n",
    "    # rely on global patient_list:\n",
    "    # list of 40 (cohort,patient_ID,first_row,last_row+1)\n",
    "    labels=[]\n",
    "    for p in patient_list:\n",
    "        cohort = p[0]\n",
    "        y = 0\n",
    "        if cohort=='R':\n",
    "            y = 1\n",
    "        labels.append(y)\n",
    "        ary = np.array(labels).astype(np.float32)\n",
    "    return ary\n",
    "\n",
    "def make_data():\n",
    "    patients=[]\n",
    "    for p in patient_list:\n",
    "        days=[]\n",
    "        first = p[2]\n",
    "        last = p[3]\n",
    "        for r in range(first,last):\n",
    "            day = scaled_features.iloc[r].values.tolist()\n",
    "            days.append(day)\n",
    "        patients.append(days)\n",
    "    # The tensor data structure has fixed size for each dimension.\n",
    "    # Thus we pad every patient to the maximum = UNIFORM_NUM_DAYS = 29.\n",
    "    # The alternative is RaggedTensor which is slow and not fully supported.\n",
    "    padded = pad_sequences(patients, padding=\"post\")  \n",
    "    ary = np.array(padded).astype(np.float32)\n",
    "    return ary\n",
    "        \n",
    "y_all = make_labels()\n",
    "X_all = make_data()\n",
    "print(\"y values, X values:\",len(y_all),len(X_all))\n",
    "UNIFORM_NUM_FEATURES=len(X_all[0][0])\n",
    "UNIFORM_NUM_DAYS=len(X_all[0])\n",
    "print(\"Days, Features:\",UNIFORM_NUM_DAYS,UNIFORM_NUM_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEURONS=32\n",
    "def build_model():\n",
    "    embed_layer = Masking(mask_value=0.,input_shape=(UNIFORM_NUM_DAYS,UNIFORM_NUM_FEATURES))\n",
    "    rnn1_layer = LSTM(NEURONS, return_sequences=True) \n",
    "    rnn2_layer = LSTM(NEURONS, return_sequences=False)\n",
    "    dense1_layer = Dense(NEURONS)\n",
    "    dense2_layer = Dense(NEURONS)\n",
    "    output_layer = Dense(1, activation=\"sigmoid\")\n",
    "    ann = keras.models.Sequential()\n",
    "    ann.add(embed_layer)\n",
    "    ann.add(rnn1_layer)\n",
    "    ann.add(rnn2_layer)\n",
    "    ann.add(dense1_layer)\n",
    "    ann.add(dense2_layer)\n",
    "    ann.add(output_layer)\n",
    "    compiled = compile_model(ann)\n",
    "    return compiled\n",
    "\n",
    "# Default weight initializers.\n",
    "# Keras Dense default = Glorot aka Xavier uniform initializer\n",
    "\n",
    "def compile_model(model):\n",
    "    bc=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    model.compile(loss=bc, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, 29, 259)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 29, 32)            37376     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 47,841\n",
      "Trainable params: 47,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm = build_model()\n",
    "lstm = compile_model(lstm)\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 11s 57ms/step - loss: 0.7025 - accuracy: 0.4896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc5fb874160>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.fit(X_all,y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_predictions(model_outs):\n",
    "    preds=[]\n",
    "    for x in model_outs:\n",
    "        y = 0\n",
    "        if x[0]>=0.5:\n",
    "            y=1\n",
    "        preds.append(y)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ccfce137f8f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "model_output=lstm.predict(data)\n",
    "predicted=convert_predictions(model_output)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(labels,predicted)\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
